{"meta":{"title":"go hard","subtitle":"focusing@","description":"","author":"JaeJun Lee","url":"https://jx2lee.github.io","root":"/"},"pages":[{"title":"","date":"2020-09-14T14:02:45.425Z","updated":"2020-09-14T14:02:45.425Z","comments":true,"path":"about/index.html","permalink":"https://jx2lee.github.io/about/index.html","excerpt":"","text":"Software Technical Engineer안녕하세요! 빅데이터와 Docker/Kubernetes 에 관심이 많은 Software Technical 엔지니어 입니다. 현재 티맥스데이터 계열사인 티맥스비아이에 재직 중이며 데이터 사이언스 플랫폼 HyperData 기술 지원, 프로젝트 및 POC 를 주로 수행합니다. 머물러 있지 않고 항상 배우기 좋아하는 성격이며, 모르는 것이 있으면 끝까지 알 때까지 물고 늘어지는 근성이 있습니다. 👨🏻‍💻Career TmaxData (2018.08.01 ~) BOAZ BigData club (#8) (2017.01 ~ 2018.01) 📖Education Master of Arts in DataScience, Kookmin Univ (2016.09 ~ 2018.08) Bachelor of Science in Math, Kookmin Univ (2010.03 ~ 2016.02) 🐶Githubjx2lee - Overview 회사 업무 및 사이드 프로젝트를 진행하는 Github 계정입니다. 📀Blog머릿속에 안남으니 기록하자: 매일 쌓여가는 지식을 어디엔가 적어두고 쌓기 위해 운영하는 개인 블로그 입니다. 🍳Skills Programming Skills: Python, SQL Backend: Docker, Kubernetes, Linux, Git 🏆Project (or POC)한국인터넷진흥원 - 개인정보노출대응체계 및 e프라이버시클린서비스 고도화: 2020.06 ~ 2020.11 (예정)수행업무: 빅데이터 플랫폼 구축 클라우드 제품(HyperCloud)을 이용해 HyperData 설치 HyperData 와 연계하는 오픈소스 Kubeflow 배포 (https://github.com/jx2lee/Kubeflower) (과제1) 홈페이지 유형 분류 탐지 웹사이트 텍스트를 이용해 홈페이지 유형 분류 수행 (TmaxAI 와 협업) 분류 결과를 File 로 떨궈 이를 external table 로 읽어들여 하나의 Table 로 생성하는 서비스 연계 방안 구성 (과제2) 키워드 매칭 탐지 웹사이트 텍스트를 이용해 게시판 / 회원가입 여부 판단 수행 Python module 구현 및 연계 서비스 구축 (https://github.com/jx2lee/KeywordMatch) (과제3) 데이터마트 구축 개인정보 상습 구매/판매자 분석을 위한 마트 구축 일 적재 배치처리와 주 단위 마감처리(FLAG)로 정합성 검증 (https://jx2lee.github.io/database-daily_batch_process/) (POC) 한솔PNS - 가상화 솔루션 제품 도입의 건: 2019.09.01 ~ 2019.10.31수행업무: 제품 설치 (On-premise) 제품 성능 비교를 위한 환경 제공 (Connect DB using Python) (POC) 안랩 - 경영진 보고를 위한 마트 구축 및 시각화: 2019.07.01 ~ 2019.08.31수행업무: 제품 설치 (On-premise) 사내 제품을 이용한 마트 시각화 (HyperData) BOAZ 컨퍼런스 - 이별가사 Generator (https://github.com/jx2lee/lyric-generator)수행 업무: 이별 가사를 학습하여 새로운 이별 가사를 생성하는 모델을 딥러닝을 이용해 구현 이별 가사 Crawling 과 모델 구현 (with Python / Tensorflow) 삼성화재 빅데이터 교육 - 자동차등록증 내 차대번호 감지 및 예측: 2017.10.31 ~ 2018.01.02 (https://github.com/jx2lee/digit-recognition)수행 업무: 교육 보조 교육 이후 프로젝트 수행 멘토 역할 (자동차 등록증 내 차대번호를 감지하고 예측하는 프로젝트 멘토 진행) python-opencv 를 이용한 차대번호 감지 support 예측 결과를 위한 CNN 모델 튜닝 Side ProjectKubeflow 설치 및 자동화 스크립트 구현: https://github.com/jx2lee/Kubeflower수행 업무: 사내 제품에 사용하는 Kubeflow 를 Kubernetes 위에 배포하는 자동화 스크립트 구현 https://github.com/jx2lee/KFimgr: Kubeflow 이미지를 도커 레지스트리에 Push 하는 스크립트 구현 팀 테스트를 위한 Kubernetes 구축: https://jx2lee.github.io/cloud-install_k8s 사내 제품 테스트를 위해 Kubernetes 환경 구축 Kubernetes 1.15.3 클러스터 구축 (마스터 삼중화) 📄PaperRNN을 이용한 한국어 감성분석 - 온라인 영화 후기를 중심으로 (학위논문): http://www.riss.kr/search/detail/DetailView.do?p_mat_type=be54d9b8bc7cdb09&amp;control_no=51909e84bd4b8282ffe0bdc3ef48d419한글 음소 단위 딥러닝 모형을 이용한 감성분석: https://www.dbpia.co.kr/journal/articleDetail?nodeId=NODE07282148 한국IT서비스학회, 17(1), 79-89, 2018, 1/3 한국어 음소 단위 LSTM 언어모델을 이용한 문장생성: https://www.dbpia.co.kr/journal/articleDetail?nodeId=NODE07189914 한국지능정보시스템학회, 23(2), 71-88, 2017, 3/4 딥러닝 프레임워크의 비교: https://www.dbpia.co.kr/Journal/articleDetail?nodeId=NODE07189911 한국지능정보시스템학회, 23(2), 1-17, 2017, 4/4"},{"title":"","date":"2020-03-30T15:06:23.592Z","updated":"2020-03-30T15:06:23.592Z","comments":true,"path":"categories/index.html","permalink":"https://jx2lee.github.io/categories/index.html","excerpt":"","text":""},{"title":"","date":"2020-03-30T15:06:23.606Z","updated":"2020-03-30T15:06:23.606Z","comments":true,"path":"tags/index.html","permalink":"https://jx2lee.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"[Jenkins] Jenkins 설치 on Instance","slug":"jenkins-install_jenkins","date":"2021-01-08T15:00:00.000Z","updated":"2021-01-09T12:24:00.377Z","comments":true,"path":"jenkins-install_jenkins/","link":"","permalink":"https://jx2lee.github.io/jenkins-install_jenkins/","excerpt":"Docker 로 젠킨스 환경을 세팅하려고 했는데.. https://hub.docker.com/_/jenkins 을 살펴보면 더 이상 지원을 안하는 것으로 파악된다. 지속적으로 업데이트되는 부분을 함께 포함하기 위해 과감히(?) docker 환경을 접고 jenkins 서버를 구축하고 환경을 설정해보도록 한다. Docker 로 설치하는 방법도 정리하였다.","text":"Docker 로 젠킨스 환경을 세팅하려고 했는데.. https://hub.docker.com/_/jenkins 을 살펴보면 더 이상 지원을 안하는 것으로 파악된다. 지속적으로 업데이트되는 부분을 함께 포함하기 위해 과감히(?) docker 환경을 접고 jenkins 서버를 구축하고 환경을 설정해보도록 한다. Docker 로 설치하는 방법도 정리하였다. &#x2728; Contents: Install Java Install Maven .bashrc Install Git Install Jenkins Edit sysconfig (jenkins config) on Docker.. Reference Install Java123456789101112131415161718$ yum list java-1.8.0-openjdk-devel*base | 3.6 kB 00:00:00epel/x86_64/metalink | 7.8 kB 00:00:00epel | 4.7 kB 00:00:00extras | 2.9 kB 00:00:00updates | 2.9 kB 00:00:00(1/7): base/7/x86_64/group_gz | 153 kB 00:00:00(2/7): extras/7/x86_64/primary_db | 222 kB 00:00:00(3/7): updates/7/x86_64/primary_db | 4.7 MB 00:00:00(4/7): epel/x86_64/group_gz | 95 kB 00:00:00(5/7): base/7/x86_64/primary_db | 6.1 MB 00:00:00(6/7): epel/x86_64/primary_db | 6.9 MB 00:00:01(7/7): epel/x86_64/updateinfo | 1.0 MB 00:00:07Available Packagesjava-1.8.0-openjdk-devel.i686 1:1.8.0.275.b01-0.el7_9 updatesjava-1.8.0-openjdk-devel.x86_64 1:1.8.0.275.b01-0.el7_9 updates$ yum install -y java-1.8.0-openjdk-devel.x86_64 이후 JAVA 환경변수 등록을 위해 실제 주소를 알아내고 이를 .bashrc 에 추가한다. 123$ readlink -f /usr/bin/javac/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.275.b01-0.el7_9.x86_64/bin/javac#JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.275.b01-0.el7_9.x86_64 Install Maven123$ sudo mkdir -p /app &amp;&amp; sudo chmod 777 /app$ wget https://downloads.apache.org/maven/maven-3/3.6.3/binaries/apache-maven-3.6.3-bin.tar.gz -P /app$ tar -xvzf apache-maven-3.6.3-bin.tar.gz .bashrc123456789# JAVA_HOMEexport JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.275.b01-0.el7_9.x86_64# MAVEN_HOMEexport MAVEN_HOME=/app/apache-maven-3.6.3# PATHPATH=$PATH:$JAVA_HOME/bin:$MAVEN_HOME/binexport PATH Install Gitsudo yum install -y git Install Jenkins1234$ sudo wget -O /etc/yum.repos.d/jenkins.repo https://pkg.jenkins.io/redhat-stable/jenkins.repo$ sudo rpm --import https://pkg.jenkins.io/redhat-stable/jenkins.io.key$ sudo yum install -y jenkins$ sudo systemctl enable jenkins Edit sysconfig (jenkins config)1234567891011121314151617181920212223242526272829303132333435363738$ sudo cat /etc/sysconfig/jenkins## Path: Development/Jenkins## Description: Jenkins Automation Server## Type: string## Default: &quot;/var/lib/jenkins&quot;## ServiceRestart: jenkins## Directory where Jenkins store its configuration and working# files (checkouts, build reports, artifacts, ...).#JENKINS_HOME=&quot;/jenkins_home&quot;## Type: string## Default: &quot;&quot;## ServiceRestart: jenkins## Java executable to run Jenkins# When left empty, we&#x27;ll try to find the suitable Java.#......JENKINS_USER=&quot;jenkins&quot;......JENKINS_PORT=&quot;9090&quot;......## Type: string## Default: &quot;&quot;## ServiceRestart: jenkins## Pass arbitrary arguments to Jenkins.# Full option list: java -jar jenkins.war --help#JENKINS_ARGS=&quot;&quot; 이후 서비스를 실행 sudo systemctl start jenkins On Docker.. Pull Jenkins Image 1234$ docker pull jenkins/jenkins:lts$ docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEjenkins/jenkins lts 1920bf702d7d 4 weeks ago 713MB Run Container 123docker run -d -p 8080:8080 --name jenkins_test \\ -v /home/centos/jenkins_home:/var/jenkins_home \\ -v /var/run/docker.sock:/var/run/docker.sock -u root jenkins/jenkins:lts /home/centos/jenkins_home 호스트 디렉토리를 container /var/jenkins_home 으로 mount /var/run/docker.sock 파일을 마운트하는 이유는 일반 user 도 해당 디렉토리를 넘나들 수 있도록 설정하기 위함 Add Outbound rule 본인은 토스트 클라우드를 사용중이므로 사용중인 클라우드의 보안그룹에 outbound rule 추가 (8080포트) Verify initial admin password 컨테이너 안으로 들어가 확인해도(ex. docker exec -ti {container_name} sh) 되지만 docker exec 에 cat 만 전달하여 initialAdminPassword 내용을 확인한다12$ docker exec -it jenkins_test cat /var/jenkins_home/secrets/initialAdminPassword75c9d1c35b674306a2ed9f114f49240f Reference https://blog.jiniworld.me/88 https://velog.io/@wimes/Jenkins를-이용해-Docker프로젝트-빌드해보기 made by jaejun.lee","categories":[{"name":"Jenkins","slug":"Jenkins","permalink":"https://jx2lee.github.io/categories/Jenkins/"}],"tags":[]},{"title":"[Node.js] NVM 설치","slug":"nodejs-install_nvm","date":"2021-01-08T15:00:00.000Z","updated":"2021-01-09T12:45:16.815Z","comments":true,"path":"nodejs-install_nvm/","link":"","permalink":"https://jx2lee.github.io/nodejs-install_nvm/","excerpt":"node.js 버전 매니저인 NVM 을 설치하고 살펴본다.","text":"node.js 버전 매니저인 NVM 을 설치하고 살펴본다. &#x2728; Contents: NVM? Install NVM .bashrc (or .zsh) Installing a specific version of Nodejs Installing a specific version of Nodejs Reference NVM? node js 버전 매니저로 시스템에 여러 개의 nodejs 를 설치하고 사용할 버전을 쉽게 전환할 수록 도와주는 shell script rvm(Ruby Version Manager) 와 비슷한 역할을 수행 Install NVMcurl -o- https://raw.githubusercontent.com/creationix/nvm/v0.33.8/install.sh | bash .bashrc (or .zsh)1234$ vi ~/.zshrc# Nodejsexport NVM_DIR=&quot;$HOME/.nvm&quot;[ -s &quot;$NVM_DIR/nvm.sh&quot; ] &amp;&amp; \\. &quot;$NVM_DIR/nvm.sh&quot; # This loads nvm Installing a specific version of Nodejs123456789101112131415$ nvm install 13.1.0$ nvm ls-&gt; v13.1.0 systemdefault -&gt; 13.1.0 (-&gt; v13.1.0)node -&gt; stable (-&gt; v13.1.0) (default)stable -&gt; 13.1 (-&gt; v13.1.0) (default)iojs -&gt; N/A (default)lts/* -&gt; lts/fermium (-&gt; N/A)lts/argon -&gt; v4.9.1 (-&gt; N/A)lts/boron -&gt; v6.17.1 (-&gt; N/A)lts/carbon -&gt; v8.17.0 (-&gt; N/A)lts/dubnium -&gt; v10.23.1 (-&gt; N/A)lts/erbium -&gt; v12.20.1 (-&gt; N/A)lts/fermium -&gt; v14.15.4 (-&gt; N/A) Commands search node version: nvm ls-remote select specific node version: nvm use &#123;node-version&#125; set default version: nvm alias default &#123;node-version&#125; delete version: nvm uninstall &#123;node-version&#125; Reference lesstif.com/javascript/nvm-node-version-manager-nodejs-82214944.html made by jaejun.lee","categories":[],"tags":[]},{"title":"[Hadoop] Single Node Cluster 설치","slug":"hadoop-install_singloenode_cluster","date":"2021-01-07T15:00:00.000Z","updated":"2021-01-09T12:21:49.923Z","comments":true,"path":"hadoop-install_singloenode_cluster/","link":"","permalink":"https://jx2lee.github.io/hadoop-install_singloenode_cluster/","excerpt":"CentOS 7 환경에서 Single Node Hadoop Clsuter 를 설치한다. version java: 1.8.0 hadoop: 2.10.1","text":"CentOS 7 환경에서 Single Node Hadoop Clsuter 를 설치한다. version java: 1.8.0 hadoop: 2.10.1 &#x2728; Contents: Install Java SSH setting Install Hadoop Configuring Hadoop $HADOOP_HOME/etc/hadoop/core-site.xml $HADOOP/etc/hadoop/hdfs-site.xml $HADOOP_HOME/etc/hadoop/mapred-site.xml $HADOOP_HOME/etc/hadoop/yarn-site.xml Running Hadoop Before starting the Cluster, we need to format the Hadoop NN in our local system Start NameNode daemon and DataNode daemon Reference Install Java$ yum install -y java-1.8.0-openjdk SSH settinglocalhost ssh 접속을 위해 아래와 같은 작업을 수행한다. 123456$ ssh-keygen -t rsa -P &#x27;&#x27;$ cat $HOME/.ssh/id_rsa.pub &gt;&gt; $HOME/.ssh/authorized_keys$ sudo vi /etc/ssh/sshd_config#PasswordAuthentication yes$ sudo systemctl restart sshd$ ssh localhost #Test Install Hadoop binary 다운로드 및 환경변수 설정 12345678910$ wget https://downloads.apache.org/hadoop/common/hadoop-2.10.1/hadoop-2.10.1.tar.gz$ tar -xvzf hadoop-2.10.1.tar.gz$ vi ~/.bashrc# Hadoop envexport HADOOP_HOME=/app/hadoop-2.10.1export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/nativeexport HADOOP_OPTS=&quot;-Djava.library.path=$HADOOP_HOME/lib&quot;export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.275.b01-0.el7_9.x86_64PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin$ source ~/.bashrc Configuring Hadoop $HADOOP_HOME/etc/hadoop/core-site.xml123456789101112131415161718192021222324&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;&lt;!-- Licensed under the Apache License, Version 2.0 (the &quot;License&quot;); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an &quot;AS IS&quot; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. See accompanying LICENSE file.--&gt;&lt;!-- Put site-specific property overrides in this file. --&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://hadoop-test.novalocal:8020&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; fs.defaultFS value format: hdfs://&#123;hostname&#125;:&#123;port&#125; $HADOOP/etc/hadoop/hdfs-site.xml1234567891011121314151617181920212223242526272829303132&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;&lt;!-- Licensed under the Apache License, Version 2.0 (the &quot;License&quot;); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an &quot;AS IS&quot; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. See accompanying LICENSE file.--&gt;&lt;!-- Put site-specific property overrides in this file. --&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;/hdata/name&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;/hdata/data&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 이후 namenode/datanode directory 를 생성한다. $HADOOP_HOME/etc/hadoop/mapred-site.xml123456789101112131415161718192021222324&lt;?xml version=&quot;1.0&quot;?&gt;&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;&lt;!-- Licensed under the Apache License, Version 2.0 (the &quot;License&quot;); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an &quot;AS IS&quot; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. See accompanying LICENSE file.--&gt;&lt;!-- Put site-specific property overrides in this file. --&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapred.job.tracker&lt;/name&gt; &lt;value&gt;localhost:9001&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; $HADOOP_HOME/etc/hadoop/yarn-site.xml123456789101112131415161718192021&lt;?xml version=&quot;1.0&quot;?&gt;&lt;!-- Licensed under the Apache License, Version 2.0 (the &quot;License&quot;); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an &quot;AS IS&quot; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. See accompanying LICENSE file.--&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; Running Hadoop Before starting the Cluster, we need to format the Hadoop NN in our local system1$ $HADOOP_HOME/bin/hadoop namenode -format 이후 namenode path 로 설정한 폴더에 뭔가가 생김을 확인할 수 있다. 123456789$ tree/hdata/name/└── current ├── fsimage_0000000000000000000 ├── fsimage_0000000000000000000.md5 ├── seen_txid └── VERSION1 directory, 4 files Start NameNode daemon and DataNode daemon123456789$ $HADOOP_HOME/sbin/start-dfs.sh$ $HADOOP_HOME/sbin/start-yarn.sh$ jps12960 Jps10161 NodeManager12625 SecondaryNameNode10051 ResourceManager12435 DataNode12287 NameNode Reference https://helei.pro/doc/Setup-Hadoop-2.7.3-(single-node)-on-AWS-EC2-Ubuntu-AMI.pdf https://www.tecmint.com/install-hadoop-single-node-on-centos-7/ made by jaejun.lee","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://jx2lee.github.io/categories/Hadoop/"}],"tags":[]},{"title":"[Cloud] Docker 설치 On CentOS7","slug":"cloud-install_docker_on_centos","date":"2021-01-06T15:00:00.000Z","updated":"2021-01-09T12:23:07.452Z","comments":true,"path":"cloud-install_docker_on_centos/","link":"","permalink":"https://jx2lee.github.io/cloud-install_docker_on_centos/","excerpt":"CentOS 7 환경에서 Docker 설치 과정을 다룬다. CentOS Linux release 7.5.1804 (Core)","text":"CentOS 7 환경에서 Docker 설치 과정을 다룬다. CentOS Linux release 7.5.1804 (Core) &#x2728; Contents: 이전 설치된 Docker 삭제 yum-utils 패키지 설치 및 Docker repo 추가 Docker 설치 (with yum) Referenece 이전 설치된 Docker 삭제12345678$ sudo yum remove docker \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-engine yum-utils 패키지 설치 및 Docker repo 추가123$ sudo yum install -y yum-utils$ sudo yum-config-manager \\ --add-repo https://download.docker.com/linux/centos/docker-ce.repo Docker 설치 (with yum)123456789101112131415161718192021222324252627$ yum install -y docker-ce$ systemctl enable dockerCreated symlink from /etc/systemd/system/multi-user.target.wants/docker.service to /usr/lib/systemd/system/docker.service.$ systemctl status docker● docker.service - Docker Application Container Engine Loaded: loaded (/usr/lib/systemd/system/docker.service; enabled; vendor preset: disabled) Active: active (running) since Mon 2021-01-04 13:19:54 KST; 1s ago Docs: https://docs.docker.com Main PID: 11889 (dockerd) Tasks: 13 Memory: 47.3M CGroup: /system.slice/docker.service └─11889 /usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sockJan 04 13:19:54 test-centos.novalocal dockerd[11889]: time=&quot;2021-01-04T13:19:54.036061899+09:00&quot; leve...rpcJan 04 13:19:54 test-centos.novalocal dockerd[11889]: time=&quot;2021-01-04T13:19:54.036084134+09:00&quot; leve...rpcJan 04 13:19:54 test-centos.novalocal dockerd[11889]: time=&quot;2021-01-04T13:19:54.036094510+09:00&quot; leve...rpcJan 04 13:19:54 test-centos.novalocal dockerd[11889]: time=&quot;2021-01-04T13:19:54.066676014+09:00&quot; leve...t.&quot;Jan 04 13:19:54 test-centos.novalocal dockerd[11889]: time=&quot;2021-01-04T13:19:54.197938418+09:00&quot; leve...ss&quot;Jan 04 13:19:54 test-centos.novalocal dockerd[11889]: time=&quot;2021-01-04T13:19:54.247887261+09:00&quot; leve...e.&quot;Jan 04 13:19:54 test-centos.novalocal dockerd[11889]: time=&quot;2021-01-04T13:19:54.275547502+09:00&quot; leve...0.1Jan 04 13:19:54 test-centos.novalocal dockerd[11889]: time=&quot;2021-01-04T13:19:54.275733738+09:00&quot; leve...on&quot;Jan 04 13:19:54 test-centos.novalocal systemd[1]: Started Docker Application Container Engine.Jan 04 13:19:54 test-centos.novalocal dockerd[11889]: time=&quot;2021-01-04T13:19:54.303691925+09:00&quot; leve...ck&quot;Hint: Some lines were ellipsized, use -l to show in full.$ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES Reference https://docs.docker.com/engine/install/centos/ made by jaejun.lee","categories":[{"name":"Cloud","slug":"Cloud","permalink":"https://jx2lee.github.io/categories/Cloud/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://jx2lee.github.io/tags/Docker/"}]},{"title":"[Cloud] 자체 튜닝 Docker 이미지를 만들어 보자구요.","slug":"cloud-docker_image_test","date":"2020-10-27T15:00:00.000Z","updated":"2020-10-29T15:00:24.596Z","comments":true,"path":"cloud-docker_image_test/","link":"","permalink":"https://jx2lee.github.io/cloud-docker_image_test/","excerpt":"급히 나주 출장을 갈 예정인데 무슨 클라우드에서 제공하는 서비스인데 서버 1개로 구축을 해달라는 황당한 요청이 있었다. 우선 설치를 하고 봐야 하니.. 각 파드 resource 를 최저로 주고 테스트 하던 도중 우리 제품 바이너리에 수정사항이 발생하였다. 이슈를 올려서 해도 되지만 어차피 검수 목적으로 캡쳐만 뜨는 설치건이라 바이너리를 수정하여 덮어 씌우는 자체 튜닝 과정을 살펴본다. 역시 GOD정희님 최고","text":"급히 나주 출장을 갈 예정인데 무슨 클라우드에서 제공하는 서비스인데 서버 1개로 구축을 해달라는 황당한 요청이 있었다. 우선 설치를 하고 봐야 하니.. 각 파드 resource 를 최저로 주고 테스트 하던 도중 우리 제품 바이너리에 수정사항이 발생하였다. 이슈를 올려서 해도 되지만 어차피 검수 목적으로 캡쳐만 뜨는 설치건이라 바이너리를 수정하여 덮어 씌우는 자체 튜닝 과정을 살펴본다. 역시 GOD정희님 최고 &#x2728; Contents: 문제 발생 바이너리 수정 후 docker image 디렉토리 생성하기 빌드하자! 문제 발생JEUS 를 통해 두 개 서버를 생성하는 스크립트에서, ThreadPool min/max 값이 잘못 설정되어 배포하는 문제가 발생하였다. 이 때문에 3개 중 하나의 서비스가 동작하지 않는 문제가 발생하였고, 이를 해결하기 위해 install script 내 modify command 를 추가하여 min 값을 수정하였다. 근데.. 당장 내일 나오는 바이너리로 나갈 필요도 없고 서버 스펙도 충분하지 못한 상황에서 이전 버젼으로 나가는 게 베스트 일 것 같아 자체적으로 Docker 이미지를 튜닝해보고자 한다. 바이너리 수정 후 docker image 디렉토리 생성하기위 문제 발생에서 해결한 바이너리를 다시 똑같은 바이너리로 압축한 뒤 docker 이미지 빌드를 위한 곳에 위치한다. 그리고 Dockerfile 를 생성하여 다음과 같이 작성한다. 12FROM hyperdata8.3_hd_v8.3.2:20201016_v1COPY HyperData_8.3_r874bb4-20201016030033_v8.3.2.tar.gz /deploy_src/src/hyperdata/HyperData_8.3_r874bb4-20201016030033_v8.3.2.tar.gz 기존 이미지를 base 로 FROM 으로 호출 굉장히 쉬웠다. 해당 수정한 바이너리 파일을 이미지 내 원래 있던 디렉토리에 같은 이름으로 작성만 하면 Docker build 시 최종 바이너리는 내가 수정한 바이너리로 압축을 해제할 것이다. Dockerfile 에 순서는 무관하다는 걸 오늘 깨달았고 나의 무지함 또는 깨달았다. Docker image build 에 대해 더 살펴볼 필요가 있어보인다. 빌드하자!해당 디렉토리 안에서 다음과 같은 커맨드로 빌드한다. 1docker build -t &#123;기존_이미지명&#125;:&#123;태그_변경_아무거나_하세요&#125; . 마지막 줄 .(점) 중요하다. 물론 디렉토리를 지정할 수 있는 옵션이 있지만 주지 않을 경우 점을 통해 빌드 디렉토리를 인식한다. 태그명의 경우 이전 태그명에서 나는 고객사 영문 줄임만 붙였다. 양아취 docker images 커맨드로 생성이 잘 되었는지 확인한다. 나의 경우 이렇게 새로운 이미지를 만들고 테스트를 해보니, 내가 수정한 스크립트가 정상적으로 반영되었고 모든게 Dockerfile 두 줄로 끝나버렸다. 조금은 허무하지만 Container 이미지 빌드 과정에 대해 관심을 가질 수 있는 계기가 되었다! 공부할 게 산더미다. made by jaejun.lee","categories":[{"name":"Cloud","slug":"Cloud","permalink":"https://jx2lee.github.io/categories/Cloud/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://jx2lee.github.io/tags/Docker/"}]},{"title":"[Cloud] cri-o 컨테이너 런타임 사용 시 docker registry 세팅","slug":"cloud-crio_registry_setting","date":"2020-10-02T15:00:00.000Z","updated":"2020-10-29T14:32:47.546Z","comments":true,"path":"cloud-crio_registry_setting/","link":"","permalink":"https://jx2lee.github.io/cloud-crio_registry_setting/","excerpt":"kubernetes 버젼이 올라감에 따라 (1.15.3 → 1.17.6) 컨테이너 런타임을 cri-o 로 변경하였다. 이미지 저장소는 그대로 docker 사용하는데 이때 docker registry 와 연동하여 image 를 관리하는 방법을 살펴본다. Updata Note 2020.10.29 : config 추가 수정","text":"kubernetes 버젼이 올라감에 따라 (1.15.3 → 1.17.6) 컨테이너 런타임을 cri-o 로 변경하였다. 이미지 저장소는 그대로 docker 사용하는데 이때 docker registry 와 연동하여 image 를 관리하는 방법을 살펴본다. Updata Note 2020.10.29 : config 추가 수정 &#x2728; Contents: config 수정 cri-o 재기동 테스트 해보자! config 수정/etc/crio/crio.conf 내 중간에서 마지막 부분 사이 insecure_registries 에 docker registry endpoint 를 추가한다. 1234# List of registries to skip TLS verification for pulling images. Please# consider configuring the registries via /etc/containers/registries.conf before# changing them here.insecure_registries = [\"192.168.179.185:5000\", \"192.168.179.189:5000\"] 첫 번째 endpoint 는 기존, 뒤에 추가한 endpoint 가 새로 테스트할 docker registry endpoint 이다. 추가적으로 config 을 다음과 같이 수정해야한다. registries = [“{registry}:{port}” , “docker.io”] plugin_dirs : “/opt/cni/bin” 추가 만약 폐쇄망 환경일 경우, pause_image 부분 앞에 registry enpoint를 추가해야한다. cri-o 재기동cri-o 서비스를 재기동한다. 1$ systemctl restart crio 테스트 해보자!crictl 로 추가한 docker registry 와 연동이 잘 되었는지 테스트 해보자! 1234$ crictl pull 192.168.179.189:5000/hyperdata8.3_tb:20200717_v3Image is up to date for 192.168.179.189:5000/hyperdata8.3_tb@sha256:85e0f94a90a26090488b929e3ef9475b2cab35ad425effdf5e79a7f2f7029a06$ crictl images |grep hyperdata8.3_tb192.168.179.189:5000/hyperdata8.3_tb 20200717_v3 76063ea49aadd 3.1GB 같은 방법으로 push 하면 정상적으로 작동하는 것을 확인할 수 있다. 컨테이너 런타임을 관리하는 놈이 crio 이고, crictl config 는 /etc/crio/crio.conf 이며 docker registry 와 연동을 위해 해당 config 를 수정하고 crio service 를 재 시작한다. 이전에는 컨테이너 런타임, 이미지 관리를 모두 docker 에서 관리하여 편하긴 했는데.. cri-o에 대한 개념을 정리해볼 필요가 있다. made by jaejun.lee","categories":[{"name":"Cloud","slug":"Cloud","permalink":"https://jx2lee.github.io/categories/Cloud/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://jx2lee.github.io/tags/Kubernetes/"}]},{"title":"[Linux] Timezone 변경","slug":"linux-change_timezone","date":"2020-10-02T15:00:00.000Z","updated":"2020-10-03T14:14:42.565Z","comments":true,"path":"linux-change_timezone/","link":"","permalink":"https://jx2lee.github.io/linux-change_timezone/","excerpt":"tzdata 패키지를 설치하고 기존 UTC timezone 을 KST 로 변경하는 과정을 살펴본다.","text":"tzdata 패키지를 설치하고 기존 UTC timezone 을 KST 로 변경하는 과정을 살펴본다. &#x2728; Contents: 패키지 설치 Symbolic link 설정 및 패키지 재설정 Reference 패키지 설치Ubuntu 패키지 매니저인 apt 를 이용하여 tzdata 를 설치한다. 1$ apt-get install tzdata Symbolic link 설정 및 패키지 재설정default timezone UTC 에서 KST 로 바꾸기 위해 link 를 생성한다. 123$ ln -sf /usr/share/zoneinfo/Asia/Seoul /etc/localtime$ dateFri Sep 18 13:19:40 KST 2020 이후 재 설정을 위해 아래 커맨드를 실행하여 timezone 이 KST 로 적용하였는지 확인한다. 1$ dpkg-reconfigure tzdata 끝! Reference https://www.lesstif.com/lpt/ubuntu-linux-timezone-setting-61899162.html made by jaejun.lee","categories":[{"name":"Linux","slug":"Linux","permalink":"https://jx2lee.github.io/categories/Linux/"}],"tags":[]},{"title":"[Linux] ssh 비밀번호 묻지마!","slug":"linux-ssh_nopasswd","date":"2020-10-02T15:00:00.000Z","updated":"2020-10-03T14:14:41.094Z","comments":true,"path":"linux-ssh_nopasswd/","link":"","permalink":"https://jx2lee.github.io/linux-ssh_nopasswd/","excerpt":"Kubernetes 노드 관리를 위해 각 노드별 접근 시 비밀번호 없이 편하게 접속하는 방법을 정리하였다. A서버에서 B 서버로 접속하는 상황이다.","text":"Kubernetes 노드 관리를 위해 각 노드별 접근 시 비밀번호 없이 편하게 접속하는 방법을 정리하였다. A서버에서 B 서버로 접속하는 상황이다. &#x2728; Contents: Key 생성 Key 복사 Reference Key 생성이미 A 서버에 ssh Key 가 존재한다면 이 부분은 패스해도 좋다. 만약 처음 세팅하는거라면, 아래 커맨드를 통해 ssh Key 를 생성하자. 12$ # A 서버에서 실행$ ssh-keygen Key 복사생성한 Key 를 ssh-copy-id 를 이용해 B 서버에 등록한다. 123$ ssh-copy-id -i ~/.ssh/id_rsa.pub [user]@[ip]$ # user/ip는 당연 B 서버 정보를 입력$ # 위 커맨드 이후 비밀번호를 물어볼텐데 잘 입력하자. 이후 A 서버 (host: k8s-master) 에서 B 서버 (host: k8s-node1) 로 원격접속 해보자. 12345678910111213141516171819202122232425262728293031$ ssh k8s-node1Welcome to Ubuntu 18.04.3 LTS (GNU/Linux 4.15.0-112-generic x86_64) * Documentation: https://help.ubuntu.com * Management: https://landscape.canonical.com * Support: https://ubuntu.com/advantage System information as of Sat Oct 3 23:03:28 KST 2020 System load: 0.34 Users logged in: 1 Usage of /home: 0.2% of 19.56GB IP address for enp6s0: 192.168.179.173 Memory usage: 16% IP address for docker0: 172.17.0.1 Swap usage: 0% IP address for tunl0: 10.244.36.64 Processes: 332 * Kubernetes 1.19 is out! Get it in one command with: sudo snap install microk8s --channel=1.19 --classic https://microk8s.io/ has docs and details. * Canonical Livepatch is available for installation. - Reduce system reboots and improve kernel security. Activate at: https://ubuntu.com/livepatch92 packages can be updated.1 update is a security update.*** System restart required ***Last login: Mon Sep 28 17:41:53 2020 from 192.168.188.101 끝! Reference [https://itzone.tistory.com/694]https://itzone.tistory.com/694) made by jaejun.lee","categories":[{"name":"Linux","slug":"Linux","permalink":"https://jx2lee.github.io/categories/Linux/"}],"tags":[]},{"title":"[TroubleShoot] PyTorch 1.5.0+cu101 버젼 설치 시 에러","slug":"troubleshoot-torch_error","date":"2020-10-02T15:00:00.000Z","updated":"2020-10-03T14:29:56.984Z","comments":true,"path":"troubleshoot-torch_error/","link":"","permalink":"https://jx2lee.github.io/troubleshoot-torch_error/","excerpt":"pytorch 관련 프로젝트를 clone 하여 해보던 중에, requirements.txt 내 torch==1.5.0+cu101 부분에서 에러가 발생하였다. 에러를 피하고 제대로 설치해보자!","text":"pytorch 관련 프로젝트를 clone 하여 해보던 중에, requirements.txt 내 torch==1.5.0+cu101 부분에서 에러가 발생하였다. 에러를 피하고 제대로 설치해보자! &#x2728; Contents: error 메시지 해결은 은근 쉽네요? Reference error 메시지pip install -r requirements.txt 로 관련 패키지 설치 시 아래와 같은 에러를 맞이하였다. 12ERROR: Could not find a version that satisfies the requirement torch==1.5.0+cu101 (from -r requirements.txt (line 59)) (from versions: 0.1.2, 0.1.2.post1, 0.1.2.post2, 0.3.1, 0.4.0, 0.4.1, 1.0.0, 1.0.1, 1.0.1.post2, 1.1.0, 1.2.0, 1.3.0, 1.3.1, 1.4.0, 1.5.0, 1.5.1, 1.6.0)ERROR: No matching distribution found for torch==1.5.0+cu101 (from -r requirements.txt (line 59)) 해결은 은근 쉽네요?별 거 없었다. pip install 시 -f 옵션을 이용해 stable version 을 찾아가 직접 다운로드 할 수 있다. 1$ pip install torch==1.5.0+cu101 torchvision==0.6.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html f 옵션: html 파일의 URL 또는 경로를 이용해 패키지를 설치 Reference https://huvso.github.io/2020/07/02/python-pip-option.html made by jaejun.lee","categories":[{"name":"TroubleShoot","slug":"TroubleShoot","permalink":"https://jx2lee.github.io/categories/TroubleShoot/"}],"tags":[]},{"title":"[Cloud] Kubernetes 아키텍처에 대해","slug":"cloud-kubernetes_chapter_02.md","date":"2020-09-27T15:00:00.000Z","updated":"2020-10-03T13:50:53.848Z","comments":true,"path":"cloud-kubernetes_chapter_02.md/","link":"","permalink":"https://jx2lee.github.io/cloud-kubernetes_chapter_02.md/","excerpt":"신입 직원 교육자료를 위해 작성한 Kubernetes 소개 자료이다. 많은 블로그를 참고하여 작성하였고, 이번 장에는 Kubernetes 아키텍쳐를 간략히 소개한다.","text":"신입 직원 교육자료를 위해 작성한 Kubernetes 소개 자료이다. 많은 블로그를 참고하여 작성하였고, 이번 장에는 Kubernetes 아키텍쳐를 간략히 소개한다. Contents: Architecture Control Plane (Master) 특징 및 기능 Component in Control Plane Node Worker Node 특징 및 기능 Component in Worker Node Object Basic Object Controller Reference Architecture [그림] kubernetes architecture ver1.17 클러스터를 관리하는 Controlplane 와 컨테이너가 배포되는 머신 (가상머신이거나 실제 서버) 인 Worker Node로 구성한다. Control Plane (한글 번역 시 Master 라고 나와있는데 Control Plane == Master 라고 생각하면 된다.) 과 Worker Node 를 확인하고 Kubernetes 내에서의 Object 를 살펴본다. Control Plane (Master) 특징 및 기능 관리자만 접속하여 보안 설정이 필요하다. Conrol Plane Node 다운이 발생하면 클러스터 관리에 장애가 생기므로 보통 3대로 구성하여 클러스터를 구성한다. 홀수대로 구성하는 이유는 컨테이너 배포에 대한 voting 를 수월하게 하기 위함이라고 한다. 소규모 환경에서는 Control Plane 과 Worker Node 를 분리하지 않고 같은 서버에 구성한다. 즉, Control Plane 이면서 동시에 컨테이너를 띄운다고 생각하면 된다. Component in Control Plane NodeAPI server: 모든 컴포넌트 간 통신의 메카이다. kubectl 요청 및 내부 모듈의 요청을 처리한다. kubectl 명령어는 Control Plane 에서만 가능하다. (물론 설치는 모든 노드에서 수행한다.) 권한 체크를 통해 요청을 허용하거나 거부한다. Etcd 를 기반으로 필요한 데이터를 조회한다. RESTful API 제공한다. Etcd: 분산형 key/value 오픈소스 Storage. Kubernetes cluster의 DB 역할을 하는 서버로 설정값이나 cluster 상태를 저장한다. Etcd 백업을 통해 클러스터 상태 복구가 가능하다. (ex. 오늘 컨트롤 플레인이 사망했다. 이를 복구할 수 있는 방법은 백업한 Etcd 스냅샷을 기반으로 클러스터에 조인할 수 있다.) kube-scheduler: 할당이 필요한 Pod를 여러 조건(source, label)에 따라 적절한 노드에 할당하는 역할을 한다. kube-controller-manager: Kubernetes 의 Object (Pod, ReplicaSet, Deployment 등) 상태들을 관리한다. Kubernetes 의 Controller 실행을 담당한다. cloud-controller-manager: 오픈 클라우드 서비스와 연결하여 관리하는 controller manger 라고 생각하면 된다. AWS, GCE, Azure 등의 클라우드 특화되어 있다고 한다! 이번 자료를 준비하면서 안 사실은 api-server, etcd, kube-scheduler, kube-controller-manager 요놈이 container 단위로 실행하여 관리하는 줄 알았는데 막상 보니 아니었다. ps -ef |grep {each_component} 를 수행하면 다음과 같다. 12345root@k8s-node1:~# ps -ef|grep -E \"kube-apiserver|etcd|kube-controller-manager|kube-scheduler\"root 3368 3311 1 Aug31 ? 10:17:51 kube-apiserver --advertise-address=192.&gt; 168.179.185 --allow-privileged=true --authorization-mode=Node,RBAC ...root 3584 3554 1 Aug31 ? 05:58:22 etcd --advertise-client-urls=https://192.&gt; 168.179.173:2379 --cert-file=/etc/kubernetes/pki/etcd/server.crt ...root 3677 3636 0 Aug31 ? 00:20:07 kube-controller-manager &gt; --allocate-node-cidrs=true --authentication-kubeconfig=/etc/kubernetes/&gt; controller-manager.conf ... root 3678 3626 0 Aug31 ? 00:38:15 kube-scheduler --bind-address=127.0.0.1 &gt; --kubeconfig=/etc/kubernetes/scheduler.conf --leader-elect=true Worker Node 특징 및 기능 Pod를 생성하고 네트워크와 볼륨을 설정합니다. kubelet 이라는 kubernetes Agent 를 통해 “어떤 조건을 만족하는 Pod 를 띄워!” 라는 임무를 받는다. 실제 컨테이너를 생성하는 서버이다. 노예라고 생각하면 편하다. 각 서버에 라벨을 붙여 사용목적에 따라 나눌 수 있다. 예를들어, GPU 노드로만 사용하여 이 리소스를 사용하는 서비스만 배포하고자 하면 yaml 작성 시 관련 label 을 설정한다. Component in Worker Nodekubelet: 클러스터 내 모든 노드로 배포되는 Agent 이다. Control Plane 에도 배포하지만 Control Plane 에 작성하면 너무 없어보일까 Worker Node 에 작성하였다. 노드에 할당한 Pod 생명주기를 관리하다. Pod 안 컨테이너 상태를 체크하고 주기적으로 Master에 전달한다. Control Plane 의 API서버와 통신을 하고 임무를 받게되면 노드가 수행해야 할 임무를 수행한다. kube-proxy: Kubernetes 내부 별도의 가상 네트워크를 설정하고 관리하는데, 이러한 가상 네트워크가 동작할 수 있게 하는 역할을 한다. 호스트의 (각 노드의) 네트워크 규칙을 관리하고 connection forwarding 을 수행한다. 참고자료 ObjectKubernets는 상태를 관리하기 위한 대상을 Object라 칭하며 크게 Basic Object(기본 오브젝트)와 Controller(컨트롤러)로 구분한다. Basic ObjectBasic Object 는 Namespace, Pod, Service, Volume 4가지가 존재한다. Namespace: Kubernetes cluster 내 논리적인 구분 단위이다. Pod, Service 등을 namespace 별로 생성하고 관리할 수 있으며, user 권한 역시 namespace 별 부여(고구려)할 수 있다. 예를 들어 서비스 테스트를 위해 세 개의 namepsace 를 생성하여 각각 테스트가 가능하다. 대신, 논리적으로 분리했기 때문에 두 namepsace 간 파드의 통신이 가능하다. 따라서 높은 수준의 분리를 원하면 클러스터 자체를 분리하는 것을 권장한다. namespace 내 object 명은 유일해야하지만 전체 namespace 내 object 명은 유일하지 않아도 된다. 예를 들어, A namespace 내 Pod 이름을 test 로 배포하면 B namespace 내 Pod 이름을 test 로 배포할 수 있다. 예시: kubernetes cluster 내 namespace 리스트를 확인한다. 1234567891011121314151617root@k8s-master:~# kubectl get namespaceNAME STATUS AGEanonymous Active 23ddefault Active 191dhyperdata Active 170distio-system Active 23dkfserving-system Active 23dknative-serving Active 23dkube-node-lease Active 191dkube-public Active 191dkube-system Active 191dkubeflow Active 23dmetallb-system Active 171dnps Active 191drook-ceph Active 24dtibero Active 7d23h Pod: Kubernetes의 최소 실행 단위이다. Kubernetes는 컨테이너를 개별적으로 배포하는 것이 아니라 Pod 단위로 배포한다. 즉, 여러 개 컨테이너가 하나의 Pod 로 배포할 수도 있고 하나의 컨테이너를 하나의 Pod 로 배포할 수 있다. (후자의 경우 Pod concept 과 맞지 않아 지양해야 할 부분이다) Pod 내 네트워크 환경(IP, Port)과 디스크(Volume) 공유한다. A Container(Port 8080)와 B Container(Port 7001)가 하나의 Pod로 배포되었을때, localhost:{각 포트}를 통해 두 컨테이너 간 통신이 가능하다. 디스크를 공유하고 있기 때문에 다른 두 성격의 컨테이너를 배포할 때 타 컨테이너의 파일을 읽을 수 있다. YAML / JSON 형식으로 선언(config)하는데, 주로 선언할 때는 YAML 을 주로 사용한다. Service: Pod 는 Controller 에 의해 관리되는데, 만약 노드 내 장애가 발생하여 다른 노드에 Pod 를 재 배포하게되면 Pod 의 IP가 변경된다. 이렇게 Pod IP가 변경되는 것을 방지하고자 IP를 특정하기 위해 사용하는 것이 Service다. label 과 label selector 를 이용하여 같은 서비스로 묶을 파드를 정의할 수 있고, 같은 서비스로 묶이면 고정 주소를 이용해 묶인 파드간 통신을 원활히 할 수 있다. Service Type ClusterIP: kubernetes cluster 내에서만 사용가능한 IP로 외부에서는 접근이 불가하다. NodePort: cluster 내 모든 노드의 지정된 포트를 할당하는 방식이다. 예를 들어, A 노드에 떠있는 a 파드로 접근하고 싶을 때 a 파드를 NodePort 서비스로 묶어주면 B 노드 IP:Port 로 접근할 수 있다. LoadBalancer: NodePort 의 확장으로 서비스 IP, 등 외부로 노출하고자 하는 특정 IP 를 이용하여 외부에서 특정 IP를 통해 접근할 수 있는 Service Type 이다. ExternalName: 외부 서비스를 Kubernetes 내부에 호출하고자 할 때 사용하는 Service Type 이다. (사용을 안해봤다..) Volume Container 재시작에 상관없이 파일을 영구적으로 저장해야하는 스토리지이다. Pod 내 Container 들은 해당 디스크를 공유하여 사용할 수 있다. 종류: emptyDir: Pod 와 함께 생성하고 사라지는 임시 볼륨이다. hostPath: 해당 노드의 디스크 경로를 Pod 에 마운트하여 사용하는 방식이다. 해당 노드 디스크 경로가 존재하지는 꼭 확인이 필요하다. Kubernetes 에서는 emptyDir, hostPath 이외에 PV(PersistentVolumne) 와 PVC(PersistentVolumnClaim) 라는 추상화한 개념을 사용하고 있다. Pod 와 별도의 생명주기를 가지고 있어 해당 볼륨을 물고 있는 Pod 가 죽더라도 PV 또는 PVC 를 지우지 않는 한 데이터는 소멸하지 않는다. 쉽게 말해 노트북 디스크를 효율적으로 사용하기 위해 외장 하드를 이용한다고 생각하자. PV: 클러스터 내에서 자원으로 다뤄지는 볼륨 자체를 의미한다. 하나의 저장소로 생각하자. PVC: 사용자가 PV에 요청하는 것입니다. 얼만큼, 읽고쓰기가 가능한 지 spec 을 정의하여 배포하면 그에 맞는 PV가 생성된다. Controller Controller 는 다중 Pod 를 생성하고 관리하는 역할로 클러스터 내에서 replication handling, rollout 등의 기능을 제공한다. 예를 들어, 클러스터 내 한 Node 가 다운되었을 경우 Controller 는 다른 노드에 해당 파드를 배포(스케쥴링) 함으로써 관리한다. 굉장히 많은 Controller 가 있는데 그 중에서 ReplicaSet, Deployment, DaemonSet 을 살펴본다. ReplicaSet Pod 를 복제하여 생성하고 이를 지속적으로 유지하는 Controller 이다. 실행하고자 하는 Pod 의 가용성을 보장하기 위해 원하는 갯수만큼 설정하여 배포하면, 그 갯수만큼 Pod 가 실행되게끔 관리하는 Controller 이다. 직접적으로 사용하기 보단 Deployment 등 다른 오브젝트에 의해 사용되는 경우가 많다. 예시: nginx 이미지를 기반으로 하는 컨테이너 (이 예시는 1 container == 1 pod) 를 spec.replicas value 만큼 유지하세요. 1234567891011121314151617181920apiVersion: apps/v1kind: ReplicaSetmetadata: name: test-replicasetspec: template: metadata: name: test-replicaset labels: app: test-replicaset spec: containers: - name: test-replicaset image: nginx ports: - containerPort: 80 replicas: 3 selector: matchLabels: app: test-replicaset Deployment Kubernetes 에서 상태가 없는(stateless) 앱을 배포할 때 가장 기본적인 컨트롤러, 이전 버젼에서는 ReplicaSet 이 담당했지만 이제는 Deployment 가 담당한다. Pod 개수를 유지하는 것 뿐 아니라 무중단 배포를 위한 방식 (blue/green deployment, canary deployment, rolling deployment) 을 제공한다. ReplicaSet 과 비교하면 Pod 개수 유지는 같지만 추가적으로 무중단 배포 방식을 설정할 수 있다. deployment 배포 시 업데이트 방식을 설정할 수 있다. (조대협님 블로그에 따르면 가장 많이 사용하는 배포 방식이며, 따로 설정하지 않으면 rolling 배포를 수행한다. 참고) 무중단 배포방식: 참고자료 DaemonSet 클러스터 전체에 Pod를 띄워주는 Controller 이다. Daemonset 을 배포하면 항상 그 Pod 가 전체 노드에 배포한다. 새롭게 노드가 추가되었을때 자동으로 Pod 를 배포한다. 반대로 노드가 클러스터에서 빠졌을 때 그 노드에 있던 포드는 그대로 사라지고 다른 곳으로 옮겨가서 배포하진 않는다. 이러한 특징 때문에 보통 로그수집기를 실행하거나 모니터링용 데몬등 클러스터 전체에 항상 실행시켜 두어야 할 때 사용한다. StatefulSet 앞서 살펴본 Controller 와 달리 상태를 가지고 있는 Pod 들을 관리하는 Controller 로, 볼륨을 사용해 특정 데이터를 기록해두고 Pod 가 재시작 해도 유지할 수 있다. 여러 Pod 를 배포할 때 순서를 지정하여 순서대로 배포할 수 있다. Pod 이름을 연속성 있게 설정할 수 있다. (name-number 와 같이 표현) 각 Pod 에 대한 PVC 를 설정할 수 있는 장점이 있다. Reference https://ooeunz.tistory.com/118 https://www.slideshare.net/lahuman1/kubernetes-object https://bcho.tistory.com/1259 https://blog.2dal.com/2018/04/30/kubernetes-02-replicaset/ https://perfectacle.github.io/2019/04/21/non-stop-deployment/ https://arisu1000.tistory.com/27834 https://www.joinc.co.kr/w/man/12/kubernetes/overview","categories":[{"name":"Cloud","slug":"Cloud","permalink":"https://jx2lee.github.io/categories/Cloud/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://jx2lee.github.io/tags/Kubernetes/"}]},{"title":"[Cloud] Kubernetes 소개","slug":"cloud-kubernetes_chapter_01","date":"2020-09-23T15:00:00.000Z","updated":"2020-10-03T13:50:21.081Z","comments":true,"path":"cloud-kubernetes_chapter_01/","link":"","permalink":"https://jx2lee.github.io/cloud-kubernetes_chapter_01/","excerpt":"신입 직원 교육자료를 위해 작성한 Kubernetes 소개 자료이다. 많은 블로그를 참고하여 작성하였고, 이번 장에는 배경을 시작으로 특징까지 살펴본다.","text":"신입 직원 교육자료를 위해 작성한 Kubernetes 소개 자료이다. 많은 블로그를 참고하여 작성하였고, 이번 장에는 배경을 시작으로 특징까지 살펴본다. &#x2728; Contents: 배경 그럼 왜 Kubernetes 가 떴을까? Kubernetes ? Kubernetes 특징 Reference 배경 [그림] 배포 환경의 변화 컨테이너 기반의 환경은 서비스 배포에 장점이 있고 마이크로 서비스 아키텍쳐(MSA)에 부합한 듯 싶지만, 가상 머신으로도 충분히 패키징이 가능하고 로컬의 개발환경을 동기화 시키는 일은 vagrant 로도 충분하였다. MSA: 소프트웨어 개발 기법 중 하나로 전체 어플리케이션을 특정 목적을 가진 어플리케이션 단위로 나눈 것, 단위 어플리케이션 간 약한 결합도와 강한 응집도를 목표로 한다. vargrant: 간소화한 VM 관리 서비스로 가상 머신과 로컬간의 동기화 서비스도 제공한다. 그리고 결정적으로 컨테이너를 운용하기 위한 관리 환경이 성숙하지 않았다. Mesos DC/OS, Docker Swarm, Kubernetes 등 다양한 환경이 나오기는 하였지만 기능적으로 부족하며 어떤 플랫폼이 대세라고 정해지지 않았다. Mesos DC/OS: Apache Mesos 분산 시스템 커널을 기반으로하는 오픈 소스, 참고자료 Docker Swarm: 도커가 공식적으로 개발한 Container Orchestration 그럼에도 컨테이너 환경으로 바뀐 이유가 무엇일까? 두 꼭지로 살펴보면 다음과 같다. MSA의 발전:MSA(Micro Service Architecture)가 단순 개념에서 발전하기 시작하며 이를 구현하기 위한 다양한 인프라 플랫폼들이 등장하기 시작했다. [그림] Micro Service Architecture 예시- 전체 애플리케이션을 특정 단위 애플리케이션으로 분리한 모습을 확인할 수 있다. 또한, 서비스 크기가 작아지며 CPU 1~2 Core 로도 운영할 수 있는 서비스들이 등장하면서 이를 VM 환경에서 서비스 하기엔 리소스 낭비가 심해졌다. VM 환경을 위해서 필요한 이미지 크기가 크고, 다양한 서비스를 VM으로 관리 배포하기에는 속도 등에서 효율적이지 못하다. 솔루션 발전 및 DevOps 안착:서비스 배포 방식도 VM 혹은 컨테이너 단위로 배포하는 피닉스 서버 패턴이 등장하였고 이를 구현하기 위한 Spinnaker와 같은 솔루션이 등장하였다. 또한, 지능형 라우팅과 분산 트렌젝션 로그 추적을 하는 기능들이 Envoy 라는 솔루션으로 나오고 이를 중앙 통제하기 위한 Istio.io 와 같은 Service Mesh 솔루션 까지 주목을 받기 시작하였다. Phoenix Server Patern: SW 테스트를 위해 변경한 설정을 현 서버에 적용하는 것이 아닌 OS설치부터 SW 설치-설정 변경까지 다시 &gt; 반복하는 패턴을 의미한다. Envoy: 대형 MSA의 단일 Application과 Service를 위해 설계된 고성능 분산 c++프록시 Istio.io: Envoy proxy를 사용하며 이를 컨트롤 해주는 Control Plane의 오픈소스 솔루션, 참고자료 DevOps 개념도 나온지 오래되었지만, 운영을 DevOps라는 이름으로 바꾼 것일뿐 실제적인 변화가 없는 팀들이 많았다고 한다. 또는 DevOps라는 이름 아래 개발팀이 개발과 운영 역할을 병행해서 하는 사례가 오히려 많았다. 이런 DevOps의 개념도 근래 들어 개발팀이 개발과 시스템에 대한 배포/운영을 담당한다면, DevOps 팀은 개발팀이 이를 편리하게 사용할 수 있게끔 플랫폼과 자동화를 하는데 목표를 두는 역할로 구분지어졌다. 그럼 왜 Kubernetes 가 떴을까?컨테이너 운용 환경은 오픈소스에 의해 표준없이 혼돈상태였는데, 2017년 말을 기점으로 Kubernetes가 de-facto 표준으로 자리잡았다. 아래 트랜드 그래프를 보면 알 수 있듯 kubernetes의 트랜드가 지속적으로 올라가서 가장 높은 것을 확인할 수 있다. 추가로 github repo star 통계에 대해서도 2015 년을 기점으로 크게 증가하여 2018년 말에는 엄청 큰 차이를 보이고 있다. [그림] Container Orchestration Trends [그림] github repo star for Container Orchestration Kubernetes ? 컨테이너 운영환경 중 가장 널리 사용되는 솔루션이다. (약어로 k8s, 8의 의미는 k와 s사이 알파벳 갯수) 구글의 내부 컨테이너 서비스를 Borg라고 하는데 이 구조를 오픈소스화 한 것이다. GO 언어로 구현하였다. golang 벤더나 플랫폼에 종속되지 않기 때문에 Public Cloud (Google, Amazone, MS Azure) 와 Private CLoud 및 베어메탈 환경 (가상화 환경을 사용하지 않는 일반 서버 하드웨어, like 깡통) 에서 구축 가능 이러한 이유로 Hybrid Cloud 환경 가능 (Private + Public) 다양한 Container Runtime 제공 Docker 또한 Container Runitme 으로 사용가능하지만, CRI-O 가 현재 Kubernetes 를 위한 표준 Container Runtime 으로 자리잡고 있다. 관련 자료 http://www.opennaru.com/kubernetes/cri-o/ https://www.samsungsds.com/global/ko/support/insights/docker.html https://bcho.tistory.com/1353 Kubernetes 특징 상태관리 : 상태를 선언하고 선언한 상태를 계속해서 유지한다. 노드가 죽거나 Container 응답이 없을 경우, 자동으로 새로운 Container 를 실행하거나 자동으로 특정 상태에 도달하지 못한 Conatiner 를 중지하는 등 선언한 상태를 계속해서 유지한다. 스케줄링 : 어떤 노드에 Container 를 실행할지 고민하지 않아도 Kubernetes 조건에 맞는 노드를 찾아서 Conatiner 를 배치한다. (특정 노드에 실행하게 조건 설정도 가능하다) 클러스터 : 가상 네트워크를 통해 통신하기 때문에 하나의 서버처럼 관리할 수 있다. 서비스 디스커버리 : 서로 다른 서비스를 쉽게 찾고 통신할 수 있습니다. Reference 쿠버네티스 #1 - 소개 [Kubernetes] 쿠버네티스의 등장 배경 [초보개발자 일지] 대세 MSA? 너 뭐니? made by jaejun.lee","categories":[{"name":"Cloud","slug":"Cloud","permalink":"https://jx2lee.github.io/categories/Cloud/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://jx2lee.github.io/tags/Kubernetes/"}]},{"title":"[Database] 일 단위 적재 프로세스 훑어보기","slug":"database-daily_batch_process","date":"2020-09-09T15:00:00.000Z","updated":"2020-09-26T14:08:33.079Z","comments":true,"path":"database-daily_batch_process/","link":"","permalink":"https://jx2lee.github.io/database-daily_batch_process/","excerpt":"프로젝트 시범과제 중 [데이터 마트 구축]을 수행하면서 고객요건에 맞는 테이블을 생성하고 갱신하는 프로세스를 경험하였다. 별 어려운 내용은 없지만 내 머릿속에 저장히기 위해 글로 남겨놓는다. (도움을 주신 갓정희님께 감사의 인사를)","text":"프로젝트 시범과제 중 [데이터 마트 구축]을 수행하면서 고객요건에 맞는 테이블을 생성하고 갱신하는 프로세스를 경험하였다. 별 어려운 내용은 없지만 내 머릿속에 저장히기 위해 글로 남겨놓는다. (도움을 주신 갓정희님께 감사의 인사를) &#x2728; Contents: 유저 생성 TARGET 테이블 생성 주 단위 데이터 마감 Reference 유저 생성해당 테이블을 관리하는 계정을 생성하였다. user/pw: mart_20/*** role: connect, resource User create query123 --Create \"MART_20\" UserCREATE USER MART_20 IDENTIFIED BY '*******';GRANT CONNECT, RESOURCE TO MART_20; TARGET 테이블 생성고객이 사용하던 SQL 쿼리를 살펴보면, 총 12개 Table 및 View 를 조합하여 네 덩어리 SELECT 조회 결과를 UNION ALL 한다. 운영자가 수정하는 부분은 날짜 칼럼을 입력하게끔 쿼리를 작성하여 이를 EXCEL 로 다운받아 요청자에게 전달하는 방식이다. 우리는 이 쿼리의 결과물을 미리 테이블로 정의하고, 일 적재를 통해 고객이 쉽게 우리 제품을 이용해 분석하고 싶을 때 분석 가능하게 환경을 구성할 예정이다. 하지만 요청자를 위해 작성된 Query는 다음과 같은 제약사항이 있다. Primary Key 로 설정한 SEQ 는 추후 중복이 발생할 수 있다. 수집을 하다보면 언젠가 UNION ALL 전의 한 덩어리에서의 SEQ 칼럼과 나머지 세 덩어리에서의 SEQ 와 동일해질 수 있다. 서로 다르게 SEQ Value 관리하기 때문이다. 이를 해결할 방법으로 UNION ALL 전의 네 덩어리를 적재 테이블로 각각 관리하면 된다. 이후 생성한 네 개 테이블을 UNION ALL 한 VIEW 를 바라보면 되는데, 이는 1) 관리 포인트가 많아지고 2) View를 사용하기 대문에 성능 이슈가 발생할 수 있다. 그럼에도 나는 하나의 테이블로 관리하고자 한다. Source Table 에서 수정이 발생하면 이에 대한 Target Source 처리가 필요하다. 즉, 이전 날짜의 데이터에 대한 수정이 발생하면 Target Source 에 대한 반영이 필요하다. 이는 주 또는 월 단위 DEL_FLAG 변수를 활용해 삭제 또는 추가 여부를 확인하는 배치 작업 으로 반영할 수 있다. 단, Update period 는 협의가 필요한 상황! HABITAUL_PRACTICE_TEMP 테이블을 생성하여 마감 처리를 진행할 것이다. 테이블 Create Query 는 아래와 같다. 1234567891011121314151617181920212223242526CREATE TABLE MART_20.HABITAUL_PRACTICE_ANALYSIS ( SEQ NUMBER(22) NOT NULL PRIMARY KEY, DOMAIN_SEQ NUMBER(22),...... \"위변조 통장\" NUMBER, \"위변조 기타\" NUMBER, DEL_FLAG VARCHAR(2) DEFAULT 'N' );CREATE INDEX MART_20.HABITAUL_PRACTICE_ANALYSIS_IDX_01 ON MART_20.HABITAUL_PRACTICE_ANALYSIS ( TFD_DATE, DOMAIN_LINK, DEL_FLAG );CREATE TABLE MART_20.HABITAUL_PRACTICE_TEMP ( SEQ NUMBER(22) NOT NULL PRIMARY KEY, DOMAIN_SEQ NUMBER(22),...... \"위변조 통장\" NUMBER, \"위변조 기타\" NUMBER ); PK: SEQ INDEX: DOMAIN_LINK, TFD_DATE, DEL_FLAG TEMP 테이블과 원본 테이블의 차이점은 DEL_FLAG 칼럼의 여부이다. 주 단위 데이터 마감운영진에서 수정된 데이터를 반영하기 위해 주 단위 데이터를 마감할 것이다. 아래와 같은 순서로 진행하는데, 이는 실시간 수정 반영이 어려워(구조상) 고객와 협의하여 진행하였다. (마감 주기, 즉 Update 반영을 한 주 전까지만 반영) 1) 현재 기준 이 전 데이터 조회 후 저장데이터 주 단위 갱신 마감을 위한 조회 Query는 다음과 같다. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889SELECT * FROM ( SELECT SEQ, DOMAIN_SEQ, DOMAIN_LINK,...... CASE WHEN INSTR(ILLE_EXPR, '0209') &gt; 0 THEN 1 END AS \"위변조 기타\" FROM ( SELECT A.SEQ, DECODE(WEB_CACHE_URL,NULL,A.DOMAIN_SEQ,100000001) AS DOMAIN_SEQ, DECODE(WEB_CACHE_URL,NULL,B.DOMAIN_LINK,'webcache.googleusercontent.com') AS DOMAIN_LINK, DECODE(WEB_CACHE_URL,NULL,F.DOMAIN_NAME,'구글웹캐시') AS DOMAIN_NAME, F.DOMAIN_CODE2,...... WM_CONCAT (E.EXPR_CATEGORY || E.EXPR_SUB_CATE) ILLE_EXPR FROM PIRST_19.DETECTED_URL@DS16 A INNER JOIN PIRST_19.DOMAIN_MASTER@DS16 B ON A.DOMAIN_SEQ = B.DOMAIN_SEQ INNER JOIN PIRST_19.DOMAIN_INFORMATION@DS16 F ON A.DOMAIN_SEQ = F.DOMAIN_SEQ LEFT OUTER JOIN PIRST_19.KEYWORD_MASTER@DS16 C ON A.KEYWORD_SEQ = C.KEYWORD_SEQ LEFT OUTER JOIN PIRST_19.A_ILLEGAL_TRADER_MST@DS16 D ON A.SEQ = D.URL_SEQ LEFT OUTER JOIN PIRST_19.DETECTED_URL_EXPR@DS16 E ON A.SEQ = E.URL_SEQ AND E.EXPR_USE = 'Y' LEFT OUTER JOIN (SELECT * FROM PIRST_19.DETECTED_IMG@DS16 WHERE ROWID IN (SELECT MAX(ROWID) FROM PIRST_19.DETECTED_IMG@DS16 GROUP BY DETECTED_SEQ)) G ON A.SEQ = G.DETECTED_SEQ...... AND C.KEYWORD_REG_USER_ID != 'covid19' GROUP BY A.SEQ, DECODE(WEB_CACHE_URL,NULL,A.DOMAIN_SEQ,100000001), DECODE(WEB_CACHE_URL,NULL,B.DOMAIN_LINK,'webcache.googleusercontent.com'), DECODE(WEB_CACHE_URL,NULL,F.DOMAIN_NAME,'구글웹캐시'), F.DOMAIN_CODE2, F.DOMAIN_CODE3, ...... D.URL_SEQ, A.POST_DATE ) A LEFT OUTER JOIN PIRST_19.A_ILLEGAL_TRADER_MST@DS16 B ON A.TRADER_SEQ = B.TRADER_SEQ AND A.URL_SEQ = B.URL_SEQ)--한 덩어리 끝 UNION ALL SELECT A.MANUAL_SEARCH_SEQ, A.DETECTED_DOMAIN_SEQ,...... DECODE(SUBSTR(D.exposure_type, INSTR(D.exposure_type, 'C09'),3),'C09',1) as SIXTEEN FROM PIRST_19.MANUAL_DETECTED_URL@DS16 A,...... PIRST_19.A_ILLEGAL_TRADER_MST@DS16 E WHERE A.DETECTED_DOMAIN_SEQ = B.DOMAIN_SEQ AND A.DETECTED_DOMAIN_SEQ = C.DOMAIN_SEQ...... UNION ALL--두 덩어리 끝 SELECT A.MANUAL_SEARCH_SEQ, A.DETECTED_DOMAIN_SEQ,...... DECODE(SUBSTR(D.exposure_type, INSTR(D.exposure_type, 'C08'),3),'C08',1) as FIFTEEN, DECODE(SUBSTR(D.exposure_type, INSTR(D.exposure_type, 'C09'),3),'C09',1) as SIXTEEN FROM PIRST_19.MANUAL_CACHE_URL@DS16 A, PIRST_19.DOMAIN_MASTER@DS16 B,...... WHERE A.DETECTED_DOMAIN_SEQ = B.DOMAIN_SEQ...... AND A.MBER_ID != 'itno_cmbok'----세 덩어리 끝 UNION ALL SELECT A.MANUAL_SEARCH_SEQ,...... DECODE(SUBSTR(D.exposure_type, INSTR(D.exposure_type, 'C08'),3),'C08',1) as FIFTEEN, DECODE(SUBSTR(D.exposure_type, INSTR(D.exposure_type, 'C09'),3),'C09',1) as SIXTEEN FROM PIRST_19.MANUAL_IMAGE_URL@DS16 A, PIRST_19.DOMAIN_MASTER@DS16 B,...... WHERE A.DETECTED_DOMAIN_SEQ = B.DOMAIN_SEQ AND A.DETECTED_DOMAIN_SEQ = C.DOMAIN_SEQ......--네 덩어리 끝; TFD_DATE 를 SYSDATE(현재 시간) 기준 7일 기준으로 검색한다. 즉, 이전 일주일 치 데이터를 조회한다. 20.09.03~20.09.09: 3993건 결과물을 HABITUAL_TEMP 테이블에 저장한다. 2) 현재 기준 일주일 전 데이터의 DEL_FLAG 변경HABITUAL_BUYER 에 위 기간 데이터의 DEL_FLAG 변경 Query 는 다음과 같다. 1234UPDATE HABITUAL_BUYERSET DEL_FLAG='Y'WHERE TFD_DATE &gt;= TO_DATE(TO_CHAR(SYSDATE, 'YYYYMMDD'),'YYYYMMDD') - 7 AND TFD_DATE &lt; TO_DATE(TO_CHAR(SYSDATE, 'YYYYMMDD'),'YYYYMMDD') 3) TEMP 테이블과 TARGET 테이블 간 Merge두 테이블 간 (HABITUAL_BUYER // HABITUAL_TEMP) Merge Ouery는 다음과 같다. 12345678910111213MERGE INTO (SELECT * FROM HABITUAL_BUYER WHERE TFD_DATE &gt;= TO_DATE(TO_CHAR(SYSDATE, 'YYYYMMDD'),'YYYYMMDD') - 7 AND TFD_DATE &lt; TO_DATE(TO_CHAR(SYSDATE, 'YYYYMMDD'),'YYYYMMDD') ) AS AUSING HABITUAL_TEMP AS BON (A.SEQ = B.SEQ)WHEN MATCHED THENUPDATE SETA.DEL_FLAG = 'N'WHEN NOT MATCHED THENINSERT (A.SEQ,A.DOMAIN_SEQ ,A.DOMAIN_LINK ,A.DOMAIN_NAME ,A.CODE_NM2 ,A.CODE_NM3,A.DETECTED_CRAWL_COUNT,A.DETECTED_LINK,A.DETECTED_COUNT ,A.TRUE_DETECTION_COUNT ,A.FALSE_DETECTION_COUNT ,A.DETECTED_NEW_EXPOSURE_COUNT ,A.DETECTED_RE_EXPOSURE,A.DETECTED_RE_EXPOSURE_COUNT ,A.DOMAIN_CATEGORY01,A.DOMAIN_CATEGORY02,A.SEARCH_COL_TYPE,A.DOMAIN_GROUP,A.DETECTED_DEPTH,A.DOMAIN_COUNTRY_CODE,A.DETECTED_STATUS,A.DETECTED_GROUP,A.DETECTED_TYPE,A.DETECTED_CHECK_TYPE,A.KEYWORD_SEQ,A.KEYWORD_VALUE,A.INDI_DOMAIN_SEQ,A.WEB_CACHE_URL,A.DETECTED_TIME ,A.TFD_DATE ,A.DEL_DONE_DATE ,A.UPDATOR,A.REQUEST_ORDINAL,A.DETECTED_POST_TYPE,A.GATHERING_TYPE,A.IMG_DETECTION_TYPE,A.POST_DATE ,A.REFERENCE,A.TRADER_TYPE,A.\"판매자ID\",A.\"이메일\",A.\"카카오톡\",A.\"네이트온\",A.\"MSN메신저\",A.\"스카이프\",A.\"위챗\",A.QQ,A.\"텔레그램\",A.\"기타1\",A.\"기타2\",A.\"핸드폰번호\",A.\"일반번호\",A.\"거래 개인정보DB\" ,A.\"거래 통장\" ,A.\"거래 ID판매\" ,A.\"거래 아이핀\" ,A.\"거래 대포폰\" ,A.\"거래 해킹\" ,A.\"거래 기타\" ,A.\"위변조 증명서\" ,A.\"위변조 성적표\" ,A.\"위변조 신분증\" ,A.\"위변조 자격증\" ,A.\"위변조 여권\" ,A.\"위변조 기록부\" ,A.\"위변조 내역서\" ,A.\"위변조 통장\" ,A.\"위변조 기타\" )VALUES (B.SEQ,B.DOMAIN_SEQ ,B.DOMAIN_LINK ,B.DOMAIN_NAME ,B.CODE_NM2 ,B.CODE_NM3,B.DETECTED_CRAWL_COUNT,B.DETECTED_LINK,B.DETECTED_COUNT ,B.TRUE_DETECTION_COUNT ,B.FALSE_DETECTION_COUNT ,B.DETECTED_NEW_EXPOSURE_COUNT ,B.DETECTED_RE_EXPOSURE,B.DETECTED_RE_EXPOSURE_COUNT ,B.DOMAIN_CATEGORY01,B.DOMAIN_CATEGORY02,B.SEARCH_COL_TYPE,B.DOMAIN_GROUP,B.DETECTED_DEPTH,B.DOMAIN_COUNTRY_CODE,B.DETECTED_STATUS,B.DETECTED_GROUP,B.DETECTED_TYPE,B.DETECTED_CHECK_TYPE,B.KEYWORD_SEQ,B.KEYWORD_VALUE,B.INDI_DOMAIN_SEQ,B.WEB_CACHE_URL,B.DETECTED_TIME ,B.TFD_DATE ,B.DEL_DONE_DATE ,B.UPDATOR,B.REQUEST_ORDINAL,B.DETECTED_POST_TYPE,B.GATHERING_TYPE,B.IMG_DETECTION_TYPE,B.POST_DATE ,B.REFERENCE,B.TRADER_TYPE,B.\"판매자ID\",B.\"이메일\",B.\"카카오톡\",B.\"네이트온\",B.\"MSN메신저\",B.\"스카이프\",B.\"위챗\",B.QQ,B.\"텔레그램\",B.\"기타1\",B.\"기타2\",B.\"핸드폰번호\",B.\"일반번호\",B.\"거래 개인정보DB\" ,B.\"거래 통장\" ,B.\"거래 ID판매\" ,B.\"거래 아이핀\" ,B.\"거래 대포폰\" ,B.\"거래 해킹\" ,B.\"거래 기타\" ,B.\"위변조 증명서\" ,B.\"위변조 성적표\" ,B.\"위변조 신분증\" ,B.\"위변조 자격증\" ,B.\"위변조 여권\" ,B.\"위변조 기록부\" ,B.\"위변조 내역서\" ,B.\"위변조 통장\" ,B.\"위변조 기타\" ) SEQ 기준으로 데이터가 존재하면 DEL_FLAG를 N 으로 수정한다. 만약 기존 적재된 SEQ가 없는 데이터가 있다면, 새롭게 INSERT (삭제될 경우도 있지만 추가될 경우도 존재) 한다. 각 테이블 조회해보면 FLAG가 변경된 값을 확인할 수 있다. 12345678910111213141516171819--20.09.03~20.09.09 새로 수집한 데이터 조회SELECT COUNT(*)FROM MART_20.HABITUAL_TEMP;--3993--BUYER 테이블에서 일주일 전 데이터 FLAG Y로 수정 후 COUNTSELECT COUNT(*)FROM MART_20.HABITUAL_BUYERWHERE DEL_FLAG='Y';--3993--MERGE 이후 COUNTSELECT COUNT(*)FROM MART_20.HABITUAL_BUYERWHERE DEL_FLAG='Y';--0, 왜냐면 수정된 데이터가 없으므로.. 데이터 주 단위 마감 순서를 한 줄로 요약하면, 마감 기준 치 데이터를 조회 후 TEMP 테이블에 저장하고 TARGET 테이블(TEMP 테이블에 저장한 데이터의 같은 날짜)의 DEL_FLAG 를 Y로 변경하여 TEMP 테이블과 Merge 한다. 직접 수행하면서 느낀점은 Merge 로 인한 성능 이슈가 발생할 수 있는데, 이를 어떻게 처리할 수 있는지는 더 연구해봐야겠다는 생각이 들었다. 배치 작업을 수행하면서 울팀 선배님이 많은 도움을 주셨다. 앞으로 나도 내 후배에게 많은 도움을 줄 수 있는 선배로 거듭나길.. Reference https://kookyungmin.github.io/db/2018/07/30/oracle_32/ https://jennylee4517.github.io/sql/oracle-1일차/ https://m.blog.naver.com/PostView.nhn?blogId=ojini21c&amp;logNo=221193420479&amp;proxyReferer=https:%2F%2Fwww.google.com%2F made by jaejun.lee","categories":[{"name":"Database","slug":"Database","permalink":"https://jx2lee.github.io/categories/Database/"}],"tags":[{"name":"Tibero","slug":"Tibero","permalink":"https://jx2lee.github.io/tags/Tibero/"}]},{"title":"[Rook Ceph] Rook ceph dashboard 사용하기","slug":"cloud-export_rook_ceph_dashboard","date":"2020-09-02T15:00:00.000Z","updated":"2020-09-26T13:42:48.682Z","comments":true,"path":"cloud-export_rook_ceph_dashboard/","link":"","permalink":"https://jx2lee.github.io/cloud-export_rook_ceph_dashboard/","excerpt":"Toolbox POD 로 접근하여 ceph cluster 를 확인하곤 한다. alias 로 바로 접근할 수 있게 설정하였지만, 좀 더 직관적으로 보기 위해 dashboard 를 배포해 보자!","text":"Toolbox POD 로 접근하여 ceph cluster 를 확인하곤 한다. alias 로 바로 접근할 수 있게 설정하였지만, 좀 더 직관적으로 보기 위해 dashboard 를 배포해 보자! &#x2728; Contents: rook-ceph service 확인 rook-ceph-mgr-dashboard-external-http 서비스 생성 dashboard 접속 Reference rook-ceph Service 확인kubectl get service -n rook-ceph 명령어로 기동중인 서비스를 확인한다. 123456789root@k8s-master:~# kubectl get service -n rook-cephNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEcsi-cephfsplugin-metrics ClusterIP 10.96.205.129 &lt;none&gt; 8080/TCP,8081/TCP 2d17hcsi-rbdplugin-metrics ClusterIP 10.96.158.126 &lt;none&gt; 8080/TCP,8081/TCP 2d17hrook-ceph-mgr ClusterIP 10.96.133.224 &lt;none&gt; 9283/TCP 2d17hrook-ceph-mgr-dashboard ClusterIP 10.96.37.245 &lt;none&gt; 8443/TCP 2d17hrook-ceph-mon-a ClusterIP 10.96.59.113 &lt;none&gt; 6789/TCP,3300/TCP 2d17hrook-ceph-mon-b ClusterIP 10.96.26.250 &lt;none&gt; 6789/TCP,3300/TCP 2d17hrook-ceph-mon-c ClusterIP 10.96.235.59 &lt;none&gt; 6789/TCP,3300/TCP 2d17h rook-ceph-mgr-dashboard 서비스가 8443 포트로 통신 가능하게 설정되어 있다. 외부로 dashboard 를 노출하는 방법은 1) rook-ceph-mgr 서비스 type 바꾸기 와 2) external service 가 있다. 이번 글에서는 2번 방법을 이용해 dashboard 를 사용해보겠다. rook-ceph-mgr-dashboard-external-http 서비스 생성하기다음과 같은 yaml 를 작성한다. 1234567891011121314151617181920root@k8s-master:~/hypercloud-rook-ceph-master/deploy# cat dashboard-external-http.yamlapiVersion: v1kind: Servicemetadata: name: rook-ceph-mgr-dashboard-external-http namespace: rook-ceph labels: app: rook-ceph-mgr rook_cluster: rook-cephspec: ports: - name: dashboard port: 8443 protocol: TCP targetPort: 8443 selector: app: rook-ceph-mgr rook_cluster: rook-ceph sessionAffinity: None type: NodePort Service 타입은 NodePort 로 설정한다. 이외 외부로 노출하고 싶다면 LoadBalancer 로 변경한다. (로드밸런서 경우 퍼블릭 IP가 존재해야한다.) rook-ceph-mgr Port 가 만약 나와 다른 번호(예. 7775) 로 설정되어 있다면, 위 yaml 에서도 같은 포트로 설정해야한다. 생성한 yaml 을 이용해 kubectl apply -f dashboard-external-http.yaml 커맨드로 service 를 생성한다. 이후 접속하기 위한 초기 admin 비밀번호 확인을 위해 다음과 같은 커맨드를 수행한다. kubectl -n rook-ceph get secret rook-ceph-dashboard-password -o jsonpath=&quot;{[&#39;data&#39;][&#39;password&#39;]}&quot; | base64 --decode &amp;&amp; echo dashboard 접속클러스터 노드 중 하나를 골라 https://[노드ip]:[port] 로 접속한다. ID/PW: admin/[바로 위 커맨드 수행 결과] 최초 접속 후 설정에 들어가 PW를 바꾸도록 하자. dashboard UI 를 통해 ceph cluster 상태를 직관적으로 볼 수 있어 편리하긴 하다. 그래도 조금 더 쿠버네티스와 친해지기 위해 커맨드로 ceph cluster 상태를 확인하는 것이 더 나을듯 하다! Reference https://github.com/rook/rook/blob/master/Documentation/ceph-dashboard.md made by jaejun.lee","categories":[{"name":"Cloud","slug":"Cloud","permalink":"https://jx2lee.github.io/categories/Cloud/"}],"tags":[]},{"title":"[Python] 단순 키워드 매칭 모듈","slug":"python-keyword_match","date":"2020-08-19T15:00:00.000Z","updated":"2020-10-29T14:35:49.291Z","comments":true,"path":"python-keyword_match/","link":"","permalink":"https://jx2lee.github.io/python-keyword_match/","excerpt":"프로젝트 시범 과제 중 하나인 “게시판 및 회원가입 여부 판단”을 위해 파이썬으로 모듈을 하나 작성했다. 알고리즘 자체는 정말 단순한데, 해당 게시판과 회원가입 여부에 대한 키워드를 통해 있는지 없는지만 확인하여 판단한다. 모듈 작성 배경과 간단한 테스트를 수행한다.","text":"프로젝트 시범 과제 중 하나인 “게시판 및 회원가입 여부 판단”을 위해 파이썬으로 모듈을 하나 작성했다. 알고리즘 자체는 정말 단순한데, 해당 게시판과 회원가입 여부에 대한 키워드를 통해 있는지 없는지만 확인하여 판단한다. 모듈 작성 배경과 간단한 테스트를 수행한다. &#x2728; Contents: 모듈 작생 배경 어떻게 작성할까? 테스트 결과는요? 어땠나요? 모듈 작성 배경원래 키워드 매칭 관련 시범과제는 주 사업자 영역인 크롤러에서 수행하기로 하였지만, 빅데이터 플랫폼을 구축하면서 빅데이터 플랫폼 안에서 수행해야 그림이 이쁘지 않을까? 라는 의견이 나와 우리쪽에서 수행하기로 결정하였다. 관련하여 연구소에 문의했지만, 키워드 포함 여부에 대한 연구는 이루어지지 않아 우리 팀에서 맡아 진행하기로 협의하였다. 구글링을 통해서 많은 키워드 매칭 자료를 찾았지만, 시범과제 성격이 크기 때문에 거창한 알고리즘을 쓰지 않기로 하였다. 단순 해당 여부에 대한 키워드를 받아 이 키워드가 있는지? 없는지? 만 판단하는 쪽으로 시범과제를 수행하기로 했다. 우리는 총 2개 DB table 에서 데이터를 받기로 했다. 첫 번째 는 URL / URL 내 텍스트 / timestamp 칼럼으로 이루어진 테이블이다. 두 번째 는 keyword type / keyword value (type: 해당 프로젝트에서는 회원가입 과 게시판) 으로 이루어진 테이블이다. 위 테이블 데이터는 우리 제품 python API 를 이용해 dataframe 으로 가져올 수 있는 상황이다. 모듈 작성 배경: 프로젝트 시범과제 중 키워드 매칭을 Pyhthon 으로 수행하기 위해 작성! 어떻게 작성할까?사실 막막했다. 아니, 대충대충 짜면 금방 짤 수 있는 부분이지만 Python 역량을 키워볼 겸 모듈화로 진행하기로 결정했다. 그러던 중, FlashText 알고리즘을 찾았고 이 패키지를 참고하고자 했다. 본 패키지는 string/list/dictionary/file 로 데이터를 읽어와 해당 키워드 사전을 만들고, 변환할 수 있는 기능을 제공한다. 하지만 내가 수행해야 하는 것은 Pandas Dataframe 형태의 데이터를 다뤄야 하기 때문에, FlashText KeywordProcessor 클래스를 차용하여 작성하였다. 관련 모듈 사용법은 Github에 올려두었다. 참고하길 바라며, 프로젝트 테스트를 위해 금융인지, 주택인지? 에 대한 키워드 매칭 결과를 확인해본다. 어떻게? FlashText 모듈 기반으로 작성! 테스트 결과는요?Github에 example.py 로 작성하였다. 규칙으로는 test 할 수 있는 document 등을 작성해야지만 그건 추후에 시간이 남는다면 진행하도록 한다. exmaple.py 는 다음과 같고 각 줄 마다 짧은 설명을 달도록 하겠다. 1234567891011121314151617181920212223242526from datetime import datetimefrom keywordmatch import MatchingProcessorimport pickleimport pandas as pdif __name__ == \"__main__\": with open(\"example.pickle\", \"rb\") as f: test_df = pickle.load(f) test_df_keyword = pd.DataFrame(&#123;'타입':['금융', '주택', '금융', '주택', '금융', '주택'], '키워드':['은행', '중랑구', '송금', '부산', '출금', '경남']&#125;) instance = MatchingProcessor(test_df, '기사내용', ['주택', '금융']) instance.set_logger('t1', is_file=False) instance.add_column() instance.get_keyword_processor(test_df_keyword, '타입', '키워드') result = instance.is_keyword() instance._data['수집시간'] = datetime(2020, 8, 19).strftime('%Y-%m-%d') tibero = &#123;'ip': '192.168.179.166', 'port': '8629', 'sid': 'tibero', 'id_pw': ['tibero', 'tmax'], 'output_columns': ['기사제목', '기사내용', '수집시간'], 'table': 'CRAWLER_DATA', 'table_columns': ['DETECTED_LINK', 'DETECTED_CONTENTS', 'DETECTED_TIME']&#125; instance.save_output_database(jar_file='/Users/jj/python/coding-test/tibero6-jdbc.jar', db_info=tibero) Line 1-4: 패키지 import 부분, 설치한 패키지는 Jaydebeapi, pandas Line 7-10: 첫 번째 테이블과 두 번째 테이블 데이터를 DataFrame 으로 로드 Line 12-16: MatchingProcessor 클래스로 생성한 객체로 테스트 Line 18: 테스트를 위한 TimeStamp 열 추가 Line 19-25: DB info 를 dictionary 형태로 표현 Line 26: 해당 DB 로 Insert 함수 실행 결과는 다음과 같다: 1234567891011121314~/python/keywordmatch masterkeyword-matching ❯ python example.py[2020-08-20 16:29:16,632][INFO] Finished Adding Columns: ['주택', '금융'][2020-08-20 16:29:16,636][INFO] Finished Setting Keyword_processor: &#123;'중랑구': '주택', '부산': '주택', '경남': '주택', '은행': '금융', '송금': '금융', '출금': '금융'&#125;[2020-08-20 16:29:16,636][INFO] Start Keyword Match.100%|███████████████████████████████████████████████████████████████████████████████████████| 8468/8468 [00:05&lt;00:00, 1530.62it/s][2020-08-20 16:29:22,200][INFO] Finished Keyword Match.[2020-08-20 16:29:23,781][INFO] Connected Tibero: 192.168.179.166:8629:tibero[2020-08-20 16:29:23,781][INFO] Started Creating SQL dump.100%|██████████████████████████████████████████████████████████████████████████████████████| 8468/8468 [00:00&lt;00:00, 50532.56it/s][2020-08-20 16:29:23,964][INFO] Finished Creating SQL dump. dump size: 8468[2020-08-20 16:29:23,964][INFO] Started pushing data. SQL Query: INSERT INTO CRAWLER_DATA VALUES (?,?,?)[2020-08-20 16:29:25,305][INFO] Finished pushing data.[2020-08-20 16:29:25,305][INFO] Disconnected Tibero. 어땠나요?키워드 매칭이 제대로 수행되었는지는 아무도 모른다. 나는 이번 프로젝트를 통해 Python 을 좀 더 살펴볼 수 있는 기회였고, 좀 더 다양한 모듈을 작성할 수 있는 역량을 확보한 것으로 만족한다. 물론 프로젝트에는 이 모듈을 들고 들어갈 생각이다. 왜? 결과가 제대로 나왔는지는 고객도 모르기 때문이다. 물론 좀 더 결과를 향상시킬 수 있는 방안을 생각하고 추가해보도록 노력하겠지만.. 10월 말 목표인 이 프로젝트에서(아 물론 한 달 정도 딜레이 될 예정) 튜닝은 필요할 지 모르겠다. 아, 추가적으로 필요한 부분은 아마 이러한 키워드 매칭을 배치성으로 수행할지는 직접 들어가봐야 알 것 같다. 하여튼, Python을 더 알아보고 싶은 마음이 생겼다. 결론: Python 을 알게되면서 점점 빠져들었다. made by jaejun.lee","categories":[{"name":"Python","slug":"Python","permalink":"https://jx2lee.github.io/categories/Python/"}],"tags":[]},{"title":"[SQL] SQL 함수 정리","slug":"sql-function","date":"2020-07-21T15:00:00.000Z","updated":"2020-08-04T04:58:14.240Z","comments":true,"path":"sql-function/","link":"","permalink":"https://jx2lee.github.io/sql-function/","excerpt":"프로젝트 참여 중 데이터 마트 구축을 위한 SQL 쿼리를 분석하면서, 관련된 지식을 내 머릿속에 쌓기(?)위해 정리한다. 필요 시 업데이트 할 예정","text":"프로젝트 참여 중 데이터 마트 구축을 위한 SQL 쿼리를 분석하면서, 관련된 지식을 내 머릿속에 쌓기(?)위해 정리한다. 필요 시 업데이트 할 예정 DECODE표준 SQL 함수는 아니지만 잘 사용하면 편한 오라클 함수, CASE WHEN 구문을 권장하기도 하지만 이왕 나왔으니 공부해보자.DECODE 는 Programming 에서의 if else 와 같은 역할을 한다. 사용방법은 다음과 같다. 12345DECODE(&#123;column&#125;, &#123;condition_01&#125;, &#123;return_01&#125;, &#123;condition_02&#125;, &#123;return_02&#125;, &#123;condition_03&#125;, &#123;return_03&#125;, ...)--exampleDECODE(WEB_CACHE_URL,NULL,A.DOMAIN_SEQ,100000001) WEB_CACHE_URL 칼럼이 만약 NULL 이라면 A 테이블 DOMAIN_SEQ 대입 만약 NULL이 아니면 100000001 대입 결론: WEB_CACHE_URL 칼럼이 만약 NULL 이라면 A 테이블 DOMAIN_SEQ 값을 대입하고 아니면 100000001 대입 CASE WHENDECODE 대신 CASE WHEN 을 권장한다니 알아보도록 한다. DECODE 랑 같이 if else 와 같은 역할을 하며 CASE WHEN 이 가독성이 더 좋다고들 한다. 사용방법은 다음과 같다. 123456789101112131415CASE WHEN &#123;condition_01&#125; THEN &#123;return_01&#125; WHEN &#123;condition_02&#125; THEN &#123;return_02&#125; ... ELSE &#123;return_other&#125;END--exampleCASE WHEN ( SELECT CODE_NM FROM COMTCCMMNDETAILCODE WHERE CODE = D.DOMAIN_CODE2) IS NOT NULL OR ( SELECT CODE_NM FROM COMTCCMMNDETAILCODE WHERE CODE = D.DOMAIN_CODE2) != '' THEN ( SELECT CODE_NM FROM COMTCCMMNDETAILCODE WHERE CODE = D.DOMAIN_CODE2) ELSE NULL END AS CODE_NM2 condition COMTCCMMNDETAILCODE 테이블에서 CODE가 D 테이블 DOMATIN_CODE2 와 같을때, CODE_NM 이 NULL 이 아니거나 (or) COMTCCMMNDETAILCODE 테이블에서 CODE가 D 테이블 DOMATIN_CODE2 와 같을때, CODE_NM 이 ‘’ 이 아니라면 return: COMTCCMMNDETAILCODE 테이블에서 CODE가 D 테이블 DOMATIN_CODE2 와 같을때 CODE_NM 대입 ELSE: NULL 대입 결론: CODE_NM 값이 있으면 CODE_NM, 없으면 NULL CAST칼럼 형변환에 사용하는 함수로 UNION(UNION ALL) 사용 시 칼럼 형태를 일치시킬 때 사용할 수 있다. 사용방법은 다음과 같다. 12345CAST(&#123;column&#125; as &#123;data_type&#123;length&#125;&#125;)--exmapleCAST(A.DETECTED_LINK AS NVARCHAR(4000)) AS DETECTED_LINK A 테이블 DETECTED_LINK 칼럼을 NVARCHAR length 4000 형태로 변환 INSTR문자열에 특정 문자열(substring)을 검색해서 위치를 return(만약 없다면 0) 하는 함수로 사용방법은 다음과 같다. 12345INSTR(&#123;string or column&#125; &#123;substring&#125;)--exampleINSTR(D.exposure_type, 'S01') D 테이블 EXPOSURE_TYPE 칼럼 내 S01 문자열 위치를 반환 Reference https://gent.tistory.com/227 https://gent.tistory.com/311 http://www.incodom.kr/SQL/INSTR%2CINSTRB made by jaejun.lee","categories":[{"name":"SQL","slug":"SQL","permalink":"https://jx2lee.github.io/categories/SQL/"}],"tags":[{"name":"Tibero","slug":"Tibero","permalink":"https://jx2lee.github.io/tags/Tibero/"}]},{"title":"[Cloud] Etcd 백업을 위한 CronJob 생성","slug":"cloud-etcd_cronjob","date":"2020-06-16T15:00:00.000Z","updated":"2020-09-26T13:42:38.732Z","comments":true,"path":"cloud-etcd_cronjob/","link":"","permalink":"https://jx2lee.github.io/cloud-etcd_cronjob/","excerpt":"최근 Control Plane 노드 복구 때문에 Etcd 백업에 대한 중요성을 깨달았다. 매 특정 시간 스냅샷을 찍어내는 CronJob 을 배포하는 과정을 다룬다.","text":"최근 Control Plane 노드 복구 때문에 Etcd 백업에 대한 중요성을 깨달았다. 매 특정 시간 스냅샷을 찍어내는 CronJob 을 배포하는 과정을 다룬다. 준비사항control plane 노드의 인증서 파일 /etc/kubernetes/pki/ca.crt /etc/kubernetes/pki/ca.key etcd 버전 확인 cluster 내 etcd 파드를 조회하여 버젼 체크12$ kubectl describe pod/etcd-k8s-master -n kube-system | grep Image: Image: k8s.gcr.io/etcd:3.3.10 setup.sh apply 할 yaml 파일 내 docker registry 와 node name 을 변경하는 스크립트 Usage123$ ./setup.shUsage : ./script.sh &#123;registry_endpoint&#125; &#123;master_node&#125;Example : ./script.sh 192.168.179.185:5000 k8s-master etcd_snapshot.yamlcontrol plane 인증서 파일을 volume 부분 /etc/kubernetes/pki/etcd 폴더에 복사etcd 백업 스냅샷의 저장 위치를 volume 부분 backup 으로 설정 스냅샷이 해당 디렉토리에 저장 원하는 스냅샷 저장 주기를 crontab 으로 표현 본인은 매일 오전 6시에 수행하는 0 6 * * * 으로 설정 1234567891011121314151617181920212223242526272829303132333435363738394041apiVersion: batch/v1beta1kind: CronJobmetadata: name: etcd-backup namespace: kube-systemspec: schedule: \"0 6 * * *\" jobTemplate: spec: template: spec: containers: - name: etcd-backup image: &#123;registry_endpoint&#125;/k8s.gcr.io/etcd:3.3.10 env: - name: ETCDCTL_API value: \"3\" command: [\"/bin/sh\"] args: [\"-c\", \"etcdctl --endpoints=https://127.0.0.1:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/healthcheck-client.crt --key=/etc/kubernetes/pki/etcd/healthcheck-client.key snapshot save /backup/etcd-snapshot-$(date +%Y-%m-%d_%H-%M-%S_%Z).db\"] volumeMounts: - mountPath: /etc/kubernetes/pki/etcd name: etcd-certs readOnly: true - mountPath: /backup name: backup restartPolicy: OnFailure nodeSelector: kubernetes.io/hostname: &#123;master_node&#125; tolerations: - effect: NoSchedule operator: Exists hostNetwork: true volumes: - name: etcd-certs hostPath: path: /etc/kubernetes/pki/etcd type: DirectoryOrCreate - name: backup hostPath: path: /etc/kubernetes/pki/etcd/snapshot type: DirectoryOrCreate script 실행으로 etcd_snapshot.yaml 내 node name 과 private registry 주소 변경1$ ./setup.sh 192.168.179.185:5000 k8s-master 배포$ kubectl apply -f etcd_snapshot.yaml 배포 확인123456789$ kubectl get cronjob -n kube-systemNAME SCHEDULE SUSPEND ACTIVE LAST SCHEDULE AGEetcd-backup 0 6 * * * False 0 &lt;none&gt; 14m$ ll /etc/kubernetes/pki/etcd/snapshotdrwxr-xr-x 2 root root 4096 6월 18 15:00 ./drwxr-xr-x 3 root root 4096 6월 17 15:00 ../-rw-r--r-- 1 root root 41095200 6월 17 15:00 etcd-snapshot-2020-06-17_06-00-04_UTC.db-rw-r--r-- 1 root root 41095200 6월 18 15:00 etcd-snapshot-2020-06-18_06-00-08_UTC.db hostPath 로 설정한 /etc/kubernetes/pki/etcd/snapshot 내 snapshot.db 확인 소스코드는 https://github.com/jx2lee/etcd-cronjob 에서 확인할 수 있다. Refernce https://labs.consol.de/kubernetes/2018/05/25/kubeadm-backup.html made by jaejun.lee","categories":[{"name":"Cloud","slug":"Cloud","permalink":"https://jx2lee.github.io/categories/Cloud/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://jx2lee.github.io/tags/Kubernetes/"}]},{"title":"[TroubleShoot] Control Plane Node 추가","slug":"troubleshoot-add_controlplane_node","date":"2020-06-15T15:00:00.000Z","updated":"2020-09-26T13:46:18.422Z","comments":true,"path":"troubleshoot-add_controlplane_node/","link":"","permalink":"https://jx2lee.github.io/troubleshoot-add_controlplane_node/","excerpt":"클러스터 운영 시 컨트롤 플레인 노드가 삭제되었을 때 클러스터에 재 추가하는 과정을 살펴본다.","text":"클러스터 운영 시 컨트롤 플레인 노드가 삭제되었을 때 클러스터에 재 추가하는 과정을 살펴본다. Kubernetes 컨트롤 플레인 노드 추가 방법기존 클러스터 노드 내 /etc/kubernetes/pki 폴더를 추가 노드의 /etc/kubernetes에 복사기존 클러스터 노드 내 /var/lib/etcd/member 폴더를 추가 노드 내 특정 디렉토리 (나의 경우 init 폴더) 로 복사기존 클러스터 노드 내 etcd 파드에 접근하여 다음 명령어로 snap db 생성123ETCDCTL_API=3 etcdctl --endpoints 127.0.0.1:2379 \\--cacert /etc/kubernetes/pki/etcd/ca.crt --cert /etc/kubernetes/pki/etcd/server.crt \\--key /etc/kubernetes/pki/etcd/server.key snapshot save snap 생성한 snap db 를 추가하고자 하는 노드의 특정 디렉토리 (나의 경우 init 폴더) 로 이동초기 클러스터 구축 시 사용한 kubeadm-config.yaml 준비12345678910apiVersion: kubeadm.k8s.io/v1beta2kind: ClusterConfigurationkubernetesVersion: \"v1.15.3\"controlPlaneEndpoint: \"192.168.179.185:6443\" # VIP:6443networking: serviceSubnet: \"10.96.0.0/16\" podSubnet: \"10.244.0.0/16\" # must equal CIDR in calico.yamlapiServer: extraArgs: advertise-address: \"192.168.179.185\" # VIP 새로 추가할 노드(k8s-node5 라 가정)에 다음 명령어를 통해 etcd 데이터 복구123456docker run --rm \\ -v '/root/init:/backup' \\ -v '/var/lib/etcd:/var/lib/etcd' \\ --env ETCDCTL_API=3 \\ 'k8s.gcr.io/etcd:3.3.10' \\ /bin/sh -c \"etcdctl snapshot restore '/backup/snap.db' ; mv /default.etcd/member/ /var/lib/etcd/\" 새로 추가할 노드 (k8s-node5) 에 kubeadm init 수행12kubeadm init --config=kubeadm-config.yaml\\--ignore-preflight-errors=DirAvailable--var-lib-etcd Reference https://codefarm.me/2019/05/22/kubernetes-recovery-master-failure/ made by jaejun.lee","categories":[{"name":"TroubleShoot","slug":"TroubleShoot","permalink":"https://jx2lee.github.io/categories/TroubleShoot/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://jx2lee.github.io/tags/Kubernetes/"}]},{"title":"[Etc] DevOps 인터뷰 기술질문 정리","slug":"etc-devops_interview_questions","date":"2020-06-10T15:00:00.000Z","updated":"2020-09-26T13:49:36.378Z","comments":true,"path":"etc-devops_interview_questions/","link":"","permalink":"https://jx2lee.github.io/etc-devops_interview_questions/","excerpt":"DevOps 직무 실무자 면접에서 받은 인터뷰 내용을 정리하였다. 기술적인 내용만 정리하였고 이외에는 이직 사유에 대해 많이 궁금해 하였다. 나에게 이직사유 보단 기술을 알고 넓히는게 중요할 것 같아.. 추가적으로 면접을 진행하면 앞으로 계속 추가할 예정이며 질문은 기술을 기준으로 나누었다.","text":"DevOps 직무 실무자 면접에서 받은 인터뷰 내용을 정리하였다. 기술적인 내용만 정리하였고 이외에는 이직 사유에 대해 많이 궁금해 하였다. 나에게 이직사유 보단 기술을 알고 넓히는게 중요할 것 같아.. 추가적으로 면접을 진행하면 앞으로 계속 추가할 예정이며 질문은 기술을 기준으로 나누었다. 구분은 다음과 같다.&#x2728; Contents: Kubernetes Python Database Linux Network Hadoop Kubernetes VM vs container:두 개 모두 가상화 기술로 큰 차이는 HyperVisor 유무이다. VM의 경우 운영체제에서 프로세스가 시작하는 반면 컨테이너는 호스트 운영체제의 내부에서 실행되어 좀 더 가볍고 MSA 구현이 가능하다. 또한, 운영체제 커널의 공유함으로써 빠르며 메모리 사용량이 적다. Docker Swarm:컨테이너 오케스트레이션 도구 중 하나로 Docker 호스트들을 하나인 것처럼 만들어주는 도구. Master/Worker 노드로 시스템을 구성한다.:https://medium.com/@chrisjune_13837/infra-docker-swarm이란-595d33160379 proxy (k8s):Pod 로 연결되는 네트워크를 관리하며 파드 간 통신을 위해 라우팅을 돕는다. 파드 간 통신을 관리하기 때문에 파드 네트워크를 관리하는 서비스와의 통신을 원활히 해준다. istio:Data Plane(프록시들로 이루어져 트래픽을 설정값에 따라 컨트롤 하는 부분)*의 메인 프록시로 Envoy Proxy를 사용하며 이를 컨트롤 해주는 Control Plane(프록시들에 설정값을 전달하고 관리하는 컨트롤러)*의 오픈소스 솔루션. 이 부분에 대해서는 아래 포스트가 잘 정리되어 있다.:https://gruuuuu.github.io/cloud/service-mesh-istio/# readiness/liveness probe:컨테이너가 살아있는지 확인하는 health check 방법인 liveness probe, 컨테이너가 서비스 가능한 상태인지 확인하는 health check 방법인 readiness probe:https://bcho.tistory.com/m/1264 Python global interpreter lock:특정 시점에서의 하나의 쓰레드만 실행하도록 만드는 것:https://m.blog.naver.com/alice_k106/221566619995 async:하나의 쓰레드로 동시 처리를 할 수 있는 비동기 프로그래밍을 위한 파이썬 패키지:https://www.daleseo.com/python-asyncio/ garbage collection:파이썬은 보통 garbage collection 과 reference counting 을 통해 할당된 메모리를 관리한다. 기본적으로 참조 횟수가 0이 된 객체를 해제하는 reference counting 방식을 사용하지만, reference cycles (순환참조) 가 발생하면 garbage collection 으로 이를 해결한다.:https://winterj.me/python-gc/ decorator:대상 함수를 wrapping 하고 이 wrapping 된 함수의 앞뒤에 추가적으로 꾸며질 구문들을 정의해 손쉽게 재사용 가능하게 해주는 기능으로 함수를 꾸며주는 함수 라고 표현할 수 있다.:https://bluese05.tistory.com/30:https://nachwon.github.io/decorator/ Database Tibero vs MySQL:사실 이 질문에 답변은 오픈소스 vs 상용이라고 말했다.. 구글링을 하면 비교자료가 나오긴 하지만 어떻게 작성해야될 지 몰라 링크만 남겨두고 면접전에 보고 들어가자!:https://db-engines.com/en/system/MySQL%3BTibero Dead Lock:데이터 일관성을 보장하기 위한 방법 중 하나로 트랜잭션 간 교착상태를 의미한다. 두 개의 트랜잭션 간 각각의 트랜잭션이 가지고 있는 리소스의 Lock 을 획득하려고 할 때 발생한다. (예를들어, A-&gt;D1 트랜잭션 발생 / B-&gt;D2 트랜잭션 발생 이후 A가 D2에 커밋을 하게 되면 Dead Lock 발생):https://medium.com/@chrisjune_13837/db-lock-락이란-무엇인가-d908296d0279 Linux fstrim:디스크 IO(주로 SSD) 성능 저하를 피하기 위해 사용하는 리눅스 명령어 /dev 디렉토리:장치 파일을 위한 디렉토리로 Node(노드) 라고 불리는 요소를 포함하며, 각 노드는 시스템의 한 장치를 나타낸다. /dev/null 은 가상장치로써 프로그램의 출력을 무시하여 화면 상 텍스트를 표시하지 않을 때 유용하다. /dev/0(zero) 는 write 수행 시 성공적인 리턴 코드를 제공하며 특정 크기의 파일을 생성하거나 저장 장치를 포맷하기 위해 주로 사용한다. LAID (리눅스 디스크):여러 개 HDD 를 하나의 HDD 로 사용하는 방식:https://infrajp.tistory.com/9 pipe / redirect:pipe) 프로세스 혹은 실행된 프로그램의 결과를 다른 프로그램으로 전달하거나 남길 때 사용:redirect) 프로그램의 결과 혹은 출력을 파일이나 다른 스트림으로 전달하거나 남길 때 사용 0: standard input (stdin) 1: standard output (stdout) 2: standard error (stderr) kernel parameter 변경 이유 (사용 이유):kernel parameter 란 커널(시스템을 관리하는 거대한 어플리케이션)이 메모리와 프로세스를 할당하기 위한 값으로, /proc/sys (OS마다 상이할 수 있음) 디렉토리에 존재한다. Network DNS(Domain Name Server):TCP/IP 네트워크에서 사람이 기억하기 쉬운 문자로 만들어진 도메인을, 컴퓨터가 처리할 수 있는 인터넷 주소(IP)로 바꾸는 시스템인 Domain Name System 또는 이런 역할을 수행하는 Domain Name Server 를 의미:https://samsikworld.tistory.com/489 CNAME, A record:CNAME) Canonical Name 으로 하나의 도메인에 다른 이름을 부여하는 방식:A record) 도메인 이름에 하나의 IP address 가 있음을 의미:https://twpower.github.io/40-difference-between-cname-and-a-record TCP / UDP:TCP/IP의 전송계층에 사용하는 프로토콜로, 전송계층이란 IP에 의해 전달하는 패킷의 오류를 검사하고 재전송 요구 등의 제어를 담당한다.:TCP(Transmission Control Protocol) 신뢰성을 요구하는 애플리케이션에 사용(양방향 전송):UDP(User Datagram Protocol) 간단한 데이터를 빠른 속도로 전송하고자 하는 애플리케이션에 사용 Hadoop / Spark broadcasting join vs shuffle join:broadcasting join) 데이터를 executor 로 복사하지만 executor 간 데이터 복사가 없어 속도가 빨라질 수 있다.:shuffle join 은 조인된 데이터로 동일한 executor 로 이동하기 위해 셔플 연산을 사용하여 데이터 이동이 많이 발생한다.:https://knight76.tistory.com/entry/spark-스파크-조인-전략-셔플-조인-브로캐스트-조인-shuffle-join-broadcast-join made by jaejun.lee","categories":[{"name":"Etc","slug":"Etc","permalink":"https://jx2lee.github.io/categories/Etc/"}],"tags":[{"name":"Review","slug":"Review","permalink":"https://jx2lee.github.io/tags/Review/"}]},{"title":"[Cloud] 스토리지 클래스를 이용한 파드 배포 시 transport endpoint is not connected","slug":"cloud-mount_failed","date":"2020-06-07T15:00:00.000Z","updated":"2020-10-29T14:38:30.347Z","comments":true,"path":"cloud-mount_failed/","link":"","permalink":"https://jx2lee.github.io/cloud-mount_failed/","excerpt":"kubernetes 클러스터 운영 중 control plane 노드 장애가 발생해 여러 삽질을 하던 중.. 기존 운영중이던 Pod 가 떠 있지를 못해 확인(describe)해보니 transport endpoint is not connected 문구와 함께 Mount failed 하였다. 이 문제를 해결하는 과정을 다룬다.","text":"kubernetes 클러스터 운영 중 control plane 노드 장애가 발생해 여러 삽질을 하던 중.. 기존 운영중이던 Pod 가 떠 있지를 못해 확인(describe)해보니 transport endpoint is not connected 문구와 함께 Mount failed 하였다. 이 문제를 해결하는 과정을 다룬다. Describe Pod장애가 발생한 파드를 Describe 한 결과이다.총 3 Waring-FailedMount 에러가 발생하였는데, Rook-ceph 의 cephfs 스토리지 클래스를 이용해 공유볼륨(rserver-share)/Home디렉토리(rserver-home)/토큰(default-token-4w7gr) 저장소를 생성하고자 했다. 12345Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedMount 13m (x143 over 5h2m) kubelet, k8s-node1 MountVolume.SetUp failed for volume \"pvc-3a160803-5d2e-4a45-9c9f-b78a0a2e7339\" : stat /var/lib/kubelet/pods/eea991e6-9908-4d41-bd01-141af0566079/volumes/kubernetes.io~csi/pvc-3a160803-5d2e-4a45-9c9f-b78a0a2e7339/mount: transport endpoint is not connected Warning FailedMount 3m1s (x125 over 4h44m) kubelet, k8s-node1 Unable to mount volumes for pod \"rserver-3.6.3-deployment-776f6b8ddc-gwbk5_nps(eea991e6-9908-4d41-bd01-141af0566079)\": timeout expired waiting for volumes to attach or mount for pod \"nps\"/\"rserver-3.6.3-deployment-776f6b8ddc-gwbk5\". list of unmounted volumes=[rserver-home]. list of unattached volumes=[rserver-share rserver-home default-token-6w7gr] Pod log 를 확인하고 싶었지만 애초에 뜨기도 전에 Mount Failed 되었기에 해당 노드로 접속하여 Kubelet 로그를 확인하였다. Kubelet log해당 노드에 journal -f | grep kubelet 으로 로그를 살펴보니 Describe 와 다른 점이 없어보이지만, 각 Pod 의 볼륨의 Path (메세지에는 stat / 으로 표현) 를 출력해준다. 123Jun 08 13:49:57 k8s-node1 kubelet[4638]: E0608 13:49:57.738465 4638 nestedpendingoperations.go:270] Operation for \"\\\"kubernetes.io/csi/rook-ceph.cephfs.csi.ceph.com^0001-0009-rook-ceph-0000000000000001-d90262eb-a499-11ea-a466-52c8f6e260ed\\\"\" failed. No retries permitted until 2020-06-08 13:51:59.738429723 +0000 UTC m=+21911.512876014 (durationBeforeRetry 2m2s). Error: \"MountVolume.SetUp failed for volume \\\"pvc-3a160803-5d2e-4a45-9c9f-b78a0a2e7339\\\" (UniqueName: \\\"kubernetes.io/csi/rook-ceph.cephfs.csi.ceph.com^0001-0009-rook-ceph-0000000000000001-d90262eb-a499-11ea-a466-52c8f6e260ed\\\") pod \\\"rserver-3.6.3-deployment-776f6b8ddc-gwbk5\\\" (UID: \\\"eea991e6-9908-4d41-bd01-141af0566079\\\") : stat /var/lib/kubelet/pods/eea991e6-9908-4d41-bd01-141af0566079/volumes/kubernetes.io~csi/pvc-3a160803-5d2e-4a45-9c9f-b78a0a2e7339/mount: transport endpoint is not connected\"Jun 08 13:50:01 k8s-node1 kubelet[4638]: E0608 13:50:01.755308 4638 kubelet.go:1665] Unable to mount volumes for pod \"rserver-3.6.3-deployment-776f6b8ddc-gwbk5_nps(eea991e6-9908-4d41-bd01-141af0566079)\": timeout expired waiting for volumes to attach or mount for pod \"nps\"/\"rserver-3.6.3-deployment-776f6b8ddc-gwbk5\". list of unmounted volumes=[rserver-home]. list of unattached volumes=[rserver-share rserver-home default-token-6w7gr]; skipping podJun 08 13:50:01 k8s-node1 kubelet[4638]: E0608 13:50:01.755344 4638 pod_workers.go:190] Error syncing pod eea991e6-9908-4d41-bd01-141af0566079 (\"rserver-3.6.3-deployment-776f6b8ddc-gwbk5_nps(eea991e6-9908-4d41-bd01-141af0566079)\"), skipping: timeout expired waiting for volumes to attach or mount for pod \"nps\"/\"rserver-3.6.3-deployment-776f6b8ddc-gwbk5\". list of unmounted volumes=[rserver-home]. list of unattached volumes=[rserver-share rserver-home default-token-6w7gr] 1stat /var/lib/kubelet/pods/eea991e6-9908-4d41-bd01-141af0566079/volumes/kubernetes.io~csi/pvc-3a160803-5d2e-4a45-9c9f-b78a0a2e7339/mount 이 부분을 주목해서 보자. 해당 Path 으로 접근하여 mount 폴더를 확인하면 다음과 같다. 123456789$ pwd/var/lib/kubelet/pods/eea991e6-9908-4d41-bd01-141af0566079/volumes/kubernetes.io~csi/pvc-3a160803-5d2e-4a45-9c9f-b78a0a2e7339$ llls: cannot access 'mount': Transport endpoint is not connectedtotal 12drwxr-x--- 3 root root 4096 Jun 2 06:27 ./drwxr-x--- 4 root root 4096 Jun 2 06:27 ../d????????? ? ? ? ? ? mount/-rw-r--r-- 1 root root 328 Jun 8 13:54 vol_data.json mount 폴더의 표현이 이상한 것을 확인할 수 있다. 분명 폴더에 대한 읽기쓰기실행 권한과 소유자/그룹이 나와야 하지만 모두 물음표로 출력한다. 이에 대한 구글링을 해본 결과, 해당 디렉토리의 mount 가 비정상 작동하여 이를 해제해야 한다고 한다. 정확한 원인은 모르겠지만, 해당 폴더에 대한 잘못된 mount 로 인해 PVC 가 PV 를 제대로 바라보지 못하여 Mount Failed 난 것으로 이해했다. 해당 디렉토리를 umount 를 이용해 마운트를 해제하고, Kubelet 로그를 더 확인하여 다른 PV stat 으로 접근해 마운트 해제를 수행한다. 보아하니 한 번의 umount 로 끝나지 않아 나의 경우,/var/lib/kubelet/pods/{위UUID}/volumes/kubernetes.io~csi/ 내 모든 PVC 로 접근하여 폴더가 이상하면 마운트를 해제 하였다. 무식할 수 있지만.. 어쨌든 모두 수행하여 Kubelet 로그에 이상이 없는 것을 확인한다. 정상 작동 확인kubectl get all -n nps 명령어로 기존에 떠있지 못한 파드가 정상 기동하였는지 확인한다. 1234567root@k8s-node2:~# kubectl get all -n nps -o wideNAME READY STATUS RESTARTS AGEpod/jupyter-3.7-deployment-f798bb7f4-n2kqq 1/1 Running 0 5h21mpod/jupyter-3.8-deployment-d47c78475-9gsb8 0/1 ContainerCreating 0 4d12hpod/pypiserver-deployment-7bbbb48f8c-rkbhk 1/1 Running 0 6d3hpod/rserver-3.5.3-deployment-96f5d6b4-dhgkg 1/1 Running 0 6d7hpod/rserver-3.6.3-deployment-776f6b8ddc-gwbk5 1/1 Running 0 6d7h 3.8 jupyter 의 경우 해당 파드는 노드가 달라.. 귀찮아서 아직 고치지 않은 모습이다. 이를 고치려면 같은 방법으로 해당 노드로 접근하여 해결해야 할 것 같다. 결론이번 장애를 겪으면서 얻은 교훈으로는,Pod Describe 도 좋지만 장애가 나는 리소스의 노드로 접근하여 Kubelet 로그도 함께 보는 습관을 가져야 겠다. 물론 Describe로도 충분히 알아낼 수 있는 정보지만, 노드들 간 통신을 담당하는 Kubelet 로그를 통해 좀 더 세부적인 Info 와 Warning 을 확인하여 일석이조의 효과를 누릴 수 있다 생각한다. 엔지니어의 삶은.. 로그라는 선배의 말이 생각난다. made by jaejun.lee","categories":[{"name":"Cloud","slug":"Cloud","permalink":"https://jx2lee.github.io/categories/Cloud/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://jx2lee.github.io/tags/Kubernetes/"}]},{"title":"[Python] Jupyter Notebook 로그 파일 생성","slug":"python-jupyter_logging","date":"2020-05-27T15:00:00.000Z","updated":"2020-09-26T13:43:50.676Z","comments":true,"path":"python-jupyter_logging/","link":"","permalink":"https://jx2lee.github.io/python-jupyter_logging/","excerpt":"Jupyter Notebook 실행 이력을 로그파일로 생성하는 과정을 다룬다.","text":"Jupyter Notebook 실행 이력을 로그파일로 생성하는 과정을 다룬다. &#x2728; Contents: Environment log_history.py Customize IPython/core/history.py Result Reference EnvironmentKubernetes 환경에서 Jupyter Notebook 도커 이미지를 이용해 deployment 로 관리하고 서비스는 노드포트로 구성하여 특정 포트로 Jupyter Notebook 에 접근할 수 있게 설정하였다. Python 버젼은 3.8에서 진행하였으며, Docker image는 3.8, 3.7 등 소수 첫 째 자리까지의 버젼만 빌드할 수 있게 작성되었다. 환경 구성은 다음과 같다. 123456789101112$ kg all -n jupyter-loggingNAME READY STATUS RESTARTS AGEpod/jupyter-3.8-deployment-6676cc56d6-xth7b 1/1 Running 0 13hNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEservice/jupyter-38-svc NodePort 10.96.187.185 &lt;none&gt; 8888:30180/TCP 24dNAME READY UP-TO-DATE AVAILABLE AGEdeployment.apps/jupyter-3.8-deployment 1/1 1 1 13hNAME DESIRED CURRENT READY AGEreplicaset.apps/jupyter-3.8-deployment-6676cc56d6 1 1 1 13h Dockerfile 를 통한 빌드 환경은 다음과 같다. 12345678910111213$ lltotal 104drwxr-xr-x 2 root root 4096 May 28 09:05 ./drwxr-xr-x 6 root root 4096 May 26 09:56 ../-rw-r--r-- 1 root root 71 May 26 01:52 create_pwd_hash.py-rw-r--r-- 1 root root 8160 May 26 08:10 Dockerfile-rw-r--r-- 1 root root 965 May 26 01:52 fix-permissions-rw-r--r-- 1 root root 33023 May 28 09:05 history.py-rw-r--r-- 1 root root 1877 May 26 09:54 jupyter_notebook_config.py-rw-r--r-- 1 root root 949 May 28 09:04 log_history.py-rwxr-xr-x 1 root root 524 May 26 01:52 start-notebook.sh*-rwxr-xr-x 1 root root 6302 May 26 01:52 start.sh*-rwxr-xr-x 1 root root 1181 May 26 01:52 start-singleuser.sh* 파일 목록 중 새로 추가한 것은 log_history.py와 history.py이다. 추후에 Dockerfile 커스텀화 한 결과를 추가할 예정이다. log_history.py생소할 것이다. 맞다. 왜냐하면 파일 네이밍은 내가 생각한 것이기 때문이다. 고객사 요건은 간단하다. Jupyter Notebook 을 이용해 분석가가 코드를 실행하였을 때, 코드 이력을 남기는 log가 필요했다. 물론 python 로그의 경우 python 을 실행한 디렉토리에서 .python_history 가 작성되지만, 이놈의 Jupyter Notebook 은 어디에다 로그를 남기는지 파악이 어려웠다. But, Jupyter Notebook 과 IPython 의 관계를 파악하면 쉽게 풀린다. 구글링 하면 쉽게 차이점을 살필 수 있겠지만, 간단히 설명하면 차이가 있는 것 보다 포함관계를 가진다. 즉, Jupyter Notebook 은 다양한 언어를 지원하는데 그 중 IPython 이 파이썬 언어를 이용할 수 있게 도와준다. 다시 말해 Jupyter Notebook은 IPython 을 포함한다. 서론이 길었다. 결국 Jupyter POD 를 기동하면 $HOME 디렉토리에 .ipython 폴더가 생성된다. 구조를 살피면 다음과 같다. 12345678910111213root@jupyter-3:~/.ipython# tree ..├── extensions├── nbextensions└── profile_default ├── db ├── history.sqlite ├── jupyter.log ├── log ├── pid ├── security └── startup └── README 우리가 주의있게 볼 디렉토리는 startup 폴더다. 이름에서 보이듯, startup 폴더의 README 를 열어보면 쉽게 파악할 수 있다. Notebook 을 실행하기 전 환경 구성을 도와주는 공간이다. 이 폴더에 다음과 같이 log_history.py 를 작성한다. 1234567891011121314151617181920212223242526import atexitimport osimport datetimeip = get_ipython()LIMIT = 100000 # limit the size of the historydef save_history(): \"\"\"save the IPython history to a plaintext file\"\"\" histfile = os.path.join(ip.profile_dir.location, \"jupyter.log\") print(\"[INFO] Saving plaintext history to %s\" % histfile) lines = [] lines.extend('[&#123;0&#125;]\\n&#123;1&#125;'.format(record[2][1], record[2][0]) + '\\n' for record in ip.history_manager.get_range()) # get previous lines # this is only necessary because we truncate the history, # otherwise we chould just open with mode='a' if os.path.exists(histfile): with open(histfile, 'a') as f: #lines = f.readlines() f.writelines(lines) else: with open(histfile, 'w') as f: f.writelines(lines)# do the save at exitatexit.register(save_history) 코드를 간단히 설명하면 log 파일이 있다면 파일을 수정모드로 열어 Notebook 안에서 실행한 Python 실행 이력을 추가하고, 없으면 새로 만들어 처음으로 생성한 로그를 추가한다. 여기서 주의깊게 볼 곳은 history_manager 오브젝트이다. Jupyter Notebook 의 History 를 관리하는 클래스로 다음 챕터에서 간단히 설명하고 수정한 부분을 설명하겠다. Customize IPython/core/history.pypython 설치 디렉토리 내 site-package 에는 IPython 이 존재할 것이다. $PY_PKG_DIR/IPython/core 에 history.py 를 수정할 것이다. 수정이 왜 필요한가? 기본적으로 Jupyter Notebook 은 sqlite3 DB에 history 를 남기지만 Timestamp 는 없다. 이를 위해 수정하는 것이기 때문에 history.py 를 열어 def store_inputs 함수를 찾아본다. 그 다음은 쉽다, 한 줄만 추가하면 된다. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253def store_inputs(self, line_num, source, source_raw=None): \"\"\"Store source and raw input in history and create input cache variables ``_i*``. Parameters ---------- line_num : int The prompt number of this input. source : str Python input. source_raw : str, optional If given, this is the raw input without any IPython transformations applied to it. If not given, ``source`` is used. \"\"\" if source_raw is None: source_raw = source source = source.rstrip('\\n') source_raw = source_raw.rstrip('\\n') # do not store exit/quit commands if self._exit_re.match(source_raw.strip()): return self.input_hist_parsed.append(source) ############# ###append self.input_hist_raw.append([source_raw, datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')]) ###append-end ############# with self.db_input_cache_lock: self.db_input_cache.append((line_num, source, source_raw)) # Trigger to flush cache and write to DB. if len(self.db_input_cache) &gt;= self.db_cache_size: self.save_flag.set() # update the auto _i variables self._iii = self._ii self._ii = self._i self._i = self._i00 self._i00 = source_raw # hackish access to user namespace to create _i1,_i2... dynamically new_i = '_i%s' % line_num to_main = &#123;'_i': self._i, '_ii': self._ii, '_iii': self._iii, new_i : self._i00 &#125; if self.shell is not None: self.shell.push(to_main, interactive=False) 이 함수를 간단히 설명하면, Jupyter Notebook 에서 발생한 python history input 을 sqlite3 DB 에 저장하는 함수다. 추가 내용은 다음과 같다.self.input_hist_raw.append([source_raw, datetime.datetime.now().strftime(&#39;%Y-%m-%d %H:%M:%S&#39;)]) 이 함수를 추적한 과정은 history_manager.get_range() 가 generator 형태로 반환하기 때문에 yield 검색을 해보니 담당 함수는 _get_range_session 임을 확인했다. 첫 줄 input_hist_raw 가 history 의 raw data 를 포함하는 것을 확인하고 input_hist_raw 검색하여 store_inputs 함수를 trace 하였다. Result간단한 Notebook 을 생성하고 log 파일을 확인해본다. log 경로는 $HOME/.ipython/profile-default 경로이다. (log_history.py 에서 수정 가능) 1234567891011121314151617181920212223242526272829303132$ cat jupyter.log[2020-05-28 08:43:46]print(1)[2020-05-28 08:43:47]def sum(a, b): # 3과 5가 각각 매개변수 a와 b에 할당된다. result = a + b # 변수 result에는 매개변수 a와 b의 합이 할당되므로 이 경우에는 3과 5의 합인 8이 할당된다. return result # 8을 함수 바깥으로 반환한다.[2020-05-28 08:43:47]sum(10,20)[2020-05-28 08:43:47]print(2)[2020-05-28 08:43:47]def say_hello(func): # 1 def wrapper2(*args, **kwargs): # 8 print('Hello') # 11 return func(*args, **kwargs) # 12 return wrapper2 # 9def say_hi(func): # 2 def wrapper1(*args, **kwargs): # 5 print('Hi') # 13 return func(*args, **kwargs) # 14 return wrapper1 # 6@say_hello # 7@say_hi # 4def introduce(name): # 3 print(f'My name is &#123;name&#125;!') # 15introduce('Jaejun') # 10 추가적으로 고려할 사항이 있다. log 파일 관리인데 최대 크기를 지정해서 용량이 커지면 이전 History 를 삭제하는 방안을 마련하고 있다. 조만간 Dockerfile 도 완성되면 추가할 예정이다. Reference https://anaconda.org/conda-forge/python-json-logger https://velog.io/@log327 https://stackoverflow.com/questions/16858724/how-to-log-ipython-history-to-text-file made by jaejun.lee","categories":[{"name":"Python","slug":"Python","permalink":"https://jx2lee.github.io/categories/Python/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://jx2lee.github.io/tags/Docker/"}]},{"title":"[Hadoop] CDH(ClouDera Hadoop) Client 설정","slug":"hadoop-client","date":"2020-05-14T15:00:00.000Z","updated":"2020-09-26T13:43:28.431Z","comments":true,"path":"hadoop-client/","link":"","permalink":"https://jx2lee.github.io/hadoop-client/","excerpt":"Python 을 이용해 CDH 연동하는 과정에서 Hadoop Client 설정 방법에 대해 정리한다.","text":"Python 을 이용해 CDH 연동하는 과정에서 Hadoop Client 설정 방법에 대해 정리한다. &#x2728; Contents: Bianry Download CDH Hadoop Config Client 압축 해제 Config 적용 /etc/hosts 적용 ENV Bianry Download wget 을 이용해 CDH hadoop 3 버젼을 다운 $ wget https://archive.apache.org/dist/hadoop/core/hadoop-3.0.0/hadoop-3.0.0.tar.gz CDH Hadoop Config CDH 웹에서 hadoop config 을 다운로드 core-site.xml hdfs-site.xml yarn-site.xml (Yarn 구성 시) Client 압축 해제 특정 directory 에 압축 해제 $ tar -xvf hadoop-3.0.0.tar.gz &amp;&amp; mv hadoop-3.0.0 hadoop Config 적용 $HADOOP_HOME/etc/hadoop 에 위에 준비한 CDH config 을 copy &amp; paste $ cp *-site.xml $HADOOP_HOME/etc/hadoop /etc/hosts 적용 Client 서버의 IP hostname 에 마찬가지로 클러스터 노드 정보를 작성 example123456#/etc/hosts......192.168.179.181 chd1192.168.179.182 chd2192.168.179.183 chd3 $ touch /etc/hosts 로 파일 적용 ENV1234export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64export HADOOP_HOME=/home/jovyan/hadoopexport HADOOP_CMD=/home/jovyan/hadoop/bin/hadoopexport PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$JAVA_HOME/bin:$HADOOP_CMD made by jaejun.lee","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://jx2lee.github.io/categories/Hadoop/"}],"tags":[]},{"title":"[Python] Jupyter Notebook 을 이용한 Hadoop 및 Database 연동","slug":"python-connection_test","date":"2020-05-14T15:00:00.000Z","updated":"2020-09-26T13:49:34.429Z","comments":true,"path":"python-connection_test/","link":"","permalink":"https://jx2lee.github.io/python-connection_test/","excerpt":"Python 을 이용한 Hadoop 및 Database 연동 테스트를 진행하였다.","text":"Python 을 이용한 Hadoop 및 Database 연동 테스트를 진행하였다. 환경구성: Hadoop Cluster (CDH 6.3.2) cdh1: 192.168.179.181 cdh2: 192.168.179.182 cdh3: 192.168.179.183 (master) Jupyter Notebook docker custom image (python version: 3.7) kubernetes deployment 배포 namespace: nps 컨테이너 환경 : Ubuntu 18.04.4 LTS &#x2728; Contents: HDFS Hive HBase Impala Spark Sybase Oracle HDFS설치 패키지LinuxPython hdfs Requirement already satisfied: docopt in /opt/conda/lib/python3.8/site-packages (from hdfs) (0.6.2) Requirement already satisfied: six&gt;=1.9.0 in /opt/conda/lib/python3.8/site-packages (from hdfs) (1.14.0) Requirement already satisfied: requests&gt;=2.7.0 in /opt/conda/lib/python3.8/site-packages (from hdfs) (2.23.0) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /opt/conda/lib/python3.8/site-packages (from - requests&gt;=2.7.0-&gt;hdfs) (1.25.9) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /opt/conda/lib/python3.8/site-packages (from requests&gt;=2.7.0-&gt;hdfs) (3.0.4) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /opt/conda/lib/python3.8/site-packages (from requests&gt;=2.7.0-&gt;hdfs) (2.9) Requirement already satisfied: certifi&gt;=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests&gt;=2.7.0-&gt;hdfs) (2020.4.5.1) Code123456789101112131415161718192021222324252627282930313233343536373839import pandas as pdfrom hdfs import InsecureClient# Connectionclient_hdfs = InsecureClient('http://192.168.179.183:9870', user='hdfs')## Createcreate_df = pd.DataFrame([1000, 2000, 3000, 4000])with client_hdfs.write('/tmp/t2.csv', encoding = 'utf-8') as writer: create_df.to_csv(writer)print(client_hdfs.list('/tmp'))## Readclient_hdfs.list('/tmp')with client_hdfs.read('/tmp/test.csv', encoding = 'utf-8') as reader: df = pd.read_csv(reader) print(df)## Updatewith client_hdfs.read('/tmp/t2.csv', encoding = 'utf-8') as reader: df = pd.read_csv(reader)df.loc[4] = [4,5000]client_hdfs.delete('/tmp/t2.csv')with client_hdfs.write('/tmp/t2.csv', encoding = 'utf-8') as writer: df.to_csv(writer) print(df)## Deleteclient_hdfs.delete('/tmp/t2.csv')'t2.csv' in client_hdfs.list('/tmp') Ref https://pypi.org/project/hdfs/ https://hdfscli.readthedocs.io/en/latest/api.html Hive설치 패키지Linux libsasl2-dev libsasl2-modules Python pyhive Requirement already satisfied: future in /opt/conda/lib/python3.8/site-packages (from pyhive) (0.18.2) Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.8/site-packages (from pyhive) (2.8.1) Requirement already satisfied: six&gt;=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil-&gt;pyhive) (1.14.0) thrift sasl thrift_sasl Requirement already satisfied: thrift&gt;=0.10.0 in /opt/conda/lib/python3.8/site-packages (from thrift_sasl) (0.13.0) Requirement already satisfied: sasl&gt;=0.2.1 in /opt/conda/lib/python3.8/site-packages (from thrift_sasl) (0.2.1) Requirement already satisfied: six&gt;=1.13.0 in /opt/conda/lib/python3.8/site-packages (from thrift_sasl) (1.14.0) Code12345678910111213141516171819202122232425262728293031323334import pyhive.hive as hiveconn = hive.Connection(host='cdh3', port=10000, username='root', password='tmaxtmax', database='default', auth='CUSTOM')# Createcur = conn.cursor()sql = \"CREATE TABLE `default`.`create_test` ( `field_1` bigint , `field_2` string , `field_3` string , `field_4` string , `field_5` string , `field_6` bigint , `field_7` string , `field_8` double , `field_9` string , `field_10` string ) ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' COLLECTION ITEMS TERMINATED BY '' MAP KEYS TERMINATED BY '' STORED AS TextFile\"cur.execute(sql)cur = conn.cursor()cur.execute('select * from create_test')res = cur.fetchall()print(res)# Readcur.execute('select count(*) from create_test')res = cur.fetchall()print(res)# Updatecur.execute('alter table create_test rename to create_test_rev')cur.execute('describe create_test_rev')res = cur.fetchall()print(res)# Deletecur.execute('drop table create_test_rev')cur.execute('show tables')res = cur.fetchall()print(res) Ref https://sungwookkang.com/m/1367 HBase Hbase 서버 내 Thrift 가 동작하고 있어야함 클러스터 내 아무 노드에서 $ hbase thrift &amp; 명령어로 thrift 실행 backgroud 구동 설치 패키지LinuxPython happybase Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from happybase) (1.14.0) Requirement already satisfied: thriftpy2&gt;=0.4 in /opt/conda/lib/python3.8/site-packages (from happybase) (0.4.11) Requirement already satisfied: ply&lt;4.0,&gt;=3.4 in /opt/conda/lib/python3.8/site-packages (from thriftpy2&gt;=0.4-&gt;happybase) (3.11) Code1234567891011121314151617181920212223242526272829import happybase# Connectionconn = happybase.Connection('192.168.179.183', 9090, autoconnect=True)## Create conn.create_table('test',&#123;'cf':&#123;&#125;&#125;)conn.tables()## Readtbl = conn.table('test')for key, data in tbl.scan(): print(key, data)## Updatetable = conn.table('test')table.put('row-key',&#123;'cf:col1':'value1','cf:col2':'value2'&#125;)row = dict(table.row('row-key'))print(row)## Deleteconn.delete_table('test', disable=True)conn.tables() Ref https://creatorw.tistory.com/entry/Hbase%EC%99%80-python-%EC%97%B0%EA%B2%B0-%ED%95%98%EA%B8%B0%EA%B8%B0 Impala connection 시 host 는 마스터로 작성하니 connection 이 되지 않음 마스터 외 노드 ip로 host 설정 필요 설치 패키지LinuxPython impyla==0.15a1 *(꼭 0.15a1 버젼 설치 필요) Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from impyla==0.15a1) (1.14.0) Requirement already satisfied: bitarray in /opt/conda/lib/python3.8/site-packages (from impyla==0.15a1) (1.2.1) Requirement already satisfied: thrift&gt;=0.9.3 in /opt/conda/lib/python3.8/site-packages (from impyla==0.15a1) (0.13.0) Requirement already satisfied: thriftpy2==0.4.0; python_version &gt;= “3.0” in /opt/conda/lib/python3.8/site-packages (from impyla==0.15a1) ( - 0.4.0) Requirement already satisfied: ply&lt;4.0,&gt;=3.4 in /opt/conda/lib/python3.8/site-packages (from thriftpy2==0.4.0; python_version &gt;= - “3.0”-&gt;impyla==0.15a1) (3.11) Code1234567891011121314151617181920212223242526272829303132333435363738394041424344454647from impala.dbapi import connect# Connectionconn = connect(host='192.168.179.181', port=21050)cursor = conn.cursor()cursor.get_databases()drop = \"DROP TABLE IF EXISTS default.tab;\"create = \"\"\"CREATE EXTERNAL TABLE default.tab(id INT,col_1 BOOLEAN,col_2 DOUBLE,col_3 TIMESTAMP)ROW FORMAT DELIMITED FIELDS TERMINATED BY ','\"\"\"## Createcursor.execute(create)cursor.execute('select count(*) from default.tab')res = cursor.fetchall()print(res)## Readcursor.execute('select * from tab')res = cursor.fetchall()print(res)## Updatecursor.execute('alter table default.tab rename to default.tab_rev')cursor.execute('describe default.tab_rev')res = cursor.fetchall()print(res)## Deletecursor.execute('drop table default.tab_rev')cursor.execute('show tables')res = cursor.fetchall()print(res) Ref https://euriion.com/?p=411856 https://m.blog.naver.com/PostView.nhn?blogId=hancury&amp;logNo=220755129588&amp;proxyReferer=https:%2F%2Fwww.google.com%2F Spark python 3.8 패키지 미지원 (pyspark) python 3.7 환경에서만 진행 컨테이너 내 hadoop client 및 spark 설치 필요 hadoop: $ wget https://archive.apache.org/dist/hadoop/core/hadoop-3.0.0/hadoop-3.0.0.tar.gz cdh 클라이언트 구성파일 중 core-site.xml, hdfs-site.xml, yarn-site.xml 을 $HADOOP_HOME/etc/hadoop 으로 copy&amp;paste spark: spark-2.4.0-bin-hadoop2.7.tgz .bashrc12345678export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64export HADOOP_HOME=/home/jovyan/hadoopexport HADOOP_CMD=/home/jovyan/hadoop/bin/hadoopexport HADOOP_CONF_DIR=/home/jovyan/hadoop/etc/hadoopexport PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$JAVA_HOME/bin:$HADOOP_CMDexport SPARK_HOME=/home/jovyan/sparkexport PATH=$PATH:$SPARK_HOME/bin CDH spark 구성 master: yarn deploy mode: client 설치 패키지Linux default-jdk Python pyspark Requirement already satisfied: py4j==0.10.7 in /opt/conda/lib/python3.8/site-packages (from pyspark) (0.10.7) Ref https://www.sicara.ai/blog/2017-05-02-get-started-pyspark-jupyter-notebook-3-minutes https://docs.cloudera.com/documentation/enterprise/6/6.3/topics/spark_ipython.html https://github.com/mike-wendt/cloudera-jupyter-notebook-spark 위와 같은 환경에서 Spark 연동은 사실 어렵다. 왜냐하면 CDH 내 Yarn 에서 컨테이너 호스트명을 인식하지 못하는 문제가 발생하기 때문이다. 이는 CDH Yarn config 를 수정해서, 컨테이너 호스트명을 바라볼 수 있게끔 변경하면 yarn-client 모드로 연동이 가능하다. 아니면 Livy 라는 패키지를 CDH 클러스터에 설치하여 API 로 연동하는 방법이 존재한다. (구글링 추천) Sybase jar file jconn3.jar (jdk 버젼별로 상이할 수 있으므로 jconn4.jar 준비 필요) 설치 패키지Linux default-jdk Python jpype1==0.6.3 Jaydebeapi Code12345678910111213141516171819202122232425262728293031import jpypeimport jaydebeapi# Connectionconn = jaydebeapi.connect('com.sybase.jdbc3.jdbc.SybDriver', 'jdbc:sybase:Tds:192.168.179.169:2638', &#123;'user': 'DBA', 'password': 'sql'&#125; ,['/home/jovyan/jconn3.jar'])c1 = conn.cursor()## Createc1.execute('create table aa(a int)')c1.execute('select * from aa')print(c1.fetchall())## Readc1.execute('select * from aa')print(c1.fetchall())## Updatec1.execute('insert into aa (a) values (1000)')c1.execute('select * from aa')print(c1.fetchall())## Deletec1.execute('drop table aa')c1.execute('select * from aa')print(c1.fetchall()) Ref https://stackoverflow.com/questions/3319788/what-is-the-best-way-to-connect-to-a-sybase-database-from-python Oracle jar file ojdbc6.jar 설치 패키지Linux default-jdk Python jpype1==0.6.3 Jaydebeapi Code1234567891011121314151617181920212223242526272829303132import jpypeimport jaydebeapi# Connection#jpype.startJVM(jpype.getDefaultJVMPath(), classpath='/home/jovyan/ojdbc6.jar', convertStrings=True)conn = jaydebeapi.connect('oracle.jdbc.driver.OracleDriver', 'jdbc:oracle:thin:scott/tiger@192.168.179.167:1521:ORCL1', jars='/home/jovyan/ojdbc6.jar')c1 = conn.cursor()## Createc1.execute('create table bb(a number)')c1.execute('select * from bb')res = c1.fetchone()print(res)## Readc1.execute('select * from emp')res = c1.fetchone()print(res)## Updatec1.execute('insert into bb values (1000)')c1.execute('select * from bb')print(c1.fetchall())## Deletec1.execute('drop table scott.bb') Ref https://m.blog.naver.com/PostView.nhn?blogId=axzswq&amp;logNo=221533592504&amp;proxyReferer=https:%2F%2Fwww.google.com%2F http://blog.naver.com/PostView.nhn?blogId=delfood&amp;logNo=221666021769&amp;categoryNo=33&amp;parentCategoryNo=0&amp;viewDate=&amp;currentPage=1&amp;postListTopCurrentPage=1&amp;from=search made by jaejun.lee","categories":[{"name":"Python","slug":"Python","permalink":"https://jx2lee.github.io/categories/Python/"}],"tags":[]},{"title":"[Cloud] Kubeflow ","slug":"cloud-kubeflow_katib","date":"2020-05-03T15:00:00.000Z","updated":"2020-10-29T14:38:28.109Z","comments":true,"path":"cloud-kubeflow_katib/","link":"","permalink":"https://jx2lee.github.io/cloud-kubeflow_katib/","excerpt":"Kubeflow 책을 공부하며 내용을 정리하고자 한다. 이번 파트는 Katib 이다.","text":"Kubeflow 책을 공부하며 내용을 정리하고자 한다. 이번 파트는 Katib 이다. KatibKubeflow 설치 시 자동으로 설치하는 컴포넌트로, 하이퍼파라미터 최적화와 뉴럴 아키텍처 탐색(NAS)으로 구성한다. 하이퍼파라미터 최적화와 뉴럴 아키텍처 탐색(NAS)에 대한 개념은 이번 글에서 다루지 않는다. 하이퍼파라미터 참고 https://jx2lee.github.io/2019/07/02/ml-introduction_to_grid_search/ Hyperparameters in Machine /Deep Learning 뉴럴 아키텍쳐 탐색(NAS) AutoML을 이용한 Architecture Search 소개 및 NASNet 논문 리뷰 Network Architecture Search - Samsung Software Membership ArchitectureKatib 은 크게 4가지 개념으로 이루어졌다. Experiment : 하나의 실행단위로 Job 개념으로 생각하면 된다. K8s 커스텀 리소스로 Trial 를 실행하는 역할을 하며 Experiment 는 5개 영역으로 나뉜다. Trial Count : 실행 횟수 (병렬) Trial Template : Trial 파드 템플릿 Objective : 목표 수치 (최곳값 또는 최솟값 설정) Search Parameter : 탐색하고자 하는 파라미터 값의 range Search Algorithm : 탐색 알고리즘 Trial : 최적화 과정의 반복 단위. Experiment의 Trial Count 값 만큼 Trial을 생성하고 순차적으로 실행. 하나의 Trial 에서 하나의 worker job을 실행, Trial 도 K8s 커스텀 리소스 Suggestion : Search Algorithm 으로 생성한 하이퍼파라미터 값의 모음. 하나의 Experiment 에서 하나의 Suggestion 을 생성 Worker job : 파라미터와 Suggestion 값을 이용해 Trial, 각각의 값을 평가하고 계사하는 프로세스. 실제로 학습을 수행하며 K8s Job 과 TFJob, PyTorch 을 사용 Experiment exampleMNIST 예제 템플릿을 함께 보며 위에서 설명한 구조와 비교하며 살펴본다. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455apiVersion: \"kubeflow.org/v1alpha3\"kind: Experimentmetadata: namespace: kubeflow labels: controller-tools.k8s.io: \"1.0\" name: handson-experiment-1spec: parallelTrialCount: 5 # 병렬로 실행할 Trial 수로, 리소스 허용한도까지 동시에 5개 Trial 을 실행한다. maxTrialCount: 30 # 최대로 실행할 Trial 수로 총 30개. parallelTrialCount 가 5 이므로 병렬로 5개 Trial 을 실행하고 6번 반복 maxFailedTrialCount: 3 # 실패 한도 수로 Trial 이 3번 실패하면 Experiment 를 중지한다. objective: # 수집 대상의 메트릭 설정 단계 type: maximize # 최댓값 또는 최솟값 설정 (본 예제는 maximize, 최댓값) goal: 0.99 # 목표 수치 설정 objectiveMetricName: validation-accuracy # 수집할 메트릭 name, validation-accuracy 로 설정 additionalMetricNames: # 이외 수집할 메트릭을 정의 (accuracy, loss, validation-loss 3개를 추가 수집할 예정) - accuracy - loss - Validation-loss algorithm: # Search Algorithm 설정 (그리드, 랜덤, 하이퍼밴드, 베이지안최적화 선택 가능) algorithmName: random trialTemplate: # Trial 템플릿 정의 goTemplate: rawTemplate: |- apiVersion: batch/v1 kind: Job metadata: name: &#123;&#123;.Trial&#125;&#125; namespace: &#123;&#123;.NameSpace&#125;&#125; spec: template: spec: containers: - name: &#123;&#123;.Trial&#125;&#125; image: brightfly/katib-job:handson command: - \"python\" - \"/app/katib_keras_mnist.py\" &#123;&#123;- with .HyperParameters&#125;&#125; # 설정 파라미터의 iteration 구문. .Name=.Value 형태로 메트릭 수집 &#123;&#123;- range .&#125;&#125; - \"&#123;&#123;.Name&#125;&#125;=&#123;&#123;.Value&#125;&#125;\" &#123;&#123;- end&#125;&#125; &#123;&#123;- end&#125;&#125; restartPolicy: Never parameters: # 하이퍼파라미터 입력값으로 learning rate 와 dropout 설정 - name: --learning_rate parameterType: double feasibleSpace: # 각 하이퍼파라미터의 range 설정 min: \"0.01\" max: \"0.03\" - name: --dropout_rate parameterType: double feasibleSpace: min: \"0.1\" max: \"0.9\" 본 템플릿으로 Experiment 를 실행하면 python /app/katib_keras_mnist.py -learning_rate=0.012--dropout_rate=0.381 와 같이 실행한다. Katib ComponentKatib 을 구성하는 컴포넌트로는 총 4개가 존재한다. 각 컴포넌트는 kubectl 로 조회 가능하며 K8s의 Deployment 로 관리한다. katib-manager : GRPC API server katib-db : Katib 의 백엔드 저장소, mysql katib-ui : Katib UI katib-controller : katib CRD의 컨트롤러 Katib UIWeb UI를 제공하는데 크게 Hyperparameter Tuning 과 NAS 2개 메뉴가 존재한다. Hyperparameter Tuning 에서는 직접 YAML 을 작성하거나 마우스와 키보드로 값을 추가할 수 있는 페이지를 제공한다. Katib Command-line interfaceUI 외에 커맨드라인 인터페이스를 제공하는데, kfctl 또는 kubectl 을 이용해 Experiment 을 실행할 수 있다. 단, Experiment 리소스 권한이 존재해야한다. kubectl apply -f mnist_experiment_random.yaml 123456789[root@master my-kubeflow]# kubectl get pod -n kubeflowNAME READY STATUS RESTARTS AGEadmission-webhook-bootstrap-stateful-set-0 1/1 Running 1 26dadmission-webhook-deployment-68c6dd4cc5-sb6hq 1/1 Running 0 6d23happlication-controller-stateful-set-0 1/1 Running 1 26dargo-ui-78bf45b698-r5zhr 1/1 Running 0 26dcentraldashboard-6957f8bcbc-6nktd 1/1 Running 1 26dhandson-experiment-1-random-57698b477b-dzmwm 1/1 Running 0 98s... mnist_experimnet_random.yaml : (https://github.com/mojokb/handson-kubeflow/blob/master/katib/mnist_experiment_random.yaml) Reference 쿠버네티스에서 머신러닝이 처음이라면! 쿠브플로우! Kubeflow.org made by jaejun.lee","categories":[{"name":"Cloud","slug":"Cloud","permalink":"https://jx2lee.github.io/categories/Cloud/"}],"tags":[{"name":"Kubeflow","slug":"Kubeflow","permalink":"https://jx2lee.github.io/tags/Kubeflow/"}]},{"title":"[Cloud] Kubeflow ","slug":"cloud-kubeflow_fairing","date":"2020-04-27T15:00:00.000Z","updated":"2020-10-29T14:38:26.422Z","comments":true,"path":"cloud-kubeflow_fairing/","link":"","permalink":"https://jx2lee.github.io/cloud-kubeflow_fairing/","excerpt":"Kubeflow 책을 공부하며 내용을 정리하고자 한다. 이번 파트는 Faring 이다.","text":"Kubeflow 책을 공부하며 내용을 정리하고자 한다. 이번 파트는 Faring 이다. FairingKubeflow 환경에서 ML 모델을 손쉽게 학습-배포할 수 있는 Python Package Architecture work flow python 으로 작성한 파일을 도커 이미지로 빌드 빌드된 이미지를 레지스트리 push 배포 리소스에 따라 k8s Job, TFJob, KFServing 등으로 변환하여 k8s API 서버 요청 위 work flow 는 preprocessor, builder, deployer 구조로 설계 preprocessor : Python 파일을 도커 이미지로 빌드할 수 있게 패키지화 builder : 패키지된 파일을 도커 이미지화 deployer : 생성한 이미지 배포 Fairing 은 Kubeflow 설치 후 생성한 노트북 이미지에는 default 로 설정되어 있어 따로 설치를 하지 않아도 테스트가 가능하다. Faring 예제Fairing 예제는 Kubeflow 노트북 서버에서 진행한다. 앞서 소개했듯이 Fairing 은 K8s 리소스를 이용하기 때문에 docker registry 와 kubeflow 접근 권한이 필요하다. 테스트 결과, private docker registry 에 fairing 이미지를 push/pull 할 때 에러가 발생하였다. 장애 해결이 이루어지지 않아 docker hub 의 공개 레지스트리를 사용할 예정이며, 책 순서에 따라 진행한다. fairing_config.ipynb1234567891011121314151617181920# in Jupyter Notebookimport kubeflow.fairing as fairingdocker_registry = \"jaejunlee\"fairing.config.set_builder( 'append', base_image='gcr.io/kubeflow-images-public/tensorflow-1.14.0-notebook-cpu:v0.7.0', image_name='fairing-test', registry=docker_registry, push=True)fairing.config.set_deployer('job')def train(): hostname = tf.constant(os.environ['HOSTNAME']) sess = tf.Session() print('Hostname : ', sess.run(hostname).decode('utf-8'))remote_train = fairing.config.fn(train)remote_train() 위 예제는 HOSTNAME 환경변수를 출력한다. gcr.io/kubeflow-images-public/tensorflow-1.14.0-notebook-cpu:v0.7.0 기본 이미지에 위 출력함수를 입힌 이미지를 생성하여 jaejunlee public registry 에 푸쉬하고 fairing-job 이란 이름의 k8s job 이 k8s 에 실행을 요청한다. 쉽게 말해, 기본 이미지 위에 담아 fairing 이미지를 생성하고 k8s 를 이용해 정의한 함수를 k8s 에서 실행한다. hub.docker.io 에 접속하여 확인하면 해당 프로세스를 실행할 때 마다 이미지를 push 한다. (set_builder 부분 push=True 로 설정했기 때문이다.) 코드를 통해 Config 클래스는 preprocessor, builder, deployer 에 대응하는 setter들을 가지고 있다. 각각의 default 값은 다음과 같다. preprocessor : Notebook 환경은 “notebook”, else “python” builder : “append” deploy” : job Fairing 구조 3개를 살펴보도록 한다. Preprocessor in Fairingpreprocessor 는 도커 이미지로 패키지화 할 대상을 설정하는데, 타입은 총 4개이다. python : 파이썬 file 패키지화 도커 이미지 내 app/{파일명}.py 로 cmd 생성 example123456789101112131415161718192021222324#!/usr/bin/env pythonimport os, timedef train(): print(\"Training...\") import tensorflow as tf x = tf.constant([1,2,3,4,5,6], shape=[2,3]) print(x) time.sleep(1)if __name__ == '__main__': if os.getenv('FAIRING_RUNTIME', None) is not None: # if else 구조를 통해 이미지가 생성되면 \bFAIRING_RUNTIME 변수가 \b1 로 변환된다. 즉, 이미지 생성후에는 train() 만 작업 train() else: from kubeflow import fairing DOCKER_REGISTRY = 'jaejunlee' file_name = os.path.basename(__file__) print(\"Executing &#123;&#125; remotely.\".format(file_name)) fairing.config.set_preprocessor('python', executable=file_name, input_files=[file_name]) fairing.config.set_builder( 'append', base_image='gcr.io/kubeflow-images-public/tensorflow-1.14.0-notebook-cpu:v0.7.0', registry=DOCKER_REGISTRY, push=True) fairing.config.run() 123456789101112131415161718192021222324252627$ chmod +x fairing_preprocessor_python.py$ ./fairing_preprocessor_python.py[I 200428 09:36:11 config:134] Using preprocessor: &lt;kubeflow.fairing.preprocessors.base.BasePreProcessor object at 0x7f8b2fc1e278&gt;[I 200428 09:36:11 config:136] Using builder: &lt;kubeflow.fairing.builders.append.append.AppendBuilder object at 0x7f8b2fc1e2b0&gt;[I 200428 09:36:11 config:138] Using deployer: &lt;kubeflow.fairing.deployers.job.job.Job object at 0x7f8b2fc1e9b0&gt;[W 200428 09:36:11 append:50] Building image using Append builder...[I 200428 09:36:11 base:107] Creating docker context: /tmp/fairing_context_jvkk0mqm[I 200428 09:36:11 docker_creds_:234] Loading Docker credentials for repository 'gcr.io/kubeflow-images-public/tensorflow-1.14.0-notebook-cpu:v0.7.0'[W 200428 09:36:13 append:54] Image successfully built in 1.9876068960002158s.[W 200428 09:36:13 append:94] Pushing image jaejunlee/fairing-job:13B00B9B...[I 200428 09:36:13 docker_creds_:234] Loading Docker credentials for repository 'jaejunlee/fairing-job:13B00B9B'[W 200428 09:36:15 append:81] Uploading jaejunlee/fairing-job:13B00B9B[I 200428 09:36:16 docker_session_:280] Layer sha256:823f4685c03b26a545ca41dcdca1e782ad5e52cf85bac03113edaa6aebdca1b3 exists, skipping......sha256:19f71f3a178549ee1bacd534f061e4b465d85c3378ecddb4dc716f1283aff2a8 pushed.[I 200428 09:36:22 docker_session_:334] Finished upload of: jaejunlee/fairing-job:13B00B9B[W 200428 09:36:22 append:99] Pushed image jaejunlee/fairing-job:13B00B9B in 9.045510983996792s.[W 200428 09:36:22 job:101] The job fairing-job-gm8hb launched.[W 200428 09:36:22 manager:296] Waiting for fairing-job-gm8hb-87z5h to start...[W 200428 09:36:22 manager:296] Waiting for fairing-job-gm8hb-87z5h to start...[W 200428 09:36:22 manager:296] Waiting for fairing-job-gm8hb-87z5h to start...[W 200428 09:36:23 manager:296] Waiting for fairing-job-gm8hb-87z5h to start...[I 200428 09:36:29 manager:302] Pod started running TrueTraining...Tensor(\"Const:0\", shape=(2, 3), dtype=int32)[W 200428 09:36:31 job:173] Cleaning up job fairing-job-gm8hb... notebook : jupyter notebook 내용을 파이썬 file 로 변환하여 파이썬 file 을 패키지화 example1234567891011121314# jupyter notebook# fairing_preprocessor_notebook.ipynbimport os, timedef train(): print(&quot;Training...&quot;) import tensorflow as tf x &#x3D; tf.constant([1,2,3,4,5,6], shape&#x3D;[2,3]) print(x) time.sleep(1)if __name__ &#x3D;&#x3D; &#39;__main__&#39;:train() 1234567891011121314#!/usr/bin/env python# fairing_preprocessor_notebook.pyimport osfrom kubeflow import fairingDOCKER_REGISTRY = 'jaejunlee'file_name = os.path.basename(__file__)print(\"Executing &#123;&#125; remotely.\".format(file_name))fairing.config.set_preprocessor('notebook', notebook_file='fairing_preprocessor_notebook.ipynb')fairing.config.set_builder( 'append', base_image='gcr.io/kubeflow-images-public/tensorflow-1.14.0-notebook-cpu:v0.7.0', registry=DOCKER_REGISTRY, push=True)fairing.config.run() 12345678910111213141516171819202122232425262728$ chmod +x fairing_preprocessor_notebook.py$ ./fairing_preprocessor_notebook.py[I 200428 09:44:27 config:134] Using preprocessor: &lt;kubeflow.fairing.preprocessors.converted_notebook.ConvertNotebookPreprocessor object at 0x7fd480eb0908&gt;[I 200428 09:44:27 config:136] Using builder: &lt;kubeflow.fairing.builders.append.append.AppendBuilder object at 0x7fd480eb0860&gt;[I 200428 09:44:27 config:138] Using deployer: &lt;kubeflow.fairing.deployers.job.job.Job object at 0x7fd4809d6240&gt;[W 200428 09:44:27 append:50] Building image using Append builder...[I 200428 09:44:27 base:107] Creating docker context: /tmp/fairing_context_b254bkb_[I 200428 09:44:27 converted_notebook:127] Converting fairing_preprocessor_notebook.ipynb to fairing_preprocessor_notebook.py[I 200428 09:44:27 docker_creds_:234] Loading Docker credentials for repository 'gcr.io/kubeflow-images-public/tensorflow-1.14.0-notebook-cpu:v0.7.0'[W 200428 09:44:29 append:54] Image successfully built in 1.9862057870050194s.[W 200428 09:44:29 append:94] Pushing image jaejunlee/fairing-job:7FCC2CF6...[I 200428 09:44:29 docker_creds_:234] Loading Docker credentials for repository 'jaejunlee/fairing-job:7FCC2CF6'[W 200428 09:44:31 append:81] Uploading jaejunlee/fairing-job:7FCC2CF6[I 200428 09:44:31 docker_session_:280] Layer sha256:380fe9d3ba2fe8c69d05cc9038b72aa9ec669cd0d51b0e61f312edb13586d5a8 pushed.......[I 200428 09:44:38 docker_session_:334] Finished upload of: jaejunlee/fairing-job:7FCC2CF6[W 200428 09:44:38 append:99] Pushed image jaejunlee/fairing-job:7FCC2CF6 in 9.018262255005538s.[W 200428 09:44:38 job:101] The job fairing-job-bbpjs launched.[W 200428 09:44:38 manager:296] Waiting for fairing-job-bbpjs-5jq22 to start...[W 200428 09:44:38 manager:296] Waiting for fairing-job-bbpjs-5jq22 to start...[W 200428 09:44:38 manager:296] Waiting for fairing-job-bbpjs-5jq22 to start...[W 200428 09:44:39 manager:296] Waiting for fairing-job-bbpjs-5jq22 to start...[I 200428 09:44:44 manager:302] Pod started running TrueTraining...Tensor(\"Const:0\", shape=(2, 3), dtype=int32)[W 200428 09:44:47 job:173] Cleaning up job fairing-job-bbpjs... full_notebook : jupyter notebook 패키지화 하는데, 수행 후 결과를 다시 노트북 파일로 생성 function : 단일 함수 패키지화 Builder in FairingBuilder 는 preprocessor 가 생성한 패키지를 도커 이미지화 한다. 빌드 타입 append : docker client 를 사용하지 않고 파이선 라이브러리를 이용해 이미지를 빌드하는 방식 self-signed 로 인증된 레지스트리는 사용 불가 .local 로 설정한 주소는 허용 가능 (kubeflow 용 레지스트리 구축시) 로그인이 필요한 레지스트리, 즉 docker hub 를 이용하려면 노트북 컨테이너 내 .docker/config.json 파일을 수정해야한다. 수정하는 방법은, 도커 서버에서 docker loging 을 통해 생성한 config.json 파일을 노트북 컨테이너 ~/.docker/config.json 으로 복사한다. cluster : 구글 컨테이너 툴인 Kaniko 를 이용해 이미지를 빌드하는 방식 docker : 도커 클라이언트를 이용해 이미지를 빌드하는 방식 해당 환경이 도커 레지스트리에 접근할 수 있는 권한이 존재해야함 format 123456DOCKER_REGISTRY='&#123;이미지를 push/pull 주소&#125;'fairing.config.set_builder( '&#123;build type&#125;', base_image='&#123;python 을 실행한 기본 이미지 명&#125;:&#123;태그&#125;', registry=DOCKER_REGISTRY, push=True) Deployer in FairingDeployer 는 builder 로 생성한 이미지를 배포한다. format 1234fairing.config.set_deployer('job', namespace=?, pod_spec_mutators=? ) 배포형태 : job, tfjob, pytorchjob, serving, kfserving, gcpjob, gcpserving등 namespace : 배포가 실행할 namespace pod_spec_mutators : 배포 pod 의 spec 정의 이렇게 각각 정의한 Preprocessor, Builder, Deployer 를 통해 Config.run 으로 Fairing 을 실행한다. 실행 순서는 설명한 흐름대로 preprocessor 로 패키지화 할 대상을 선택하고, Builder 로 이미지를 생성하며 Deployer 로 배포한다. Reference 쿠버네티스에서 머신러닝이 처음이라면! 쿠브플로우! made by jaejun.lee","categories":[{"name":"Cloud","slug":"Cloud","permalink":"https://jx2lee.github.io/categories/Cloud/"}],"tags":[{"name":"Kubeflow","slug":"Kubeflow","permalink":"https://jx2lee.github.io/tags/Kubeflow/"}]},{"title":"[Shell] k8s Pod 내 alias 적용 스크립트","slug":"shell-append_alias_to_pod","date":"2020-04-23T15:00:00.000Z","updated":"2020-05-07T08:31:47.888Z","comments":true,"path":"shell-append_alias_to_pod/","link":"","permalink":"https://jx2lee.github.io/shell-append_alias_to_pod/","excerpt":"사내 클라우드 제품을 이용해 업무를 보던 중, 계속되는 패치 작업으로 매번 생성하는 컨테이너의 alias 가 사라지는 문제가 발생하였다. 이러한 문제를 해결하고자 패치 이후 새로 생성하는 파드를 검색하고 .bashrc 에 alias 를 추가하는 스크립트를 작성하였다. Update Note 2020.05.07 : 스크립트 개선 (exec, alias 두 개 함수로 분리)","text":"사내 클라우드 제품을 이용해 업무를 보던 중, 계속되는 패치 작업으로 매번 생성하는 컨테이너의 alias 가 사라지는 문제가 발생하였다. 이러한 문제를 해결하고자 패치 이후 새로 생성하는 파드를 검색하고 .bashrc 에 alias 를 추가하는 스크립트를 작성하였다. Update Note 2020.05.07 : 스크립트 개선 (exec, alias 두 개 함수로 분리) 문제 발생패치가 진행되면 해당 파드의 이미지를 교체해야 한다. 이 작업을 수행하면 기존 파드를 삭제하고 교체된 이미지로 파드를 재생성하는데, 그럼 기존 파드에서 작업을 하던 alias 들이 사라진다. (당연히 컨테이너가 재기동하면서 .bashrc 초기화) 기존 사용하는 alias 1234567891011alias dasboot='startDomainAdminServer -u jeus -p jeus'alias dasdown='stopServer -host localhost:9736 -u jeus -p jeus'alias hdstart='startManagedServer -server hyperdata -u jeus -p jeus'alias hdstop='stopServer -host localhost:19736 -u jeus -p jeus'alias pastart='startManagedServer -server ProAuth -u jeus -p jeus'alias pastop='stopServer -host localhost:29736 -u jeus -p jeus'alias polog='tail -100f /hyperdata/proobject7/logs/ProObject.log'alias slog='tail -100f /db/tibero6/instance/tibero/log/slog/sys.log' Script 설계 방안우선 고려할 점은 교체한 이미지로 기동한 파드를 찾아야 한다. 이는 grep 과 awk 를 이용해 파드 정보를 조회하여 파드 이름을 검색하고 kubectl exec 으로 .bashrc 에 alias 를 추가한다. 당연히 추가만 한다고 적용이 안되므로 마지막에 source ~/.bashrc 명령어를 파드에 넘겨주고 접속하는 방향으로 설계를 하였다. 정리하면 다음과 같다. 파드 이름 검색 alias 를 .bashrc 에 추가 bash 적용을 위한 source 명령어 전달 파드 접근 사내 클라우드에서는 namespace 가 default 로 정의되어 있다. 이는 바뀌지 않기 때문에 스크립트 안에 default 로 설정하였다. Script12345678910111213141516171819202122232425262728293031323334353637383940414243#!/bin/sh# script for managing hd container# jaejun.lee.1991@gmail.comnamespace=$(kubectl get namespace | grep hpcd | awk '&#123;print $1&#125;')pod=$(kubectl describe pod -n hpcd-510fdc58 | grep -B 20 hyperdata8.3_hd | grep \"Name: hpcd\" | awk '&#123;print $2&#125;')function hd_container_exec() &#123; echo \"[INFO] exec $pod in $namespace\" | grep \"[INFO]\" --color kubectl exec -ti -n $namespace $pod -- bash&#125;function append_alias() &#123; echo \"[INFO] append alias to $pod\" | grep \"[INFO]\" --color kubectl exec -n $namespace $pod -- bash -c 'echo alias dasboot=\"\\\"startDomainAdminServer -u jeus -p jeus\"\\\" &gt;&gt; ~/.bashrc' kubectl exec -n $namespace $pod -- bash -c 'echo alias dasdown=\"\\\"stopServer -host localhost:9736 -u jeus -p jeus\"\\\" &gt;&gt; ~/.bashrc' kubectl exec -n $namespace $pod -- bash -c 'echo alias pastart=\"\\\"startManagedServer -server ProAuth -u jeus -p jeus\"\\\" &gt;&gt; ~/.bashrc' kubectl exec -n $namespace $pod -- bash -c 'echo alias pastop=\"\\\"stopServer -host localhost:29736 -u jeus -p jeus\"\\\" &gt;&gt; ~/.bashrc' kubectl exec -n $namespace $pod -- bash -c 'echo alias hdstart=\"\\\"startManagedServer -server hyperdata -u jeus -p jeus\"\\\" &gt;&gt; ~/.bashrc' kubectl exec -n $namespace $pod -- bash -c 'echo alias hdstop=\"\\\"stopServer -host localhost:19736 -u jeus -p jeus\"\\\" &gt;&gt; ~/.bashrc' kubectl exec -n $namespace $pod -- bash -c 'echo alias polog=\"\\\"tail -f /hyperdata/proobject7/logs/ProObject.log\"\\\" &gt;&gt; ~/.bashrc' kubectl exec -n $namespace $pod -- bash -c 'echo alias slog=\"\\\"tail -f /db/tibero6/instance/tibero/log/slog/sys.log\"\\\" &gt;&gt; ~/.bashrc' kubectl exec -n $namespace $pod -- bash -c 'source ~/.bashrc'&#125;function main() &#123; case \"$&#123;1:-&#125;\" in exec) hd_container_exec ;; alias) append_alias ;; *) set +x echo \"usage:\" &gt;&amp;2 echo \" $0 exec\" &gt;&amp;2 echo \" $0 alias\" &gt;&amp;2 ;; esac&#125;main $1 6번째 줄 : alias 를 적용할 파드 이름을 검색 9번째 줄 ~ 29번째 줄 : source ~/.bashrc 이전 : alias 를 .bashrc 에 추가 30번째 줄 : .bashrc 적용 31번째 줄 : 파드 접근 hd_container_exec : 특정 컨테이너(hd) 로 접근하는 함수 append_alias : 특정 컨테이너(hd) 내 .bashrc 파일에 alias 를 추가하고 적용하는 함수 main : exec or alias 인자를 받는 메인 함수 made by jaejun.lee","categories":[{"name":"Shell","slug":"Shell","permalink":"https://jx2lee.github.io/categories/Shell/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://jx2lee.github.io/tags/Kubernetes/"}]},{"title":"[Shell] 두 docker registry 간 이미지 최신화","slug":"shell-image_updater","date":"2020-04-08T15:00:00.000Z","updated":"2020-04-23T08:18:01.866Z","comments":true,"path":"shell-image_updater/","link":"","permalink":"https://jx2lee.github.io/shell-image_updater/","excerpt":"최근들어 이미지 패치가 자주 이루어지면서, 이미지를 일일이 풀(pull)하고 울팀 레지스트리에 푸쉬(push)하는 작업이 지속적으로 발생하였다. 최신화가 이루어지는 레지스트리를 daemon / insecure-registry 에 등록해 사용해도 되지만 스크립트 짜는 연습도 할 겸 이미지 최신화 스크립트를 작성하였고 이를 소개하고자 한다.","text":"최근들어 이미지 패치가 자주 이루어지면서, 이미지를 일일이 풀(pull)하고 울팀 레지스트리에 푸쉬(push)하는 작업이 지속적으로 발생하였다. 최신화가 이루어지는 레지스트리를 daemon / insecure-registry 에 등록해 사용해도 되지만 스크립트 짜는 연습도 할 겸 이미지 최신화 스크립트를 작성하였고 이를 소개하고자 한다. 상황간단히 A팀, B팀(내가 속한)에 대해 간략히 설명하면 다음과 같다. A팀 : 이미지를 최신화 하며 이미지 태그는 날짜_v?으로 설정한다. 최신 이미지는 a 레지스트리에 등록한다. B팀 : 테스트 작업을 마친 최신 이미지를 사용해 패치하고 이를 다시 테스트 한다. b 레지스트리를 이용한다. 사실 수작업으로 일일이 이미지 태그를 확인하며 pull&amp;push 해도 된다. docker pull a/이미지:태그, docker tag a/이미지:태그 b/이미지:태그, docker push b/이미지:태그.. 하나의 제품을 정상기동하려면 4개의 이미지를 사용하니 굉장히 불편하였다. 이미지 최적화가 안되어서 그런지 한 이미지의 pull&amp;push 가 2분정도 걸리는 경우도 존재한다. 이를 타파하고자 간단한 쉘 스크립트를 작성하여 모두 update 하던가, 한 이미지만 update 하게끔 만들었다. Usage컨피그 파일 하나와 쉘 스크립트 하나가 존재한다. 컨피그 파일(registry.config)은 pull 하기 위한 레지스트리 주소와 push 하기 위한 레지스트리 주소를 작성한다. 이 중 하나라도 작성하지 않으면 ERROR가 발생한다. 간단히 살펴보자. 1234❯ ./updateImage.shusage: ./updateImage.sh all ./updateImage.sh img &#123;image_name&#125; 쉘 스크립트는 크게 all 과 img가 있다. registry.config 를 모두 작성했다는 가정하에 스크립트를 실행해본다. all : 총 4개의 최신 이미지를 pull&amp;push 12345❯ ./updateImage.sh all[PULL REGISTRY] 192.xxx.xxx.xxx[PUSH REGISTRY] 192.yyy.yyy.yyy...... img : 4개 중 원하는 이미지를 pull&amp;push 123456❯ ./updateImage.sh img[PULL REGISTRY] 192.xxx.xxx.xxx[PUSH REGISTRY] 192.yyy.yyy.yyyEnter the image name : ???...... image name 을 작성하면 그 이미지의 최신 태그를 찾고 이를 pull&amp;push 한다. 최사 제품명이 혹시나 노출되면 안될까 싶어 수행 결과는 작성하지 않았따. Script스크립트는 내 깃헙에 올려두었지만 하기에도 있다. 난 착한 편이다. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768#!/bin/bash# Thu, 08.04.2020# jaejun.lee.1991@gmail.com#includebase_dir=$(dirname \"$0\"). $&#123;base_dir&#125;/registry.configfunction check_env()&#123; if [ -z $&#123;pull_registry&#125; ] || [ -z $&#123;push_registry&#125; ]; then echo \"[ERROR] You must set registry variables in registry.config!\" | grep \"[ERROR]\" --color exit 1 else echo \"[PULL REGISTRY] $&#123;pull_registry&#125;\" | grep \"[PULL REGISTRY]\" --color echo \"[PUSH REGISTRY] $&#123;push_registry&#125;\" | grep \"[PUSH REGISTRY]\" --color fi&#125;function check_img() &#123; image_pattern='hyperdata8.3_' echo -n \"Enter the image name : \" read name if [[ $&#123;name&#125; == *$&#123;image_pattern&#125;* ]];then echo \"Update $name image..\" else echo \"[ERROR] you must enter the image on hyperdata8.3_&#123;tb, hl, efa, hd&#125;!\" | grep \"[ERROR]\" --color exit 1 fi&#125;function update_all()&#123; image_list=( \"hyperdata8.3_hd\" \"hyperdata8.3_tb\" \"hyperdata8.3_hl\" \"hyperdata8.3_eda\" ) for image in $&#123;image_list[@]&#125;;do version=$(curl -X GET $&#123;pull_registry&#125;/v2/$&#123;image&#125;/tags/list | jq -r '.tags | .[-1]') docker pull $&#123;pull_registry&#125;/$&#123;image&#125;:$&#123;version&#125; docker tag $&#123;pull_registry&#125;/$&#123;image&#125;:$&#123;version&#125; $&#123;push_registry&#125;/$&#123;image&#125;:$&#123;version&#125; docker push $&#123;push_registry&#125;/$&#123;image&#125;:$&#123;version&#125; done&#125;function update_once()&#123; version=$(curl -X GET $&#123;pull_registry&#125;/v2/$&#123;name&#125;/tags/list | jq -r '.tags | .[-1]') docker pull $&#123;pull_registry&#125;/$&#123;name&#125;:$&#123;version&#125; docker tag $&#123;pull_registry&#125;/$&#123;name&#125;:$&#123;version&#125; $&#123;push_registry&#125;/$&#123;name&#125;:$&#123;version&#125; docker push $&#123;push_registry&#125;/$&#123;name&#125;:$&#123;version&#125;&#125;function main() &#123; case \"$&#123;1:-&#125;\" in all) check_env update_all ;; img) check_env check_img update_once ;; *) set +x echo \"usage:\" &gt;&amp;2 echo \" $0 all\" &gt;&amp;2 echo \" $0 img hyperdata8.3_hd\" &gt;&amp;2 ;; esac&#125;main $1 made by jaejun.lee","categories":[{"name":"Shell","slug":"Shell","permalink":"https://jx2lee.github.io/categories/Shell/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://jx2lee.github.io/tags/Docker/"}]},{"title":"[Cloud] Metallb for Kubernetes 설치","slug":"cloud-install_metallb","date":"2020-04-05T15:00:00.000Z","updated":"2020-04-23T08:18:41.677Z","comments":true,"path":"cloud-install_metallb/","link":"","permalink":"https://jx2lee.github.io/cloud-install_metallb/","excerpt":"Kubernetes 에 로드밸런서 생성을 위해 설치한 Metallb 설치 과정을 다뤄본다. 서비스를 특정 IP 로 노출하기 위해 Metallb config 설정과 서비스로 노출하는 단계로 설명한다.","text":"Kubernetes 에 로드밸런서 생성을 위해 설치한 Metallb 설치 과정을 다뤄본다. 서비스를 특정 IP 로 노출하기 위해 Metallb config 설정과 서비스로 노출하는 단계로 설명한다. Metallb 란MetalLB는 표준 라우팅 프로토콜을 사용하여 베어 메탈 (깡통) Kubernetes 클러스터에 대한 로드 밸런서 구현 이라고 한다. 표준 네트워크 장비와 통합되는 네트워크 LB 구현을 제공하여 베어 메탈 클러스터의 외부 서비스도 가능하게 만들어주는 것을 목표로 한다. 프라이빗 클라우드 환경에서 벤더사를 이용하지 않는 kubernetes 운영의 경우, 공유 IP 만 존재한다면 이를 로드밸런서로 활용할 수 있게 만들어주는 장점이 있다. Metallb 설치github 의 공식 배포 야믈을 특정 디렉토리 (ex. metallb) 에 다운받아 배포한다. 123mkdir -p /data/jlee/metallbwget -O metallb.yaml https://raw.githubusercontent.com/google/metallb/v0.7.3/manifests/metallb.yamlkubectl apply -f metallb.yaml 이후 생성하는 metallb-system 네임스페이스에 파드를 정상 배포하였는지 확인한다. 1234567root@k8s-node2:/data/jlee# kubectl get pods -n metallb-systemNAME READY STATUS RESTARTS AGEcontroller-547d466688-25w69 1/1 Running 0 2d21hspeaker-dfhwc 1/1 Running 0 2d21hspeaker-kpc2n 1/1 Running 0 2d21hspeaker-lfjv9 1/1 Running 0 2d21hspeaker-p9qlt 1/1 Running 0 2d21h Metallb Configmap 배포로드밸런서 생성에 대한 컨피그 맵 야믈을 생성하고 배포한다. 이름은 metallb-Configmap.yaml 로 설정하였다. 12345678910111213root@k8s-node2:/data/jlee/metallb# cat metallb-ConfigMap.yamlapiVersion: v1kind: ConfigMapmetadata: namespace: metallb-system name: configdata: config: | address-pools: - name: my-ip-space protocol: layer2 addresses: - 192.168.179.184-192.168.179.195 adresses 부분은 로드밸런서로 사용할 IP 대역대를 설정한다. 나의 경우, 184-195 까지 여유 IP 가 존재하기 때문에 range 를 추가하였다. 만약 하나의 IP 로 설정할 경우 IP 하나만 작성한다. 정상 배포하였는지 configmap 을 출력한다.1234567891011121314151617181920212223242526root@k8s-node2:/data/jlee/metallb# kubectl get configmap -n metallb-system -o yamlapiVersion: v1items:- apiVersion: v1 data: config: | address-pools: - name: my-ip-space protocol: layer2 addresses: - 192.168.179.184-192.168.179.195 kind: ConfigMap metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | &#123;\"apiVersion\":\"v1\",\"data\":&#123;\"config\":\"address-pools:\\n- name: my-ip-space\\n protocol: layer2\\n addresses:\\n - 192.168.179.184-192.168.179.195\\n\"&#125;,\"kind\":\"ConfigMap\",\"metadata\":&#123;\"annotations\":&#123;&#125;,\"name\":\"config\",\"namespace\":\"metallb-system\"&#125;&#125; creationTimestamp: \"2020-04-03T09:56:44Z\" name: config namespace: metallb-system resourceVersion: \"13110630\" selfLink: /api/v1/namespaces/metallb-system/configmaps/config uid: 49765aca-9364-48c2-9f9a-4fa2304651e8kind: Listmetadata: resourceVersion: \"\" selfLink: \"\" 서비스 배포 후 확인4개 파드 통신을 위한 서비스 야믈 파일에 type 을 로드밸런서로 설정하고 서비스를 배포한다. loadbalancer.yaml 12345678910apiVersion: v1kind: Service....spec: ... ... ... type: LoadBalancer loadBalancerIP: 192.168.179.184 # 이 부분 수정 필요 loadBalancerIP 값을 위 metallb-Configmap.yaml 내 IP 범위에 포함한 하나로 사용한다. 이후 배포한 서비스를 확인하면 EXTERNA-IP 에 IP 가 표시될 것이다. 반드시 ip 값은 configmap ip range 안에서 사용해야한다. kubectl get svc12NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEhyper-lb LoadBalancer 10.96.25.50 192.168.179.184 20:31010/TCP,21:31020/TCP,22:31030/TCP,23:31040/TCP,80:31060/TCP,8080:31080/TCP,9736:31085/TCP,8629:31090/TCP,8630:31100/TCP,28080:31160/TCP,1883:31190/TCP,2883:31200/TCP 6h42m Reference https://metallb.universe.tf https://ssup2.github.io/record/Kubernetes_MetalLB_설치_Ubuntu_18.04/ made by jaejun.lee","categories":[{"name":"Cloud","slug":"Cloud","permalink":"https://jx2lee.github.io/categories/Cloud/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://jx2lee.github.io/tags/Kubernetes/"}]},{"title":"[Cloud] kubectl 명령어 수행 시 실행속도 저하 문제","slug":"cloud-kubectl_hang","date":"2020-03-30T15:00:00.000Z","updated":"2020-04-02T09:04:40.977Z","comments":true,"path":"cloud-kubectl_hang/","link":"","permalink":"https://jx2lee.github.io/cloud-kubectl_hang/","excerpt":"도커 루트 디렉토리를 변경하던 도중, k8s 클러스터에서 kubectl 명령어가 느려지는 문제가 발생하였다. 원인을 파악하고 이를 해결하는 과정을 다룬다.","text":"도커 루트 디렉토리를 변경하던 도중, k8s 클러스터에서 kubectl 명령어가 느려지는 문제가 발생하였다. 원인을 파악하고 이를 해결하는 과정을 다룬다. 문제 발생kubectl 를 사용하면 결과는 나오지만 엄청 오래 걸리는 문제가 발생하였다. time kubectl get nodes 를 수행하면 다음과 같이 결과가 나왔다. 12345678910NAME STATUS ROLES AGE VERSIONk8s-master Ready master 14d v1.15.3k8s-node1 Ready master 14d v1.15.3k8s-node2 Ready master 14d v1.15.3k8s-node3 Ready &lt;none&gt; 14d v1.15.3k8s-node4 Ready &lt;none&gt; 14d v1.15.3real 2m1.032suser 2m0.089ssys 2m0.041s 원인 파악 kube-system 의 파드 상태 12345678910111213141516171819202122232425262728root@k8s-master:/data# kubectl get pods -n kube-systemNAME READY STATUS RESTARTS AGEcalico-kube-controllers-56cd854695-hvnkl 1/1 Running 0 7dcalico-node-4f2bt 1/1 Running 0 7dcalico-node-bhk4z 1/1 Running 0 7dcalico-node-kmvm9 1/1 Running 0 7dcalico-node-q928k 1/1 Running 0 7dcalico-node-snf8z 0/1 Evicted 0 90mcoredns-5c98db65d4-7665n 1/1 Running 35 10dcoredns-5c98db65d4-7hpxb 1/1 Running 34 10detcd-k8s-master 1/1 Running 3 14detcd-k8s-node1 1/1 Running 0 14detcd-k8s-node2 1/1 Running 0 14dkube-apiserver-k8s-master 1/1 Running 3 5d23hkube-apiserver-k8s-node1 1/1 Running 0 5d23hkube-apiserver-k8s-node2 1/1 Running 0 5d23hkube-controller-manager-k8s-master 1/1 Running 3 14dkube-controller-manager-k8s-node1 1/1 Running 7 14dkube-controller-manager-k8s-node2 1/1 Running 8 14dkube-proxy-4dhqc 1/1 Running 0 14dkube-proxy-9v87c 1/1 Running 0 14dkube-proxy-pfrwf 1/1 Running 0 14dkube-proxy-tsdb6 0/1 Evicted 0 91mkube-proxy-vsk6r 1/1 Running 0 14dkube-scheduler-k8s-master 1/1 Running 2 14dkube-scheduler-k8s-node1 1/1 Running 6 14dkube-scheduler-k8s-node2 1/1 Running 8 14dtiller-deploy-758bcdc94f-c92cw 1/1 Running 0 14d Evcited 된 캘리코 노드 파드가 보인다. 분명 k8s-master 노드에서 발생한 것으로 보인다. (미리 캡쳐를 떠놓지 못해 -o wide 옵션을 준 결과는 없다) calico-node-snf8z 파드를 describe 해보자. calico-node-snf8z 캘리코 파드 12345678910111213141516root@k8s-master:/data# kd pod -n kube-system calico-node-snf8zName: calico-node-snf8zNamespace: kube-systemPriority: 2000001000Priority Class Name: system-node-criticalNode: k8s-master/Start Time: Tue, 31 Mar 2020 14:14:44 +0900Labels: controller-revision-hash=5744776c47 k8s-app=calico-node pod-template-generation=1Annotations: scheduler.alpha.kubernetes.io/critical-pod:Status: FailedReason: EvictedMessage: The node was low on resource: ephemeral-storage....... The node was low on resource: ephemeral-storage. 메세지가 눈에 띈다. 구글링을 통해 알아본 결과, 해당 파드가 배포된 노드 용량이 부족하다 느끼면 파드를 띄우지 못하고 Evicted 상태로 변한다고 한다. 상태를 보아하니 docker root directory 가 /var/lib 가 default 로 설정되어 있어 이미지 등 데이터가 많이 쌓이는 문제가 발생하였다. (루트 전체 디렉토리의 약 80% 이상을 차지하고 있었다. 이는 full 나지 않아도 kubernetest 가 설정한 적정 용량(?)에서만 파드를 띄우게끔 설계된 것으로 보인다-뇌피셜) 이를 해결하고자 docker root directory 를 변경하였다. 참고 이후 Evicted 된 캘리코 노드 파드를 재기동하였다. 1docker restart 7feda4bd2162 결과해당 컨테이너를 재시작하고 kube-system 파드 상태를 확인한 결과, 모두 정상 작동하고 있으며 kubectl 명령어 실행도 이전처럼 빠르게 복구되었다. 이를 통해 kubectl 명령어어가 느려진 이유는 *”노드 용량 및 노드 간 통신일 수도 있다”* 라는 것을 깨달았다. 이외에도 많은 부분에서 명령어 실행 시간이 길어질 수 있는데 그때마다 글에 추가할 예정이다. 123456789101112131415161718192021222324252627282930313233343536373839root@k8s-master:/data# kgpo -n kube-systemNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATEScalico-kube-controllers-56cd854695-hvnkl 1/1 Running 0 7d 10.244.169.130 k8s-node2 &lt;none&gt; &lt;none&gt;calico-node-2wvj8 1/1 Running 0 54s 192.168.179.172 k8s-master &lt;none&gt; &lt;none&gt;calico-node-4f2bt 1/1 Running 0 7d 192.168.179.176 k8s-node4 &lt;none&gt; &lt;none&gt;calico-node-bhk4z 1/1 Running 0 7d 192.168.179.174 k8s-node2 &lt;none&gt; &lt;none&gt;calico-node-kmvm9 1/1 Running 0 7d 192.168.179.175 k8s-node3 &lt;none&gt; &lt;none&gt;calico-node-q928k 1/1 Running 0 7d 192.168.179.173 k8s-node1 &lt;none&gt; &lt;none&gt;coredns-5c98db65d4-7665n 1/1 Running 35 10d 10.244.36.71 k8s-node1 &lt;none&gt; &lt;none&gt;coredns-5c98db65d4-7hpxb 1/1 Running 34 10d 10.244.169.148 k8s-node2 &lt;none&gt; &lt;none&gt;etcd-k8s-master 1/1 Running 3 14d 192.168.179.172 k8s-master &lt;none&gt; &lt;none&gt;etcd-k8s-node1 1/1 Running 0 14d 192.168.179.173 k8s-node1 &lt;none&gt; &lt;none&gt;etcd-k8s-node2 1/1 Running 0 14d 192.168.179.174 k8s-node2 &lt;none&gt; &lt;none&gt;kube-apiserver-k8s-master 1/1 Running 3 5d23h 192.168.179.172 k8s-master &lt;none&gt; &lt;none&gt;kube-apiserver-k8s-node1 1/1 Running 0 5d23h 192.168.179.173 k8s-node1 &lt;none&gt; &lt;none&gt;kube-apiserver-k8s-node2 1/1 Running 0 5d23h 192.168.179.174 k8s-node2 &lt;none&gt; &lt;none&gt;kube-controller-manager-k8s-master 1/1 Running 3 14d 192.168.179.172 k8s-master &lt;none&gt; &lt;none&gt;kube-controller-manager-k8s-node1 1/1 Running 7 14d 192.168.179.173 k8s-node1 &lt;none&gt; &lt;none&gt;kube-controller-manager-k8s-node2 1/1 Running 8 14d 192.168.179.174 k8s-node2 &lt;none&gt; &lt;none&gt;kube-proxy-4dhqc 1/1 Running 0 14d 192.168.179.175 k8s-node3 &lt;none&gt; &lt;none&gt;kube-proxy-8qtnp 1/1 Running 0 54s 192.168.179.172 k8s-master &lt;none&gt; &lt;none&gt;kube-proxy-9v87c 1/1 Running 0 14d 192.168.179.174 k8s-node2 &lt;none&gt; &lt;none&gt;kube-proxy-pfrwf 1/1 Running 0 14d 192.168.179.173 k8s-node1 &lt;none&gt; &lt;none&gt;kube-proxy-vsk6r 1/1 Running 0 14d 192.168.179.176 k8s-node4 &lt;none&gt; &lt;none&gt;kube-scheduler-k8s-master 1/1 Running 2 14d 192.168.179.172 k8s-master &lt;none&gt; &lt;none&gt;kube-scheduler-k8s-node1 1/1 Running 6 14d 192.168.179.173 k8s-node1 &lt;none&gt; &lt;none&gt;kube-scheduler-k8s-node2 1/1 Running 8 14d 192.168.179.174 k8s-node2 &lt;none&gt; &lt;none&gt;tiller-deploy-758bcdc94f-c92cw 1/1 Running 0 14d 10.244.122.75 k8s-node4 &lt;none&gt; &lt;none&gt;root@k8s-node2:/data# time kubectl get nodesNAME STATUS ROLES AGE VERSIONk8s-master Ready master 14d v1.15.3k8s-node1 Ready master 14d v1.15.3k8s-node2 Ready master 14d v1.15.3k8s-node3 Ready &lt;none&gt; 14d v1.15.3k8s-node4 Ready &lt;none&gt; 14d v1.15.3real 0m1.032suser 0m0.089ssys 0m0.041s 2020.03.31 made by jaejun.lee","categories":[{"name":"Cloud","slug":"Cloud","permalink":"https://jx2lee.github.io/categories/Cloud/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://jx2lee.github.io/tags/Kubernetes/"}]},{"title":"[Cloud] Kubeflow 폐쇄망 설치 중 Namespace 생성 화면이 나오지 않는 문제","slug":"cloud-kubeflow_error","date":"2020-03-29T15:00:00.000Z","updated":"2020-06-24T03:29:13.139Z","comments":true,"path":"cloud-kubeflow_error/","link":"","permalink":"https://jx2lee.github.io/cloud-kubeflow_error/","excerpt":"Kubeflow 를 폐쇄 환경에서 설치하던 중 Namespace 생성화면이 나오지 않고 심지어 선택할 수 없는 문제가 발생하였다. 문제의 원인을 파악하고 해결하는 과정을 다룬다. Update Note 2020.05.19 : Image list 변경 2020.06.24 : Image List 변경 (kfserving-system 에 필요한 이미지 추가)","text":"Kubeflow 를 폐쇄 환경에서 설치하던 중 Namespace 생성화면이 나오지 않고 심지어 선택할 수 없는 문제가 발생하였다. 문제의 원인을 파악하고 해결하는 과정을 다룬다. Update Note 2020.05.19 : Image list 변경 2020.06.24 : Image List 변경 (kfserving-system 에 필요한 이미지 추가) 문제 UI 접속 후 namespace 생성이 되지 않음 .cache 폴더 삭제 후 재 배포해도 계속되는 문제 발생 kubectl get pod -n kubeflow123456789101112131415161718192021222324252627282930313233NAME READY STATUS RESTARTS AGEadmission-webhook-bootstrap-stateful-set-0 1/1 Running 0 15madmission-webhook-deployment-68c6dd4cc5-sgtn6 1/1 Running 0 14mapplication-controller-stateful-set-0 1/1 Running 0 15margo-ui-78bf45b698-2vhm9 1/1 Running 0 15mcentraldashboard-fd9549bd5-nv68z 1/1 Running 0 15mjupyter-web-app-deployment-7797778d74-5lq9d 1/1 Running 0 15mkatib-controller-55545cb4c8-8xdtn 1/1 Running 1 15mkatib-db-d99f776cd-zww8n 0/1 Running 1 15mkatib-manager-67d9689545-9j6cf 0/1 CrashLoopBackOff 6 15mkatib-ui-889499864-fd4hc 1/1 Running 0 15mmetacontroller-0 1/1 Running 0 15mmetadata-db-68df96445c-br8vt 1/1 Running 0 15mmetadata-deployment-865fddd777-dpkbv 1/1 Running 0 15mmetadata-envoy-deployment-68f64489cc-ts88j 1/1 Running 0 15mmetadata-grpc-deployment-7f5d6c8ccb-j99d2 1/1 Running 1 15mmetadata-ui-84c76df48f-4sxdq 1/1 Running 0 15mminio-75d8cbbb5c-2dsgt 1/1 Running 0 15mml-pipeline-7f6548ff8-74lh6 0/1 ImagePullBackOff 0 15mml-pipeline-ml-pipeline-visualizationserver-559c875d6b-9mth7 0/1 ImagePullBackOff 0 15mml-pipeline-persistenceagent-796c6c4c75-l8vx2 0/1 ImagePullBackOff 0 15mml-pipeline-scheduledworkflow-f86df57bd-87j5p 0/1 ImagePullBackOff 0 15mml-pipeline-ui-fb8b6778f-zgx59 1/1 Running 0 15mml-pipeline-viewer-controller-deployment-78bdcc54fc-pf8lb 1/1 Running 0 15mmysql-6c5ddbd98b-pkqqr 1/1 Running 0 15mnotebook-controller-deployment-7694b76c89-fb6g5 1/1 Running 0 15mprofiles-deployment-5c48f8d6d8-kxfx5 1/2 CrashLoopBackOff 7 15mpytorch-operator-dfb77d487-92dqx 1/1 Running 0 15mseldon-operator-controller-manager-0 1/1 Running 1 15mspartakus-volunteer-74f96589f9-g2xd5 1/1 Running 0 15mtensorboard-6867797f97-kqxsc 1/1 Running 0 15mtf-job-operator-58d7d7d976-jhpk4 1/1 Running 0 15mworkflow-controller-66dd745699-9bqcx 1/1 Running 0 15m logprofiles-deployment managerk logs -n kubeflow profiles-deployment-5c48f8d6d8-kxfx5 -c manager 123456789101112flag provided but not defined: -workload-identityUsage of /manager: -kubeconfig string Paths to a kubeconfig. Only required if out-of-cluster. -master string The address of the Kubernetes API server. Overrides any value in kubeconfig. Only required if out-of-cluster. -metrics-addr string The address the metric endpoint binds to. (default \":8080\") -userid-header string Key of request header containing user id (default \"x-goog-authenticated-user-email\") -userid-prefix string Request header user id common prefix (default \"accounts.google.com:\") profiles-deployment kfamk logs -n kubeflow profiles-deployment-5c48f8d6d8-kxfx5 -c kfam 1time=\"2020-03-26T06:02:16Z\" level=info msg=\"Server started\" centraldashboard pod logs123456789101112131415161718192021222324252627282930313233343536373839Initializing Kubernetes configurationUnable to fetch Nodes &#123; kind: 'Status', apiVersion: 'v1', metadata: &#123;&#125;, status: 'Failure', message: 'nodes is forbidden: User \"system:serviceaccount:kubeflow:centraldashboard\" cannot list resource \"nodes\" in API group \"\" at the cluster scope', reason: 'Forbidden', details: &#123; kind: 'nodes' &#125;, code: 403 &#125;Unable to fetch Application information: &#123; kind: 'Status', apiVersion: 'v1', metadata: &#123;&#125;, status: 'Failure', message: 'applications.app.k8s.io is forbidden: User \"system:serviceaccount:kubeflow:centraldashboard\" cannot list resource \"applications\" in API group \"app.k8s.io\" in the namespace \"kubeflow\"',Using Profiles service at http://profiles-kfam.kubeflow:8081/kfam reason: 'Forbidden', details: &#123; group: 'app.k8s.io', kind: 'applications' &#125;, code: 403 &#125;Unable to fetch Nodes &#123; kind: 'Status', apiVersion: 'v1', metadata: &#123;&#125;, status: 'Failure', message: 'nodes is forbidden: User \"system:serviceaccount:kubeflow:centraldashboard\" cannot list resource \"nodes\" in API group \"\" at the cluster scope', reason: 'Forbidden', details: &#123; kind: 'nodes' &#125;, code: 403 &#125;\"other\" is not a supported platform for MetricsServer listening on port http://localhost:8082 (in production mode)Unable to contact Profile Controller [object Object]Unable to contact Profile Controller [object Object]Unable to contact Profile Controller [object Object]Unable to contact Profile Controller [object Object]Unable to contact Profile Controller [object Object]Unable to contact Profile Controller [object Object]Unable to contact Profile Controller [object Object]Unable to contact Profile Controller [object Object] Unable to contact Profile Controller [object Object] centraldashboard 에서 profile controller 요청을 받지 못하는 상황 각 파드에 접속하여 (centraldashboard, profile controller-manager) 핑을 때려봤는데 모두 통신이 원활하였음 해결 방안 엄청나게 많은 방법을 시도했다. kfctl verion, 각종 이미지 변경 등.. 시행착오 끝에 centraldashobard 와 profile-deployment 파드에 생성한 컨테이너 이미지 버젼 문제였다. gcr.io/kubeflow-images-public/centraldashboard:latest gcr.io/kubeflow-images-public/profile-controller:v20191024-v0.7.0-rc.5-12-g956569ba-e3b0c4 gcr.io/kubeflow-images-public/kfam@sha256:3b0d4be7e59a3fa5ed1d80dccc832312caa94f3b2d36682524d3afc4e45164f0 digest 로 이루어진 이미지는 pull 이후에 tag 를 생성하여 레지스트리에 push 하였다. (ex. gcr.io/kubeflow-images-public/kfam:3b0d4be7e59a3fa5ed1d80dccc832312caa94f3b2d36682524d3afc4e45164f0) 최종적으로 kubeflow 0.7.1 버젼에 사용한 이미지 리스트는 다음과 같다. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475argoproj/argoui:v2.3.0argoproj/workflow-controller:v2.3.0docker.io/istio/citadel:1.1.6docker.io/istio/galley:1.1.6docker.io/istio/kubectl:1.1.6docker.io/istio/mixer:1.1.6docker.io/istio/pilot:1.1.6docker.io/istio/proxy_init:1.1.6docker.io/istio/proxyv2:1.1.6docker.io/istio/sidecar_injector:1.1.6docker.io/jaegertracing/all-in-one:1.9docker.io/kiali/kiali:v0.16docker.io/prom/prometheus:v2.3.1docker.io/seldonio/seldon-core-operator:0.4.1gcr.io/google_containers/spartakus-amd64:v1.1.0gcr.io/kfserving/alibi-explainer:0.2.2gcr.io/kfserving/kfserving-controller:0.2.2gcr.io/kfserving/logger:0.2.2gcr.io/kfserving/pytorchserver:0.2.2gcr.io/kfserving/sklearnserver:0.2.2gcr.io/kfserving/storage-initializer:0.2.2 gcr.io/kfserving/xgbserver:0.2.2gcr.io/kubebuilder/kube-rbac-proxy:v0.4.0gcr.io/kubeflow-images-public/admission-webhook:v20190520-v0-139-gcee39dbc-dirty-0d8f4cgcr.io/kubeflow-images-public/ingress-setup:latestgcr.io/kubeflow-images-public/jupyter-web-app:9419d4dgcr.io/kubeflow-images-public/katib/v1alpha3/katib-controller:v0.7.0gcr.io/kubeflow-images-public/katib/v1alpha3/katib-manager:v0.7.0gcr.io/kubeflow-images-public/katib/v1alpha3/katib-ui:v0.7.0gcr.io/kubeflow-images-public/kubernetes-sigs/application:1.0-betagcr.io/kubeflow-images-public/metadata-frontend:v0.1.8gcr.io/kubeflow-images-public/metadata:v0.1.11gcr.io/kubeflow-images-public/pytorch-operator:v0.7.0gcr.io/kubeflow-images-public/tensorflow-1.14.0-notebook-cpu:v-base-ef41372-1177829795472347138gcr.io/kubeflow-images-public/tensorflow-1.14.0-notebook-cpu:v0.7.0gcr.io/kubeflow-images-public/tensorflow-1.14.0-notebook-gpu:v0.7.0gcr.io/kubeflow-images-public/tensorflow-2.0.0a0-notebook-cpu:v0.7.0gcr.io/kubeflow-images-public/tensorflow-2.0.0a0-notebook-gpu:v0.7.0gcr.io/kubeflow-images-public/tf_operator:kubeflow-tf-operator-postsubmit-v1-5adee6f-6109-a25cgcr.io/ml-pipeline/api-server:0.1.31gcr.io/ml-pipeline/envoy:metadata-grpcgcr.io/ml-pipeline/frontend:0.1.31gcr.io/ml-pipeline/persistenceagent:0.1.31gcr.io/ml-pipeline/scheduledworkflow:0.1.31gcr.io/ml-pipeline/viewer-crd-controller:0.1.31gcr.io/ml-pipeline/visualization-server:0.1.27gcr.io/tfx-oss-public/ml_metadata_store_server:0.15.1grafana/grafana:6.0.2mcr.microsoft.com/onnxruntime/server:v0.5.1metacontroller/metacontroller:v0.3.0minio/minio:RELEASE.2018-02-09T22-40-05Zmysql:5.6mysql:8mysql:8.0.3nvcr.io/nvidia/tensorrtserver:19.05-py3tensorflow/serving:1.11.0tensorflow/serving:1.11.0-gputensorflow/serving:1.12.0tensorflow/serving:1.12.0-gputensorflow/serving:1.13.0tensorflow/serving:1.13.0-gputensorflow/serving:1.14.0tensorflow/serving:1.14.0-gputensorflow/tensorflow:1.8.0gcr.io/knative-releases/knative.dev/serving/cmd/activator@sha256:88d864eb3c47881cf7ac058479d1c735cc3cf4f07a11aad0621cd36dcd9ae3c6gcr.io/knative-releases/knative.dev/serving/cmd/autoscaler-hpa@sha256:a7801c3cf4edecfa51b7bd2068f97941f6714f7922cb4806245377c2b336b723gcr.io/knative-releases/knative.dev/serving/cmd/autoscaler@sha256:aeaacec4feedee309293ac21da13e71a05a2ad84b1d5fcc01ffecfa6cfbb2870gcr.io/knative-releases/knative.dev/serving/cmd/controller@sha256:3b096e55fa907cff53d37dadc5d20c29cea9bb18ed9e921a588fee17beb937dfgcr.io/knative-releases/knative.dev/serving/cmd/networking/istio@sha256:057c999bccfe32e9889616b571dc8d389c742ff66f0b5516bad651f05459b7bcgcr.io/knative-releases/knative.dev/serving/cmd/queue@sha256:e0654305370cf3bbbd0f56f97789c92cf5215f752b70902eba5d5fc0e88c5acagcr.io/knative-releases/knative.dev/serving/cmd/webhook@sha256:c2076674618933df53e90cf9ddd17f5ddbad513b8c95e955e45e37be7ca9e0e8gcr.io/kubeflow-images-public/centraldashboard@sha256:4299297b8390599854aa8f77e9eb717db684b32ca9a94a0ab0e73f3f73e5d8b5gcr.io/kubeflow-images-public/kfam@sha256:3b0d4be7e59a3fa5ed1d80dccc832312caa94f3b2d36682524d3afc4e45164f0gcr.io/kubeflow-images-public/notebook-controller@sha256:6490f737000bd1d2520ac4b8cbde2b09749cdb291b1967ddda95d05131db49dbgcr.io/kubeflow-images-public/profile-controller@sha256:e601b2226e534a4f8e0722cfc44ae4a919a90265c4c6c9e7a7a55fcb57032f25 tar 파일로 용량을 계산한 결과 약 30G(no gzip option) 추가로 발생한 문제 Kubeflow 배포 전 istio-system 와 knative-serving 네임스페이스가 존재하여 배포할 때 에러가 발생 해결 방안 배포 전 istio-system / knative-serving 네임스페이스를 삭제 후 재 배포 Ref https://github.com/kubeflow/kubeflow/issues/3859 https://github.com/kubeflow/kubeflow/issues/4788 https://github.com/kubeflow/kubeflow/issues/4718 https://github.com/kubeflow/kubeflow/issues/3900 https://stackoverflow.com/questions/54203646/kubernetes-how-to-increase-ephemeral-storage https://www.kangwoo.kr/2020/02/18/pc에-kubeflow-설치하기-3부-kubeflow-설치하기/ 2020.03.30 made by jaejun.lee","categories":[{"name":"Cloud","slug":"Cloud","permalink":"https://jx2lee.github.io/categories/Cloud/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://jx2lee.github.io/tags/Kubernetes/"}]},{"title":"[Cloud] Docker private registry 설정","slug":"cloud-docker_registry","date":"2020-03-17T15:00:00.000Z","updated":"2020-09-26T13:34:58.520Z","comments":true,"path":"cloud-docker_registry/","link":"","permalink":"https://jx2lee.github.io/cloud-docker_registry/","excerpt":"docker private registry 를 생성하고, tar 파일로 변환한 이미지를 load 및 registry 에 push 하는 과정을 다룬다. 사이트에 나가게 되면 폐쇄망인 경우가 대부분인데, 이럴 경우를 대비해서 이미지 리스트를 읽어들여 docker image 를 tar로 변환하고 registry 에 push 하는 쉘 스크립트를 작성하였다. Update Note 2020.03.30 : docker run 시 mount 방법(-v 옵션) 및 docker image pull / push 스크립트 추가 2020.04.13 : docker run 시 restart argument 추가","text":"docker private registry 를 생성하고, tar 파일로 변환한 이미지를 load 및 registry 에 push 하는 과정을 다룬다. 사이트에 나가게 되면 폐쇄망인 경우가 대부분인데, 이럴 경우를 대비해서 이미지 리스트를 읽어들여 docker image 를 tar로 변환하고 registry 에 push 하는 쉘 스크립트를 작성하였다. Update Note 2020.03.30 : docker run 시 mount 방법(-v 옵션) 및 docker image pull / push 스크립트 추가 2020.04.13 : docker run 시 restart argument 추가 Private registry 생성registry 이미지를 Pull 하고 container 를 기동한다.12docker pull registrydocker run -dit --name bips-registry --restart=always -p 5000:5000 -v /data/registry:/var/lib/registry registry:latest -v 플래그를 이용해 레지스트리 저장소를 /data/registry 마운트 시켜 컨테이너를 동작한다. 이는 이미지가 많아질 경우 용량이 큰 디바이스에 직접 설정하여 이미지를 관리할 수 있다. --restart=alyways 인자를 추가하여 도커 데몬 재시작 시 기동할 수 있도록 설정한다. Image pushtar 파일로 묶은 이미지를 docker load -i {tar_name} 으로 이미지를 생성하고, 이를 registry 에 push 한다. 원래 이미지 : 192.168.17.131:5000/ubuntu_t6:2020303_v2 태그 변경 후 이미지 : 92.168.179.185:5000/ubuntu_t6:2020303_v2 12docker tag 192.168.17.131:5000/ubuntu_t6:2020303_v2 192.168.179.185:5000/ubuntu_t6:2020303_v2docker push 192.168.179.185:5000/ubuntu_t6:2020303_v2 Image 확인registry 에 해당 이미지가 존재하는지 curl 명령어로 확인한다.1234root@k8s-master:~# curl -X GET 192.168.179.185:5000/v2/_catalog&#123;\"repositories\":[\"dfa-module\",\"hd8.3_rel\",\"hl_r172919\",\"ubuntu_t6\"]&#125;root@k8s-master:~# curl -X GET 192.168.179.185:5000/v2/ubuntu_t6/tags/list&#123;\"name\":\"ubuntu_t6\",\"tags\":[\"2020303_v2\"]&#125; 잘~ 등록되었다! Docker Image Pull &amp; Push 스크립트폐쇄망 환경에서 docker hub 의 이미지를 가져올 수 없는 상황이 발생하였다. 이를 위해 폐쇄망이 아닌 환경에서 이미지를 pull 하고, 이를 폐쇄망 환경에서 특정 registry (private docker registry) 에 push 하는 스크립트를 작성하였다. 기능 pull : docker hub 의 이미지를 가져와 tars 디렉토리에 저장한다. push : tars 디렉토리에 tar 이미지를 특정 registry 에 push 한다. 1234❯ ./imageLoaderusage: ./imageLoader pull ./imageLoader push &#123;registry_endpoint&#125; 이미지:태그 로 이루어진 .imageList 가 선행으로 준비해야 한다. 또한, .imageListHash 는 태그명이 아닌 해쉬값으로 되어있는 이미지를 받을 때 사용한다. 만약 둘 중 사용하지 않는 이미지가 있다면 선택에 따라 삭제하고 해당 스크립트에 주석을 설정한다. 첫 번째 pull/push 하는 부분이 이미지:태그, 두 번째가 이미지@해쉬 본인은 Kubeflow 를 폐쇄망 환경에서 설치할 이슈가 생겨 필요한 이미지 리스트를 .imageList / .imageListHash 에 작성하였다. 참고로 Kubeflow 폐쇄망 설치 시 이미지 버젼 때문에 애를 많이 먹었다.. github 에 올려두었다. 허접한 스크립트 이지만, 현 직장에서는 편하게(?) 사용할 수 있을 것 같아 공유한다. 많은 별 부탁드립니다. 2020.03.18 made by jaejun.lee","categories":[{"name":"Cloud","slug":"Cloud","permalink":"https://jx2lee.github.io/categories/Cloud/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://jx2lee.github.io/tags/Docker/"}]},{"title":"[Cloud] Orphaned pod found 에러 해결","slug":"cloud_orphaned_pod_error","date":"2020-03-11T15:00:00.000Z","updated":"2020-09-24T14:34:50.011Z","comments":true,"path":"cloud_orphaned_pod_error/","link":"","permalink":"https://jx2lee.github.io/cloud_orphaned_pod_error/","excerpt":"본 포스트에서는 kubelet log 에 Orphaned pod found , but volume subpaths are still present on disk 로그가 발생하는 문제를 해결한다.","text":"본 포스트에서는 kubelet log 에 Orphaned pod found , but volume subpaths are still present on disk 로그가 발생하는 문제를 해결한다. Kubelet logjournalctl -f | grep kubelet 1Mar 12 01:31:04 k8s-node2 kubelet[19210]: E0312 01:31:04.964272 19210 kubelet_volumes.go:154] Orphaned pod \"1a2f73f1-77a2-4053-975f-57a89fdba1db\" found, but volume subpaths are still present on disk : There were a total of 1 errors similar to this. Turn up verbosity to see them. 문제 원인해당 노드를 재부팅했을 때 기존에 남아있던 파드 정보로 인하여 에러가 발생한 것 같다. 사라진 파드의 정보에 volume subpath 도 삭제되지 않은 것이다. 문제 해결의외로 간단하다. 해당 pod 정보를 삭제하면 되는데, 본인 환경으로는 /var/lib/kubelet/pods/ 에 파드 UUID 폴더를 삭제하면 된다. 이후 kubelet 을 재기동하면 위 로그가 삭제됨을 확인할 수 있다. 이외 문제들오늘 있었던 다양한 에러들을 한 번 정리했다 (네트워크 문제로 인해 file stroage 를 배포하는 문제는 더 파악해본 후 포스팅 할 예정이다). kubelet 로그에 Unable to read config path “/etc/kubernetes/manifests”: path does not exist, ignoring 메세지 발생 /etc/kubernets/path 에 manifests 폴더를 생성하여 해결 노드에 node.kubernetes.io/unreachable:NoSchedule Taint 가 걸려있는 경우 해당 노드가 Not Ready 인 상태 이런 경우에는 대게 노드가 재부팅 후 스왑 메모리가 켜져 있을 확률이 높다. free 명령어를 통해 확인한 다음 swap off 이후 kubelet 을 재기동 한다. 2020.03.12 made by jaejun.lee","categories":[{"name":"Cloud","slug":"Cloud","permalink":"https://jx2lee.github.io/categories/Cloud/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://jx2lee.github.io/tags/Kubernetes/"}]},{"title":"[Cloud] R docker 컨테이너에서 Hadoop HDFS 연동","slug":"cloud-r_hadoop_connection","date":"2020-02-19T15:00:00.000Z","updated":"2020-04-10T02:58:19.324Z","comments":true,"path":"cloud-r_hadoop_connection/","link":"","permalink":"https://jx2lee.github.io/cloud-r_hadoop_connection/","excerpt":"본 포스트에서는 R Container 에서 Hadoop HDFS 데이터를 연동하는 과정을 다룬다.","text":"본 포스트에서는 R Container 에서 Hadoop HDFS 데이터를 연동하는 과정을 다룬다. R Container 기동R container image 를 이용해 컨테이너를 기동한다. docker run -d -p 8787:8787 -e PASSWORD=tmaxtmax rocker_with_java:latest 1234root@k8s-master:~# docker rename competent_dubinsky r-hadoop-testroot@k8s-master:/app# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESaa91a42054dc rocker_with_java:latest \"/init\" 23 minutes ago Up 23 minutes 0.0.0.0:8787-&gt;8787/tcp r-hadoop-test http://{NODE_IP}:8787 로 접속하여 rstudio/tmaxtmax 입력하여 Rstduio 화면으로 이동한다. rocker_with_java image 는 rocker-rstudio 이미지 보다 상위 이미지인 rocker/tidyverse 를 사용한 새로운 이미지다. 참고 R Container 환경설정Hadoop 연동을 위한 Hadoop client 와 java 를 설치한다. 로컬에 있는 hadoop client 와 java tar 파일들을 R container 로 옮긴다. docker cp hadoop-client.tar r-hadoop-test:/root docker cp jdk.tar r-hadoop-test:/root R container로 접속하여 hadoop client 를 설치한다 docker exec -it r-hadoop-test /bin/bash 명령어 이후 hadoop client 설치 설치 확인 1234567891011rstudio@4935816e695d:~$ java -versionjava version \"1.8.0_241\"Java(TM) SE Runtime Environment (build 1.8.0_241-b07)Java HotSpot(TM) 64-Bit Server VM (build 25.241-b07, mixed mode)rstudio@aa91a42054dc:~$ hdfs versionHadoop 2.10.0Subversion ssh://git.corp.linkedin.com:29418/hadoop/hadoop.git -r e2f1f118e465e787d8567dfa6e2f3b72a0eb9194Compiled by jhung on 2019-10-22T19:10ZCompiled with protoc 2.5.0From source with checksum 7b2d8877c5ce8c9a2cca5c7e81aa4026This command was run using /app/hadoop/2.10.0/share/hadoop/common/hadoop-common-2.10.0.jar Hadoop 연동을 위한 R package 설치Hadoop 연동을 위한 R 패키지를 설치한다. R Studio 에서 아래 커맨드를 이용해 패키지를 설치한다. HADOOP_CMD와 JAVA_HOME 은 각 환경에 맞는 PATH 로 변경하여 수행한다. 12345678library(devtools)library(rJava)install_github(c(\"RevolutionAnalytics/rmr2/pkg\", \"RevolutionAnalytics/plyrmr2/pkg\"))Sys.setenv(HADOOP_CMD=\"/app/hadoop/2.10.0/bin/hadoop\")Sys.setenv(JAVA_HOME=\"/usr/lib/jvm/java-8-openjdk-amd64\")install_github(\"RevolutionAnalytics/rhdfs/pkg\") Packeges 관리 화면에 설치가 되었는지 확인한다. R - Hadoop 연동R 환경변수를 설정하고 HDFS 에 접근하여 목록과 데이터 일부를 확인한다. rhdfs / rmr2 패키지를 활성화 시키고 hdfs 객체를 초기화한다. 1234library(rmr2)library(rhdfs)hdfs.init() hdfs.ls / hdfs.cat 함수를 이용해 디렉토리 목록과 데이터 일부를 확인한다. 1234567hdfs.ls(\"/user/spark\")hdfs.cat(\"/user/spark/test.csv\")&gt; hdfs.ls(\"/user/spark\") permission owner group size modtime file1 drwxr-xr-x spark supergroup 0 2020-02-19 05:02 /user/spark/.sparkStaging2 -rw-r--r-- rstudio supergroup 970491 2020-02-19 09:52 /user/spark/test.csv (참고) R - Hadoop 연동 (REST API 방식)이전의 방법은 R container 에 hadoop client 를 이용해 연동하였다. 또다른 방법으로써 hadoop 에서 제공하는 REST API 를 이용해 R과 연동하는 방법이다. R package httr 을 설치한다. install.packages(&quot;httr&quot;) hadoop URI 변수를 생성한다. 형태는 http://namenodedns:port/webhdfs/v1/user/username/myfile.csv?user.name=MYUSERNAME&amp;op=OPEN 으로 hadoop 설정에 맞게끔 변경한다. 123456hdfsUri=\"http://192.168.179.178:50070/webhdfs/v1\"fileUri=\"/user/spark/test.csv\"readParameter=\"?user.name=\"usernameParameter=\"spark&amp;\"optionnalParameters=\"op=OPEN\"uri &lt;- paste0(hdfsUri, fileUri, readParameter, usernameParameter, optionnalParameters) URI 형태로 데이터를 불러오고 상단 부분 (head) 을 확인한다. 12data = read.csv(uri)head(data)) Reference https://github.com/RevolutionAnalytics/RHadoop/wiki/Installing-RHadoop-on-RHEL https://niceguy1575.tistory.com/40 https://hub.docker.com/r/rocker/tidyverse https://github.com/gadenbuie/docker-tidyverse-rjava/blob/master/Dockerfile https://wikidocs.net/52630 2020.02.20 made by jaejun.lee","categories":[{"name":"Cloud","slug":"Cloud","permalink":"https://jx2lee.github.io/categories/Cloud/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://jx2lee.github.io/tags/Docker/"}]},{"title":"[Cloud] Elasticsearch 배포 on K8s","slug":"cloud-deploy_es","date":"2020-02-06T15:00:00.000Z","updated":"2020-09-24T14:35:37.607Z","comments":true,"path":"cloud-deploy_es/","link":"","permalink":"https://jx2lee.github.io/cloud-deploy_es/","excerpt":"toy project를 위해 es를 구축하려던 찰나, 사내에 K8s cluster를 구축하였다. 클라우드 공부 겸 Elasticsearch와 kibana를 K8s에 배포하는 과정을 다룬다","text":"toy project를 위해 es를 구축하려던 찰나, 사내에 K8s cluster를 구축하였다. 클라우드 공부 겸 Elasticsearch와 kibana를 K8s에 배포하는 과정을 다룬다 Elasticsearch 배포하기3-node로 구성된 elasticsearch 클러스터를 배포한다. 한 개의 노드로만 구성될 경우 장애가 발생하면 고가용성을 확보할 수 없으므로 이와 같이 3개 노드로 구성된 클러스트를 배포함으로써 split-brain을 피하고자 한다.Namespace 생성elastic_ns.yaml을 작성하고 namespace를 생성한다 kubectl create -f elastic_ns.yaml elastic_ns.yaml 1234kind: NamespaceapiVersion: v1metadata: name: elasticsearch Service 생성elastic_svc.yaml을 작성하고 service를 생성한다 kubectl create -f elastic_svc.yaml elastic_svc.yaml 12345678910111213141516kind: ServiceapiVersion: v1metadata: name: elastic-svc namespace: elastic labels: app: elasticsearchspec: type: NodePort selector: app: elasticsearch ports: - port: 9200 name: rest - port: 9300 name: inter-node kubectl get service -n {namespace-name}으로 서비스 생성을 확인한다 123root@k8s-master:~/jlee/elastic# kg svc -n elasticNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEelastic-svc NodePort 10.96.42.114 &lt;none&gt; 9200:30117/TCP,9300:30395/TCP 56m 기존 사이트에서 소개한 service와는 조금 다르게 작성하였다. 클러스터에 배보된 es node에 접속하기 위해서는 service의 타입을 NodePort로 설정하였다. NodePort로 설정하지 않는다면 이후에 curl 하는 명령이 connected refused 될 것이다, K8s Service 정리가 필요! StatefulSet 생성elastic_statefulset.yaml을 작성하고 StatefulSet을 생성한다 StatefulSet이란, 상태를 가지고 있는 Pod들을 관리하는 컨트롤러로 순서를 지정하여 Pod를 실행하고 volume을 지정하여 Pod가 내려가도 정보를 잃지 않게 한다 kubectl create -f elastic_statefulset.yaml elastic_statefulset.yaml 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677apiVersion: apps/v1kind: StatefulSetmetadata: name: es-cluster namespace: elastic # namepsace name for elasticsearchspec: serviceName: elasticsearch # service name for elasticsearch replicas: 3 selector: matchLabels: app: elasticsearch template: metadata: labels: app: elasticsearch spec: containers: - name: elasticsearch image: docker.elastic.co/elasticsearch/elasticsearch:7.5.2 resources: limits: cpu: 1000m requests: cpu: 100m ports: - containerPort: 9200 name: rest protocol: TCP - containerPort: 9300 name: inter-node protocol: TCP volumeMounts: - name: data mountPath: /usr/share/elasticsearch/data env: - name: cluster.name value: k8s-elastic - name: node.name valueFrom: fieldRef: fieldPath: metadata.name - name: discovery.seed_hosts value: \"es-cluster-0.elasticsearch,es-cluster-1.elasticsearch,es-cluster-2.elasticsearch\" # hostname for each container - name: cluster.initial_master_nodes value: \"es-cluster-0,es-cluster-1,es-cluster-2\" # node name in es-cluster - name: ES_JAVA_OPTS value: \"-Xms512m -Xmx512m\" initContainers: - name: fix-permissions image: busybox command: [\"sh\", \"-c\", \"chown -R 1000:1000 /usr/share/elasticsearch/data\"] securityContext: privileged: true volumeMounts: - name: data mountPath: /usr/share/elasticsearch/data - name: increase-vm-max-map image: busybox command: [\"sysctl\", \"-w\", \"vm.max_map_count=262144\"] securityContext: privileged: true - name: increase-fd-ulimit image: busybox command: [\"sh\", \"-c\", \"ulimit -n 65536\"] securityContext: privileged: true volumeClaimTemplates: - metadata: name: data labels: app: elasticsearch spec: accessModes: [ \"ReadWriteOnce\" ] storageClassName: rook-ceph-block # your storageclass resources: requests: storage: 10Gi kubectl get pods -n elastic으로 생성한 파드를 확인한다. 12345root@k8s-master:~/jlee/elastic# kgpo -n elasticNAME READY STATUS RESTARTS AGEes-cluster-0 1/1 Running 0 159mes-cluster-1 1/1 Running 0 158mes-cluster-2 1/1 Running 0 158m Check Status정상적으로 ES 노드가 배포되었는지 kubectl get svc -n elastic을 통해 확인된 포트로 (9200에 포트포워딩 된 포트 확인) curl 명령을 수행한다. curl http://{ip}:{port} 123456789101112131415161718root@k8s-master:~/jlee/elastic# curl http://192.168.179.172:30117&#123; \"name\" : \"es-cluster-2\", \"cluster_name\" : \"k8s-elastic\", \"cluster_uuid\" : \"GqGwbyKYSfGZoEcqFumVzw\", \"version\" : &#123; \"number\" : \"7.2.0\", \"build_flavor\" : \"default\", \"build_type\" : \"docker\", \"build_hash\" : \"508c38a\", \"build_date\" : \"2019-06-20T15:54:18.811730Z\", \"build_snapshot\" : false, \"lucene_version\" : \"8.0.0\", \"minimum_wire_compatibility_version\" : \"6.8.0\", \"minimum_index_compatibility_version\" : \"6.0.0-beta1\" &#125;, \"tagline\" : \"You Know, for Search\"&#125; Kibana 배포하기Kibana 배포에 경우 Service와 Deployment가 명시된 yaml 파일을 생성하고 배포하면 된다. 위와 같은 서비스 방식인 NodePort로 서비스를 배포하고 포트번호 5601만 명시해주면 노드포트 형식의 서비스가 포트 포워딩을 수행하여 웹에서 접속할 수 있다.kubectl create -f kibana.yaml kibana.yaml 1234567891011121314151617181920212223242526272829303132333435363738394041424344apiVersion: v1kind: Servicemetadata: name: kibana namespace: elastic labels: app: kibanaspec: type: NodePort ports: - port: 5601 selector: app: kibana---apiVersion: apps/v1kind: Deploymentmetadata: name: kibana namespace: elastic labels: app: kibanaspec: replicas: 1 selector: matchLabels: app: kibana template: metadata: labels: app: kibana spec: containers: - name: kibana image: docker.elastic.co/kibana/kibana:7.2.0 resources: limits: cpu: 1000m requests: cpu: 100m env: - name: ELASTICSEARCH_URL value: http://elasticsearch:9200 ports: - containerPort: 5601 Check Status정상적으로 Kibana 가 배포되었는지 kubectl get svc -n elastic을 통해 확인된 포트로 (9200에 포트포워딩 된 포트 확인) curl 명령을 수행한다. curl http://{ip}:{port} Reference How To Set Up an Elasticsearch, Fluentd and Kibana (EFK) Logging Stack on Kubernetes Elasticsearch와 Kibana, filebeat 를 활용한 쿠버네티스 로깅 아키텍쳐 2020.02.06 made by jaejun.lee","categories":[{"name":"Cloud","slug":"Cloud","permalink":"https://jx2lee.github.io/categories/Cloud/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://jx2lee.github.io/tags/Kubernetes/"}]},{"title":"[Cloud] rook-ceph을 이용한 Ceph cluster 구성","slug":"cloud-install_rook_ceph","date":"2020-02-05T15:00:00.000Z","updated":"2020-03-30T15:06:23.567Z","comments":true,"path":"cloud-install_rook_ceph/","link":"","permalink":"https://jx2lee.github.io/cloud-install_rook_ceph/","excerpt":"K8s 클러스터 내 rook-ceph을 이용한 Ceph cluster를 구성한다. Update Note 2020.03.30 : yaml 파일이 포함된 github 주소 추가","text":"K8s 클러스터 내 rook-ceph을 이용한 Ceph cluster를 구성한다. Update Note 2020.03.30 : yaml 파일이 포함된 github 주소 추가 Architecture rook-ceph은 Ceph 클러스터 및 다른 component들을 CRD(Custom Resource Definition)으로 관리하며 CRD의 변경사항을 Rook Operator를 이용해 일괄 적용할 수 있다 설치 순서아래 순서와 같이 설치를 진행 특정 위치에 git repository를 clone 한다. 이후 common.yaml, operator.yaml 을 이용해 rook-ceph에서 제공하는 CRD와 operator를 생성한다. 123git clone https://github.com/rook/rook.gitcd /rook/cluster/example/kubenetes/cephfskubectl apply -f common.yamlkubectl apply -f operator.yaml ceph_config_override.yaml 과 cluster.yaml 을 이용해 configmap을 생성하고 cluster를 구성한다. 12kubectl apply -f ceph_config_override.yamlkubectl apply -f cluster.yaml ceph_config_override.yaml 12345678910apiVersion: v1kind: ConfigMapmetadata: name: rook-config-override namespace: rook-cephdata: config: | [global] mon osd down out interval = &#123;osd_down_out_interval&#125; mon clock drift allowed = 0.2 cluster.yaml 12345678910111213141516171819202122232425262728293031323334353637apiVersion: ceph.rook.io/v1kind: CephClustermetadata: name: rook-ceph namespace: rook-cephspec: cephVersion: image: ceph/ceph:v14.2.4-20190917 allowUnsupported: true dataDirHostPath: /var/lib/rook skipUpgradeChecks: false mon: count: 1 # Recommendation: Use odd numbers (ex. 3, 5) dashboard: enabled: true ssl: true monitoring: enabled: false # Require Prometheus to be pre-installed rulesNamespace: rook-ceph network: hostNetwork: false rbdMirroring: workers: 0 mgr: modules: # The pg_autoscaler is only available on nautilus or newer. remove this if testing mimic. - name: pg_autoscaler enabled: true storage: useAllNodes: true # Apply ceph-osd to all nodes. useAllDevices: false deviceFilter: config: journalSizeMB: \"1024\" # This value can be removed for environments with normal sized disks (20 GB or larger) osdsPerDevice: \"1\" # This value can be overridden at the node or device level directories: - path: /var/lib/rook toolbox.yaml 을 이용해 ceph 클러스터 이용을 위한 client를 설치한다 kubectl apply -f toolbox.yaml block_pool.yaml 과 file_system.yaml을 이용해 block / file storage 배포 준비 12kubectl apply -f block_pool.yamlkubectl apply -f file_system.yaml block_pool.yaml 123456789apiVersion: ceph.rook.io/v1kind: CephBlockPoolmetadata: name: replicapool namespace: rook-cephspec: failureDomain: host replicated: size: 2 file_system.yaml 12345678910111213141516171819apiVersion: ceph.rook.io/v1kind: CephFilesystemmetadata: name: myfs namespace: rook-cephspec: metadataPool: # failureDomain - values are possible for 'osd' and 'host' failureDomain: host # ceph-osd must exist equal or more than replicated size replicated: size: 2 dataPools: # failureDomain - values are possible for 'osd' and 'host' - failureDomain: host # ceph-osd must exist equal or more than replicated size replicated: size: 2 metadataServer: activeCount: 1 activeStandby: true 마지막으로 block_sc.yaml 과 file_sc.yaml을 이용해 각 block / file storageclass를 생성한다 12kubectl apply -f block_sc.yamlkubectl apply -f file_sc.yaml block_sc.yaml 1234567891011121314151617181920212223242526272829apiVersion: storage.k8s.io/v1kind: StorageClassmetadata: name: rook-ceph-blockprovisioner: rook-ceph.rbd.csi.ceph.comparameters: # clusterID is the namespace where the rook cluster is running # If you change this namespace, also change the namespace below where the secret namespaces are defined clusterID: rook-ceph # Ceph pool into which the RBD image shall be created pool: replicapool # RBD image format. Defaults to \"2\". imageFormat: \"2\" # RBD image features. Available for imageFormat: \"2\". CSI RBD currently supports only `layering` feature. imageFeatures: layering # The secrets contain Ceph admin credentials. These are generated automatically by the operator # in the same namespace as the cluster. csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph # Specify the filesystem type of the volume. If not specified, csi-provisioner # will set default as `ext4`. csi.storage.k8s.io/fstype: ext4# uncomment the following to use rbd-nbd as mounter on supported nodes file_sc.yaml 1234567891011121314151617181920212223242526272829303132333435apiVersion: storage.k8s.io/v1kind: StorageClassmetadata: name: csi-cephfs-scprovisioner: rook-ceph.cephfs.csi.ceph.comparameters: # clusterID is the namespace where operator is deployed. clusterID: rook-ceph # CephFS filesystem name into which the volume shall be created fsName: myfs # Ceph pool into which the volume shall be created # Required for provisionVolume: \"true\" pool: myfs-data0 # Root path of an existing CephFS volume # Required for provisionVolume: \"false\" # rootPath: /absolute/path # The secrets contain Ceph admin credentials. These are generated automatically by the operator # in the same namespace as the cluster. csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph # (optional) The driver can use either ceph-fuse (fuse) or ceph kernel client (kernel) # If omitted, default volume mounter will be used - this is determined by probing for ceph-fuse # or by setting the default mounter explicitly via --volumemounter command-line argument. # mounter: kernelreclaimPolicy: DeletemountOptions: # uncomment the following line for debugging #- debug 위 단계를 거치고 난 뒤 rook-ceph namespace의 pod와 storageclass 를 확인한다 kubectl get pods -n rook-ceph 12345678910111213141516171819202122232425262728293031323334root@k8s-master:~/jlee/rook-ceph-master# kubectl get pods -n rook-cephNAME READY STATUS RESTARTS AGEcsi-cephfsplugin-7kz7v 3/3 Running 0 4d19hcsi-cephfsplugin-9rt7t 3/3 Running 0 4d19hcsi-cephfsplugin-dnggh 3/3 Running 0 4d19hcsi-cephfsplugin-provisioner-974b566d9-7k2rb 4/4 Running 0 4d19hcsi-cephfsplugin-provisioner-974b566d9-kxg2f 4/4 Running 0 4d19hcsi-cephfsplugin-xzt9b 3/3 Running 0 4d19hcsi-rbdplugin-2npvg 3/3 Running 0 4d19hcsi-rbdplugin-drzkp 3/3 Running 0 4d19hcsi-rbdplugin-hhsm5 3/3 Running 0 4d19hcsi-rbdplugin-provisioner-579c546f5-qprb8 5/5 Running 0 4d19hcsi-rbdplugin-provisioner-579c546f5-svhlw 5/5 Running 0 4d19hcsi-rbdplugin-qhsw6 3/3 Running 0 4d19hrook-ceph-mds-myfs-a-58ddc89fc8-s4f44 1/1 Running 0 4d19hrook-ceph-mds-myfs-b-85dc7c7cf4-x68lk 1/1 Running 0 4d19hrook-ceph-mgr-a-69df8d6794-glbjb 1/1 Running 0 4d19hrook-ceph-mon-a-7b9cb64846-zfbwf 1/1 Running 0 4d19hrook-ceph-mon-b-7fc7c8fbb4-75j9j 1/1 Running 0 4d19hrook-ceph-mon-c-6c59c89fbc-rn8nv 1/1 Running 0 4d19hrook-ceph-operator-7985c4b57d-8qtht 1/1 Running 0 4d19hrook-ceph-osd-0-55888686c-pf6wn 1/1 Running 0 4d19hrook-ceph-osd-1-f56d885d4-tnrmv 1/1 Running 0 4d19hrook-ceph-osd-2-68f99d999f-zlrl4 1/1 Running 0 4d19hrook-ceph-osd-3-7545f4df9b-ng4tf 1/1 Running 0 4d19hrook-ceph-osd-prepare-k8s-node1-msfs9 0/1 Completed 0 4d19hrook-ceph-osd-prepare-k8s-node2-z858m 0/1 Completed 0 4d19hrook-ceph-osd-prepare-k8s-node3-lwh4c 0/1 Completed 0 4d19hrook-ceph-osd-prepare-k8s-node4-w8rfw 0/1 Completed 0 4d19hrook-ceph-tools-8648fbb998-5q7v2 1/1 Running 0 4d19hrook-discover-85fzl 1/1 Running 0 4d19hrook-discover-djj97 1/1 Running 0 4d19hrook-discover-p7cwx 1/1 Running 0 4d19hrook-discover-zvn5f 1/1 Running 0 4d19h kubectl get storageclass 1234root@k8s-master:~# kubectl get storageclassNAME PROVISIONER AGEcsi-cephfs-sc rook-ceph.cephfs.csi.ceph.com 16hrook-ceph-block rook-ceph.rbd.csi.ceph.com 16h 위 언급한 yaml 파일은 github 에 올려 두었다. https://github.com/jaejuning/rook-ceph-deploy Ceph cluster 상태 확인설치가 완료되었다면 구축한 ceph cluster가 정상 구동되었는지 확인하여야 한다. toolbox pod 를 통해 pod 네임을 확인한다kubectl -n rook-ceph get pod -l &quot;app=rook-ceph-tools&quot; 123root@k8s-master:~# kubectl -n rook-ceph get pod -l \"app=rook-ceph-tools\"NAME READY STATUS RESTARTS AGErook-ceph-tools-8648fbb998-dzbbd 1/1 Running 0 15h 확인된 pod 네임을 통해 exec 명령어로 해당 컨테이너로 접속 kubectl exec -it -n rook-ceph [위 결과로 나온 pod NAME] -- /bin/bash Ceph cluster 상태 확인ceph -s 12345678910111213141516[root@k8s-node3 /] ceph -s cluster: id: 9d3a534e-797f-4659-af8d-4bfb5f60f76c health: HEALTH_OK services: mon: 1 daemons, quorum a (age 16h) mgr: a(active, since 15h) mds: myfs:1 &#123;0=myfs-a=up:active&#125; 1 up:standby-replay osd: 2 osds: 2 up (since 15h), 2 in (since 15h) data: pools: 3 pools, 24 pgs objects: 537 objects, 1.4 GiB usage: 53 GiB used, 45 GiB / 98 GiB avail pgs: 24 active+clean Ceph cluster disk 확인ceph df 1234567891011[root@k8s-node3 /] ceph dfRAW STORAGE: CLASS SIZE AVAIL USED RAW USED %RAW USED ssd 98 GiB 45 GiB 53 GiB 53 GiB 53.87 TOTAL 98 GiB 45 GiB 53 GiB 53 GiB 53.87 POOLS: POOL ID STORED OBJECTS USED %USED MAX AVAIL replicapool 1 1.4 GiB 509 1.4 GiB 3.50 19 GiB myfs-metadata 2 2.2 KiB 28 2.2 KiB 0 19 GiB myfs-data0 3 0 B 0 0 B 0 19 GiB RBD image 사용량rbd du -p replicapoll 12345678NAME PROVISIONED USED csi-vol-22712573-4815-11ea-9f90-aea9eb69a9f1 10 GiB 344 MiB csi-vol-6d1536b2-47fc-11ea-9f90-aea9eb69a9f1 10 GiB 300 MiB csi-vol-d38c964b-4814-11ea-9f90-aea9eb69a9f1 10 GiB 404 MiB csi-vol-d4745340-4814-11ea-9f90-aea9eb69a9f1 10 GiB 396 MiB csi-vol-d4937470-4814-11ea-9f90-aea9eb69a9f1 20 GiB 244 MiB csi-vol-d4a291d9-4814-11ea-9f90-aea9eb69a9f1 20 GiB 400 MiB &lt;TOTAL&gt; 80 GiB 2.0 GiB reclaimPolicy를 Retain으로 설정할 경우, pv를 지워도 RBD image가 ceph cluster에 남게 되는데, 이 경우에는 rbd ls, rbd rm 등을 통해 rbd 리스트를 확인하고 삭제해야 한다. Troubleshooting내 경우 클러스터의 한 노드에서 csi-rbdplugin pod가 생성되지 않고 CrashLoopBack 이 걸리는 현상이 발생하였다. 노드 문제를 해결하지 못하여 우회하는 방안으로 해당 노드에 파드가 설정되지 않게 taint 조건을 추가하여 문제를 해결하였다. kubectl taint nodes {csi-rbdplugin pod를 생성하지 못하는 노드} key=value:NoSchedule- 이후에 rook-ceph cluster를 재 구축하면 csi-rbdplugin이 정상 작동함을 확인할 수 있다. 2020.02.06 made by jaejun.lee","categories":[{"name":"Cloud","slug":"Cloud","permalink":"https://jx2lee.github.io/categories/Cloud/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://jx2lee.github.io/tags/Kubernetes/"}]},{"title":"[Cloud] Terminating State에 빠진 Namespace 삭제","slug":"cloud-delete_ns_at_terminating_state","date":"2020-02-03T15:00:00.000Z","updated":"2020-09-24T14:34:59.240Z","comments":true,"path":"cloud-delete_ns_at_terminating_state/","link":"","permalink":"https://jx2lee.github.io/cloud-delete_ns_at_terminating_state/","excerpt":"K8s namespace를 삭제하다보면 state가 Terminating이면서 get namespace 결과에 계속 남아있는 문제가 발생하는데, 이를 해결하는 방법을 다룬다. Updata Note 2020.03.13 : Advanced 추가 (shell script)","text":"K8s namespace를 삭제하다보면 state가 Terminating이면서 get namespace 결과에 계속 남아있는 문제가 발생하는데, 이를 해결하는 방법을 다룬다. Updata Note 2020.03.13 : Advanced 추가 (shell script) 문제 발생rook-ceph 네임스페이스를 생성하다 삭제하면서 아래와 같은 문제가 발생하였다 123456789$ kubectl get nsNAME STATUS AGEdefault Active 2d21histio-system Active 6h40mknative-serving Active 6h40mkube-node-lease Active 2d21hkube-public Active 2d21hkube-system Active 2d21hrook-ceph Terminating 16m Terminating 중인 네임스페이스를 한 번 더 지우는 명령어를 수행하면 에러가 발생한다. 1Error from server (Conflict): Operation cannot be fulfilled on namespaces \"rook-ceph\": The system is ensuring all content is removed from this namespace. Upon completion, this namespace will automatically be purged by the system. 문제 해결해당 Namespace의 yaml 파일을 살펴보면, .spec/finalizers 부분에 Kubernetes라 명시되어 있다. 이를 빈 공백으로 바꾸고 적용하는 순서로 진행한다. jq 패키지를 설치하고 (apt get install jq) 아래 명령어를 수행한다.kubectl get namespace $NAMESPACE -o json |jq &#39;.spec = {&quot;finalizers&quot;:[]}&#39; &gt; temp.json 명령어를 수행한 디렉토리에 temp.json이 생성되는데, 이를 yaml 파일로 적용할 것이다. 이때 필요한 Ip/port를 아래 명령어로 확인한다.kubectl proxy &amp; curl 명령어로 수정사항을 반영한다.curl -k -H &quot;Content-Type: application/json&quot; -X PUT --data-binary @temp.json http://127.0.0.1:8001/api/v1/namespaces/{Namespace-name}/finalize 12345678$ kubectl get namespacesNAME STATUS AGEdefault Active 2d21histio-system Active 6h43mknative-serving Active 6h43mkube-node-lease Active 2d21hkube-public Active 2d21hkube-system Active 2d21h (Advanced) Shell script이런 에러가 발생할 때마다 일일이 찾기 귀찮아서 쉘 스크립트 공부도 할 겸 deleteNS.sh 스크립트를 구현하였다. 123456789#! /bin/bash# delete namespacesNS=$1kubectl get namespace $NS -o json |jq '.spec = &#123;\"finalizers\":[]&#125;' &gt; temp.jsonkubectl proxy &amp;curl -k -H \"Content-Type: application/json\" -X PUT --data-binary @temp.json http://127.0.0.1:8001/api/v1/namespaces/$NS/finalizekill %1 &amp;&amp; rm tmp.json # proxy process 를 다운하고 tmp.json 파일을 삭제 Reference Delete Namespace Stuck At Terminating State, https://nasermirzaei89.net/2019/01/27/delete-namespace-stuck-at-terminating-state/ 2020.02.04 made by jaejun.lee","categories":[{"name":"Cloud","slug":"Cloud","permalink":"https://jx2lee.github.io/categories/Cloud/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://jx2lee.github.io/tags/Kubernetes/"}]},{"title":"[Cloud] K8s cluster node 추가 및 삭제","slug":"cloud-manage_node","date":"2020-01-28T15:00:00.000Z","updated":"2020-03-30T15:06:23.543Z","comments":true,"path":"cloud-manage_node/","link":"","permalink":"https://jx2lee.github.io/cloud-manage_node/","excerpt":"K8s 클러스터에 노드를 추가 및 삭제하는 과정을 다룬다. Update Node 2020.03.10 : kubectl drain 설명 추가","text":"K8s 클러스터에 노드를 추가 및 삭제하는 과정을 다룬다. Update Node 2020.03.10 : kubectl drain 설명 추가 상태 확인kubectl get nodes로 노드 상태를 확인한다 12345678root@k8s-master:~# kubectl get nodesNAME STATUS ROLES AGE VERSIONai.bips NotReady &lt;none&gt; 11m v1.15.3bigdata-svr Ready &lt;none&gt; 8m24s v1.15.3k8s-master Ready master 6d17h v1.15.3k8s-node1 Ready master 6d17h v1.15.3k8s-node2 Ready master 6d17h v1.15.3k8s-node3 Ready &lt;none&gt; 4m14s v1.15.3 k8s-node3 노드를 삭제하고 추가해보도록 하자 Delete (k8s-master) &amp; Reset (k8s-node3)마스터 노드에서 k8s-node3를 delete 한다 12$ kubectl delete node k8s-node3node \"k8s-node3\" deleted kubectl delete 말고 kubectl drain {node_name} 을 수행하면 이미 띄워져있는 해당 노드의 파드들을 클러스터 내 다른 노드로 이동시키는 명령이다. delete 보다 drain을 수행하여 클러스터에서 제외시키는 것이 관리에 더 용이할 것으로 보인다. 이후, 삭제한 노드에서 kubeadm을 통해 reset 한다. reset을 하게되면 이후 마스터 노드에서 k8s-node3가 삭제되었음을 확인한다 kubeadm reset reset 하지 않으면 이전 정보가 남아있어 추후에 join 수행 시 error 발생 In master node,kubeadm init 을 통해 생성된 토큰을 확인하기 위해서 3개 마스터 중 한 대 노드에서 token 을 확인한다 kubeadm token list 12345TOKEN TTL EXPIRES USAGES DESCRIPTION EXTRA GROUPS4e3bad.gzp017frj86g4ngi 23h 2020-01-30T01:53:56Z authentication,signing &lt;none&gt; system:bootstrappers:kubeadm:default-node-tokeneo7vb4.3ck3l5ja3ry78bef &lt;invalid&gt; 2020-01-23T08:03:36Z authentication,signing &lt;none&gt; system:bootstrappers:kubeadm:default-node-tokenm8470a.r8fo1tbwmdhb39eo 22h 2020-01-30T00:58:34Z authentication,signing &lt;none&gt; system:bootstrappers:kubeadm:default-node-tokensds92v.mk937ek75jygtrlo &lt;invalid&gt; 2020-01-22T10:03:36Z &lt;none&gt; Proxy for managing TTL for the kubeadm-certs secret &lt;none&gt; EXPIERS : invalid 하지 않는 토큰이 없는 경우, kubeadm token create(or generate)로 토큰을 설정한다. 만약 expired 되지 않았다면, join 명렁어의 토큰으로 사용한다 이후 hash 값이 필요하므로 다음 명령어를 통해 hash 값을 확인한다 openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2&gt;/dev/null | openssl dgst -sha256 -hex | sed &#39;s/^.* //&#39; In k8s-node3,k8s-node3에서 위 master 노드를 통해 확인한 값으로 join 을 수행한다 12345678$ kubeadm join 192.168.179.171:6443 --token 4e3bad.gzp017frj86g4ngi \\ --discovery-token-ca-cert-hash sha256:b141f77ea7c5749767bd7a1dfc54f256ef374969b08f660f1c131453ebed7091......This node has joined the cluster:* Certificate signing request was sent to apiserver and a response was received.* The Kubelet was informed of the new secure connection details. IP는 마스터 IP(주의 : 삼중화를 진행하였으므로, 3개 마스터 통신을 담당하는 VIP로 작성) port는 6443 추가를 완료하였다. 이후에 master 노드에서 kubectl get nodes 를 하면 하기 출력을 확인할 수 있다 123456NAME STATUS ROLES AGE VERSIONbigdata-svr Ready &lt;none&gt; 23m v1.15.3k8s-master Ready master 6d18h v1.15.3k8s-node1 Ready master 6d18h v1.15.3k8s-node2 Ready master 6d18h v1.15.3k8s-node3 Ready &lt;none&gt; 3m16s v1.15.3 # Check! SummaryK8s cluster에 노드를 추가하고 삭제하는 과정을 다뤘다. 노드를 삭제하고 삭제한 노드에서 리셋을 진행한 다음, 마스터에서의 token 및 hash value를 추가 노드에 join에 이용하였다 노드 삭제 kubectl delete node {node-name} 노드 리셋 kubeadm reset cluster token 확인 kubadm token list, expired 된 토큰 발견 시 kubeadm token create로 새 토큰 생성 hash값 확인 openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2&gt;/dev/null | openssl dgst -sha256 -hex | sed &#39;s/^.* //&#39; 2020.01.29 made by jaejun.lee","categories":[{"name":"Cloud","slug":"Cloud","permalink":"https://jx2lee.github.io/categories/Cloud/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://jx2lee.github.io/tags/Kubernetes/"}]},{"title":"[Shell] .DS_Store 삭제 script","slug":"shell-delete_ds_store","date":"2020-01-26T15:00:00.000Z","updated":"2020-04-01T14:18:05.613Z","comments":true,"path":"shell-delete_ds_store/","link":"","permalink":"https://jx2lee.github.io/shell-delete_ds_store/","excerpt":"Mac Finder로 파일을 탐색하다 보면 .DS_Store 이라는 파일이 생성된다. 성가시다! git을 사용할 때항상 .gitignore로 명시를 해야되며, 모든 폴더에 적용하고 push 한 경험이 있을거다. 쉘 공부하면서 간단한 스크립트를 이번 포스트에서 작성해본다. Update Note 2020.04.01 : 스크립트 수정 (변수 입력 및 usage)","text":"Mac Finder로 파일을 탐색하다 보면 .DS_Store 이라는 파일이 생성된다. 성가시다! git을 사용할 때항상 .gitignore로 명시를 해야되며, 모든 폴더에 적용하고 push 한 경험이 있을거다. 쉘 공부하면서 간단한 스크립트를 이번 포스트에서 작성해본다. Update Note 2020.04.01 : 스크립트 수정 (변수 입력 및 usage) .DS_Store?DS_STORE 파일이란 Desktop Services Store의 약자로 애플에서 정의한 파일 포맷이다. 애플의 맥 OS X 시스템이 폴더에 접근할 때 생기는 해당 폴더에 대한 메타데이터를 저장하는 파일이다. 윈도우의 thumb.db 파일과 비슷하다. 분석해보면 해당 디렉토리 크기, 아이콘의 위치, 폴더의 배경에 대한 정보들을 얻을 수 있다. 맥 OS 환경에서만 생성 및 사용되지만,파일을 공유하는 과정에서 이 파일도 같이 공유되는 경우가 있다. 출처 Shell script간단한 한 줄 짜리 명령으로 루트 디렉토리부터 삭제할 수 있다. 하지만 나는 이게 귀찮았다. 그리고 Finder로는 / 디렉토리까지 갈 일이 없고, /Users/jj 아래 하위 디렉토리에 생성되는 DS_Store 파일이 거슬렸다. hackerrank shell 문제를 푸는 디렉토리에 rm-ds-store.sh 파일을 생성하고 스크립트를 작성하였다 sudo와 find로 쉽게 작성할 수 있다. 쉘 실행 시 stdin으로 패스워드 입력값을 받아 sudo 명렁을 실행하고, 삭제되는 내역을 print 한다. 내용은 하기와 같다 12345678910111213#!/bin/bash# delete all .DS_Store file from path# Wed, 01.04.2020if [ \"$#\" -ne 1 ]; then echo \"usage : $0 &#123;path&#125;\" echo \"example : $0 /Users/jj\" exit 0fipath=$1sudo --stdin find $&#123;path&#125; -name \".DS_Store\" -print -deleteecho \".DS_Store clear in $&#123;path&#125; !\" Result./rm-ds-store.sh 명령어를 실행하면 Usage 를 확인할 수 있다. 정리하고자 하는 디렉토리를 삽입하면 된다. 123456789~/shell/custom❯ ./rm-ds-store.shusage : ./rm-ds-store.sh &#123;path&#125;example : ./rm-ds-store.sh /Users/jj~/shell/custom❯ ./rm-ds-store.sh /Users/jjPassword:.DS_Store clear in /Users/jj..! 삭제된 위치가 프린트하며 동시에 삭제하여 완료됨을 확인할 수 있다. 원래는 / 하위 디렉토리 모두 검사하였지만 굳이 검사할 필요가 없었다 (이유는 .DS_Store 의 정의를 생각하면 된다). 쉘 스크립트 공부하면서 실제로 써볼 수 있는 toy project 였고 더 활용할 수 있는 방안을 고민해야겠다. 인자값이 없으면 Usage 를 출력하고 원하는 디렉토리를 추가하여 하위 디렉토리를 모두 검색하는 스크립트로 수정하였다. 조금씩 더 활용할 방안을 생각하면서 수정할 계획이다. Reference .DS_STORE 파일이란, https://chp747.tistory.com/54 2020.01.27 made by jaejun.lee","categories":[{"name":"Shell","slug":"Shell","permalink":"https://jx2lee.github.io/categories/Shell/"}],"tags":[]},{"title":"[Cloud] Kubeflow 설치","slug":"cloud-install_kubeflow","date":"2020-01-22T15:00:00.000Z","updated":"2020-03-30T15:06:23.561Z","comments":true,"path":"cloud-install_kubeflow/","link":"","permalink":"https://jx2lee.github.io/cloud-install_kubeflow/","excerpt":"K8s cluster 위 Kubeflow를 설치하는 과정을 다룬다. K8s cluster 구축은 포스트 참고 Update Note 2020.02.25 : Trouble Shooting 추가 2020.03.13 : Kubeflow 삭제 추가, kfctl 버젼 명시 2020.03.18 : Dashboard UI 수정","text":"K8s cluster 위 Kubeflow를 설치하는 과정을 다룬다. K8s cluster 구축은 포스트 참고 Update Note 2020.02.25 : Trouble Shooting 추가 2020.03.13 : Kubeflow 삭제 추가, kfctl 버젼 명시 2020.03.18 : Dashboard UI 수정 K8s 환경Storageclass default 설정 확인storage class name default 설정을 위해 아래 커맨드라인을 수행 kubectl patch storageclass [storage class 명] -p &#39;{&quot;metadata&quot;: {&quot;annotations&quot;:{&quot;storageclass.kubernetes.io/is-default-class&quot;:&quot;true&quot;}}}&#39; 1234$ kubectl get sc #sc : storageclassNAME PROVISIONER AGEcsi-cephfs-sc rook-ceph.cephfs.csi.ceph.com 16hrook-ceph-block (default) rook-ceph.rbd.csi.ceph.com 16h Kubeflow 설치 파일 다운https://github.com/kubeflow/kubeflow/releases에서 최신 kfctl 바이너리를 다운받아 압축을 해제한다. 본인은 v1.0.1-0-gf3edb9b 을 사용하였다. Kubeflow 환경 설정.bashrc12345export KF_NAME=my-kubeflowexport BASE_DIR=/root/kubeflowexport KF_DIR=$&#123;BASE_DIR&#125;/$&#123;KF_NAME&#125;export CONFIG_FILE=$&#123;KF_DIR&#125;/kfctl_k8s_istio.0.7.0.yamlexport CONFIG_URI=\"https://raw.githubusercontent.com/kubeflow/manifests/v0.7-branch/kfdef/kfctl_k8s_istio.0.7.0.yaml\" kfctl_k8s_istio.0.7.0.yamlwget -O kfctl_k8s_istio.0.7.0.yaml $CONFIG_URI Kubeflow deployBinary 이동123cd $&#123;BASE_DIR&#125;chmod 755 kfctlmv kfctl /usr/bin 설치12cd $KF_DIRkfctl apply -V -f $&#123;CONFIG_FILE&#125; Access Kubeflow UIhttp://{NODE_IP}:31380로 접속한다 Master가 많은 경우에 centraldashboard pod 를 확인하여, 해당 host ip를 사용하면 접속이 가능하다 (Master, Worker IP로 접속이 모두 가능) Service 를 NodePort 로 설정하였기때문에 클러스터 내 모든 노드의 IP로 접근이 가능하다. Truuble ShootingArtifacts 또는 Executions 탭에서 error mysql_query failed errno 2006해당 탭으로 이동하면 mysql_query failed 에러가 발생하는 경우가 있다. 이때에는 metadata-grpc-deployment pod 를 재시작 하면 된다. kubectl get pod {pod_name} -n kubeflow -o yaml | kubectl replace --force -f- Kubeflow 삭제Kubeflow 를 삭제하는 방법은 아래 kfctl delete 명령어를 사용한다. 12cd $&#123;KF_DIR&#125;kfctl delete -f $&#123;CONFIG_FILE&#125; 명령어 수행 후 kubectl get all -n kubeflow 로 모든 내용이 삭제되었는-f 지 확인한다. 12 root@k8s-master:~/kubeflow/my-kubeflow# kubectl get all -n kubeflowNo resources found. 2020.01.23 made by jaejun.lee","categories":[{"name":"Cloud","slug":"Cloud","permalink":"https://jx2lee.github.io/categories/Cloud/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://jx2lee.github.io/tags/Kubernetes/"}]},{"title":"[Cloud] K8s cluster 구축","slug":"cloud-install_k8s","date":"2020-01-21T15:00:00.000Z","updated":"2020-09-26T13:39:01.939Z","comments":true,"path":"cloud-install_k8s/","link":"","permalink":"https://jx2lee.github.io/cloud-install_k8s/","excerpt":"K8s cluster는 Master 3대(k8s-master/k8s-node1/k8s-node2) / Worker 2대(k8s-node3/k8s-node4)로 구성한다. k8s는 1.15.3 version 이며 서버는 ubuntu 18.04 이다. Update Note 2020.02.25 : kubeadm init 시 --upload-certs 옵션 부가 설명 2020.03.12 : Calico CNI 설정 추가 2020.03.17 : Trouble shooting 추가 - Calico node not working 2020.06.04 : Taint NoSchedule 변경 (/ to =) 2020.09.26 : VIP 포트 추가 설명","text":"K8s cluster는 Master 3대(k8s-master/k8s-node1/k8s-node2) / Worker 2대(k8s-node3/k8s-node4)로 구성한다. k8s는 1.15.3 version 이며 서버는 ubuntu 18.04 이다. Update Note 2020.02.25 : kubeadm init 시 --upload-certs 옵션 부가 설명 2020.03.12 : Calico CNI 설정 추가 2020.03.17 : Trouble shooting 추가 - Calico node not working 2020.06.04 : Taint NoSchedule 변경 (/ to =) 2020.09.26 : VIP 포트 추가 설명 공통모든 노드에 공통으로 수행한다. 123456789apt-get install -y apt-transport-https curlapt-get updatecurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - # Docker 공식 GPG keysudo apt-key fingerprint 0EBFCD88sudo add-apt-repository \\ \"deb [arch=amd64] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) \\ stable\"apt-get install -y docker-ce 1.15.3 버젼에 맞는 kubelet / kubeadm / kubectl 를 설치한다. 1234curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -echo deb http://apt.kubernetes.io/ kubernetes-xenial main &gt; /etc/apt/sources.list.d/kubernetes.listapt-get updateapt-get install -y kubeadm=1.15.3-00 kubelet=1.15.3-00 kubectl=1.15.3-00 Kubernets는 swap를 off시켜야 작동하므로 swap 메모리를 끈다. 1234freeswapoff -avi /etc/fstab# 재부팅시 자동으로 swapoff 하려면 위 파일에서 swap 부분 주석 처리 방화벽을 내려준다 (ubnut : ufw) 12systemctl stop ufw systemctl disable ufw /etc/docker/daemon.json 의 파일을 수정하고 도커 데몬을 재실행한다 1234567891011cat &gt; /etc/docker/daemon.json &lt;&lt;EOF&#123;\"exec-opts\": [\"native.cgroupdriver=systemd\"], \"log-driver\": \"json-file\",\"log-opts\": &#123;\"max-size\": \"100m\" &#125;,\"storage-driver\": \"overlay2\", \"insecure-registries\": [\"192.168.179.185:5000\"]&#125;EOFsystemctl daemon-reloadsystemctl restart docker insecure-registries 는 VIP(virtual ip)를 나타내는데, 삼중화 했을 때 마스터 간 통신을 위한 ip. 뒤에 포트 5000은 후에 마스터 부분에 설치할 keepalived 에서 사용한다 VIP Port: kubernetes API Server 통신에 사용하는 default port 위 커맨드 라인을 정리한 script는 다음과 같다. 해당 스크립트를 작성하여 5개 노드에서 모두 실행한다 1234567891011121314151617181920212223242526272829303132333435#!/bin/bash# install basic package &amp; docker-ce &amp; k8s utilsapt-get install -y apt-transport-https curlapt-get updatecurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - # Docker 공식 GPG keysudo apt-key fingerprint 0EBFCD88sudo add-apt-repository \\ \"deb [arch=amd64] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) \\ stable\"apt-get install -y docker-cecurl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -echo deb http://apt.kubernetes.io/ kubernetes-xenial main &gt; /etc/apt/sources.list.d/kubernetes.listapt-get updateapt-get install -y kubeadm=1.15.3-00 kubelet=1.15.3-00 kubectl=1.15.3-00freeswapoff -aufw disableufw statuscat &gt; /etc/docker/daemon.json &lt;&lt;EOF&#123;\"exec-opts\": [\"native.cgroupdriver=systemd\"], \"log-driver\": \"json-file\",\"log-opts\": &#123;\"max-size\": \"100m\" &#125;,\"storage-driver\": \"overlay2\", \"insecure-registries\": [\"192.168.179.185:5000\"]&#125;EOFsystemctl daemon-reload &amp;&amp; echo \"restart docker-daemon\"systemctl restart docker &amp;&amp; echo \"restart docker\" Master NodeKeepalived 설치12apt-get install -y keepalivedvi /etc/keepalived/keepalived.conf keepalived.conf 123456789101112131415vrrp_instance VI_1 &#123; state BACKUP interface enp0s8 virtual_router_id 50 priority 100 # 이후 마스터부터 1씩 감소하여 수정 advert_int 1 nopreempt authentication &#123; auth_type PASS auth_pass $ place secure password here. &#125; virtual_ipaddress &#123; 192.168.179.185 &#125;&#125; interface : ifconfig -a로 확인 priority : master 마다 다른 값 설정 priority 값이 높으면 최우선적으로 master 역할 수행 100, 99, 98 로 설정 virtual_ipaddress : 앞전에 docker daemon의 vip 주소 기입 VIP 이어도 아무 ip나 사용하면 혹여나 충돌이 일어날까봐 할당받은 ip 사용 keepalived 서비스 재시작 후 상태($ ip addr)를 확인한다 $ systemctl restart keepalived &amp;&amp; systemctl status keepalived 123456783: enp6s0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UP group default qlen 1000 link/ether 64:e5:99:fa:52:c1 brd ff:ff:ff:ff:ff:ff inet 192.168.179.172/24 brd 192.168.179.255 scope global enp6s0 valid_lft forever preferred_lft forever inet 192.168.179.185/32 scope global enp6s0 valid_lft forever preferred_lft forever inet6 fe80::66e5:99ff:fefa:52c1/64 scope link valid_lft forever preferred_lft forever VIP로 설정한 192.168.179.171 이 보이는 것을 확인 // 하나의 master에만 보임 (나머지는 stand by) K8s 설치kubeadm-config.yaml 12345678910apiVersion: kubeadm.k8s.io/v1beta2kind: ClusterConfigurationkubernetesVersion: \"v1.15.3\"controlPlaneEndpoint: \"192.168.179.185:6443\"networking: serviceSubnet: \"10.96.0.0/16\" podSubnet: \"10.244.0.0/16\"apiServer: extraArgs: advertise-address: \"192.168.179.185” 위 Yaml 파일은 첫 번째 master에서만 작성 후 init 을 수행한다. v1.15.3으로 작성한다. controlPlaneEndpoint와 advertise-address 는 VIP이고 포트로 6443으로 지정 Yaml 파일을 이용해 클러스터 초기화를 진행한다. $ kubeadm init --config=kubeadm-config.yaml --upload-certs 1234567891011121314151617181920212223242526Your Kubernetes control-plane has initialized successfully!To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/configYou should now deploy a pod network to the cluster.Run \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/You can now join any number of the control-plane node running the following command on each as root: kubeadm join 192.168.179.185:6443 --token iicz2g.0a8b07vasikwuthz \\ --discovery-token-ca-cert-hash sha256:249ee21a200d807f21dad0102eb638e50904102c7e7ae8f6388c1654f60ae1a0 \\ --control-plane --certificate-key 553c75881e6825bf9e5f3887b2100de4a3d979777a313b70281c5bbe5ddeef80Please note that the certificate-key gives access to cluster sensitive data, keep it secret!As a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use \"kubeadm init phase upload-certs --upload-certs\" to reload certs afterward.Then you can join any number of worker nodes by running the following on each as root:kubeadm join 192.168.179.185:6443 --token iicz2g.0a8b07vasikwuthz \\ --discovery-token-ca-cert-hash sha256:249ee21a200d807f21dad0102eb638e50904102c7e7ae8f6388c1654f60ae1a0 *–upload-certs 옵션은 Master 이중화 시 \bkey 인증을 init 명령과 수행해주는 역할을 한다. \b이 옵션을 주지 않으면 이중화 대상 Master 에 key 인증을 수행해야 한다.* 서버 주소 및 인증서를 복사하는 단계로 아래 명령어를 수행한다. 123mkdir -p $HOME/.kubecp -i /etc/kubernetes/admin.conf $HOME/.kube/configchown $(id -u):$(id -g) $HOME/.kube/config $ kubectl get pods --all-namespaces 명령어로 아래와 같은 파드가 생성되기 기다린다. 123456789root@k8s-master:~# kgpo --all-namespacesNAMESPACE NAME READY STATUS RESTARTS AGEkube-system coredns-5c98db65d4-9fpzf 0/1 Pending 0 3m2skube-system coredns-5c98db65d4-vn8d5 0/1 Pending 0 3m2skube-system etcd-k8s-master 1/1 Running 0 119skube-system kube-apiserver-k8s-master 1/1 Running 0 2m14skube-system kube-controller-manager-k8s-master 1/1 Running 0 2m21skube-system kube-proxy-c4cnn 1/1 Running 0 3m1skube-system kube-scheduler-k8s-master 1/1 Running 0 2m7s coredns 는 CNI를 설치해야 Running 상태가 된다. calico yaml을 다운받아 CALICO_IPV4POOL_CIDR 값을 10.244.0.0/16 으로 변경한 후 CNI 를 설치한다. $ kubectl apply -f kube-calico.yaml CALICO_IPV4POOL_CIDR 값을 10.244.0.0/16 으로 변경되어있는지 확인한 후 배포한다. \b클러스터 구성한 노드 IP 대역과 같으면 충돌이 일어난다고 한다. 1234567891011root@k8s-master:~# kgpo --all-namespacesNAMESPACE NAME READY STATUS RESTARTS AGEkube-system calico-kube-controllers-56cd854695-65j42 1/1 Running 0 89skube-system calico-node-ptq8x 1/1 Running 0 89skube-system coredns-5c98db65d4-9fpzf 1/1 Running 0 7m29skube-system coredns-5c98db65d4-vn8d5 1/1 Running 0 7m29skube-system etcd-k8s-master 1/1 Running 0 6m26skube-system kube-apiserver-k8s-master 1/1 Running 0 6m41skube-system kube-controller-manager-k8s-master 1/1 Running 0 6m48skube-system kube-proxy-c4cnn 1/1 Running 0 7m28skube-system kube-scheduler-k8s-master 1/1 Running 0 6m34s kubeadm init 시 생성된 커맨드를 다른 마스터 노드에서 실행하여 Join 한다. 123kubeadm join 192.168.179.185:6443 --token iicz2g.0a8b07vasikwuthz \\ --discovery-token-ca-cert-hash sha256:249ee21a200d807f21dad0102eb638e50904102c7e7ae8f6388c1654f60ae1a0 \\ --control-plane --certificate-key 553c75881e6825bf9e5f3887b2100de4a3d979777a313b70281c5bbe5ddeef80 서버 주소 및 인증서를 복사하는 명령어를 수행한다. 123mkdir -p $HOME/.kubecp -i /etc/kubernetes/admin.conf $HOME/.kube/configchown $(id -u):$(id -g) $HOME/.kube/config Worker Nodekubeadm init 시 나온 토큰과 해쉬값으로 각 워커 노드에서 Join 한다. 12kubeadm join 192.168.179.185:6443 --token iicz2g.0a8b07vasikwuthz \\ --discovery-token-ca-cert-hash sha256:249ee21a200d807f21dad0102eb638e50904102c7e7ae8f6388c1654f60ae1a0 설치 확인Kubectl get nodeskubectl get nodes -o widemaster node에서 확인하면 클러스터에 포함된 노드들을 확인할 수 있다 123456NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIMEk8s-master Ready master 44m v1.15.3 192.168.179.172 &lt;none&gt; Ubuntu 18.04.4 LTS 5.3.0-28-generic docker://19.3.8k8s-node1 Ready master 41m v1.15.3 192.168.179.173 &lt;none&gt; Ubuntu 18.04.3 LTS 4.15.0-88-generic docker://19.3.6k8s-node2 Ready master 40m v1.15.3 192.168.179.174 &lt;none&gt; Ubuntu 18.04.3 LTS 4.15.0-88-generic docker://19.3.6k8s-node3 Ready &lt;none&gt; 40m v1.15.3 192.168.179.175 &lt;none&gt; Ubuntu 18.04.3 LTS 4.15.0-88-generic docker://19.3.8k8s-node4 Ready &lt;none&gt; 40m v1.15.3 192.168.179.176 &lt;none&gt; Ubuntu 18.04.3 LTS 4.15.0-88-generic docker://19.3.8 (선택사항) Master node에도 pod 배포가 가능한 상태로 변환master에 Pod를 배포할 수 있는 상태로 변환하기 위해 taint 조건을 해제한다. kubectl taint nodes {node-name} node-role.kubernetes.io=master:NoSchedule- 만약 노드를 리스케쥴 되지 않게 복구하려면 kubectl taint nodes {node-name} node-role.kubernetes.io/master:NoSchedule Trouble-Shootingcalico node 가 Running 상태이지만 Unhealthy 문제https://github.com/projectcalico/calico/issues/2904 문제와 같다. calico-node 가 클러스터 내 노드 수만큼 기동하지만 0/1 Running 상태였다. 문제가 있는 파드를 describe 해본 결과 다음과 같다. 123456789101112131415161718192021222324252627282930313233343536Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Pulled 45m kubelet, k8s-master Container image \"calico/cni:v3.9.5\" already present on machine Normal Created 45m kubelet, k8s-master Created container upgrade-ipam Normal Started 45m kubelet, k8s-master Started container upgrade-ipam Normal Scheduled 45m default-scheduler Successfully assigned kube-system/calico-node-v5rgk to k8s-master Normal Started 45m kubelet, k8s-master Started container install-cni Normal Pulled 45m kubelet, k8s-master Container image \"calico/cni:v3.9.5\" already present on machine Normal Created 45m kubelet, k8s-master Created container install-cni Normal Started 45m kubelet, k8s-master Started container flexvol-driver Normal Pulled 45m kubelet, k8s-master Container image \"calico/pod2daemon-flexvol:v3.9.5\" already present on machine Normal Created 45m kubelet, k8s-master Created container flexvol-driver Normal Pulled 45m kubelet, k8s-master Container image \"calico/node:v3.9.5\" already present on machine Normal Created 45m kubelet, k8s-master Created container calico-node Normal Started 45m kubelet, k8s-master Started container calico-node Warning Unhealthy 30m (x34 over 45m) kubelet, k8s-master Readiness probe failed: calico/node is not ready: felix is not ready: readiness probe reporting 503 Warning Unhealthy 25m (x76 over 45m) kubelet, k8s-master Readiness probe failed: calico/node is not ready: felix is not ready: Get http://localhost:9099/readiness: dial tcp 127.0.0.1:9099: connect: connection refused Warning Unhealthy 15m (x116 over 45m) kubelet, k8s-master Liveness probe failed: calico/node is not ready: Felix is not live: Get http://localhost:9099/liveness: dial tcp 127.0.0.1:9099: connect: connection refused Warning FailedMount 12m kubelet, k8s-master MountVolume.SetUp failed for volume \"calico-node-token-9j8gt\" : couldn't propagate object cache: timed out waiting for the condition Normal SandboxChanged 12m kubelet, k8s-master Pod sandbox changed, it will be killed and re-created. Normal Pulled 12m kubelet, k8s-master Container image \"calico/cni:v3.9.5\" already present on machine Normal Started 12m kubelet, k8s-master Started container upgrade-ipam Normal Created 12m kubelet, k8s-master Created container upgrade-ipam Normal Started 12m (x2 over 12m) kubelet, k8s-master Started container install-cni Normal Pulled 12m (x2 over 12m) kubelet, k8s-master Container image \"calico/cni:v3.9.5\" already present on machine Normal Created 12m (x2 over 12m) kubelet, k8s-master Created container install-cni Normal Started 12m kubelet, k8s-master Started container flexvol-driver Normal Pulled 12m kubelet, k8s-master Container image \"calico/pod2daemon-flexvol:v3.9.5\" already present on machine Normal Created 12m kubelet, k8s-master Created container flexvol-driver Normal Pulled 12m kubelet, k8s-master Container image \"calico/node:v3.9.5\" already present on machine Normal Created 12m kubelet, k8s-master Created container calico-node Normal Started 12m kubelet, k8s-master Started container calico-node Warning Unhealthy 11m (x2 over 12m) kubelet, k8s-master Readiness probe failed: calico/node is not ready: felix is not ready: readiness probe reporting 503 Warning Unhealthy 7m34s (x19 over 12m) kubelet, k8s-master Liveness probe failed: calico/node is not ready: Felix is not live: Get http://localhost:9099/liveness: dial tcp 127.0.0.1:9099: connect: connection refused Warning Unhealthy 2m30s (x38 over 12m) kubelet, k8s-master Readiness probe failed: calico/node is not ready: felix is not ready: Get http://localhost:9099/readiness: dial tcp 127.0.0.1:9099: connect: connection refused 너무 길어 Event 부분만 작성하였다. 좀 더 deep 한 로그를 찾고자 해당 파드 log 를 찾아본 결과 다음과 같다. 12342019-10-03 04:34:38.296 [WARNING][16449] int_dataplane.go 781: failed to wipe the XDP state error=failed to load BPF program (/tmp/felix-bpf-824808941): stat /sys/fs/bpf/calico/xdp/prefilter_v1_calico_tmp_A: no such file or directorylibbpf: Error in bpf_object__probe_name():Operation not permitted(1). Couldn't load basic 'r0 = 0' BPF program.libbpf: failed to load object '/tmp/felix-bpf-824808941'Error: failed to load object file 위 깃헙 이슈 링크를 확인해보면 OS 커널단에서 BPF 관련 오브젝트를 읽어드리지 못한 상태였다. 이슈 내용들을 살펴보니 OS secure boot 한 경우 생길 수 있다하며 $ mokutil --disable-validation 명령어로 안전 모드로 부팅하지 못하게 설정하고 재부팅 하였다. 재부팅 시 명령어로 설정한 패스워드로 secure-mode boot 을 disabled 로 설정하고 재부팅 하니 calico node 가 문제없이 기동됨을 확인하였다. 2020.01.22 made by jaejun.lee","categories":[{"name":"Cloud","slug":"Cloud","permalink":"https://jx2lee.github.io/categories/Cloud/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://jx2lee.github.io/tags/Kubernetes/"}]},{"title":"[Cloud] Kubeflow overview","slug":"cloud-introduction_to_kubeflow","date":"2020-01-12T15:00:00.000Z","updated":"2020-04-10T02:58:26.204Z","comments":true,"path":"cloud-introduction_to_kubeflow/","link":"","permalink":"https://jx2lee.github.io/cloud-introduction_to_kubeflow/","excerpt":"Kubeflow 는 Kubernetes 위에서 동작하는 ML toolkit 이자, ML 파이프 라인을 구축하고 실험하는 data scientist를 위한 플랫폼이다. 머신러닝 시스템 개발, 테스트 및 프로덕션 수준의 서비스를 위해 다양한 환경에 배포하려는 머신러닝 엔지니어 및 운영 팀을 위한 것이다. 포스트는 Conceptual overview, ML workflow, Kuberflow Components, interface, example 순서로 작성하였다. kubeflow 0.7.0 version 기준으로 작성하였다","text":"Kubeflow 는 Kubernetes 위에서 동작하는 ML toolkit 이자, ML 파이프 라인을 구축하고 실험하는 data scientist를 위한 플랫폼이다. 머신러닝 시스템 개발, 테스트 및 프로덕션 수준의 서비스를 위해 다양한 환경에 배포하려는 머신러닝 엔지니어 및 운영 팀을 위한 것이다. 포스트는 Conceptual overview, ML workflow, Kuberflow Components, interface, example 순서로 작성하였다. kubeflow 0.7.0 version 기준으로 작성하였다 Conceptual overviewKubernetes 위 ML 시스템의 구성 요소를 배치한 그림은 다음과 같다. Kubeflow는 ML 시스템을 배포, 확장 및 관리하기 위한 플랫폼으로 Kubernetes 기반으로 구성한다. Kubeflow interface 를 통해 ML tools을 지정할 수 있고 클라우드 뿐 아니라 on-premise 에 동일한 worflow를 배포할 수 있어 특정 플랫폼에 종속되지 않는다. 각 구성 요소들이 어떤 역할을 하는지는 뒤 kubeflow Components에서 살펴본다. ML workflow많은 사람들의 선입견 중 하나가 머신러닝 모델 개발이 ML 시스템의 대부분을 차지할 것이라 생각한다. 하지만, 실제 모델을 개발하는 시간 보다 데이터 탐색부터 데이터 분석, 그리고 개발된 모델을 반복적으로 학습하며 튜닝하는 시간이 훨씬 길다. 즉, ML 시스템 개발은 반복적인 프로세스로, workflow의 각 단계를 평가하고 최고의 퍼포먼스를 낼 수 있게 모델 및 파라미터 변경 사항을 적용해야 한다. 아래는 실험 및 생산(실제 배포) 관점에서의 workflow를 나타낸다. 각 단계별 내용은 다음과 같다. Experimental phase : 실험 단계 Identify problem and collect and analyse data : ML 시스템으로 해결하고자 하는 문제를 식별하고 학습 훈련을 위한 데이터를 수집하고 분석한다 Choose an ML algorithm and code your model : 사용하고자 하는 ML Framework 및 알고리즘을 선별하고 모델 초기 버젼을 코딩한다 Experiment with data and model training : 앞서 준비된 데이터와 모델 코드를 통해 학습을 진행한다 Tune the model hyperparameters : 모델 결과에 영향을 주는 hyperparameter를 조정하며 학습을 반복적으로 진행한다 (이후에는 반복적인 parameter 튜닝과 학습을 진행한다) Production phase : 생산 단계(배포 단계) Transform data : 학습과 예측에 필요한 데이터를 변환한다. 이때, 모델 정합성을 위해 위 실험 단계에서 진행한 데이터와 같은 형태로 변환해야 함을 주의한다 Train model : 모델을 학습한다 Serve the model for online/batch prediction : 온라인 또는 배치 모드를 위한 모델을 제공한다 Monitor the model’s performance : 모델 성능을 모니터링한다. 이를 통해 모델을 수정하고 재 학습을 진행하여 모델 성능을 높여간다 Kubeflow components in the ML workflow위에 설명한 ML workflow에 Kubeflow 컴포넌트를 대입한 그림이다. 각 컴포넌트들을 experiment/production 별 나누어 설명한다. Experiment Pytorch / scikit-learn / Tensorflow / XGBoost : ML 알고리즘을 제공하는 패키지. 가장 유명한 Tensorflow 부터 손쉽게 ML 모델을 생성할 수 있는 scikit-learn 까지, 현재 진행형으로 다양한 ML 알고리즘 패키지를 지원하고 있다. Jupyter notebook / Fairing / pipelines Jupyter notebook : web 기반 파이선 인터프리터로, 인터렉티브한 환경을 제공하며 데이터 분석에 많이 사용하는 tool Fairing : Kubeflow에서 ML 모델을 쉽게 학습하고 배포할 수 있는 Python 패키지 pipelines : Kubeflow의 pipeline은 ML workflow의 모든 구성 요소를 설명하는 개념이다. 헷갈릴 수 있겠지만, ML workflow의 모든 과정을 담은 것으로 인지하고 workflow를 실행하는데 필요한 입력값 &amp; 구성 요소의 모든 입출력에 대한 정의를 포함하고 있다 (공식 Doc에는 pipeline을 Kubeflow 안의 플랫폼이라 표현한다). pipeline은 다음 기능들을 포함하고 있다. ML workflow을 추적하고 관리하는 UI 다중 ML workflow scheduling ML workflow를 정의하고 실행하기 위한 SDK (python) SDK를 이용해 ML system과 연결하는 Notebook [참고 - pipeline architecture] Katib : ML 모델의 Hyper parameter 및 아키텍쳐를 자동으로 튜닝하는 kubeflow의 컴포넌트 (Hyperparameter란, 모델 학습 과정을 제어하는 변수로 learning rate / neural network의 layer 수 / layer 내 node 수 등이 있다) Prodiction Chainer / MPI MXNet / PyTorch / TFJob Chainer : CUDA(GPU), 다양한 딥러닝 아키텍쳐를 지원하는 유연한 딥러닝 프레임워크 MPI : ? MXNet / Pytorch / TFJob : 오픈소스 딥러닝 소프트웨어 프레임워크로 Deep Neural Network를 학습 및 배포 KFServing / NVDIA TensorRT / PyTorch / TFServing / Seldon : 학습된 모델을 실제 배포할 때 사용하는 컴포넌트로, 크게 KFServing 과 Seldon 을 이용해 프레임워크를 배포. 각 컴포넌트들에 대한 내용은 방대하여 공식 Doc 참조 Metadata / TensorBoard Metadata : 모델, 모델 실행, 데이터 셋 등 기타 Artifact에 대한 정보를 의미하는 컴포넌트 (Artifact란, ML workflow 구성 요소의 input / output을 형성하는 file 또는 오브젝트) TensorBoard : Tensorflow가 포함하는 graph visualization tool #Kubeflow interface Kubeflow는 사용자 인터페이스와 커맨드라인 인터페이스를 모두 제공한다. User interface 배포된 컴포넌트에 액세스하는데 사용할 수 있는 대시 보드를 제공한다. (참고) Command line interface Kfctl 은 Kubeflow를 설치 및 구성하는데 사용할 수 있는 Kubeflow CLI. (참고) 공식 도큐먼트를 참고해 전체적인 개념을 살펴보았다. 다음 포스트에는 Kubernetes 클러스터에 Kubeflow를 설치하는 과정을 다룬다. Reference Kubeflow org, https://www.kubeflow.org/docs/about/kubeflow/ 쿠버네티스 기반의 End2End 머신러닝 플랫폼 Kubeflow #1 - 소개, https://bcho.tistory.com/1301 2020.01.13 made by jaejun.lee","categories":[{"name":"Cloud","slug":"Cloud","permalink":"https://jx2lee.github.io/categories/Cloud/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://jx2lee.github.io/tags/Kubernetes/"}]},{"title":"[Database] Install ElasticSearch using Docker","slug":"cloud-install_elasticsearch","date":"2020-01-07T15:00:00.000Z","updated":"2020-09-26T13:36:15.963Z","comments":true,"path":"cloud-install_elasticsearch/","link":"","permalink":"https://jx2lee.github.io/cloud-install_elasticsearch/","excerpt":"데이터 마트를 구축하고 이를 시각화하는 파이프라인 구축 pilot을 수행하기 위해 ElasticSearch를 설치하고자 한다. 단일 서버에 싱글 노드로 구축하고 binary 설치를 하려고 했지만 요즘 추세에 맞게(?) Container 기반으로 구축하고자 Docker를 이용한다. Updata Note 2020.05.10 : Elasticsearch 용어 정리","text":"데이터 마트를 구축하고 이를 시각화하는 파이프라인 구축 pilot을 수행하기 위해 ElasticSearch를 설치하고자 한다. 단일 서버에 싱글 노드로 구축하고 binary 설치를 하려고 했지만 요즘 추세에 맞게(?) Container 기반으로 구축하고자 Docker를 이용한다. Updata Note 2020.05.10 : Elasticsearch 용어 정리 Elasticsearch 용어Indexelasticsearch 내 데이ㅓ 저장소로 RDBMS 의 데이터베이스와 유사 하나 또는 여러 개 Document 포괄적인 의미의 색인 또는 색인 파일 (범용적인 의미로 사용) indice: elasticsearch 내 물리적으로 사용하는 색인 또는 색인 파일 ShardLucene 을 기준으로 검색의 기본 데이터베이스가 되는 인덱스 분산 처리를 위한 개념 큰 크기의 인덱스 -&gt; 여러 개 작은 인덱스로 나누어 저장하는 것을 의미 단일 노드에서는 저장소 서능에 대한 한계를 해결, 클러스터에서는 분산 처리를 수행하며 빠른 결과 생성 Primary Shard:색인 시 가장 먼저 생성되는 인덱스로 복제의 기본 소스 Replica Shard:레플리카 설정값에 따라 Primary Shard 을 복제하여 생성한 샤드를 일컫음 Replica단어 뜻대로 복제본으로, 장애 발생 시 지속성 보장과 검색 효율성을 위해 사용 분산된 다른 노드에 Shard 와 같은 데이터를 복제 (복제된 Shard 를 Replica 라 생각하자) 생성 절차 Primary Shard 색인 각 노드에서 async 하게 Shard 복제 검색 성능 저하 최소화 Documentelasticsearch 에서 가장 기본이 되는 데이터 단위 하나의 item 또는 article RDBMS 의 테이블 내 하나의 row 에 해당 Type 은 RDBMS 테이블 Field 는 RDBMS 테이블의 하나의 column Nodeelasticsearch 를 구성하는 하나의 서버 또는 데몬 독립적으로 동작 가능한 서버 Clusterstandalone 하게 동작하는 여러 노드를 하나의 그룹으로 묶어 데이터의 분산과 공유를 할 수 있도록 서비스를 구성한 것 pull Docker-Image for ElasticSearchElasticSearch docker image를 서버로 가져오는 작업을 수행한다. 현재 포스팅 기준 최신 버젼은 7.5.1 이다. 12345678910111213$ docker pull docker.elastic.co/elasticsearch/elasticsearch:7.5.1Trying to pull repository docker.elastic.co/elasticsearch/elasticsearch ... 7.5.1: Pulling from docker.elastic.co/elasticsearch/elasticsearchc808caf183b6: Pull complete 05ff3f896999: Pull complete 82fb7fb0a94e: Pull complete c4d0024708f4: Pull complete 136650a16cfe: Pull complete 968db096c092: Pull complete 42547e91692f: Pull complete Digest: sha256:b0960105e830085acbb1f9c8001f58626506ce118f33816ea5d38c772bfc7e6cStatus: Downloaded newer image for docker.elastic.co/elasticsearch/elasticsearch:7.5.1 Run single-node clusterdocker run -i -t -p 9200:9200 -p 9300:9300 -e &quot;discovery.type=single-node&quot; docker.elastic.co/elasticsearch/elasticsearch:7.5.1 docker container 실행 시 외부 접근을 허용하기 위해서 -p 인자를 추가하여 port forwarding 하게끔 설정한다. 또한, 인터렉티브한 컨테이너를 생성하기 위해 -i -p 인자를 추가하였다. Checkcurl 을 이용해 elasticsearch 가 실행되고 있는지 확인한다. 123456789101112131415161718$ curl -XGET [설치한 서버 ip]:9200&#123; \"name\" : \"294fb8043230\", \"cluster_name\" : \"docker-cluster\", \"cluster_uuid\" : \"UOT6i7eIRjuan8ot89zNHw\", \"version\" : &#123; \"number\" : \"7.5.1\", \"build_flavor\" : \"default\", \"build_type\" : \"docker\", \"build_hash\" : \"3ae9ac9a93c95bd0cdc054951cf95d88e1e18d96\", \"build_date\" : \"2019-12-16T22:57:37.835892Z\", \"build_snapshot\" : false, \"lucene_version\" : \"8.3.0\", \"minimum_wire_compatibility_version\" : \"6.8.0\", \"minimum_index_compatibility_version\" : \"6.0.0-beta1\" &#125;, \"tagline\" : \"You Know, for Search\" 설치를 완료하였다. Reference elasticsearch 설치 (도커), https://velog.io/@pa324/elasticsearch-설치-도커-2bk2h3gh7d elasticsearch document, https://www.elastic.co/guide/en/elasticsearch/reference/current/docker.html 2020.01.08 made by jaejun.lee","categories":[{"name":"Database","slug":"Database","permalink":"https://jx2lee.github.io/categories/Database/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://jx2lee.github.io/tags/Docker/"}]},{"title":"[BI] Superset 설치","slug":"bi-install_superset","date":"2020-01-06T15:00:00.000Z","updated":"2020-09-14T14:02:47.615Z","comments":true,"path":"bi-install_superset/","link":"","permalink":"https://jx2lee.github.io/bi-install_superset/","excerpt":"Superset은 Web 기반 BI 툴로써 Python으로 개발되었고 수집-저장-처리를 거친 데이터를 마트에서 추출하여 그래프로 시각화하는데 사용한다. 제공 기능으로는 EDA, Dashboard 생성 및 공유, 보안, 권한과 다양한 소스 연결을 지원한다. 이번 포스트에는 Superset을 설치해본다.","text":"Superset은 Web 기반 BI 툴로써 Python으로 개발되었고 수집-저장-처리를 거친 데이터를 마트에서 추출하여 그래프로 시각화하는데 사용한다. 제공 기능으로는 EDA, Dashboard 생성 및 공유, 보안, 권한과 다양한 소스 연결을 지원한다. 이번 포스트에는 Superset을 설치해본다. InstallVirtualenvWrapperpython 가상환경을 관리하는 VirtualenvWrapper을 설치하고 superset 환경을 셋팅한다. 12$ mkvirtualenv superset$ workon superset Install Superset and Initializationvirtualenv 환경에서 pip 명령어를 이용해 설치한다. 1$ pip install apache-superset superset의 database를 초기화 하고 admin user를 생성한다 1234567891011$ superset db upgrade$ flask fab create-adminUsername [admin]: flask fab create-adminUser first name [admin]: adminUser last name [user]:Email [admin@fab.org]:Password:Repeat for confirmation:2020-01-07 10:51:24,272:INFO:root:Configured event logger of type &lt;class 'superset.utils.log.DBEventLogger'&gt;Recognized Database Authentications.Admin User flask fab create-admin created. 이후 테스트 할 데이터를 import한다. 이후 권한/허가등을 초기화하고 마지막으로 UI 화면을 띄운다. 123$ superset load_examples$ superset init$ superset run -p 8088 --with-threads --reload --debugger Troubleshooting설치 중 발생한 에러를 살펴본다. install error : python-geohash12345678910111213141516171819202122232425ERROR: Command errored out with exit status 1: command: /home/supset/.virtualenvs/superset/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-onzf3pi1/python-geohash/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-onzf3pi1/python-geohash/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /tmp/pip-record-oy48mkll/install-record.txt --single-version-externally-managed --compile --install-headers /home/supset/.virtualenvs/superset/include/site/python3.6/python-geohash cwd: /tmp/pip-install-onzf3pi1/python-geohash/ Complete output (19 lines): running install running build running build_py creating build creating build/lib.linux-x86_64-3.6 copying geohash.py -&gt; build/lib.linux-x86_64-3.6 copying quadtree.py -&gt; build/lib.linux-x86_64-3.6 copying jpgrid.py -&gt; build/lib.linux-x86_64-3.6 copying jpiarea.py -&gt; build/lib.linux-x86_64-3.6 running build_ext building '_geohash' extension creating build/temp.linux-x86_64-3.6 creating build/temp.linux-x86_64-3.6/src gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -fPIC -DPYTHON_MODULE=1 -I/usr/include/python3.6m -c src/geohash.cpp -o build/temp.linux-x86_64-3.6/src/geohash.o src/geohash.cpp:538:20: fatal error: Python.h: No such file or directory #include &lt;Python.h&gt; ^ compilation terminated. error: command 'gcc' failed with exit status 1 ----------------------------------------ERROR: Command errored out with exit status 1: /home/supset/.virtualenvs/superset/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-onzf3pi1/python-geohash/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-onzf3pi1/python-geohash/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /tmp/pip-record-oy48mkll/install-record.txt --single-version-externally-managed --compile --install-headers /home/supset/.virtualenvs/superset/include/site/python3.6/python-geohash Check the logs for full command output. python-geohash package 설치 시 에러가 발생하였다. 이는 설치 시 필요한 시스템 패키지가 미설치되어 발생한 에러로, sudo yum install gcc gcc-c++ python3-devel cyrus-sasl-devel 을 통해 해결할 수 있다. 프로그램 설치 시 환경 설정이 꼭 필요하니 공식 documents를 참고하자. Reference Superset Document, https://superset.incubator.apache.org/installation.html 2020.01.07 made by jaejun.lee","categories":[{"name":"BI","slug":"BI","permalink":"https://jx2lee.github.io/categories/BI/"}],"tags":[]},{"title":"[Python] 큰 수 만들기","slug":"programmers-large_number","date":"2019-12-16T15:00:00.000Z","updated":"2020-03-30T15:06:23.536Z","comments":true,"path":"programmers-large_number/","link":"","permalink":"https://jx2lee.github.io/programmers-large_number/","excerpt":"주어진 숫자에서 특정 갯수의 숫자만으로 가장 큰 수를 만드는 문제를 풀어본다","text":"주어진 숫자에서 특정 갯수의 숫자만으로 가장 큰 수를 만드는 문제를 풀어본다 문제 설명어떤 숫자에서 k개의 수를 제거했을 때 얻을 수 있는 가장 큰 숫자를 구하려 합니다. 예를 들어, 숫자 1924에서 수 두 개를 제거하면 [19, 12, 14, 92, 94, 24] 를 만들 수 있습니다. 이 중 가장 큰 숫자는 94 입니다. 문자열 형식으로 숫자 number와 제거할 수의 개수 k가 solution 함수의 매개변수로 주어집니다. number에서 k 개의 수를 제거했을 때 만들 수 있는 수 중 가장 큰 숫자를 문자열 형태로 return 하도록 solution 함수를 완성하세요. 제한 조건number는 1자리 이상, 1,000,000자리 이하인 숫자입니다. k는 1 이상 number의 자릿수 미만인 자연수입니다. 입출력 예 number k return 1924 2 94 1231234 3 3234 4177252841 4 775841 문제 풀이탐욕법으로 풀 수 있는 문제로, 우선 collected 라는 결과물 저장 List를 선언한다. number의 숫자를 for문으로 돌면서 collected 길이 &gt; 0 / collected의 마지막 숫자 비교 / k &gt; 0 조건을 만족하면, 원소 하나를 빼주고 k를 차감한다. 이후 k가 0이면 빈 리스트를 반환하는 걸 방지하고자 index i이상 만큼의 number를 담는다. for문의 마지막으로 조건에 안걸리는 num은 collected에 담는다. 이후 k가 음수일 경우를 대비해 -k 까지 slice를 수행하고 결과물이 담긴 collected List를 join하여 숫자로 변환한다. Code1234567891011121314151617def solution(number, k): collected = [] for i, num in enumerate(number): while len(collected) &gt; 0 and collected[-1] &lt; num and k &gt; 0: collected.pop() k -= 1 if k == 0: collected += list(number[i:]) break collected.append(num) collected = collected[:-k] if k &gt; 0 else collected answer = ''.join(collected) return answer 2019.12.17 made by jaejun.lee","categories":[{"name":"Python","slug":"Python","permalink":"https://jx2lee.github.io/categories/Python/"}],"tags":[{"name":"Programmers","slug":"Programmers","permalink":"https://jx2lee.github.io/tags/Programmers/"}]},{"title":"[SQL] 이동평균 - Moving Average","slug":"sql-moving_average","date":"2019-12-15T15:00:00.000Z","updated":"2020-03-30T15:06:23.587Z","comments":true,"path":"sql-moving_average/","link":"","permalink":"https://jx2lee.github.io/sql-moving_average/","excerpt":"모 기업 코딩테스트에 나온 이동 평균, Moving Average 를 SQL로 풀어본다","text":"모 기업 코딩테스트에 나온 이동 평균, Moving Average 를 SQL로 풀어본다 코딩테스트를 Hackerrank 플랫폼을 이용했던터라 문제 복원을 할 수 없었다. 이에 (https://www.sqlteam.com/articles/calculating-running-totals) 에서 제공한 create database script를 활용해 생성한 데이터로 이동 평균을 구해보고자 한다. 사용한 쿼리와 결과는 아래와 같다. 12345678910111213141516delimiter //CREATE PROCEDURE insert_row()BEGIN DECLARE DayCount smallint default 5; DECLARE Sales bigint default 10; WHILE DayCount &lt;= 5000 DO INSERT INTO Sales values (DayCount, Sales); SET DayCount = DayCount + 1; SET Sales = Sales + 15; END WHILE;END//delimiter; 12345678mysql root@localhost:practice&gt; select count(*) from Sales; +----------+| count(*) |+----------+| 5000 |+----------+1 row in setTime: 0.010s 이동평균 구하기스칼라 서브쿼리를 이용해 문제를 해결하였다. average 함수를 이용해 평균 Sales 값을 구하는데, 스칼라 서브쿼리 내 서브쿼리(Count 절)를 작성하여 count가 1과 3사이에 있을때 평균을 구하는 칼럼(MvAvg)을 조회하였다. 문제를 풀다보니 SQL 실행순서나 계획 등에 대한 지식이 부족한 것 같다. 책을 한 권 구비하여 공부하는게 좋을 듯 하다. Code123456789101112131415select DayCount, Sales, (select avg(Sales) as moving_average from Sales b where (select count(*) from Sales c where DayCount between b.DayCount and a.DayCount ) between 1 and 3 ) as MvAvg from Sales a; 2019.12.16 made by jaejun.lee","categories":[{"name":"SQL","slug":"SQL","permalink":"https://jx2lee.github.io/categories/SQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://jx2lee.github.io/tags/MySQL/"}]},{"title":"[Hadoop] Install Spark","slug":"hadoop-install_spark","date":"2019-12-12T15:00:00.000Z","updated":"2020-05-15T05:25:12.601Z","comments":true,"path":"hadoop-install_spark/","link":"","permalink":"https://jx2lee.github.io/hadoop-install_spark/","excerpt":"Spark를 설치하는 과정을 다룬다. binary를 다운받아 풀고 config를 수정하면 쉽게 설치하고 실행할 수 있다. 이 포스트에는 standalone 모드로 spark를 실행한다","text":"Spark를 설치하는 과정을 다룬다. binary를 다운받아 풀고 config를 수정하면 쉽게 설치하고 실행할 수 있다. 이 포스트에는 standalone 모드로 spark를 실행한다 PreliminariesHadoop Versionhadoop version 명령어를 통해 Hadoop의 버젼을 체크한다. 이 말인 즉슨, Hadoop Client가 Spark를 사용할 계정에 준비되어 있어야 한다. 123456Hadoop 2.9.2Subversion https://git-wip-us.apache.org/repos/asf/hadoop.git -r 826afbeae31ca687bc2f8471dc841b66ed2c6704Compiled by ajisaka on 2018-11-13T12:42ZCompiled with protoc 2.5.0From source with checksum 3a9939967262218aa556c684d107985This command was run using /app/hadoop/2.9.2/share/hadoop/common/hadoop-common-2.9.2.jar Hadoop Client는 설치가 필요없고 이미 구축된 하둡 클러스터에서 hadoop 바이너리만 가져와 사용하는 것을 말한다. 즉, 다른 서버에서 하둡을 사용할 수 있게끔만 설정해놓자. Download binarySpark Documents로 접속하여 binary를 다운로드 한다. 1234567[root@node2 app]# wget http://mirror.apache-kr.org/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz--2019-12-10 17:41:49-- http://mirror.apache-kr.org/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgzResolving mirror.apache-kr.org (mirror.apache-kr.org)... 1.201.139.179Connecting to mirror.apache-kr.org (mirror.apache-kr.org)|1.201.139.179|:80... connected.HTTP request sent, awaiting response... 200 OKLength: 230091034 (219M) [application/x-gzip]Saving to: ‘spark-2.4.4-bin-hadoop2.7.tgz’ Spark 유저를 생성하고 Spark Home 디렉토리에 다운받은 binary를 풀어준다. Set config~/.bash_profileJAVA, HADOOP 환경변수를 설정하고 SPARK 환경변수를 새로 작성하고 update 한다. 1234567891011#JAVA ENVexport JAVA_HOME=/app/java/jdk1.8.0_181export PATH=$PATH:$JAVA_HOME/bin#SPARK ENVSPARK_HOME=/app/sparkexport PATH=$PATH:$SPARK_HOME/bin#HADOOP ENVexport HADOOP_HOME=/app/hadoopexport PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin Run$SPARK_HOME/bin 폴더에 pypark를 실행한다. * 1234567891011121314[spark@node2 bin]$ ./pyspark Python 3.6.8 (default, Aug 7 2019, 17:28:10) [GCC 4.8.5 20150623 (Red Hat 4.8.5-39)] on linuxType \"help\", \"copyright\", \"credits\" or \"license\" for more information.Welcome to ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ '_/ /__ / .__/\\_,_/_/ /_/\\_\\ version 2.4.4 /_/Using Python version 3.6.8 (default, Aug 7 2019 17:28:10)SparkSession available as 'spark'.&gt;&gt;&gt; 간단한 RDD를 생성하여 README.md 에 포함한 라인을 세보도록 한다. 123&gt;&gt;&gt; lines &#x3D; sc.textFile(&#39;README.md&#39;)&gt;&gt;&gt; lines.count()105 sc.textFile로 경로를 무시하게되면 자동으로 hdfs 경로를 읽어들인다. 만약에 로컬 파일을 읽고 싶다면, file://[file path]/[file name]으로 작성하면 로컬 파일을 읽어들인다. Reference Spark Documents made by jaejun.lee","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://jx2lee.github.io/categories/Hadoop/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://jx2lee.github.io/tags/Spark/"}]},{"title":"[Python] 다리를 지나는 트럭","slug":"programmers-truck","date":"2019-12-02T15:00:00.000Z","updated":"2020-03-30T15:06:23.540Z","comments":true,"path":"programmers-truck/","link":"","permalink":"https://jx2lee.github.io/programmers-truck/","excerpt":"최대 용량을 가진 다리를 트럭이 모두 지나는 시간을 구하는 문제를 풀어본다.","text":"최대 용량을 가진 다리를 트럭이 모두 지나는 시간을 구하는 문제를 풀어본다. 문제 설명트럭 여러 대가 강을 가로지르는 일 차선 다리를 정해진 순으로 건너려 합니다. 모든 트럭이 다리를 건너려면 최소 몇 초가 걸리는지 알아내야 합니다. 트럭은 1초에 1만큼 움직이며, 다리 길이는 bridge_length이고 다리는 무게 weight까지 견딥니다.※ 트럭이 다리에 완전히 오르지 않은 경우, 이 트럭의 무게는 고려하지 않습니다. 예를 들어, 길이가 2이고 10kg 무게를 견디는 다리가 있습니다. 무게가 [7, 4, 5, 6]kg인 트럭이 순서대로 최단 시간 안에 다리를 건너려면 다음과 같이 건너야 합니다. 경과 시간 다리를 지난 트럭 다리를 건너는 트럭 대기 트럭 0 [] [] [7,4,5,6] 1~2 [] [7] [4,5,6] 3 [7] [4] [5,6] 4 [7] [4,5] [6] 5 [7,4] [5] [6] 6~7 [7,4,5] [6] [] 8 [7,4,5,6] [] [] 따라서, 모든 트럭이 다리를 지나려면 최소 8초가 걸립니다. solution 함수의 매개변수로 다리 길이 bridge_length, 다리가 견딜 수 있는 무게 weight, 트럭별 무게 truck_weights가 주어집니다. 이때 모든 트럭이 다리를 건너려면 최소 몇 초가 걸리는지 return 하도록 solution 함수를 완성하세요. 제한 조건 bridge_length는 1 이상 10,000 이하입니다. weight는 1 이상 10,000 이하입니다. truck_weights의 길이는 1 이상 10,000 이하입니다. 모든 트럭의 무게는 1 이상 weight 이하입니다. 입출력 예 bridge_length weight truck_weights return 2 10 [7,4,5,6] 8 100 100 [10] 101 100 100 [10,10,10,10,10,10,10,10,10,10] 110 문제 풀이배열을 list로 풀면 하나의 케이스가 시간초과가 발생한다. pop와 append 시 index를 재배열하므로 이를 방지하기 위해, collections 패키지의 deque 배열을 사용하여 해결하였다. queue 변수는 다리를 지나가고 있는 트럭들을 나타낸다. truck_weights 의 모든 트럭들을 하나씩 뽑아 while 문을 실행한다. queue 배열 길이가 bridge_length와 같다면 queue에서 pop을 실행하고, 만약 선택된 트럭의 무게를 더해도 버틸 수 있다면 (sum(queue) + truck &lt;= weight) queue에 트럭을 추가하고 while문을 빠져나온다. 만약 그렇지 않다면 queue에 0을 왼쪽에 추가하고 시간을 +1 한다. 위를 반복하고 마지막에 들어간 트럭의 소요시간을 구하기 위해 bridge_length를 더하고 문제를 마무리한다. Code1234567891011121314151617181920212223from collections import dequedef solution(bridge_length, weight, truck_weights): answer = 0 queue = deque([]) truck_weights = deque(truck_weights) for truck in truck_weights: while truck: if len(queue) == bridge_length: queue.pop() if sum(queue) + truck &lt;= weight: queue.appendleft(truck) truck = 0 answer += 1 else: queue.appendleft(0) answer += 1 answer += bridge_length return answer Python 의 List 타입의 경우 pop(0)을 수행할 때 인덱스를 재배열하는 것을 깨달았다. 즉, queue를 구현하기 위해서는 List보다 Collections 패키지의 deque를 사용하는 것이 시간초과를 피할 수 있다 2019.12.03 made by jaejun.lee","categories":[{"name":"Python","slug":"Python","permalink":"https://jx2lee.github.io/categories/Python/"}],"tags":[{"name":"Programmers","slug":"Programmers","permalink":"https://jx2lee.github.io/tags/Programmers/"}]},{"title":"[Hadoop] Tibero 계정의 모든 table을 HDFS로 저장","slug":"hadoop-tables_to_hdfs_using_sqoop","date":"2019-11-26T15:00:00.000Z","updated":"2020-03-30T15:06:23.546Z","comments":true,"path":"hadoop-tables_to_hdfs_using_sqoop/","link":"","permalink":"https://jx2lee.github.io/hadoop-tables_to_hdfs_using_sqoop/","excerpt":"Sqoop을 이용해 RDB 특정 계정의 모든 Table을 Import 하는 과정을 다룬다","text":"Sqoop을 이용해 RDB 특정 계정의 모든 Table을 Import 하는 과정을 다룬다 Tibero의 ERP 계정의 모든 Table을 HDFS로 저장하고 동시에 Hive Table로 생성한다. sqoop import-all-tables을 이용하며 특정 스키마가 없는 테이블은 제외하였다. 구문은 다음과 같다. 12345678sqoop import-all-tables \\--connect jdbc:tibero:thin:@[ip]:[port]:[DB SID] \\--driver com.tmax.tibero.jdbc.TbDriver \\--username [user] --password [passowrd] \\--warehouse-dir [hdfs dir] \\--hive-import --hive-overwrite --hive-database [metastore db name] \\--exclude-tables SM_GROUP,SM_GROUP_PERMISSION,SM_PERMISSION,SM_PROJECT_GROUP_AUTH,SM_ROLE,SM_USER,SM_USER_GROUP \\--m 1 –connect : 접속할 DB 정보 –driver : 접속할 DB Driver –username &amp; –password : 계정 ID/Password –warehouse-dir : HDFS 위치 –hive-import –hive-overwrite –hive-database : HDFS로 저장함과 동시에 Hive table로 import. hive-database는 MetaStore의 database –exclude-table : Import 시 제외할 Table –m : number of mappers 제외할 테이블 명을 명시할 떄 콤마 이후에 무조건 붙여줘야 argument를 인식한다 생각보다 시간이 오래걸렸다. 결과를 확인해보자. 12$ beeline$ !connect jdbc:hive2://[ip]:[port] user password 1234567891011121314151617181920use tims;show tables;+-----------------------+| tab_name |+-----------------------+| aactv00t || aactv01t || aactv10t || aactv20t || aactv24t || aactv25t |......| satch00t_03 || sbms_common_code || sbms_document_data || sbms_error_log |+-----------------------+595 rows selected (0.183 seconds) 2019.11.27 made by jaejun.lee","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://jx2lee.github.io/categories/Hadoop/"}],"tags":[]},{"title":"[Python] 문자열 압축","slug":"programmers-compress_char","date":"2019-11-25T15:00:00.000Z","updated":"2020-03-30T15:06:23.535Z","comments":true,"path":"programmers-compress_char/","link":"","permalink":"https://jx2lee.github.io/programmers-compress_char/","excerpt":"주어진 문자열을 가장 짧게 압축하는 문제를 풀어본다 (2020 카카오 1차 코딩테스트)","text":"주어진 문자열을 가장 짧게 압축하는 문제를 풀어본다 (2020 카카오 1차 코딩테스트) 문제데이터 처리 전문가가 되고 싶은 “어피치”는 문자열을 압축하는 방법에 대해 공부를 하고 있습니다. 최근에 대량의 데이터 처리를 위한 간단한 비손실 압축 방법에 대해 공부를 하고 있는데, 문자열에서 같은 값이 연속해서 나타나는 것을 그 문자의 개수와 반복되는 값으로 표현하여 더 짧은 문자열로 줄여서 표현하는 알고리즘을 공부하고 있습니다.간단한 예로 “aabbaccc”의 경우 “2a2ba3c”(문자가 반복되지 않아 한번만 나타난 경우 1은 생략함)와 같이 표현할 수 있는데, 이러한 방식은 반복되는 문자가 적은 경우 압축률이 낮다는 단점이 있습니다. 예를 들면, “abcabcdede”와 같은 문자열은 전혀 압축되지 않습니다. “어피치”는 이러한 단점을 해결하기 위해 문자열을 1개 이상의 단위로 잘라서 압축하여 더 짧은 문자열로 표현할 수 있는지 방법을 찾아보려고 합니다.예를 들어, “ababcdcdababcdcd”의 경우 문자를 1개 단위로 자르면 전혀 압축되지 않지만, 2개 단위로 잘라서 압축한다면 “2ab2cd2ab2cd”로 표현할 수 있습니다. 다른 방법으로 8개 단위로 잘라서 압축한다면 “2ababcdcd”로 표현할 수 있으며, 이때가 가장 짧게 압축하여 표현할 수 있는 방법입니다.다른 예로, “abcabcdede”와 같은 경우, 문자를 2개 단위로 잘라서 압축하면 “abcabc2de”가 되지만, 3개 단위로 자른다면 “2abcdede”가 되어 3개 단위가 가장 짧은 압축 방법이 됩니다. 이때 3개 단위로 자르고 마지막에 남는 문자열은 그대로 붙여주면 됩니다.압축할 문자열 s가 매개변수로 주어질 때, 위에 설명한 방법으로 1개 이상 단위로 문자열을 잘라 압축하여 표현한 문자열 중 가장 짧은 것의 길이를 return 하도록 solution 함수를 완성해주세요. 제한사항 s의 길이는 1 이상 1,000 이하입니다. s는 알파벳 소문자로만 이루어져 있습니다. 입출력 예123456 s result&quot;aabbaccc&quot; 7&quot;ababcdcdababcdcd&quot; 9&quot;abcabcdede&quot; 8&quot;abcabcabcabcdededededede&quot; 14&quot;xababcdcdababcdcd&quot; 17 입출력 예에 대한 설명 입출력 예 #1 문자열을 1개 단위로 잘라 압축했을 때 가장 짧습니다. 입출력 예 #2 문자열을 8개 단위로 잘라 압축했을 때 가장 짧습니다. 입출력 예 #3 문자열을 3개 단위로 잘라 압축했을 때 가장 짧습니다. 입출력 예 #4 문자열을 2개 단위로 자르면 “abcabcabcabc6de” 가 됩니다. 문자열을 3개 단위로 자르면 “4abcdededededede” 가 됩니다. 문자열을 4개 단위로 자르면 “abcabcabcabc3dede” 가 됩니다. 문자열을 6개 단위로 자를 경우 “2abcabc2dedede”가 되며, 이때의 길이가 14로 가장 짧습니다. 입출력 예 #5 문자열은 제일 앞부터 정해진 길이만큼 잘라야 합니다. 따라서 주어진 문자열을 x / ababcdcd / ababcdcd 로 자르는 것은 불가능 합니다. 이 경우 어떻게 문자열을 잘라도 압축되지 않으므로 가장 짧은 길이는 이 됩니다. 문제 풀이딱히 사용한 알고리즘은 없는 것 같다 (굳이 선택하면 브루트포스?). 반복하는 문자열 길이에 따라 모든 문자열을 압축하고, 이에 대한 길이를 res 리스트에 담아 최솟값을 출력해낸다. Code123456789101112131415161718192021222324252627def solution(s): result = '' cnt = 1 res = [] for n in range(1, len(s)+1): for i in range(0, len(s), n): if s[i:i+n] == s[i+n:i+2*n]: cnt += 1 else: if cnt &gt; 1: result += str(cnt) + s[i:i+n] else: result += s[i:i+n] cnt = 1 if i == len(s) - n: if cnt &gt; 1: result += str(cnt) + s[i+n:i+2*n] else: result += s[i+n:i+2*n] res.append(len(result)) result = '' cnt = 1 return min(res) 2019.11.26 made by jaejun.lee","categories":[{"name":"Python","slug":"Python","permalink":"https://jx2lee.github.io/categories/Python/"}],"tags":[{"name":"Programmers","slug":"Programmers","permalink":"https://jx2lee.github.io/tags/Programmers/"}]},{"title":"[Python] 자물쇠와 열쇠","slug":"programmers-unlock","date":"2019-11-25T15:00:00.000Z","updated":"2020-03-30T15:06:23.578Z","comments":true,"path":"programmers-unlock/","link":"","permalink":"https://jx2lee.github.io/programmers-unlock/","excerpt":"key를 이용해 자물쇠를 여는 문제를 풀어본다 (2020 카카오 1차 코딩테스트)","text":"key를 이용해 자물쇠를 여는 문제를 풀어본다 (2020 카카오 1차 코딩테스트) 문제 설명고고학자인 튜브는 고대 유적지에서 보물과 유적이 가득할 것으로 추정되는 비밀의 문을 발견하였습니다. 그런데 문을 열려고 살펴보니 특이한 형태의 자물쇠로 잠겨 있었고 문 앞에는 특이한 형태의 열쇠와 함께 자물쇠를 푸는 방법에 대해 다음과 같이 설명해 주는 종이가 발견되었습니다. 잠겨있는 자물쇠는 격자 한 칸의 크기가 1 x 1인 N x N 크기의 정사각 격자 형태이고 특이한 모양의 열쇠는 M x M 크기인 정사각 격자 형태로 되어 있습니다. 자물쇠에는 홈이 파여 있고 열쇠 또한 홈과 돌기 부분이 있습니다. 열쇠는 회전과 이동이 가능하며 열쇠의 돌기 부분을 자물쇠의 홈 부분에 딱 맞게 채우면 자물쇠가 열리게 되는 구조입니다. 자물쇠 영역을 벗어난 부분에 있는 열쇠의 홈과 돌기는 자물쇠를 여는 데 영향을 주지 않지만, 자물쇠 영역 내에서는 열쇠의 돌기 부분과 자물쇠의 홈 부분이 정확히 일치해야 하며 열쇠의 돌기와 자물쇠의 돌기가 만나서는 안됩니다. 또한 자물쇠의 모든 홈을 채워 비어있는 곳이 없어야 자물쇠를 열 수 있습니다. 열쇠를 나타내는 2차원 배열 key와 자물쇠를 나타내는 2차원 배열 lock이 매개변수로 주어질 때, 열쇠로 자물쇠를 열수 있으면 true를, 열 수 없으면 false를 return 하도록 solution 함수를 완성해주세요. 제한사항 key는 M x M(3 ≤ M ≤ 20, M은 자연수)크기 2차원 배열입니다. lock은 N x N(3 ≤ N ≤ 20, N은 자연수)크기 2차원 배열입니다. M은 항상 N 이하입니다. key와 lock의 원소는 0 또는 1로 이루어져 있습니다. 0은 홈 부분, 1은 돌기 부분을 나타냅니다. 입출력 예| key | lock | result || —– | —— | —————————— | —— || [[0, 0, 0], [1, 0, 0], [0, 1, 1]] | [[1, 1, 1], [1, 1, 0], [1, 0, 1]] | true | 입출력 예 설명 key를 시계 방향으로 90도 회전하고, 오른쪽으로 한 칸, 아래로 한 칸 이동하면 lock의 홈 부분을 정확히 모두 채울 수 있습니다. 문제 풀이한 줄로 문제를 풀어보면, lock을 확대하고 key를 하나씩 대입해보면서 모든 부분이 1인 경우를 찾으면 된다. 크게 세 가지 함수로 구현하였다. rotate_key는 말그래도 입력한 key 를 시계 방향 90도로 회전하는 함수이다. 입력은 key와 key의 길이 123456def rotate_key(key, M): res = [[0 * n for n in range(M)] for _ in range(M)] for y in range(M): for x in range(M): res[x][M - y - 1] = key[y][x] return res expand_lock은 lock을 key와 대조하기 위해 확장하는 함수이다. n+2 X (m-1) 만큼 확장한다. N은 lock의 길이, M은 열쇠의 길이 123456def expand_lock(lock, N, M, K): res = [[0 * i for i in range(K)] for _ in range(K)] for y in range(N): for x in range(N): res[y + M - 1][x + M - 1] = lock[y][x] return res is_open은 확장된 lock과 key를 이용해 각 구멍 value를 더해 1이 아니면 경우는 False, 1인 경우에는 True를 반환하는 함수이다. 이 함수는 확장된 lock을 겹치는 부분부터 끝까지 key를 for문으로 돌려가며 확인한다. 1234567891011def is_open(_y, _x, key, lock, N, M): _lock = copy.deepcopy(lock) for y in range(M): for x in range(M): _lock[_y + y][_x + x] += key[y][x] for y in range(N): for x in range(N): if _lock[y + M - 1][x + M - 1] != 1: return False return True 마지막 solution함수는 하나하나 키를 돌려가며 모든 값들이 1인지를 판단하고, 아니면 key를 돌려가며 확인하는 함수이다. 1234567891011121314def solution(key, lock): n, m = len(lock), len(key) k = n + 2 * (m - 1) lock = expand_lock(lock, n, m, k) for y in range(k - m +1): for x in range(k - m +1): for _ in range(4): if is_open(y, x, key, lock, n, m): return True key = rotate_key(key, m) return False Code123456789101112131415161718192021222324252627282930313233343536373839404142import copydef rotate_key(key, M): res = [[0 * n for n in range(M)] for _ in range(M)] for y in range(M): for x in range(M): res[x][M - y - 1] = key[y][x] return resdef expand_lock(lock, N, M, K): res = [[0 * i for i in range(K)] for _ in range(K)] for y in range(N): for x in range(N): res[y + M - 1][x + M - 1] = lock[y][x] return resdef is_open(_y, _x, key, lock, N, M): _lock = copy.deepcopy(lock) for y in range(M): for x in range(M): _lock[_y + y][_x + x] += key[y][x] for y in range(N): for x in range(N): if _lock[y + M - 1][x + M - 1] != 1: return False return Truedef solution(key, lock): answer = True n, m = len(lock), len(key) k = n + 2 * (m - 1) lock = expand_lock(lock, n, m, k) for y in range(k - m +1): for x in range(k - m +1): for _ in range(4): if is_open(y, x, key, lock, n, m): return True key = rotate_key(key, m) return False 2019.11.26 made by jaejun.lee","categories":[{"name":"Python","slug":"Python","permalink":"https://jx2lee.github.io/categories/Python/"}],"tags":[{"name":"Programmers","slug":"Programmers","permalink":"https://jx2lee.github.io/tags/Programmers/"}]},{"title":"[Database] MySQL 유저 관리","slug":"database-manage_user_in_mysql","date":"2019-11-24T15:00:00.000Z","updated":"2020-03-30T15:06:23.568Z","comments":true,"path":"database-manage_user_in_mysql/","link":"","permalink":"https://jx2lee.github.io/database-manage_user_in_mysql/","excerpt":"MySQL의 유저를 관리해본다","text":"MySQL의 유저를 관리해본다 User tableMySQL에서 관리하는 유저를 조회해보자. 우선, 기본적으로 mysql database에 user 테이블에서 관리한다. mysql database를 선택하고 user table의 host, user를 조회해보자. 12345678910111213141516171819202122232425mysql&gt; show databases;+--------------------+| Database |+--------------------+| hive || information_schema || mysql || performance_schema || sys |+--------------------+5 rows in set (0.00 sec)mysql&gt; use mysql;Database changedmysql&gt; select host, user from user;+-----------+------------------+| host | user |+-----------+------------------+| % | hive || localhost | mysql.infoschema || localhost | mysql.session || localhost | mysql.sys || localhost | root |+-----------+------------------+5 rows in set (0.01 sec) Create User &amp; DatabaseUserSqoop를 이용해 hdfs 데이터를 MySQL table을 저장하기 위한 계정을 생성한다. (Sqoop export를 위해서는 해당 RDB에 테이블이 존재해야한다. 굳이 이관하는 것 까진 필요없을 것 같아, 이번 포스팅에는 유저를 생성하고 관리하는 방법만 다룬다) create user [user name]@[ip] identified by [password]; user name : 생성할 계정명 ip : 접속가능 ip로 로컬 계정에서만 접속을 허용할 것이면 localhost, 본인과 같이 모든 외부 IP에서 접근이 가능하게 하려면 % password : 생성할 계정의 비밀번호 ip의 경우, grant 명령어로 수정이 가능함 123456789101112131415mysql&gt; create user 'tims'@'%' identified by '****';Query OK, 0 rows affected (0.00 sec)mysql&gt; select host, user from user;+-----------+------------------+| host | user |+-----------+------------------+| % | hive || % | tims || localhost | mysql.infoschema || localhost | mysql.session || localhost | mysql.sys || localhost | root |+-----------+------------------+6 rows in set (0.01 sec) Database생성한 tims계정에서 사용할 database를 생성한다. create database [database name] database name : 생성할 database 이름 grank all privileges on [database name].[schema] to [user name]@[ip] database naem : 생성한 database 이름 schema : 생성한 database 내 스키마 user name : 권한을 줄 계정명 ip : 접속 ip 12345mysql&gt; create database tims;Query OK, 1 row affected (0.01 sec)mysql&gt; grant all privileges on tims.* to 'tims'@'%';Query OK, 0 rows affected (0.00 sec) 테스트해보자. show tables 를 치게되면 아무 테이블이 표시되지 않을 것이다. (당연) 123456789101112131415161718192021222324252627[mysql@node2 ~]$ mysql -u tims -pEnter password: Welcome to the MySQL monitor. Commands end with ; or \\g.Your MySQL connection id is 6820Server version: 8.0.18 Source distributionCopyright (c) 2000, 2019, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement.mysql&gt; show databases;+--------------------+| Database |+--------------------+| information_schema || tims |+--------------------+2 rows in set (0.00 sec)mysql&gt; use tims;Database changedmysql&gt; show tables;Empty set (0.00 sec) ERROR 1045 (28000): Access denied for user ‘hive’@’localhost’ (using password: NO) 에러가 발생하는 경우가 있다. 이때는 해당 계정으로 Login 할 때 -p opiton을 붙여준다 2019.11.25 made by jaejun.lee","categories":[{"name":"Database","slug":"Database","permalink":"https://jx2lee.github.io/categories/Database/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://jx2lee.github.io/tags/MySQL/"}]},{"title":"[Hadoop] Install Zeppelin and Connect to RDBMS & Hive","slug":"hadoop-install_zeppelin","date":"2019-11-17T15:00:00.000Z","updated":"2020-03-30T15:06:23.583Z","comments":true,"path":"hadoop-install_zeppelin/","link":"","permalink":"https://jx2lee.github.io/hadoop-install_zeppelin/","excerpt":"Apache Zeppelin을 설치하고 Tibero와 Hive에 연동하는 과정을 살펴본다","text":"Apache Zeppelin을 설치하고 Tibero와 Hive에 연동하는 과정을 살펴본다 Install Apache ZeppelinZeppelin user를 생성하고 설치파일을 다운로드한다. 123$ adduser zepp --gid 1000 # bigdata$ wget http://apache.mirror.cdnetworks.com/zeppelin/zeppelin-0.8.2/zeppelin-0.8.2-bin-all.tgz$ cd /app &amp;&amp; tar -xvzf zeppelin-0.8.2-bin-all.tgz $ZEPPELIN_HOME/conf/zeppelin-site.xml 파일을 수정한다. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253&lt;configuration&gt;&lt;property&gt; &lt;name&gt;zeppelin.server.addr&lt;/name&gt; &lt;value&gt;192.xxx.xxx.xx&lt;/value&gt; &lt;description&gt;Server binding address, Server IP&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;zeppelin.server.port&lt;/name&gt; &lt;value&gt;8001&lt;/value&gt; &lt;description&gt;Server port.&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;zeppelin.server.ssl.port&lt;/name&gt; &lt;value&gt;8443&lt;/value&gt; &lt;description&gt;Server ssl port. (used when ssl property is set to true)&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;zeppelin.server.context.path&lt;/name&gt; &lt;value&gt;/&lt;/value&gt;&lt;property&gt; &lt;name&gt;zeppelin.server.context.path&lt;/name&gt; &lt;value&gt;/&lt;/value&gt; &lt;description&gt;Context Path of the Web Application&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;zeppelin.war.tempdir&lt;/name&gt; &lt;value&gt;webapps&lt;/value&gt; &lt;description&gt;Location of jetty temporary directory&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;zeppelin.notebook.dir&lt;/name&gt; &lt;value&gt;notebook&lt;/value&gt; &lt;description&gt;path or URI for notebook persist&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;zeppelin.notebook.homescreen&lt;/name&gt; &lt;value&gt;&lt;/value&gt; &lt;description&gt;id of notebook to be displayed in homescreen. ex) 2A94M5J1Z Empty value displays default home screen&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;zeppelin.notebook.homescreen.hide&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;description&gt;hide homescreen notebook from list when this value set to true&lt;/description&gt;&lt;/property&gt; 이미 8080포트가 사용중이므로 port를 8001로 수정하였다. zeppelin 폴더 이용 권한을 zepp에게 주고 daemon을 실행한다. 1234$ chown -R zepp:bigdata zeppelin/$ su - zepp$ bin/zeppelin-daemon.sh startZeppelin start [ OK ] http://localhost:8080 으로 접속한다. ConnectionTibero ConnectionTibero와 연동하는 방법은 간단하다. $ZEPPELIN_HOME/interpreter/jdbc 안에 tibero6-jdbc.jar 파일을 복사한다 $ cp tibero6-jdbc.jar $ZEPPELIN_HOME/interpreter/jdbc/tibero6-jdbc.jar 이후 $ZEPPELIN/conf/interpreter.json 내 jdbc 부분에 import할 DB 정보를 작성한다. 123456789101112131415161718192021222324\"jdbc\": &#123; \"id\": \"jdbc\", \"name\": \"jdbc\", \"group\": \"jdbc\", \"properties\": &#123; \"default.url\": &#123; \"name\": \"default.url\", \"value\": \"jdbc:tibero:thin:@192.168.xxx.xxx:xxxx:tibero\", \"type\": \"string\" &#125;, \"default.driver\": &#123; \"name\": \"default.driver\", \"value\": \"com.tmax.tibero.jdbc.TbDriver\", \"type\": \"string\" &#125;, \"default.password\": &#123; \"name\": \"default.password\", \"value\": \"xxxxx\", \"type\": \"password\" &#125;, \"default.user\": &#123; \"name\": \"default.user\", \"value\": \"xxx\", \"type\": \"string\" properties 안에 default.url, default.driver, default.password/user 를 해당 DB 정보를 작성한다. 나머지는 세부적인 사항이므로 https://zeppelin.apache.org/docs/0.8.0/interpreter/jdbc.html를 확인해 필요하면 수정하도록 한다. Zeppelin을 재실행한다. Notebook을 생성하고 %jdbc \\n select * from tab을 실행하여 정상적으로 연결되었는지 확인한다. 1234$ $ZEPPELIN_HOME/bin/zeppelin-daemon.sh restart%jdbcselect * from tab Hive ConnectionHive와의 연동도 마찬가지로 $ZEPPELIN/conf/interpreter.json 내 interpreterSettings 부분에 Hive 정보를 작성한다. 1234567891011121314151617181920212223242526272829303132333435363738394041424344&#123; \"interpreterSettings\": &#123; \"hive\": &#123; \"id\": \"hive\", \"name\": \"hive\", \"group\": \"jdbc\", \"properties\": &#123; \"default.url\": &#123; \"name\": \"default.url\", \"value\": \"jdbc:hive2://localhost:10000/project\", \"type\": \"string\" &#125;, \"default.driver\": &#123; \"name\": \"default.driver\", \"value\": \"org.apache.hive.jdbc.HiveDriver\", \"type\": \"string\" &#125;,...... \"default.password\": &#123; \"name\": \"default.password\", \"value\": \"hive\", \"type\": \"password\" &#125;, \"default.user\": &#123; \"name\": \"default.user\", \"value\": \"hive\", \"type\": \"string\"...... \"dependencies\": [ &#123; \"groupArtifactVersion\": \"org.apache.hive:hive-jdbc:2.3.6\", \"local\": false &#125;, &#123; \"groupArtifactVersion\": \"org.apache.hadoop:hadoop-common:2.6.0\", \"local\": false, \"exclusions\": [] &#125; ], \"option\": &#123; \"remote\": true, \"port\": -1, templete을 이용해 굳이 모든 정보를 입력하지 않아도 된다. Zeppelin 웹에서 interpreter를 생성한 후 default.driver, default.password, default.url, default.user, Dependencies 2개를 작성한 후 생성하면 자동으로 interpreter.json에 추가된다. 주의할 점은, HiveServer2 로 접근이 가능한 상태임을 체크해주어야 한다. Zeppelin을 재실행한다. Notebook을 생성하고 %jdbc \\n show tables을 실행하여 정상적으로 연결되었는지 확인한다. 1234$ $ZEPPELIN_HOME/bin/zeppelin-daemon.sh restart%hiveshow tables Reference Zeppelin Documents, https://zeppelin.apache.org/docs/0.8.0/interpreter/hive.html#dependencies 2019.11.18 made by jaejun.lee","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://jx2lee.github.io/categories/Hadoop/"}],"tags":[]},{"title":"[Hadoop] Install Presto","slug":"hadoop-install_presto","date":"2019-11-11T15:00:00.000Z","updated":"2020-05-15T05:24:08.746Z","comments":true,"path":"hadoop-install_presto/","link":"","permalink":"https://jx2lee.github.io/hadoop-install_presto/","excerpt":"Presto를 설치하는 과정을 살펴본다","text":"Presto를 설치하는 과정을 살펴본다 PreliminariesAdd user &amp; DownloadPresto user를 생성하고 hdclient 그룹에 포함한다. 이후 설치파일을 다운로드한다. 12$ adduser presto --gid 8630 # hdclient$ wget https://repo1.maven.org/maven2/com/facebook/presto/presto-server/0.228/presto-server-0.228.tar.gz .bash_profile.bash_profile에 Hadoop 및 Presto Env를 추가한다. 123456789101112131415161718# Hadoop Envexport JAVA_HOME=/app/jdkexport HADOOP_HOME=/app/hadoopexport PATH=$PATH:$JAVA_HOME/bin:\\$HADOOP_HOME/bin:\\$HADOOP_HOME/sbinexport HADOOP_PREFIX=/app/hadoopexport HADOOP_COMMON_HOME=$HADOOP_PREFIXexport HADOOP_CONF_DIR=$HADOOP_PREFIX/etc/hadoopexport HADOOP_HDFS_HOME=$HADOOP_PREFIXexport HADOOP_MAPRED_HOME=$HADOOP_PREFIXexport HADOOP_YARN_HOME=$HADOOP_PREFIXexport YARN_CONF_DIR=$HADOOP_PREFIX/etc/hadoop# Presto Envexport PRESTO_HOME=/app/prestoexport PATH=$PATH:$JAVA_HOME/bin:$HIVE_HOME/bin Configuring Presto세 개의 설정파일을 $PRESTO_HOME/etc 폴더에 생성한다. /etc/node.properties123node.environment=productionnode.id=ffffffff-ffff-ffff-ffff-ffffffffffffnode.data-dir=/app/presto/data /etc/jvm.confing12345678-server-Xmx16G-XX:+UseG1GC-XX:G1HeapRegionSize=32M-XX:+UseGCOverheadLimit-XX:+ExplicitGCInvokesConcurrent-XX:+HeapDumpOnOutOfMemoryError-XX:+ExitOnOutOfMemoryError /etc/config.properties12345678coordinator=truenode-scheduler.include-coordinator=truehttp-server.http.port=8000query.max-memory=5GBquery.max-memory-per-node=1GBquery.max-total-memory-per-node=2GBdiscovery-server.enabled=truediscovery.uri=http://192.168.xxx.xxx:8000 http-server.htt.port : Presto는 내부 및 외부 모든 통신에 HTTP를 사용하며, 내 경우 8000번 포트를 open, 이를 통해 통신 discovery.uri : Presto instance는 시작 시 Discovery service에 등록되는 URI로 Presto 구동 서버의 IP와 port (위와 같은 경우는 8000 port) 로 작성 /etc/catalog/hive.properties12connector.name=hive-hadoop2hive.metastore.uri=thrift://localhost:9083 Hive MetaStore 의 default port는 *9083*** 위에 생성한 파일들을 tree로 표현하면 다음과 같다. 123456789$ tree ..├── catalog│ └── hive.properties├── config.properties├── jvm.config└── node.properties1 directory, 4 files Start Presto Server$PRESTO_HOME/bin 폴더에 launcher 파일을 실행한다. 12345678$ ./launcher startStarted as 25072# $PRESTO_HOME/data/var/log/launcher.log2019-11-12T14:56:02.306+0900 INFO main io.airlift.log.Logging Logging to stderr2019-11-12T14:56:02.308+0900 INFO main Bootstrap Loading configuration2019-11-12T14:56:02.404+0900 INFO main Bootstrap Initializing logging2019-11-12T14:56:02.447+0900 INFO main io.airlift.log.Logging Logging to /app/presto/data/var/log/server.log2019-11-12T14:56:02.497+0900 INFO main io.airlift.log.Logging Disabling stderr output Presto CLI 을 wget을 이용해 다운로드 한다. 이후 실행권한을 주고 CLI 를 실행한다. 123$ wget https://repo1.maven.org/maven2/com/facebook/presto/presto-cli/0.228/presto-cli-0.228-executable.jar &amp;&amp; mv presto-cli-0.228-executable.jar presto$ chmod +x presto$ ./presto --server 192.168.154.156:8000 --catalog hive --schema project Presto CLI 명령어 arguments server : discovery.uri catalog : Hive MetaStore schema : Hive metaStore 중 project db ** 테이블을 조회해보자. 123456789101112presto:project&gt; show tables; Table ---------- binvt00t bprjt00t ccomp00t iprsn00t (4 rows)Query 20191112_060524_00002_bgi94, FINISHED, 1 nodeSplits: 19 total, 19 done (100.00%)0:02 [4 rows, 100B] [1 rows/s, 43B/s] 구축 완료! Query 속도 비교테이블의 row 수를 반환하는 쿼리문을 Presto와 Hive에서 수행해본다. Presto 123456789presto:project&gt; select count(*) from bprjt00t; _col0 ------- 65355 (1 row)Query 20191112_061041_00004_bgi94, FINISHED, 1 nodeSplits: 23 total, 23 done (100.00%)0:04 [65.4K rows, 9.29MB] [15.9K rows/s, 2.26MB/s] Hive 123456789101112131415161718192021222324252627hive&gt; select count(*) &gt; from bprjt00t;WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.Query ID = hive_20191112151209_8598d146-fed8-4b55-8a2b-e1ed186a82d0Total jobs = 1Launching Job 1 out of 1Number of reduce tasks determined at compile time: 1In order to change the average load for a reducer (in bytes): set hive.exec.reducers.bytes.per.reducer=&lt;number&gt;In order to limit the maximum number of reducers: set hive.exec.reducers.max=&lt;number&gt;In order to set a constant number of reducers: set mapreduce.job.reduces=&lt;number&gt;Starting Job = job_1567153359966_0111, Tracking URL = http://node5.dat:8088/proxy/application_1567153359966_0111/Kill Command = /app/hadoop/bin/hadoop job -kill job_1567153359966_0111Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 12019-11-12 15:12:19,615 Stage-1 map = 0%, reduce = 0%2019-11-12 15:12:23,860 Stage-1 map = 100%, reduce = 0%, Cumulative CPU 4.1 sec2019-11-12 15:12:27,986 Stage-1 map = 100%, reduce = 100%, Cumulative CPU 5.98 secMapReduce Total cumulative CPU time: 5 seconds 980 msecEnded Job = job_1567153359966_0111MapReduce Jobs Launched: Stage-Stage-1: Map: 2 Reduce: 1 Cumulative CPU: 5.98 sec HDFS Read: 9754283 HDFS Write: 105 SUCCESSTotal MapReduce CPU Time Spent: 5 seconds 980 msecOK65355Time taken: 20.718 seconds, Fetched: 1 row(s) 대략 Presto가 Hive 대비 쿼리속도가 *20배 가량 빠르다*** Reference Presto Documents A Single-node Installation of Presto and Simple Benchmarks made by jaejun.lee","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://jx2lee.github.io/categories/Hadoop/"}],"tags":[]},{"title":"[Hadoop] Sqoop을 이용한 Table 조회","slug":"hadoop-sqoop_example","date":"2019-11-06T15:00:00.000Z","updated":"2020-03-30T15:06:23.578Z","comments":true,"path":"hadoop-sqoop_example/","link":"","permalink":"https://jx2lee.github.io/hadoop-sqoop_example/","excerpt":"Tibero Table을 Sqoop 을 이용해 HDFS에 저장함과 동시에, Hive 로 조회하는 예제를 살펴본다.","text":"Tibero Table을 Sqoop 을 이용해 HDFS에 저장함과 동시에, Hive 로 조회하는 예제를 살펴본다. PreliminariesRDBMS TableBPRJT00T 테이블을 확인한다. 123456789101112131415161718192021222324252627282930313233343536373839$ DESC BPRJT00T;COLUMN_NAME TYPE CONSTRAINT ---------------------------------------- ------------------ --------------------PRJT_CD VARCHAR(10) PRIMARY KEYPRJT_NM VARCHAR(1000) COMP_CD VARCHAR(10) CUST_CD VARCHAR(10) PRJT_ENV VARCHAR(3000) BUSI_AMT NUMBER(13) ATTACH_NO_ORG VARCHAR(20) IMPORTANT_CLS VARCHAR(1) NOT NULLMA_PRJT_CLS VARCHAR(1) NOT NULLREPORT_CLS VARCHAR(4) NOT NULLPRIORITY_CD VARCHAR(4) NOT NULLPRJT_STATUS VARCHAR(4) NOT NULLSALE_EMP VARCHAR(7) REMARK VARCHAR(4000) REG_EMP VARCHAR(7) REG_DATE VARCHAR(8) MOD_EMP VARCHAR(7) MOD_DATE VARCHAR(8) LOSS_PROD VARCHAR(200) CURRENCY_KIND VARCHAR(4) NOT NULLRECNTR_YN VARCHAR(1) NOT NULLRECNTR_STATUS VARCHAR(4) DIST_PATH VARCHAR(100) DISTRIB_YN VARCHAR(1) NOT NULLDISTRIB_PRJTCD VARCHAR(10) INDEX_NAME TYPE COLUMN_NAME -------------------------------- ------------------------ ----------------------BPRJT00T_IDX01 NORMAL SALE_EMPBPRJT00T_IDX02 NORMAL COMP_CDBPRJT00T_PK NORMAL PRJT_CD$ SELECT COUNT(*) FROM BPRJT00T; COUNT(*)---------- 56125 Sqoop evalSqoop을 이용해 테이블 접근이 가능한지 확인한다. 123456$ sqoop eval \\-connect jdbc:tibero:thin:@[ip]:[port]:[DB SID]] \\-driver com.tmax.tibero.jdbc.TbDriver \\-username XXX -password XXX \\-e \"select * from BPRJT00T where rownum &lt; 10\"... 보안상 조회 결과는 생략하였다 RDMBS to HDFS확인이 끝났다면, 아래 명령어를 통해 Sqoop으로 HDFS에 저장하고 동시에 Hive로 Import 한다. 12345678910sqoop import \"-Dorg.apache.sqoop.splitter.allow_text_splitter=true\" \\--connect jdbc:tibero:thin:@[ip]:[port]:[DB SID] \\--driver com.tmax.tibero.jdbc.TbDriver \\--target-dir /project/BPRJT00T \\--username XXX --password XXX \\--table BPRJT00T \\--fields-terminated-by \",\" \\--hive-import \\--create-hive-table \\--hive-table project.BPRJT00T Hive로 Import 하기 위해서는 미리 Database가 구성되어 있어야 한다. (나의 경우 DB Name은 project) 12345678...19/11/07 16:28:43 INFO hive.HiveImport: OK19/11/07 16:28:43 INFO hive.HiveImport: Time taken: 4.162 seconds19/11/07 16:28:44 INFO hive.HiveImport: Loading data to table project.bprjt00t19/11/07 16:28:44 INFO hive.HiveImport: OK19/11/07 16:28:44 INFO hive.HiveImport: Time taken: 0.611 seconds19/11/07 16:28:45 INFO hive.HiveImport: Hive import complete.19/11/07 16:28:45 INFO hive.HiveImport: Export directory is contains the _SUCCESS file only, removing the directory. Result우선, Hive로 Import 한 테이블을 hdfs 명렁어로 확인한다. 123456$ hdfs dfs -ls /user/hive/warehouseFound 1 itemsdrwxrwxrwx - hive supergroup 0 2019-11-07 16:27 /user/hive/warehouse/project.db$ hdfs dfs -ls /user/hive/warehouse/project.db/Found 1 itemsdrwxrwxrwx - hive supergroup 0 2019-11-07 16:28 /user/hive/warehouse/project.db/bprjt00t 잘 들어갔다. 그럼 Hive 콘솔로 접속하여 BPRJT00T 테이블을 조회해보자. 1234567891011121314$ hiveSLF4J: Class path contains multiple SLF4J bindings.SLF4J: Found binding in [jar:file:/app/hive/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: Found binding in [jar:file:/app/hadoop/2.9.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]Logging initialized using configuration in jar:file:/app/hive/lib/hive-common-2.3.6.jar!/hive-log4j2.properties Async: trueHive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.hive&gt; $ use project;OKTime taken: 2.963 secondshive&gt; $ select * from bprjt00t limit 10;OK 이또한, 조회결과는 생략 2019.11.07 made by jaejun.lee","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://jx2lee.github.io/categories/Hadoop/"}],"tags":[]},{"title":"[Hadoop] Hive Import 시 Could not initialize class org.apache.derby.jdbc.EmbeddedDriver 문제해결","slug":"hadoop-sqoop_import_error","date":"2019-11-06T15:00:00.000Z","updated":"2020-03-30T15:06:23.585Z","comments":true,"path":"hadoop-sqoop_import_error/","link":"","permalink":"https://jx2lee.github.io/hadoop-sqoop_import_error/","excerpt":"Hive 에 RDMS 테이블을 import 하는 과정에서 발생한 문제를 해결한다","text":"Hive 에 RDMS 테이블을 import 하는 과정에서 발생한 문제를 해결한다 Status아래와 같은 명령어를 통해 Tibero 테이블 BPRJT00T를 Sqoop으로 땡겨오고 Hive로 Import 하고자 했다. 12345678910sqoop import \"-Dorg.apache.sqoop.splitter.allow_text_splitter=true\" \\--connect jdbc:tibero:thin:@192.168.154.xxx:xxxx:tibero \\--driver com.tmax.tibero.jdbc.TbDriver \\--target-dir /project/BPRJT00T \\--username ERP --password xxxx \\--table BPRJT00T \\--fields-terminated-by \",\" \\--hive-import \\--create-hive-table \\--hive-table project.BPRJT00T Error Message123Could not initialize class org.apache.derby.jdbc.EmbeddedDriver......... 12 more Solution분명 Hive의 MetaStore를 MySQL로 설정하였는데(초기화까지 완료한 상태) 자꾸 Derby Driver를 못찾았다는 에러가 발생하였다. 이는, hive-site.xml이 Hive가 인식을 못해 Default Database로 Derby를 사용했기 때문이다. 이는 .bash_profile 또는 .profile 내 HADOOP_CLASSPATH를 추가하여 해결할 수 있다. 만약 나처럼 MetaStore를 MySQL이 아닌 Derby로 설정했는데 에러가 발생한다면, $HIVE_HOME/lib 안에 connector 파일이 있는지 확인하고 없다면 copy &amp; paste 하자 123$ vi ~/.profileexport HADOOP_CLASSPATH=$HIVE_HOME/conf:$HIVE_HOME/lib$ . ~/.profile 이후 Status에서 작성한 커맨드를 실행하면 Hive에 미리 생성해놓은 Database에 테이블이 생성한 것을 확인할 수 있다. 123$ hdfs dfs -ls /user/hive/warehouseFound 1 itemsdrwxrwxrwx - hive supergroup 0 2019-11-07 15:41 /user/hive/warehouse/project.db MySQL hive 유저의 proejct DB에 bprjt00t 테이블이 들어가 있음을 확인 2019.11.07 made by jaejun.lee","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://jx2lee.github.io/categories/Hadoop/"}],"tags":[]},{"title":"[Database] Install MySQL 8.0","slug":"database-install_mysql","date":"2019-11-05T15:00:00.000Z","updated":"2020-05-15T05:21:06.069Z","comments":true,"path":"database-install_mysql/","link":"","permalink":"https://jx2lee.github.io/database-install_mysql/","excerpt":"Hive의 Meta Store로 MySQL를 사용하기 위해 설치하고, 이를 정리한다","text":"Hive의 Meta Store로 MySQL를 사용하기 위해 설치하고, 이를 정리한다 Setting Environment설치에 필요한 라이브러리 version을 맞춰줄 필요가 있다. Version Up CMakeCMake 이 하위 version이라면 올려보도록 한다. 12345$ tar -xvzf cmake-3.16.0-rc3.tar.gz$ cd cmake-3.16.0-rc3.tar.gz$ ./bootstrap$ make$ sudo make install Version up gcc마찬가지로 gcc version이 하위 버젼이면 올려보도록 한다. 123456789$ sudo yum install centos-release-scl$ sudo yum install devtoolset-7-gcc*$ scl enable devtoolset-7 bash$ which gcc$ gcc --versiongcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-36)Copyright (C) 2015 Free Software Foundation, Inc.This is free software; see the source for copying conditions. There is NOwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. Makewget을 이용해 binary 파일을 다운받고 CMake을 이용해 설치한다. 12345678910111213141516171819202122232425262728$ cd /app/$ wget https://dev.mysql.com/get/Downloads/MySQL-8.0/mysql-8.0.18.tar.gz$ tar xvfz mysql-8.0.18.tar.gz$ cd mysql-8.0.18$ cmake \\-DCMAKE_INSTALL_PREFIX=/app/mysql \\-DMYSQL_DATADIR=/home/mysql/data \\-DSYSCONFDIR=/app/mysql \\-DMYSQL_USER=mysql \\-DWITH_MYISAM_STORAGE_ENGINE=1 \\-DWITH_INNOBASE_STORAGE_ENGINE=1 \\-DWITH_PARTITION_STORAGE_ENGINE=1 \\-DWITH_FEDERATED_STORAGE_ENGINE=1 \\-DWITH_BLACKHOLE_STORAGE_ENGINE=1 \\-DWITH_MEMORY_STORAGE_ENGINE=1 \\-DWITH_READLINE=1 \\-DMYSQL_UNIX_ADDR=/app/mysql/mysql.sock \\-DMYSOL_TCP_PORT=3306 \\-DENABLED_LOCAL_INFILE=1 \\-DENABLE_DOWNLOADS=1 \\-DWITH_EXTRA_CHARSETS=all \\-DDEFAULT_CHARSET=utf8 \\-DDEFAULT_COLLATION=utf8_general_ci \\-DWITH_DEBUG=0 \\-DMYSQL_MAINTAINER_MODE=0 \\-DDOWNLOAD_BOOST=1 \\-DDOWNLOAD_BOOST=1 -DWITH_BOOST=/app/mysql-8.0.18$ make install Add Servcie123$ cp mysql.server /etc/rc.d/init.d/mysql$ ln -s /etc/rc.d/init.d/mysql /etc/rc.d/rc3.d/S97mysql $ vi /usr/lib/systemd/system/mysql.service /etc/my.cnf/etc 에 my.cnf config 파일을 생성한다. my.cnf는 MySQL의 config를 설정하는 파일이며, 본 설치에서는 DB Engine으로 InnoDB를 사용한다. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697[client]default-character-set &#x3D; utf8port &#x3D; 3306socket &#x3D; &#x2F;tmp&#x2F;mysql.sockdefault-character-set &#x3D; utf8 [mysqld]socket&#x3D;&#x2F;app&#x2F;mysql&#x2F;mysql.sockdatadir&#x3D;&#x2F;home&#x2F;mysql&#x2F;databasedir &#x3D; &#x2F;app&#x2F;mysql#user &#x3D; mysql#bind-address &#x3D; 0.0.0.0skip-external-lockingkey_buffer_size &#x3D; 384Mmax_allowed_packet &#x3D; 16Mtable_open_cache &#x3D; 2048sort_buffer_size &#x3D; 2Mread_buffer_size &#x3D; 2Mread_rnd_buffer_size &#x3D; 8Mmyisam_sort_buffer_size &#x3D; 64Mthread_cache_size &#x3D; 8 #dns queryskip-name-resolve #connectionmax_connections &#x3D; 1000max_connect_errors &#x3D; 1000wait_timeout&#x3D; 60 #slow-queries#slow_query_log &#x3D; &#x2F;home&#x2F;mysql_data&#x2F;slow-queries.log#long_query_time &#x3D; 3#log-slow-queries &#x3D; &#x2F;home&#x2F;mysql_data&#x2F;mysql-slow-queries.log ##timestampexplicit_defaults_for_timestampsymbolic-links&#x3D;0### loglog-error&#x3D;&#x2F;home&#x2F;mysql&#x2F;data&#x2F;mysqld.logpid-file&#x3D;&#x2F;home&#x2F;mysql&#x2F;mysqld.pid ###chractercharacter-set-client-handshake&#x3D;FALSEinit_connect &#x3D; SET collation_connection &#x3D; utf8_general_ciinit_connect &#x3D; SET NAMES utf8character-set-server &#x3D; utf8collation-server &#x3D; utf8_general_cisymbolic-links&#x3D;0##Password Policy#validate_password_policy&#x3D;LOW#validate_password_policy&#x3D;MEDIUM ### MyISAM Spectific options#default-storage-engine &#x3D; myisamkey_buffer_size &#x3D; 32Mbulk_insert_buffer_size &#x3D; 64Mmyisam_sort_buffer_size &#x3D; 128Mmyisam_max_sort_file_size &#x3D; 10Gmyisam_repair_threads &#x3D; 1 ### INNODB Spectific optionsdefault-storage-engine &#x3D; InnoDB#skip-innodb#innodb_additional_mem_pool_size &#x3D; 16Minnodb_buffer_pool_size &#x3D; 1024MBinnodb_data_file_path &#x3D; ibdata1:10M:autoextendinnodb_write_io_threads &#x3D; 8innodb_read_io_threads &#x3D; 8innodb_thread_concurrency &#x3D; 16innodb_flush_log_at_trx_commit &#x3D; 1innodb_log_buffer_size &#x3D; 8Minnodb_log_file_size &#x3D; 128Minnodb_log_files_in_group &#x3D; 3innodb_max_dirty_pages_pct &#x3D; 90innodb_lock_wait_timeout &#x3D; 120 [mysqldump]default-character-set &#x3D; utf8max_allowed_packet &#x3D; 512M [mysql]#no-auto-rehashdefault-character-set &#x3D; utf8 [myisamchk]key_buffer_size &#x3D; 512Msort_buffer_size &#x3D; 512Mread_buffer &#x3D; 8Mwrite_buffer &#x3D; 8M Initialize DatabaseDatabase를 mysql user로 초기화 한다. 12345678910111213141516171819202122232425$ /app/mysql/bin/mysqld --initialize-insecure --basedir=/app/mysql --datadir=/home/mysql/data --user=mysql$ ll /home/mysql/data/total 448572-rw-r----- 1 mysql mysql 56 Nov 5 17:19 auto.cnf-rw------- 1 mysql mysql 1680 Nov 5 17:19 ca-key.pem-rw-r--r-- 1 mysql mysql 1112 Nov 5 17:19 ca.pem-rw-r--r-- 1 mysql mysql 1112 Nov 5 17:19 client-cert.pem-rw------- 1 mysql mysql 1676 Nov 5 17:19 client-key.pem-rw-r----- 1 mysql mysql 6100 Nov 5 17:19 ib_buffer_pool-rw-r----- 1 mysql mysql 10485760 Nov 5 17:19 ibdata1-rw-r----- 1 mysql mysql 134217728 Nov 5 17:19 ib_logfile0-rw-r----- 1 mysql mysql 134217728 Nov 5 17:19 ib_logfile1-rw-r----- 1 mysql mysql 134217728 Nov 5 17:19 ib_logfile2drwxr-x--- 2 mysql mysql 6 Nov 5 17:19 #innodb_tempdrwxr-x--- 2 mysql mysql 143 Nov 5 17:19 mysql-rw-r----- 1 mysql mysql 1301 Nov 5 17:19 mysqld.log-rw-r----- 1 mysql mysql 25165824 Nov 5 17:19 mysql.ibddrwxr-x--- 2 mysql mysql 8192 Nov 5 17:19 performance_schema-rw------- 1 mysql mysql 1680 Nov 5 17:19 private_key.pem-rw-r--r-- 1 mysql mysql 452 Nov 5 17:19 public_key.pem-rw-r--r-- 1 mysql mysql 1112 Nov 5 17:19 server-cert.pem-rw------- 1 mysql mysql 1676 Nov 5 17:19 server-key.pemdrwxr-x--- 2 mysql mysql 28 Nov 5 17:19 sys-rw-r----- 1 mysql mysql 10485760 Nov 5 17:19 undo_001-rw-r----- 1 mysql mysql 10485760 Nov 5 17:19 undo_002 Restart Service &amp; CheckingService 재기동 후 MySQL이 제대로 설치되었는지 확인한다. 123456789101112131415161718192021222324252627282930313233343536373839404142$ systemctl stop mysql$ systemctl start mysql$ /app/mysql/bin/mysql -u root -pEnter password: Welcome to the MySQL monitor. Commands end with ; or \\g.Your MySQL connection id is 8Server version: 8.0.18 Source distributionCopyright (c) 2000, 2019, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement.mysql&gt; mysql&gt; mysql&gt; mysql&gt; \\s--------------/app/mysql/bin/mysql Ver 8.0.18 for Linux on x86_64 (Source distribution)Connection id: 8Current database: Current user: root@localhostSSL: Not in useCurrent pager: stdoutUsing outfile: ''Using delimiter: ;Server version: 8.0.18 Source distributionProtocol version: 10Connection: Localhost via UNIX socketServer characterset: utf8Db characterset: utf8Client characterset: utf8Conn. characterset: utf8UNIX socket: /tmp/mysql.sockUptime: 17 min 25 secThreads: 2 Questions: 6 Slow queries: 0 Opens: 115 Flush tables: 3 Open tables: 35 Queries per second avg: 0.005-------------- MySQL 시작 시 /tmp/mysql.sock 이 없다고 fail이 날 수 있다. 이때는 tmp 폴더안에 mysql.sock이 있는 path로 링크를 생성하면된다. (초기 설정부터 /tmp에 안들어가게끔 어떻게 설정하지..? 이건 내일하자!) Reference MYSQL 8.0 INSTALL ( mysql 8.0.17 ) / Centos 7 CentOS7에서 Mysql 8.0 소스 설치 MySQL Documents 2019.11.06 made by jaejun.lee","categories":[{"name":"Database","slug":"Database","permalink":"https://jx2lee.github.io/categories/Database/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://jx2lee.github.io/tags/MySQL/"}]},{"title":"[Hadoop] Install Hive","slug":"hadoop-install_hive","date":"2019-11-05T15:00:00.000Z","updated":"2020-05-15T05:23:03.960Z","comments":true,"path":"hadoop-install_hive/","link":"","permalink":"https://jx2lee.github.io/hadoop-install_hive/","excerpt":"Hive를 설치하는 과정을 살펴본다","text":"Hive를 설치하는 과정을 살펴본다 PreliminariesAdd user &amp; groupHive user를 생성한다. 123$ groupadd hdclient$ usermod -g hdclient sqoop $ adduser hive --gid 8630 # hdclient .bash_profile.bash_profile에 Hadoop 및 Hive ENV를 추가한다. 123456789101112131415161718# Hadoopexport JAVA_HOME=/app/jdkexport HADOOP_HOME=/app/hadoopexport PATH=$PATH:$JAVA_HOME/bin:\\$HADOOP_HOME/bin:\\$HADOOP_HOME/sbinexport HADOOP_PREFIX=/app/hadoopexport HADOOP_COMMON_HOME=$HADOOP_PREFIXexport HADOOP_CONF_DIR=$HADOOP_PREFIX/etc/hadoopexport HADOOP_HDFS_HOME=$HADOOP_PREFIXexport HADOOP_MAPRED_HOME=$HADOOP_PREFIXexport HADOOP_YARN_HOME=$HADOOP_PREFIXexport YARN_CONF_DIR=$HADOOP_PREFIX/etc/hadoop# Hiveexport HIVE_HOME=/app/hiveexport PATH=$PATH:$JAVA_HOME/bin:$HIVE_HOME/bin Setting MySQL for Hive MetaStoreHive MetaStore를 MySQL로 사용하기위해 새로운 database와 hive user를 생성한다 12345678910111213141516$ create database hive;$ create user hive@'%' identified by 'hive';$ grant all privileges on hive.* to hive@'%';$ use mysql;$ select host, user from user;+-----------+------------------+| host | user |+-----------+------------------+| % | hive || localhost | mysql.infoschema || localhost | mysql.session || localhost | mysql.sys || localhost | root |+-----------+------------------+5 rows in set (0.00 sec) user를 생성할 때 골뱅이 뒤 %는 모든 외부 ip의 접근을 허용한다는 뜻이다. jdbc to $HIVE_HOME/lib/MySQL Driver를 해당 PATH로 복사한다. $ cp tibero6-jdbc.jar /app/hive/lib/ Driver URL : https://dev.mysql.com/downloads/connector/j/8.0.html Hive Configurationshive-env.sh123$ cp $HIVE_HOME/conf/hive-env.sh.template $HIVE_HOME/conf/hive-env.sh`$ vi $HIVE_HOME/conf/hive-env.shHADOOP_HOME=/app/hadoop hive-site.xmlhive-site.xml을 아래와 같이 작성한다. Hive의 MetaStore를 외부 서버의 MySQL를 이용할 예정이다. (Hive 설치된 서버와 별개의 서버이다. 만약 같은 서버라면, javax.jdo.option.ConnectionURL의 ip:port값은 localhost:port로 작성한다.) 1234567891011121314151617181920212223242526&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hive.metastore.local&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://ip:port/hive?serverTimezone=UTC&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;hive&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;hive&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.metastore.urls&lt;/name&gt; &lt;value&gt;thrift://node5.dat:10000&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; javax.jdo.option.ConnectionURL의 port 뒤 hive는 MetaStore로 사용하기 위한 MySQL DB name이다. 뒤에 serverTimezone arg를 추가한 이유는 추가하지 않으면 SchemaTool 초기화 시 에러 메세지가 뜬다. 결국엔 시간 형식이 맞지 않아 생기는 문제이므로 arg를 추가한다 Create MetaStore Schemaschematool -initSchema -dbType mysql --verbose 을 통해 Hive의 MetaStore를 초기화 한다. 1234567891011121314151617181920212223242526272829$ schematool -initSchema -dbType mysql --verbose...beeline&gt; Initialization script completedschemaTool completed$ mysql -u hive -pmysql&gt; use hive;Reading table information for completion of table and column namesYou can turn off this feature to get a quicker startup with -ADatabase changedmysql&gt; show tables;+---------------------------+| Tables_in_hive |+---------------------------+| AUX_TABLE || BUCKETING_COLS || CDS || COLUMNS_V2 || COMPACTION_QUEUE || COMPLETED_COMPACTIONS |......| SERDE_PARAMS || TYPES || TYPE_FIELDS || VERSION || WRITE_SET |+---------------------------+57 rows in set (0.00 sec) Run HiveServer &amp; MetaStorehiveserver2와 metastore를 기동해주면 완료. 1234567891011121314$ hive --service hiveserver2 &amp;SLF4J: Class path contains multiple SLF4J bindings.SLF4J: Found binding in [jar:file:/app/hive/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: Found binding in [jar:file:/app/hadoop/2.9.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]Loading class `com.mysql.jdbc.Driver'. This is deprecated. The new driver class is `com.mysql.cj.jdbc.Driver'. The driver is automatically registered via the SPI and manual loading of the driver class is generally unnecessary.$ hive --service metastore &amp;SLF4J: Class path contains multiple SLF4J bindings.SLF4J: Found binding in [jar:file:/app/hive/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: Found binding in [jar:file:/app/hadoop/2.9.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]Loading class `com.mysql.jdbc.Driver'. This is deprecated. The new driver class is `com.mysql.cj.jdbc.Driver'. The driver is automatically registered via the SPI and manual loading of the driver class is generally unnecessary. made by jaejun.lee","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://jx2lee.github.io/categories/Hadoop/"}],"tags":[]},{"title":"[Python] Mappers and Reducers","slug":"python-map_reduce","date":"2019-10-30T15:00:00.000Z","updated":"2020-03-30T15:06:23.551Z","comments":true,"path":"python-map_reduce/","link":"","permalink":"https://jx2lee.github.io/python-map_reduce/","excerpt":"hackerrank에서 제공하는 Database 카테고리의 MapReduce 문제를 풀어본다","text":"hackerrank에서 제공하는 Database 카테고리의 MapReduce 문제를 풀어본다 문제Mappers and Reducers Here’s a quick but comprehensive introduction to the idea of splitting tasks into a MapReduce model. The four important functions involved are: 1234Map (the mapper function) EmitIntermediate(the intermediate key,value pairs emitted by the mapper functions) Reduce (the reducer function) Emit (the final output, after summarization from the Reduce functions) We provide you with a single system, single thread version of a basic MapReduce implementation. Task Joins are The input is a number of lines with pairs of name of friends, in the form: 1[Friend1] [Friend2] The required output is to print the number of friends of each person, in the format shown. The code for the MapReduce class, parts related to IO etc. has already been provided. However, the mapper and reducer functions are incomplete. Your task is to fill up the mapper and reducer functions appropriately, such that the program works, and outputs the list of number of friends of each person , in lexicographical order. Also, this program outputs certain information to the error stream. This information has been logged to help beginners gain a better understanding of the the intermediate steps in a map-reduce process. Languages Supported Currently, we provide the base code in Python. Input Format A list of single space separated pairs of friend names. We have already written the input handling code to read in this data. Output Format Again, the output handling part has already been provided in the template code. The Key contains [Person name] and the value contains the number of friends, sorted in lexicographical order. The entities in this list, will naturally be confined to only those people provided in the input data. Sample Input 1234Joe SueSue PhiPhi JoePhi Alice Sample Output 1234&#123;&quot;key&quot;:&quot;Alice&quot;,&quot;value&quot;:&quot;1&quot;&#125;&#123;&quot;key&quot;:&quot;Joe&quot;,&quot;value&quot;:&quot;2&quot;&#125;&#123;&quot;key&quot;:&quot;Phi&quot;,&quot;value&quot;:&quot;3&quot;&#125;&#123;&quot;key&quot;:&quot;Sue&quot;,&quot;value&quot;:&quot;2&quot;&#125; Explanation We have computed the number of friends for each person via the Mapper and Reducer functions. 해결full code 는 아래와 같다. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051import sysfrom collections import OrderedDictclass MapReduce: def __init__(self): self.intermediate = OrderedDict() self.result = [] def emitIntermediate(self, key, value): self.intermediate.setdefault(key, []) self.intermediate[key].append(value) def emit(self, value): self.result.append(value) def execute(self, data, mapper, reducer): for record in data: mapper(record) for key in self.intermediate: reducer(key, self.intermediate[key]) self.result.sort() print(self.result) for item in self.result: print(\"&#123;\\\"key\\\":\\\"\" + item[0] + \"\\\",\\\"value\\\":\\\"\" + str(item[1]) + \"\\\"&#125;\")mapReducer = MapReduce()def mapper(record): # Start writing the Map code here words = record.split() mapReducer.emitIntermediate(words[0], words[1]) mapReducer.emitIntermediate(words[1], words[0]) print(mapReducer.intermediate)def reducer(key, list_of_values): # Start writing the Reduce code here mapReducer.emit((key, len(list_of_values))) print(mapReducer.result)if __name__ == \"__main__\": # inputData = ['Joe Sue', 'Sue Phi', 'Phi Joe', 'Phi Alice'] inputData = [] for line in sys.stdin: inputData.append(line) mapReducer.execute(inputData, mapper, reducer) 우선, mapReduce class를 살펴본다. Class : mapReduceclss mapReduce는 init 함수를 포함 총 세 개의 함수를 갖는다. Func : initinit`함수로 인해 intermediate, result 변수를 갖는다. 이는 각각 key-value로 이루어진 dictionary (문제에서 원하는 단어 : 단어 출현 횟수를 의미)와 문제 정답에 맞는 형식의 Return 값이다. 123def __init__(self): self.intermediate = OrderedDict() self.result = [] Func : emitIntermediatekey-value 를 입력받아 dictionary에 추가하는 함수이다. 123def emitIntermediate(self, key, value): self.intermediate.setdefault(key, []) self.intermediate[key].append(value) Func : emit각 단어의 출현 횟수를 집계한 후 결과값에 담는 함수이다. 12def emit(self, value): self.result.append(value) Func : execute입력받은 데이터를 읽어들여 나중에 우리가 작성해야할 mapper / reducer함수를 이용해 최종 결과값을 알맞는 형태로 출력하는 함수이다. 12345678910def execute(self, data, mapper, reducer): for record in data: mapper(record) for key in self.intermediate: reducer(key, self.intermediate[key]) self.result.sort() print(self.result) for item in self.result: print(\"&#123;\\\"key\\\":\\\"\" + item[0] + \"\\\",\\\"value\\\":\\\"\" + str(item[1]) + \"\\\"&#125;\") 실행함수(excutable fucntion)라 생각하자 Func : mapper이제 mapper 를 살펴본다. 입력받은 한 문장은 split 함수를 통해 두 단어로 나누어주고, 첫 번째 단어만 key로 인식하면 안되기 때문에 mapReducer 클래스에서 만든 emitIntermediate 함수를 두 번 수행한다. 123456def mapper(record): # Start writing the Map code here words = record.split() mapReducer.emitIntermediate(words[0], words[1]) mapReducer.emitIntermediate(words[1], words[0]) print(mapReducer.intermediate) 과연 이 mapper 함수가 어떻게 작동되는지 문제에서 제공한 test case를 바탕으로 print 해보면 다음과 같이 출력된다. 즉, 같은 key값을 가지면 value로 append 해나간다. (value값으로 계속해서 단어를 추가하는데 이는 나중에 reducer 함수에서 집계를 할 때 사용한다) 1234OrderedDict([(&#39;Joe&#39;, [&#39;Sue&#39;]), (&#39;Sue&#39;, [&#39;Joe&#39;])])OrderedDict([(&#39;Joe&#39;, [&#39;Sue&#39;]), (&#39;Sue&#39;, [&#39;Joe&#39;, &#39;Phi&#39;]), (&#39;Phi&#39;, [&#39;Sue&#39;])])OrderedDict([(&#39;Joe&#39;, [&#39;Sue&#39;, &#39;Phi&#39;]), (&#39;Sue&#39;, [&#39;Joe&#39;, &#39;Phi&#39;]), (&#39;Phi&#39;, [&#39;Sue&#39;, &#39;Joe&#39;])])OrderedDict([(&#39;Joe&#39;, [&#39;Sue&#39;, &#39;Phi&#39;]), (&#39;Sue&#39;, [&#39;Joe&#39;, &#39;Phi&#39;]), (&#39;Phi&#39;, [&#39;Sue&#39;, &#39;Joe&#39;, &#39;Alice&#39;]), (&#39;Alice&#39;, [&#39;Phi&#39;])]) Func : reducerkey-value로 이루어진 dictionary를 집계해주는 reducer 함수이다. mapReducer 클래스의 emit함수를 통해 result 변수에 결과값을 저장한다. 이때, 위 mapper함수를 통해 각 key에 대한 value들의 길이를 key와 함께 append 한다. 1234def reducer(key, list_of_values): # Start writing the Reduce code here mapReducer.emit((key, len(list_of_values))) print(mapReducer.result) Main123456if __name__ == \"__main__\": # inputData = ['Joe Sue', 'Sue Phi', 'Phi Joe', 'Phi Alice'] inputData = [] for line in sys.stdin: inputData.append(line) mapReducer.execute(inputData, mapper, reducer) 이렇게 class 및 function을 직접 짜보면서 설계하는 단계의 중요성을 깨달았다. Python을 이런 방식으로 코딩을 해보는 연습을 해야겠다 참고 https://github.com/cielavenir/procon/blob/master/hackerrank/map-reduce-advanced-count-number-of-friends.py 하둡 맵리듀스(MapReduce) 알아보자, https://jayzzz.tistory.com/44 2019.10.31 made by jaejun.lee","categories":[{"name":"Python","slug":"Python","permalink":"https://jx2lee.github.io/categories/Python/"}],"tags":[{"name":"Hackerrank","slug":"Hackerrank","permalink":"https://jx2lee.github.io/tags/Hackerrank/"}]},{"title":"[SQL] 15 Days of Learning","slug":"hackerrank-15_days_of_learning","date":"2019-10-29T15:00:00.000Z","updated":"2020-03-30T15:06:23.575Z","comments":true,"path":"hackerrank-15_days_of_learning/","link":"","permalink":"https://jx2lee.github.io/hackerrank-15_days_of_learning/","excerpt":"hackerrank에서 제공하는 15 Days of Learning 를 SELECT sub query을 활용해 해결하였다.","text":"hackerrank에서 제공하는 15 Days of Learning 를 SELECT sub query을 활용해 해결하였다. 문제Julia conducted a days of learning SQL contest. The start date of the contest was March 01, 2016 and the end date was March 15, 2016. Write a query to print total number of unique hackers who made at least submission each day (starting on the first day of the contest), and find the hacker_id and name of the hacker who made maximum number of submissions each day. If more than one such hacker has a maximum number of submissions, print the lowest hacker_id. The query should print this information for each day of the contest, sorted by the date. Input Format The following tables hold contest data: Hackers: The hacker_id is the id of the hacker, and name is the name of the hacker. Submissions: The submission_date is the date of the submission, submission_id is the id of the submission, hacker_id is the id of the hacker who made the submission, and score is the score of the submission. Sample Input For the following sample input, assume that the end date of the contest was March 06, 2016. Hackers Table: Submissions Table: Sample Output 1234562016-03-01 4 20703 Angela2016-03-02 2 79722 Michael2016-03-03 2 20703 Angela2016-03-04 2 20703 Angela2016-03-05 1 36396 Frank2016-03-06 1 20703 Angela Explanation On March 01, 2016 hackers , , , and made submissions. There are unique hackers who made at least one submission each day. As each hacker made one submission, is considered to be the hacker who made maximum number of submissions on this day. The name of the hacker is Angela. On March 02, 2016 hackers , , and made submissions. Now and were the only ones to submit every day, so there are unique hackers who made at least one submission each day. made submissions, and name of the hacker is Michael. On March 03, 2016 hackers , , and made submissions. Now and were the only ones, so there are unique hackers who made at least one submission each day. As each hacker made one submission so is considered to be the hacker who made maximum number of submissions on this day. The name of the hacker is Angela. On March 04, 2016 hackers , , , and made submissions. Now and only submitted each day, so there are unique hackers who made at least one submission each day. As each hacker made one submission so is considered to be the hacker who made maximum number of submissions on this day. The name of the hacker is Angela. On March 05, 2016 hackers , , and made submissions. Now only submitted each day, so there is only unique hacker who made at least one submission each day. made submissions and name of the hacker is Frank. On March 06, 2016 only made submission, so there is only unique hacker who made at least one submission each day. made submission and name of the hacker is Angela. 접근Join을 이용해 문제를 풀려다 실패하였다. 이에 제공되는 Table이 2개인 점을 활용하여 SELECT 절 내 Sub query를 작성하는 것으로 접근하였다. 우선, 유일한 제출 날짜 Table로 부터 제출 날짜가 같고 제출 마감일까지 날짜 차이가 같은 유일한 hacker_id 를 조회하는 SELECT 문을 작성한다. 이때, 최종 결과물은 날짜를 기준으로 group화 시킨다. 1234567891011select submission_date, (select count(distinct hacker_id) from submissions as s2 where s2.submission_date = s1.submission_date and (select count(distinct s3.submission_date) from submissions as s3 where s3.hacker_id = s2.hacker_id and s3.submission_date &lt; s1.submission_date) = datediff(s1.submission_date, '2016-03-01')),from (select distinct submission_date from submissions) as s1group by submission_date; 이후, 제출을 가장 많이 한 hacker의 id를 조회한 id table 과 id table을 이용해 hacker 이름을 조회 하는 SELECT Sub query를 추가한다. 이때, id table의 경우 가장 많이 제출한 hacker만 뽑아야 하기 때문에 중복을 피하고자 LIMIT 1을 추가한다. 12345678910111213141516171819select submission_date, (select count(distinct hacker_id) from submissions as s2 where s2.submission_date = s1.submission_date and (select count(distinct s3.submission_date) from submissions as s3 where s3.hacker_id = s2.hacker_id and s3.submission_date &lt; s1.submission_date) = datediff(s1.submission_date, '2016-03-01')), --append (select hacker_id from submissions as s2 where s2.submission_date = s1.submission_date group by hacker_id order by count(submission_id) desc, hacker_id limit 1) as id, (select name from hackers where hacker_id = id) --/appendfrom (select distinct submission_date from submissions) as s1group by submission_date; 문제 접근할 때 Join만 바라보지 않고 SELECT / FROM / WHERE 절에서 Sub query를 작성하는 안목을 키워보자. (물론 hackerrank의 문제는 끝났다..) 해결1234567891011121314151617select submission_date, (select count(distinct hacker_id) from submissions as s2 where s2.submission_date = s1.submission_date and (select count(distinct s3.submission_date) from submissions as s3 where s3.hacker_id = s2.hacker_id and s3.submission_date &lt; s1.submission_date) = datediff(s1.submission_date, '2016-03-01')), (select hacker_id from submissions as s2 where s2.submission_date = s1.submission_date group by hacker_id order by count(submission_id) desc, hacker_id limit 1) as id, (select name from hackers where hacker_id = id)from (select distinct submission_date from submissions) as s1group by submission_date; 2019.10.30 made by jaejun.lee","categories":[{"name":"SQL","slug":"SQL","permalink":"https://jx2lee.github.io/categories/SQL/"}],"tags":[{"name":"Hackerrank","slug":"Hackerrank","permalink":"https://jx2lee.github.io/tags/Hackerrank/"}]},{"title":"[Hadoop] Sqoop import 시 JDBC-90401:Connection refused by the server Error 발생","slug":"hadoop-sqoop_jdbc_error","date":"2019-10-29T15:00:00.000Z","updated":"2020-03-30T15:06:23.562Z","comments":true,"path":"hadoop-sqoop_jdbc_error/","link":"","permalink":"https://jx2lee.github.io/hadoop-sqoop_jdbc_error/","excerpt":"상황Sqoop을 이용해 Tibero Table을 hdfs 형태로 변환하는 import 과정에서 ERROR가 발생하였다. 상황을 간단히 설명하면, 정보시스템 개발기 DB(Tibero) Tabel을 팀 서버에 구축한 Hadoop 에(51, 52 : DataNode, 53 : NameNode로 이하 숫자로 표현) 저장하고자 했다.","text":"상황Sqoop을 이용해 Tibero Table을 hdfs 형태로 변환하는 import 과정에서 ERROR가 발생하였다. 상황을 간단히 설명하면, 정보시스템 개발기 DB(Tibero) Tabel을 팀 서버에 구축한 Hadoop 에(51, 52 : DataNode, 53 : NameNode로 이하 숫자로 표현) 저장하고자 했다.정보시스템에서 허용한 IP는 총 4개였고 그 중 하나인 69(편하게 숫자로.. 대체하겠다)에 Sqoop을 설치하여 Hadoop에 저장하려는 계획이었다. 간략히 각 서버와 현황을 나타내면 아래와 같다. Hadoop 51, 52 : DataNode 53 : NameNode 정보시스템 개발기 DB에 접근이 허용되지 않음 Sqoop 69 정보시스템 개발기 DB에 접근이 허용되지 않음 Sqoop import 명령어 (sqoop import –connect jdbc:tibero:thin:@192.168.xx.xx:8629:tibero –driver com.tmax.tibero.jdbc.TbDriver –username xxxx –password xxxx –table PROJECT_INFO –delete-target-dir -m 1) 를 날리면 아래와 같은 에러가 발생하였다. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455sqoop@bips:/app/sqoop/sqoop$ sqoop import --connect jdbc:tibero:thin:@192.168.10.84:8629:tibero --driver com.tmax.tibero.jdbc.TbDriver --username tody -P --table PROJECT_INFO --delete-target-dir -m 1Warning: /app/sqoop/sqoop/../hbase does not exist! HBase imports will fail.Please set $HBASE_HOME to the root of your HBase installation.Warning: /app/sqoop/sqoop/../hcatalog does not exist! HCatalog jobs will fail.Please set $HCAT_HOME to the root of your HCatalog installation.Warning: /app/sqoop/sqoop/../accumulo does not exist! Accumulo imports will fail.Please set $ACCUMULO_HOME to the root of your Accumulo installation.Warning: /app/sqoop/sqoop/../zookeeper does not exist! Accumulo imports will fail.Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation.19/10/30 09:32:49 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7Enter password: 19/10/30 09:33:01 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time.19/10/30 09:33:01 INFO manager.SqlManager: Using default fetchSize of 100019/10/30 09:33:01 INFO tool.CodeGenTool: Beginning code generation19/10/30 09:33:01 ERROR manager.SqlManager: Error executing statement: java.sql.SQLException: JDBC-90401:Connection refused by the server. - Connection refused (Connection refused)java.sql.SQLException: JDBC-90401:Connection refused by the server. - Connection refused (Connection refused) at com.tmax.tibero.jdbc.err.TbError.makeSQLException(Unknown Source) at com.tmax.tibero.jdbc.err.TbError.newSQLException(Unknown Source) at com.tmax.tibero.jdbc.comm.TbStream.&lt;init&gt;(Unknown Source) at com.tmax.tibero.jdbc.comm.TbCommType4.createStream(Unknown Source) at com.tmax.tibero.jdbc.driver.TbConnection.openConnection(Unknown Source) at com.tmax.tibero.jdbc.TbDriver.connectInternal(Unknown Source) at com.tmax.tibero.jdbc.TbDriver.connect(Unknown Source) at java.sql.DriverManager.getConnection(DriverManager.java:664) at java.sql.DriverManager.getConnection(DriverManager.java:247) at org.apache.sqoop.manager.SqlManager.makeConnection(SqlManager.java:904) at org.apache.sqoop.manager.GenericJdbcManager.getConnection(GenericJdbcManager.java:59) at org.apache.sqoop.manager.SqlManager.execute(SqlManager.java:763) at org.apache.sqoop.manager.SqlManager.execute(SqlManager.java:786) at org.apache.sqoop.manager.SqlManager.getColumnInfoForRawQuery(SqlManager.java:289) at org.apache.sqoop.manager.SqlManager.getColumnTypesForRawQuery(SqlManager.java:260) at org.apache.sqoop.manager.SqlManager.getColumnTypes(SqlManager.java:246) at org.apache.sqoop.manager.ConnManager.getColumnTypes(ConnManager.java:327) at org.apache.sqoop.orm.ClassWriter.getColumnTypes(ClassWriter.java:1872) at org.apache.sqoop.orm.ClassWriter.generate(ClassWriter.java:1671) at org.apache.sqoop.tool.CodeGenTool.generateORM(CodeGenTool.java:106) at org.apache.sqoop.tool.ImportTool.importTable(ImportTool.java:501) at org.apache.sqoop.tool.ImportTool.run(ImportTool.java:628) at org.apache.sqoop.Sqoop.run(Sqoop.java:147) at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76) at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:183) at org.apache.sqoop.Sqoop.runTool(Sqoop.java:234) at org.apache.sqoop.Sqoop.runTool(Sqoop.java:243) at org.apache.sqoop.Sqoop.main(Sqoop.java:252)19/10/30 09:33:01 ERROR tool.ImportTool: Import failed: java.io.IOException: No columns to generate for ClassWriter at org.apache.sqoop.orm.ClassWriter.generate(ClassWriter.java:1677) at org.apache.sqoop.tool.CodeGenTool.generateORM(CodeGenTool.java:106) at org.apache.sqoop.tool.ImportTool.importTable(ImportTool.java:501) at org.apache.sqoop.tool.ImportTool.run(ImportTool.java:628) at org.apache.sqoop.Sqoop.run(Sqoop.java:147) at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76) at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:183) at org.apache.sqoop.Sqoop.runTool(Sqoop.java:234) at org.apache.sqoop.Sqoop.runTool(Sqoop.java:243) at org.apache.sqoop.Sqoop.main(Sqoop.java:252) 해결결론부터 말하면 해결하지 못하는 문제이다. Hadoop이 JDBC를 통해 정보시스템 개발기 Tibero에 접근해야 하는데 Sqoop이 아래와 같은 구조를 갖는다. 참고 : https://t1.daumcdn.net/cfile/tistory/255AAE415527751E16 사진은 Sqoop 1의 Architecture이지만, 결국에 Sqoop이 Hadoop의 Map Task에게 태스크를 넘기면 Hadoop이 Database에 접근하는 형태이다. 즉, 정보시스템 DB 접근이 허용되지 않는 Hadoop 환경에서는 Connection을 허용하지 않는다. 때문에 우리팀이 원래 시도하려던 했던 HDFS 형태로 파일을 떨구고 이를 불러오는 과정을 아예 다른 식으로 접근하거나, 우회 방안 을 생각해야 한다. 결론Connection ERROR 해결 방안은 Hadoop이 Database에 접근 가능하게 환경을 구성해주면 된다. 하지만, 우리팀의 프로젝트는 이렇게 환경을 재구성하는 것이 쉽지 않다. 이에대해 다음 두 가지 방안이 있는데 구체적이지 않다. (어떠한 방법이 더 효율적인지 더 고민해봐야겠다) If using Hadoop, Hadoop을 정보시스템 DB에 접근할 수 있는 환경에 설치 정보시스템 쪽에 접근 가능한 IP 추가 요청 else, 자사 제품 사용.. 역량 강화를 위해서는 Hadoop system을 이용하는 것이 좋다는 개인적인 바람이 있다. 2019.10.30 made by jaejun.lee","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://jx2lee.github.io/categories/Hadoop/"}],"tags":[]},{"title":"[Hadoop] Sqoop import/export with Tibero","slug":"hadoop-sqoop_with_tibero","date":"2019-10-29T15:00:00.000Z","updated":"2020-03-30T15:06:23.579Z","comments":true,"path":"hadoop-sqoop_with_tibero/","link":"","permalink":"https://jx2lee.github.io/hadoop-sqoop_with_tibero/","excerpt":"Sqoop을 이용해 Tibero table을 HDFS로 저장하고 이를 다시 table로 변환하는 테스트를 진행한다.","text":"Sqoop을 이용해 Tibero table을 HDFS로 저장하고 이를 다시 table로 변환하는 테스트를 진행한다. Create Test table우선, Import하려는 table을 생성한다. 중요한 것은 export하기 위한 table도 생성해야 한다는 점이다. Import table : RECIPES Export table : RECIPES_EXP Import table1234567891011121314151617181920CREATE TABLE USERS(USERNO NUMBER,EMAIL VARCHAR2(255) NOT NULL,PWD VARCHAR2(100) NOT NULL,NAME VARCHAR2(100) NOT NULL,PNO VARCHAR2(100) NOT NULL,ADDRESS VARCHAR2(255));INSERT INTO recipes (recipe_id, recipe_name) VALUES (1,'Tacos');INSERT INTO recipes (recipe_id, recipe_name) VALUES (2,'Tomato Soup');INSERT INTO recipes (recipe_id, recipe_name) VALUES (3,'Grilled Cheese');-- checking tableSQL&gt; SELECT * FROM RECIPES; RECIPE_ID RECIPE_NAME ---------- ------------------------------ 3 Grilled Cheese 1 Tacos 2 Tomato Soup3 rows selected. Export table123456CREATE TABLE recipes_exp ( recipe_id INT NOT NULL, recipe_name VARCHAR(30) NOT NULL, PRIMARY KEY (recipe_id), UNIQUE (recipe_name)); Sqoop Importtable이 준비되었다면 Sqoop을 이용해 HDFS 형태로 Hadoop에 저장해본다. 명령어와 수행결과는 아래와 같다. CMD12345sqoop import --connect jdbc:tibero:thin:@192.168.xxx.xx:xxxx:tibero \\--driver com.tmax.tibero.jdbc.TbDriver \\--username tibero --password [password] \\--table RECIPES \\--target-dir /t1/input Tibero 접속을 위한 string은 위와 같이 작성하고, MySQL/PostgreSQL의 경우 driver를 지정하지 않아도 되지만 Oracle/Tibero는 driver를 설정해야 한다. (MySQL/PostgreSQL : direct connect 지원이라고 document에 명시) Result1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980sqoop@bips:~$ sqoop import --connect jdbc:tibero:thin:@192.168.xxx.xx:xxxx:tibero \\&gt; --driver com.tmax.tibero.jdbc.TbDriver \\&gt; --username tibero --password tmax \\&gt; --table RECIPES \\&gt; --target-dir /t1/inputWarning: /app/sqoop/sqoop/../hbase does not exist! HBase imports will fail.Please set $HBASE_HOME to the root of your HBase installation.Warning: /app/sqoop/sqoop/../hcatalog does not exist! HCatalog jobs will fail.Please set $HCAT_HOME to the root of your HCatalog installation.Warning: /app/sqoop/sqoop/../accumulo does not exist! Accumulo imports will fail.Please set $ACCUMULO_HOME to the root of your Accumulo installation.Warning: /app/sqoop/sqoop/../zookeeper does not exist! Accumulo imports will fail.Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation.19/10/30 10:09:24 INFO sqoop.Sqoop: Running Sqoop version: 1.4.719/10/30 10:09:24 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.19/10/30 10:09:24 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time.19/10/30 10:09:24 INFO manager.SqlManager: Using default fetchSize of 100019/10/30 10:09:24 INFO tool.CodeGenTool: Beginning code generation19/10/30 10:09:25 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM RECIPES AS t WHERE 1=019/10/30 10:09:25 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM RECIPES AS t WHERE 1=019/10/30 10:09:25 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /app/hadoopNote: /tmp/sqoop-sqoop/compile/905c294a54718643bcf983498f8878ba/RECIPES.java uses or overrides a deprecated API.Note: Recompile with -Xlint:deprecation for details.19/10/30 10:09:26 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-sqoop/compile/905c294a54718643bcf983498f8878ba/RECIPES.jar19/10/30 10:09:26 INFO mapreduce.ImportJobBase: Beginning import of RECIPES19/10/30 10:09:26 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar19/10/30 10:09:26 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM RECIPES AS t WHERE 1=019/10/30 10:09:27 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps19/10/30 10:09:27 INFO client.RMProxy: Connecting to ResourceManager at node5.dat/192.168.158.53:805019/10/30 10:09:31 INFO db.DBInputFormat: Using read commited transaction isolation19/10/30 10:09:31 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(RECIPE_ID), MAX(RECIPE_ID) FROM RECIPES19/10/30 10:09:31 INFO mapreduce.JobSubmitter: number of splits:419/10/30 10:09:31 INFO Configuration.deprecation: yarn.resourcemanager.system-metrics-publisher.enabled is deprecated. Instead, use yarn.system-metrics-publisher.enabled19/10/30 10:09:31 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1567153359966_006919/10/30 10:09:31 INFO impl.YarnClientImpl: Submitted application application_1567153359966_006919/10/30 10:09:31 INFO mapreduce.Job: The url to track the job: http://node5.dat:8088/proxy/application_1567153359966_0069/19/10/30 10:09:31 INFO mapreduce.Job: Running job: job_1567153359966_006919/10/30 10:09:35 INFO mapreduce.Job: Job job_1567153359966_0069 running in uber mode : false19/10/30 10:09:35 INFO mapreduce.Job: map 0% reduce 0%19/10/30 10:09:39 INFO mapreduce.Job: map 50% reduce 0%19/10/30 10:09:40 INFO mapreduce.Job: map 100% reduce 0%19/10/30 10:09:41 INFO mapreduce.Job: Job job_1567153359966_0069 completed successfully19/10/30 10:09:41 INFO mapreduce.Job: Counters: 30 File System Counters FILE: Number of bytes read=0 FILE: Number of bytes written=830592 FILE: Number of read operations=0 FILE: Number of large read operations=0 FILE: Number of write operations=0 HDFS: Number of bytes read=447 HDFS: Number of bytes written=39 HDFS: Number of read operations=16 HDFS: Number of large read operations=0 HDFS: Number of write operations=8 Job Counters Launched map tasks=4 Other local map tasks=4 Total time spent by all maps in occupied slots (ms)=8334 Total time spent by all reduces in occupied slots (ms)=0 Total time spent by all map tasks (ms)=8334 Total vcore-milliseconds taken by all map tasks=8334 Total megabyte-milliseconds taken by all map tasks=8534016 Map-Reduce Framework Map input records=3 Map output records=3 Input split bytes=447 Spilled Records=0 Failed Shuffles=0 Merged Map outputs=0 GC time elapsed (ms)=162 CPU time spent (ms)=3530 Physical memory (bytes) snapshot=827170816 Virtual memory (bytes) snapshot=8600104960 Total committed heap usage (bytes)=585629696 File Input Format Counters Bytes Read=0 File Output Format Counters Bytes Written=3919/10/30 10:09:41 INFO mapreduce.ImportJobBase: Transferred 39 bytes in 14.9856 seconds (2.6025 bytes/sec)19/10/30 10:09:41 INFO mapreduce.ImportJobBase: Retrieved 3 records. Checking제대로 hadoop에 저장되어있는지 확인해본다. hdfs명령어를 이용해 파일이 정상적으로 저장되었는지 확인한다. hdfs dfs -cat /t1/input/* or hdfs dfs -ls /t1/input 1234567891011sqoop@bips:~$ hdfs dfs -cat /t1/input/*1,Tacos2,Tomato Soup3,Grilled Cheesesqoop@bips:~$ hdfs dfs -ls /t1/inputFound 5 items-rw-r--r-- 2 sqoop supergroup 0 2019-10-30 10:09 /t1/input/_SUCCESS-rw-r--r-- 2 sqoop supergroup 8 2019-10-30 10:09 /t1/input/part-m-00000-rw-r--r-- 2 sqoop supergroup 0 2019-10-30 10:09 /t1/input/part-m-00001-rw-r--r-- 2 sqoop supergroup 14 2019-10-30 10:09 /t1/input/part-m-00002-rw-r--r-- 2 sqoop supergroup 17 2019-10-30 10:09 /t1/input/part-m-00003 Sqoop Export이번엔 HDFS를 Tibero table로 다시 변환하는 작업을 진행한다. 명령어와 수행결과는 다음과 같다. CMD12345sqoop export --connect jdbc:tibero:thin:@192.168.xxx.xx:xxx:tibero \\--driver com.tmax.tibero.jdbc.TbDriver \\--username tibero --password ?? \\--table RECIPES_EXP \\--export-dir /t1/input 위에서도 언급했듯이 export table이 Tibero에 이미 존재해야 한다. Result12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485sqoop@bips:~$ sqoop export --connect jdbc:tibero:thin:@192.168.158.53:8729:tibero \\&gt; --driver com.tmax.tibero.jdbc.TbDriver \\&gt; --username tibero --password tmax \\&gt; --table RECIPES_EXP \\&gt; --export-dir /t1/inputWarning: /app/sqoop/sqoop/../hbase does not exist! HBase imports will fail.Please set $HBASE_HOME to the root of your HBase installation.Warning: /app/sqoop/sqoop/../hcatalog does not exist! HCatalog jobs will fail.Please set $HCAT_HOME to the root of your HCatalog installation.Warning: /app/sqoop/sqoop/../accumulo does not exist! Accumulo imports will fail.Please set $ACCUMULO_HOME to the root of your Accumulo installation.Warning: /app/sqoop/sqoop/../zookeeper does not exist! Accumulo imports will fail.Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation.19/10/30 10:40:23 INFO sqoop.Sqoop: Running Sqoop version: 1.4.719/10/30 10:40:23 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.19/10/30 10:40:23 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time.19/10/30 10:40:23 INFO manager.SqlManager: Using default fetchSize of 100019/10/30 10:40:23 INFO tool.CodeGenTool: Beginning code generation19/10/30 10:40:24 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM RECIPES_EXP AS t WHERE 1=019/10/30 10:40:24 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM RECIPES_EXP AS t WHERE 1=019/10/30 10:40:24 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /app/hadoopNote: /tmp/sqoop-sqoop/compile/bc292d345cc3a16972516454f904b6df/RECIPES_EXP.java uses or overrides a deprecated API.Note: Recompile with -Xlint:deprecation for details.19/10/30 10:40:25 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-sqoop/compile/bc292d345cc3a16972516454f904b6df/RECIPES_EXP.jar19/10/30 10:40:25 INFO mapreduce.ExportJobBase: Beginning export of RECIPES_EXP19/10/30 10:40:25 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar19/10/30 10:40:25 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM RECIPES_EXP AS t WHERE 1=019/10/30 10:40:25 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative19/10/30 10:40:25 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative19/10/30 10:40:25 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps19/10/30 10:40:25 INFO client.RMProxy: Connecting to ResourceManager at node5.dat/192.168.158.53:805019/10/30 10:40:29 INFO input.FileInputFormat: Total input files to process : 419/10/30 10:40:29 INFO input.FileInputFormat: Total input files to process : 419/10/30 10:40:29 INFO mapreduce.JobSubmitter: number of splits:419/10/30 10:40:29 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative19/10/30 10:40:29 INFO Configuration.deprecation: yarn.resourcemanager.system-metrics-publisher.enabled is deprecated. Instead, use yarn.system-metrics-publisher.enabled19/10/30 10:40:29 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1567153359966_007119/10/30 10:40:30 INFO impl.YarnClientImpl: Submitted application application_1567153359966_007119/10/30 10:40:30 INFO mapreduce.Job: The url to track the job: http://node5.dat:8088/proxy/application_1567153359966_0071/19/10/30 10:40:30 INFO mapreduce.Job: Running job: job_1567153359966_007119/10/30 10:40:35 INFO mapreduce.Job: Job job_1567153359966_0071 running in uber mode : false19/10/30 10:40:35 INFO mapreduce.Job: map 0% reduce 0%19/10/30 10:40:40 INFO mapreduce.Job: map 75% reduce 0%19/10/30 10:40:41 INFO mapreduce.Job: map 100% reduce 0%19/10/30 10:40:41 INFO mapreduce.Job: Job job_1567153359966_0071 completed successfully19/10/30 10:40:41 INFO mapreduce.Job: Counters: 31 File System Counters FILE: Number of bytes read=0 FILE: Number of bytes written=829388 FILE: Number of read operations=0 FILE: Number of large read operations=0 FILE: Number of write operations=0 HDFS: Number of bytes read=656 HDFS: Number of bytes written=0 HDFS: Number of read operations=22 HDFS: Number of large read operations=0 HDFS: Number of write operations=0 Job Counters Launched map tasks=4 Other local map tasks=1 Data-local map tasks=3 Total time spent by all maps in occupied slots (ms)=14227 Total time spent by all reduces in occupied slots (ms)=0 Total time spent by all map tasks (ms)=14227 Total vcore-milliseconds taken by all map tasks=14227 Total megabyte-milliseconds taken by all map tasks=14568448 Map-Reduce Framework Map input records=3 Map output records=3 Input split bytes=586 Spilled Records=0 Failed Shuffles=0 Merged Map outputs=0 GC time elapsed (ms)=425 CPU time spent (ms)=4140 Physical memory (bytes) snapshot=811102208 Virtual memory (bytes) snapshot=8589045760 Total committed heap usage (bytes)=606601216 File Input Format Counters Bytes Read=0 File Output Format Counters Bytes Written=019/10/30 10:40:41 INFO mapreduce.ExportJobBase: Transferred 656 bytes in 15.7366 seconds (41.6862 bytes/sec)19/10/30 10:40:41 INFO mapreduce.ExportJobBase: Exported 3 records.​ 123456789101112131415161718192021222324252627282930## Checking제대로 테이블로 데이터가 들어갔는지 확인해본다. &#96;Sqoop&#96;에서 &#96;--query&#96; argument를 주어 &#96;RECIPES_EXP&#96;를 조회해보자&#96;sqoop eval --connect jdbc:tibero:thin:@192.168.xxx.xx:xxxx:tibero --driver com.tmax.tibero.jdbc.TbDriver --username tibero --password ?? --query &#39;SELECT * FROM RECIPES_EXP&#39;&#96;&#96;&#96;&#96;bashsqoop@bips:~$ sqoop eval --connect jdbc:tibero:thin:@192.168.158.53:8729:tibero \\&gt; --driver com.tmax.tibero.jdbc.TbDriver \\&gt; --username tibero --password tmax --query &#39;SELECT * FROM RECIPES_EXP&#39;Warning: &#x2F;app&#x2F;sqoop&#x2F;sqoop&#x2F;..&#x2F;hbase does not exist! HBase imports will fail.Please set $HBASE_HOME to the root of your HBase installation.Warning: &#x2F;app&#x2F;sqoop&#x2F;sqoop&#x2F;..&#x2F;hcatalog does not exist! HCatalog jobs will fail.Please set $HCAT_HOME to the root of your HCatalog installation.Warning: &#x2F;app&#x2F;sqoop&#x2F;sqoop&#x2F;..&#x2F;accumulo does not exist! Accumulo imports will fail.Please set $ACCUMULO_HOME to the root of your Accumulo installation.Warning: &#x2F;app&#x2F;sqoop&#x2F;sqoop&#x2F;..&#x2F;zookeeper does not exist! Accumulo imports will fail.Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation.19&#x2F;10&#x2F;30 10:41:54 INFO sqoop.Sqoop: Running Sqoop version: 1.4.719&#x2F;10&#x2F;30 10:41:54 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.19&#x2F;10&#x2F;30 10:41:54 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time.19&#x2F;10&#x2F;30 10:41:54 INFO manager.SqlManager: Using default fetchSize of 1000-----------------------------------------------| RECIPE_ID | RECIPE_NAME | -----------------------------------------------| 3 | Grilled Cheese | | 1 | Tacos | | 2 | Tomato Soup | ----------------------------------------------- export 가 정상 작동됐음을 확인할 수 있다 참고 플밍장군님 블로그 : https://dlwjdcks5343.tistory.com/116 AndersonChoi 님 블로그 : https://blog.voidmainvoid.net/175 Sqoop Documents : https://sqoop.apache.org/docs/1.4.2/SqoopUserGuide.html#_free_form_query_imports 2019.10.30 made by jaejun.lee","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://jx2lee.github.io/categories/Hadoop/"}],"tags":[{"name":"Tibero","slug":"Tibero","permalink":"https://jx2lee.github.io/tags/Tibero/"}]},{"title":"[SQL] Interviews","slug":"hackerrank-interviews","date":"2019-10-27T15:00:00.000Z","updated":"2020-03-30T15:06:23.520Z","comments":true,"path":"hackerrank-interviews/","link":"","permalink":"https://jx2lee.github.io/hackerrank-interviews/","excerpt":"hackerrank에서 제공하는 Interviews 문제를 multiple join 및 group by를 활용해 해결하였다.","text":"hackerrank에서 제공하는 Interviews 문제를 multiple join 및 group by를 활용해 해결하였다. 문제Samantha interviews many candidates from different colleges using coding challenges and contests. Write a query to print the contest_id, hacker_id, name, and the sums of total_submissions, total_accepted_submissions, total_views, and total_unique_views for each contest sorted by contest_id. Exclude the contest from the result if all four sums are . Note: A specific contest can be used to screen candidates at more than one college, but each college only holds screening contest. Input Format The following tables hold interview data: Contests: The contest_id is the id of the contest, hacker_id is the id of the hacker who created the contest, and name is the name of the hacker. Colleges: The college_id is the id of the college, and contest_id is the id of the contest that Samantha used to screen the candidates. Challenges: The challenge_id is the id of the challenge that belongs to one of the contests whose contest_id Samantha forgot, and college_id is the id of the college where the challenge was given to candidates. View_Stats: The challenge_id is the id of the challenge, total_views is the number of times the challenge was viewed by candidates, and total_unique_views is the number of times the challenge was viewed by unique candidates. Submission_Stats: The challenge_id is the id of the challenge, total_submissions is the number of submissions for the challenge, and total_accepted_submission is the number of submissions that achieved full scores. Sample Input Contests Table: Colleges Table: Challenges*Table: *View_Stats Table: Submission_Stats Table: Sample Output 12366406 17973 Rose 111 39 156 5666556 79153 Angela 0 0 11 1094828 80275 Frank 150 38 41 15 Explanation The contest is used in the college . In this college , challenges and are asked, so from the view and submission stats: Sum of total submissions Sum of total accepted submissions Sum of total views Sum of total unique views Similarly, we can find the sums for contests and . 접근Join 유형 중 Left Join을 활용하여 해결하였다. 다수의 table을 특정 키를 기준으로 Join하는 것이 다소 헷갈릴 수 있지만 차근차근 Join하면 문제를 쉽게 해결할 수 있다. 우선, contests table을 기준으로 colleges, challenges table과 Left Join을 수행한다. 각 key는 contest_id 와 college_id 이다. 1234select a.contest_id, a.hacker_id, a.name,from contests as aleft join colleges as b on a.contest_id = b.contest_idleft join challenges as c on b.college_id = c.college_id; 다음 total_views와 *total_unique_views를 구하기 위해 view_stats table을 *challenge_id 기준으로 group by 한다. 이후 결과 테이블과 Left Join을 수행한다. 단, key는 challenge_id이다. 123456789select a.contest_id, a.hacker_id, a.name, sum(total_views) as total_views, sum(total_unique_views) as total_unique_viewsfrom contests as aleft join colleges as b on a.contest_id = b.contest_idleft join challenges as c on b.college_id = c.college_idleft join ( select challenge_id, sum(total_views) as total_views, sum(total_unique_views) as total_unique_views from view_stats group by challenge_id ) as d on c.challenge_id = d.challenge_id; view_stats table Join과 같은 방법으로 submission_stats table을 정제한 후 Left Join을 수행한다. 1234567891011121314select a.contest_id, a.hacker_id, a.name, sum(total_submissions) as total_submissions, sum(total_accepted_submissions) as total_accepted_submissions, sum(total_views) as total_views, sum(total_unique_views) as total_unique_viewsfrom contests as aleft join colleges as b on a.contest_id = b.contest_idleft join challenges as c on b.college_id = c.college_idleft join ( select challenge_id, sum(total_views) as total_views, sum(total_unique_views) as total_unique_views from view_stats group by challenge_id ) as d on c.challenge_id = d.challenge_idleft join ( select challenge_id, sum(total_submissions) as total_submissions, sum(total_accepted_submissions) as total_accepted_submissions from submission_stats group by challenge_id ) as e on c.challenge_id = e.challenge_id; 마지막으로 contest_id, hacker_id, name을 기준으로 group by를 수행하고, 문제의 조건인 네 가지 summation이 0보다 큰 경우만 조회하는 having을 추가하여 완성한다. 12345678910111213141516select a.contest_id, a.hacker_id, a.name, sum(total_submissions) as total_submissions, sum(total_accepted_submissions) as total_accepted_submissions, sum(total_views) as total_views, sum(total_unique_views) as total_unique_viewsfrom contests as aleft join colleges as b on a.contest_id = b.contest_idleft join challenges as c on b.college_id = c.college_idleft join ( select challenge_id, sum(total_views) as total_views, sum(total_unique_views) as total_unique_views from view_stats group by challenge_id ) as d on c.challenge_id = d.challenge_idleft join ( select challenge_id, sum(total_submissions) as total_submissions, sum(total_accepted_submissions) as total_accepted_submissions from submission_stats group by challenge_id ) as e on c.challenge_id = e.challenge_idgroup by a.contest_id, a.hacker_id, a.namehaving (total_submissions + total_accepted_submissions + total_views + total_unique_views) &gt; 0; 해결참고 12345678910111213141516select a.contest_id, a.hacker_id, a.name, sum(total_submissions) as total_submissions, sum(total_accepted_submissions) as total_accepted_submissions, sum(total_views) as total_views, sum(total_unique_views) as total_unique_viewsfrom contests as aleft join colleges as b on a.contest_id = b.contest_idleft join challenges as c on b.college_id = c.college_idleft join ( select challenge_id, sum(total_views) as total_views, sum(total_unique_views) as total_unique_views from view_stats group by challenge_id ) as d on c.challenge_id = d.challenge_idleft join ( select challenge_id, sum(total_submissions) as total_submissions, sum(total_accepted_submissions) as total_accepted_submissions from submission_stats group by challenge_id ) as e on c.challenge_id = e.challenge_idgroup by a.contest_id, a.hacker_id, a.namehaving (total_submissions + total_accepted_submissions + total_views + total_unique_views) &gt; 0; 2019.10.28 made by jaejun.lee","categories":[{"name":"SQL","slug":"SQL","permalink":"https://jx2lee.github.io/categories/SQL/"}],"tags":[{"name":"Hackerrank","slug":"Hackerrank","permalink":"https://jx2lee.github.io/tags/Hackerrank/"}]},{"title":"[Hadoop] Install Hadoop using Docker","slug":"hadoop-install_hadoop_using_docker","date":"2019-10-24T15:00:00.000Z","updated":"2020-05-15T05:22:11.687Z","comments":true,"path":"hadoop-install_hadoop_using_docker/","link":"","permalink":"https://jx2lee.github.io/hadoop-install_hadoop_using_docker/","excerpt":"Hadoop 환경을 구성해보자!","text":"Hadoop 환경을 구성해보자! 드디어 엔지니어로써의 능력을 배양할 수 있는 환경 구성이다. 우선, 회사에서 지급받은 서버로 이미 하둡 환경이 구축되어 있지만, 개인 할당받은 서버에서 Docker를 이용해 실습 환경을 구축하고자 한다. Docker Hub 에서 스타가 가장 많은 이미지를 이용할 것이다. Docker를 이용하는 것은 혹여나 나중에도 써먹을 경우를 대비한 것이다. PLAN3개의 DataNode와 1개의 NameNode로 구성된 하둡 환경을 구축한다. 여러 개 서버를 연결하는 구조 대신, 쉽게 환경을 바꾸고 입맛대로 수정이 가능한 docker를 이용해 구성한다. docker를 공부해보자는 의미도 있고 새롭게 환경을 재구성할 때 유용할 것 같다. ENVIRONMENThadoop 폴더를 생성하여 아래와 같은 구조를 갖는다. 12345678910111213141516[kuber@node2 hadoop]$ tree ..├── base│ ├── core-site.xml│ └── Dockerfile├── data-node│ ├── Dockerfile│ ├── hdfs-site.xml│ └── install.sh├── docker-compose.yml└── name-node ├── Dockerfile ├── hdfs-site.xml └── install.sh3 directories, 9 files hadoop의 기본 환경을 구성하는 base와 이를 활용해 name / data -node 폴더를 구성하였고, Dockerfile을 작성하여 직접 image를 build하고 docker-compose를 활용해 배포한다. BASE각 component의 밑바당이 되는 이미지를 구성하는 단계이다. 이는 base 폴더에서 수행하며, 고려사항은 다음과 같다. Hadoop 설치를 위한 binary Java 이미지 빌드를 위해 Dockerfile, core-site.xml을 작성해보도록 한다. DockerfileDockerfile은 아래와 같은 순서로 작성하였다. Dockerfile 작성을 많이 해버릇 해야겠다. 참고한 블로그에서 사용한 내용을 복사 붙여넣지 않고 직접 작성하니 어느정도 흐름은 파악하였다 환경변수 설정 HADOOP_VERSION : hadoop version을 의미 HADOOP_URL : hadoop 설치 binary 다운을 위한 url 환경변수를 이용해 download 및 압축해제 링크파일 생성 호스트(in base directory) 파일을 container에 추가 hadoop 실행을 위한 환경변수 설정 HADOOP_PREFIX : hadoop root directory HADOOP_CONF_DIR : hadoop config directory JAVA_HOME : Java directory 1234567891011121314151617181920212223242526# ENV for installationENV HADOOP_VERSION=2.9.2ENV HADOOP_URL=http://mirror.apache-kr.org/hadoop/common/hadoop-$HADOOP_VERSION/hadoop-$HADOOP_VERSION.tar.gz## Download Hadoop on /app/hadoopRUN curl -fSL \"$HADOOP_URL\" -o /tmp/hadoop.tar.gz \\ &amp;&amp; tar -xvf /tmp/hadoop.tar.gz -C /opt/ \\ &amp;&amp; rm /tmp/hadoop.tar.gz# make directory &amp; symbolic linkRUN ln -s /opt/hadoop-$HADOOP_VERSION /opt/hadoop \\ &amp;&amp; mkdir /opt/hadoop/dfs \\ &amp;&amp; ln -s /opt/hadoop-$HADOOP_VERSION/etc/hadoop /etc/hadoop \\ &amp;&amp; rm -rf /opt/hadoop/share/doc# copy local-site.xml file to containerADD core-site.xml /etc/hadoop/# ENV for runENV HADOOP_PREFIX /opt/hadoopENV HADOOP_CONF_DIR /etc/hadoopENV PATH $HADOOP_PREFIX/bin/:$PATHENV JAVA_HOME /usr/lib/jvm/zulu-8-amd64 core-site.xmlcore-site.xml은 아래와 같이 작성한다. 12345678&lt;connfiguration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://namenode:9000/&lt;/value&gt; &lt;description&gt;NameNode URI &lt;/description&gt; &lt;/property&gt; &lt;/connfiguration&gt; fs.defaultFS : NameNode의 위치를 찾는 설정으로 읽기/쓰기 요청을 할 때 사용되는 항목 URI hostname 은 namenode라 설정하였는데, NameNode container의 host name을 지정한 것 connfiguration tag 명을 제대로 확인하도록 하자. (내 경우 connfiguration -&gt; configuration으로 이미지를 빌드 후 실행하였더니 container가 정상적으로 작동하지 않았다) Build hadoop-base:2.9.2이제 Docker image로 빌드할 차례이다. base 폴더로 접근 후 아래와 같은 명령어를 통해 build를 수행한다. 12[kuber@node2 hadoop]$ cd base/[kuber@node2 base]$ docker build -t hadoop-base:2.9.2 . NAMENODEbase image를 생성하였다면, 이를 이용해 NameNode Container를 빌드하기 위한 환경을 구성한다. 고려 사항은 다음과 같다. NameNode 용 hdfs-site.xml FsImage, EditLog 저장을 위한 로컬 파일 시스템 경로 NameNode의 첫 구동 확인 (첫 구동이 아니라면 포맷 후 구동 필요) NameNode 이미지 빌드를 위해 Dockerfile, hdfs-site.xml, install.sh를 작성해보도록 한다. DockerfileDockerfile은 아래와 같은 순서로 작성하였다. 이전에 만든 hadoop-base:2.9.2 image를 불러온다 Web UI 응답 여부 확인을 위한 HEALTHCHECK 호스트(in name-node directory) 파일을 container에 추가 FSIMage, EditLog 파일 경로 연결 포트 노출 명령어 등록 123456789101112131415161718192021FROM hadoop-base:2.9.2# CON NameNode Web UIHEALTHCHECK --interval=30s --timeout=30s --retries=3 CMD curl -f http://localhost:50070/ || exit 1# COPY hdfs-site.xmlADD hdfs-site.xml /etc/hadoop/# FSImage/EditLog path -&gt; volumeRUN mkdir /opt/hadoop/dfs/nameVOLUME /opt/hadoop/dfs/name# COPY shell scripADD install.sh /install.shRUN chmod a+x /install.sh# EXPOSE PortEXPOSE 50070 9000# ADD command line for runCMD [\"/install.sh\", \"opt/hadoop/dfs/name\"] hdfs-site.xmlhdfs-site.xml은 아래와 같이 작성한다. 1234567891011121314151617181920212223242526272829303132&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:///opt/hadoop/dfs/name&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.blocksize&lt;/name&gt; &lt;value&gt;10485760&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.client.use.datanode.hostname&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-bind-host&lt;/name&gt; &lt;value&gt;0.0.0.0&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.servicerpc-bind-host&lt;/name&gt; &lt;value&gt;0.0.0.0&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-bind-host&lt;/name&gt; &lt;value&gt;0.0.0.0&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.https-bind-host&lt;/name&gt; &lt;value&gt;0.0.0.0&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; dfs.namenode.name.dir : FSImage / EditLog 파일을 저장하는 경로 dfs.blocksize : HDFS 파일 블록의 크기로 본 환경에서는 10MB로 설정하였다(default : 128MB) (기타 항목들은 ( https://blog.geunho.dev/posts/hadoop-docker-test-env-hdfs/ ) 확인) install.sh install.sh은 NameNode의 네임스페이스의 포맷 여부를 확인하는 쉘 스크립트이다. 만약 네임스페이스가 포맷되어 있다면 NameNode를 구동하고, 포맷되어있지 않다면 포맷을 진행한 후 구동한다. 12345678910111213141516#! /bin/bash# SET namespace directoryNAME_DIR=$1echo $NAME_DIR# CHECK if dir is emptyif [ \"$(ls -A $NAME_DIR)\" ]; then echo \"NameNode is already formatted !!\"else echo \"Format NameNode..\" $HADOOP_PREFIX/bin/hdfs --config $HADOOP_CONF_DIR namenode -formatfi# RUN$HADOOP_PREFIX/bin/hdfs --config $HADOOP_CONF_DIR namenode Build hadoop-namenode:2.9.2이제 Docker image로 빌드할 차례이다. name-node 폴더로 접근 후 아래와 같은 명령어를 통해 build를 수행한다. 12[kuber@node2 hadoop]$ cd name-node/[kuber@node2 name-node]$ docker build -t hadoop-namenode:2.9.2 . DATANODENameNode 이미지 생성과 마찬가지로, base image를 이용해 DataNode image를 생성해본다. 고려사항은 아래와 같다. DataNode 용 hdfs-site.xml 파일 블록 저장을 위한 경로 DataNode 이미지 빌드를 위해 Dockerfile, hdfs-site.xml, install.sh를 작성해보도록 한다. DockerfileDockerfile은 아래와 같은 순서로 작성하였다. 이전에 만든 hadoop-base:2.9.2 image를 불러온다 Web UI 응답 여부 확인을 위한 HEALTHCHECK host(in name-node directory) 파일을 container에 추가 FSIMage, EditLog 파일 경로 연결 port 노출 cmd 등록 123456789101112131415161718FROM hadoop-base:2.9.2# CONN NameNode Web UIHEALTHCHECK --interval=30s --timeout=30s --retries=3 CMD curl -f http://localhost:50075/ || exit 1# RUN mkdir /opt/hadoop/dfs/dataVOLUME /opt/hadoop/dfs/data# COPY shell scripADD install.sh /install.shRUN chmod a+x /install.sh# EXPOSE PortEXPOSE 50075 50010# ADD command line for runCMD [\"/install.sh\"] hdfs-site.xmlhdfs-site.xml 아래와 같이 작성한다. container 에 datanode의 dir path와 blocksize를 지정한다. 1234567891011121314&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:///opt/hadoop/dfs/data&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.blocksize&lt;/name&gt; &lt;value&gt;10485760&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.use.datanode.hostname&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; install.shDataNode의 install.sh 은 별거 없다. DataNode를 구동하는 명령어를 추가하여 작성한다. 123#! /bin/sh$HADOOP_PREFIX/bin/hdfs --config $HADOOP_CONF_DIR datanode Build hadoop-datanode:2.9.2이제 Docker image로 빌드할 차례이다. data-node 폴더로 접근 후 아래와 같은 명령어를 통해 build를 수행한다. 12345678910[kuber@node2 hadoop]$ cd data-node/[kuber@node2 data-node]$ docker build -t hadoop-datanode:2.9.2 .Sending build context to Docker daemon 4.096 kBStep 1/8 : FROM hadoop-base:2.9.2 ---&gt; 765c9acb59fdStep 2/8 : HEALTHCHECK --interval=30s --timeout=30s --retries=3 CMD curl -f http://localhost:50075/ || exit 1 ---&gt; Running in e2b20d7d5fd1......Successfully built 3f1372bf4fdb container 구동을 위한 준비가 거의 끝나간다 RUN빌드된 이미지를 하나하나 실행(ex. docker run ~)해도 되지만, docker compose라는 툴을 이용해 한꺼번에 배포해보도록 한다. yml 형식의 스크립트를 작성하여 NadeNode와 DataNode를 한 번에 배포할 것이다. Install docker-composedocker 설치 시 자동으로 설치된 줄 알았는데, 설치가 안되있었다. 아래 명령어를 통해 docker-compose를 설치한다. 1234567[kuber@node2 hadoop]$ sudo curl -L \"https://github.com/docker/compose/releases/download/1.24.1/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed100 617 0 617 0 0 1382 0 --:--:-- --:--:-- --:--:-- 1380100 15.4M 100 15.4M 0 0 2327k 0 0:00:06 0:00:06 --:--:-- 3535k[kuber@node2 hadoop]$ sudo chmod +x /usr/local/bin/docker-compose[kuber@node2 hadoop]$ sudo ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose docker-compose.ymldocker-compose.yml은 docker 실행 옵션들을 미리 적어둔 파일이다. services` 부분은 우리가 구동할 NameNode 및 DataNode에 관련된 옵션들을 작성하고, 계획에서 언급한 것처럼 DataNode 3개 구동을 위해 01/02/03으로 구분하여 작성한다. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748version: \"3.4\"x-datanode_base: &amp;datanode_base image: hadoop-datanode:2.9.2 networks: - bridgeservices: namenode: image: hadoop-namenode:2.9.2 container_name: namenode hostname: namenode ports: - \"50070:50070\" - \"9000:9000\" volumes: - namenode:/opt/hadoop/dfs/name - /tmp:/tmp networks: - bridge datanode01: &lt;&lt;: *datanode_base container_name: datanode01 hostname: datanode01 volumes: - datanode01:/opt/hadoop/dfs/data datanode02: &lt;&lt;: *datanode_base container_name: datanode02 hostname: datanode02 volumes: - datanode02:/opt/hadoop/dfs/data datanode03: &lt;&lt;: *datanode_base container_name: datanode03 hostname: datanode03 volumes: - datanode03:/opt/hadoop/dfs/datavolumes: namenode: datanode01: datanode02: datanode03:networks: bridge: version의 경우, 자신의 서버에 설치된 docker engine release에 따라 format이 정해져있으므로 이 문서를 참고 Run Container using docker-composedocker-compose를 이용해 배포해 보도록 하자. 볼륨을 생성하고 NameNode / DataNode가 구동되었다는 메세지가 보일 것이다. 12345678[kuber@node2 hadoop]$ docker-compose up -dCreating volume \"hadoop_datanode01\" with default driverCreating volume \"hadoop_datanode02\" with default driverCreating volume \"hadoop_datanode03\" with default drivernamenode is up-to-dateCreating datanode01 ... doneCreating datanode02 ... doneCreating datanode03 ... done Installation Check노드들이 정상 작동하는지 확인해보는 단계이다. 구동을 했으면 제대로 되는지 확인하는게 중요하겠죠? 아래 순서와 같이 설치 확인을 진행한다. NameNode 컨테이너의 hadoop client 실행 확인NameNode의 컨테이너에 접속해 커맨드라인을 확인하는 명령어(docker exec)를 통해 확인해본다. 그러면 아래와 같이 Usage가 출력되는 것을 확인할 수 있다. 1234567891011121314151617181920[kuber@node2 hadoop]$ docker exec namenode /opt/hadoop/bin/hadoopUsage: hadoop [--config confdir] [COMMAND | CLASSNAME] CLASSNAME run the class named CLASSNAME or where COMMAND is one of: fs run a generic filesystem user client version print the version jar &lt;jar&gt; run a jar file note: please use \"yarn jar\" to launch YARN applications, not this command. checknative [-a|-h] check native hadoop and compression libraries availability distcp &lt;srcurl&gt; &lt;desturl&gt; copy file or directories recursively archive -archiveName NAME -p &lt;parent path&gt; &lt;src&gt;* &lt;dest&gt; create a hadoop archive classpath prints the class path needed to get the Hadoop jar and the required libraries credential interact with credential providers daemonlog get/set the log level for each daemon trace view and modify Hadoop tracing settingsMost commands print help when invoked w/o parameters. 이처럼 매 번 docker exec 명령어를 작성하는 건 정말 귀찮을 것이다. alias를 등록해 간편하게 명령어를 날려보자. 12345678910111213141516171819202122232425[kuber@node2 hadoop]$ vi ~/.bash_profile # bash_profilealias hadoop=\"docker exec namenode /opt/hadoop/bin/hadoop\"#[kuber@node2 hadoop]$ source ~/.bash_profile [kuber@node2 hadoop]$ hadoopUsage: hadoop [--config confdir] [COMMAND | CLASSNAME] CLASSNAME run the class named CLASSNAME or where COMMAND is one of: fs run a generic filesystem user client version print the version jar &lt;jar&gt; run a jar file note: please use \"yarn jar\" to launch YARN applications, not this command. checknative [-a|-h] check native hadoop and compression libraries availability distcp &lt;srcurl&gt; &lt;desturl&gt; copy file or directories recursively archive -archiveName NAME -p &lt;parent path&gt; &lt;src&gt;* &lt;dest&gt; create a hadoop archive classpath prints the class path needed to get the Hadoop jar and the required libraries credential interact with credential providers daemonlog get/set the log level for each daemon trace view and modify Hadoop tracing settingsMost commands print help when invoked w/o parameters. 폴더 생성/조회/삭제 확인hadoop 명령어를 통해 폴더를 생성하고 조회하며 마지막으로 삭제하는 작업을 해본다. 12345678[kuber@node2 hadoop]$ hadoop fs -mkdir -p /tmp/test/app[kuber@node2 hadoop]$ hadoop fs -ls -R /tmpdrwxr-xr-x - root supergroup 0 2019-10-25 05:15 /tmp/testdrwxr-xr-x - root supergroup 0 2019-10-25 05:15 /tmp/test/app[kuber@node2 hadoop]$ hadoop fs -rm -r /tmp/test/appDeleted /tmp/test/app[kuber@node2 hadoop]$ hadoop fs -ls -R /tmpdrwxr-xr-x - root supergroup 0 2019-10-25 05:16 /tmp/test Web UINameNode와 DataNode 상태를 Web에서 확인할 수 있다. container 실행을 위해 작성한 docker-compose.yml 안에 NameNode의 port(50070)를 이용해 접속을 하면 아래와 같은 화면이 보일 것이다. NameNode overview DataNode overview Ref. 김근호님 블로그 : Docker로 Hadoop 테스트 환경 구축하기 - HDFS Docker documenst made by jaejun.lee","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://jx2lee.github.io/categories/Hadoop/"}],"tags":[]},{"title":"[Database] OLAP","slug":"hackerrank-olap","date":"2019-10-22T15:00:00.000Z","updated":"2020-03-30T15:06:23.577Z","comments":true,"path":"hackerrank-olap/","link":"","permalink":"https://jx2lee.github.io/hackerrank-olap/","excerpt":"hackerrank에서 제공하는 Database 카테고리의 OLAP 문제를 풀어보고 개념을 정리한다.","text":"hackerrank에서 제공하는 Database 카테고리의 OLAP 문제를 풀어보고 개념을 정리한다. OLAP - The Total view문제Which of these provides a total view of the organization? 1) OLAP2) OLTP3) Data Warehousing4) Database 풀이왜 틀렸는지를 모르겠는 문제이다. 1번, 2번, 4번이 결국 3번 : Data Warehousing에 포함된 내용이다. 각각의 개념을 간단히 정리해보면 아래와 같다. (4번의 경우 PASS) OLAP데이터 집계를 효율화하는 접근 방법 중 하나로, 다차원 모델구조를 MDX (Multidimensional expressions) 등의 쿼리 언어로 집계한다. 다차원 모델 구조를 OLAP 큐브라 하며 이러한 큐브를 이용해 크로스 집계하는 구조가 OLAP이다. OLTP (Online Transaction Processing)정의는 실시간으로 서버(DB)가 자료를 처리하는 과정 인데, 사실 OLAP vs OLTP를 비교하는 것을 주로 보았는데 정확한 의미를 모르겠다. OLAP은 하나의 기술로 보는것인지, OLTP는 기술이 아닌 실시간성 처리 과정으로 봐야하는지는 좀 더 살펴본 이후에 자세히 정리해야겠다. Data Warehousing Data Warehouse를 설계하고 사용하는 과정을 뜻하는 단어이다. Data Warehouse 특징을 살펴보면 다음과 같다. Web Server 또는 RDB와 달리 대량 데이터 장기 보존 최적화 정리된 데이터 전송 기능은 뛰어나지만, 소량 데이터의 경우 적합하지 않음 업무 처리에 있어 함부로 사용해 시스템 과부하 초래는 위험함, 이러한 문제로 필요한 데이터만을 추출하여 데이터 마트 (Data Mart)를 구성함 [참고] : 빅데이터를 지탱하는 기술, Wikipedia OLAP - OLAP Operation Types문제Consider a fact table DataPoints(D1,D2,D3,x), and the following three queries: Q1: Select D1,D2,D3,Sum(x) From DataPoints Group By D1,D2,D3Q2: Select D1,D2,D3,Sum(x) From DataPoints Group By D1,D2,D3 WITH CUBEQ3: Select D1,D2,D3,Sum(x) From DataPoints Group By D1,D2,D3 WITH ROLLUP Suppose attributes D1, D2, and D3 have n1, n2, and n3 different values respectively, and assume that each possible combination of values appears at least once in the table DataPoints. The number of tuples in the result of each of the three queries above can be specified as an arithmetic formula involving n1, n2, and n3. Pick the one tuple (a,b,c,d,e,f) in the list below such that when n1=a, n2=b, and n3=c, then the result sizes of queries Q1, Q2, and Q3 are d, e, and f respectively. 1) (2, 2, 2, 6, 18, 8)2) (2, 2, 2, 8, 64, 15)3) (5, 10, 10, 500, 1000, 550)4) (4, 7, 3, 84, 160, 117) 풀이문제를 잘못 이해해 푸는데 오래 걸렸다. 주어진 보기 4개 tuple의 앞에 3개는 각각 D1, D2, D3칼럼의 value 들이었다. 따라서, 각 지문의 3개 숫자 (ex. 1번 보기는 2,2,2 -&gt; D1, D2, D3) 로 operation CUBE 및 ROLL UP 을 수행한 후 조회되는 행의 갯수를 맞추는 문제이다. Q1은 쉽게 구할 수 있었고 CUBE 및 ROLLUP 연산을 구글을 통해 살펴보았다. 우선 CUBE operation은 모든 차원에서 모든 속성 조합을 사용한다. 이는 곧, NULL 구문을 사용하기 때문에 group by로 조회수와는 다르게 아래와 같이 계산된다. (n1 + 1) * (n2 + 1) * (n3 + 1) 마지막으로 ROLLUP operation은 NULL이 있는 속성을 포함하여 속성 tuple을 생성한다. 이에 기존 CUBE 연산을 통해 나온 수와 ROLLUP연산을 통해 계산되는 tuple 수를 더해준다. 식은 아래와 같다. [기존 CUBE로 계산된 수] + n1 * (n2 + 1) + 1 ROLLUP 이라 함은 Drill Down (operation 중 하나)과 달리 작은 범위에서 큰 범위의 단계적 접근 분석 방법을 말한다 (ex. 번지 -&gt; 동 -&gt; 구 -&gt; 시도 -&gt; 광역). 위에 기존 CUBE 연산을 통한 값과 그 뒤에 새로운 추가된 수를 더하는 내용이 확실히 이해가 가지 않아 나중에 정리해야할 것 같다. 2019.10.23 made by jaejun.lee","categories":[{"name":"Database","slug":"Database","permalink":"https://jx2lee.github.io/categories/Database/"}],"tags":[{"name":"Hackerrank","slug":"Hackerrank","permalink":"https://jx2lee.github.io/tags/Hackerrank/"}]},{"title":"[Hadoop] Hadoop Overview","slug":"hadoop-introduction_to_hadoop","date":"2019-10-21T15:00:00.000Z","updated":"2020-05-15T05:29:45.181Z","comments":true,"path":"hadoop-introduction_to_hadoop/","link":"","permalink":"https://jx2lee.github.io/hadoop-introduction_to_hadoop/","excerpt":"Hadoop 관련 책을 읽으며 개념을 간단히 정리한다.","text":"Hadoop 관련 책을 읽으며 개념을 간단히 정리한다. 맨날 하둡 공부해야지.. 공부해야지 하다가 빅데이터를 지탱하는 기술 책을 읽으며 정리하고자 한다. 무작정 실습부터 하기 보다는, 기본적인 개념을 익히고 Hadoop 환경을 구성할 것이다. 우선 내용 전체적으로 도서를 참고해 작성하고 추가적인 부분은 구글 서치를 통해 채워넣을 예정이다. 최대한 책을 기반으로 개념을 정리하는 것을 목표로 한다. Contents: Hadoop 이란 Hadoop 기본 구성 요소 분산 파일 시스템 (HDFS) 과 리소스 관리자 (YARN) 분산 데이터 처리 (MapReduce) 및 쿼리 엔진 (Hive) Hive on Tez 대화형 쿼리 엔진 (Impala &amp; Presto) Hadoop 이란개념을 배우는데 있어서 역사는 크게 중요하지 않다고 생각한다. 책에서는 역사가 기술되어있지만 나는 정의(수학과 아니랄까봐) 부터 짚어보고자 한다. Hadoop은 단일 소프트웨어가 아닌, 분산 시스템 을 구성하는 다수의 소프트웨어로 이루어진 집합체 이다. Wiki백과에 의하면, 대량의 자료를 처리할 수 있는 큰 컴퓨터 클러스터에서 동작하는 분산 응용 프로그램을 지원하는 프레임워크라고 소개한다. 일맥 상통한다. 좀 더 자세히 어떠한 언어로 작성되었는지를 표현하였지, 의미는 같다 2013년 Hadoop2부터 YARN이라는 리소트관리자 상에서 분산 애플리케이션이 동작하는 구성으로 설계되어, 대규모 분산시스템을 구축하기 위한 플랫폼 역할을 맡고 있다. [그림] - 빅데이터 관련 Apache 프로젝트 (참고 : 빅데이터를 지탱하는 기술) Hadoop 기본 구성 요소기본 구성 요소로는 분산 파일 시스템 (distributed file system)인 HDFS(Hadoop Distributed File System), *리소스 관리자** *(resource manager) 인 YARN(Yet Another Resource Negotiator), 분산 데이터 처리 (distributed data processing) 기반 MapReduce 3가지다. 이외 구성요소(프로젝트라고 표현하기도 함)는 Hadoop과 독립적으로 개발되어 분산 애플리케이션으로 동작한다. 즉, 위에서 소개한 프로젝트에서 분산 파일 시스템으로는 HDFS를 사용하고 resource manager로는 Mesos를, 분산 데이터 처리에는 Spark를 사용할 수 있다. 자신에게 맞고 상황에 맞는 프로젝트를 구성하는 것이 Hadoop을 중심으로 하는 데이터 처리의 특징이다. 분산 파일 시스템 (HDFS) 과 리소스 관리자 (YARN)Hadoop에서 처리되는 데이터는 대부분 HDFS에 저장된다. 보통 파일 서버와 비슷한 개념이지만, 다수 컴퓨터에 파일을 복사하여 중복성을 높인다는 특징이 있다. HDFS는 블록 구조의 file system이다. 파일을 특정 크기 블록으로 나누어 분산된 서버에 저장한다. 크기는 64MB 에서 Hadoop2 부터는 128M로 증가하였다(참고) 한편, CPU나 메모리 등의 계산 리소스는 resource manager인 YARN에 의해 관리된다. YARN은 CPU 코어와 메모리를 컨테이너 (Container) 단위로 관리한다 (여기서 Container는 Docker Container와는 다르다. 어떤 호스트에서 어떤 프로세스를 실행시킬 것인지 결정하는 앱 수준의 기술). Hadoop에서 분산 앱을 실행하면 YARN이 클러스터 전체의 부하를 보고 비어 있는 호스트부터 컨테이너를 할당한다. 즉, 리소스 관리자인 YARN은 어느 애플리케이션에 얼마만큼의 리소스를 할당할 지 관리함으로써 모든 애플리케이션이 차질없이 실행되도록 제어 한다 분산 데이터 처리 (MapReduce) 및 쿼리 엔진 (Hive) MapReduceYARN 상에서 동작하는 분산 애플리케이션 중 하나로 데이터 처리를 실행하는 데 사용한다. 임의의 java 프로그램을 실행할 수 있기 때문에 비구조화 데이터 (Unstructured Data) 가공에 적합하다. 초기 목적은 대량의 데이터를 Batch*처리하기 위함이었다. 한 번 실행하면 대량의 데이터를 읽을 수 있지만, 작은 프로그램을 *(작은 데이터가 존재하는) 을 실행하면 과한 오버헤드로 몇 초 안에 끝나버리는 쿼리에는 어울리지 않다. [그림] - MapReduce Process 쿼리 엔진 (Hive)Hive는 SQL 등 쿼리 언어에 의한 데이터 집계가 목적으로 설계된 쿼리 엔진 중 하나이다. 이는 SQL 쿼리를 자동으로 MapReduce 프로그램으로 변환시킨다. 실행 특성 상 MapReduce에 의존하고 있다. 쿼리 엔진 Hive도 결국 MapReduce에 의존하고 있기 때문에, 시간이 오래 걸리는 대량의 데이터를 처리하는 배치 처리에는 적합하나, 애드 훅 분석을 위한 쿼리(간단하고 바로바로 볼 수 있는)를 여러 번 수행하는 데 적절하지 않다 [그림] - Hive Architecture (https://medium.com/@yigiterbas/apache-hive-and-applications-1-31735b8823c7) Hive on TezHive 가속화를 위해 개발된 것으로 MapReduce에서 보인 몇 가지 단점을 해결하고 고속화를 실현하고 있다. 예를 들어, MapReduce의 경우 하나의 stage가 끝날 때 까지 다음의 처리를 진행할 수 없었다. 이에 Tez는 stage 종료를 기다리지 않고 처리가 끝난 데이터를 차례대로 후속 처리로 넘겨 전체 쿼리 시간의 단축을 실현했다. 현재의 Hive는 MapReduce 뿐 아니라 Tez를 사용해도 동작하므로 Hive를 Hive on Tez 와 Hive on MR로 구분한다. (MR은 MapReduce 줄임말) [그림] - Hive on MR &amp; Hive on Tez Process 대화형 쿼리 엔진 (Impala &amp; Presto)Hive 고속화가 아닌 대화형 쿼리 실행만을 위한 엔진도 있다. 그 중 Impala와 Presto가 대표적이다. Imapala와 Presto를 간단히 살펴보면 다음과 같다. Impala Impala는 크게 impalad와 impala state store 프로세스로 구성한다. impalad는 분산 질의 엔진 역할을 담당하는 프로세스로, Hadoop 클러스터 내 데이터노드 위에서 질의에 대한 plan 설계와 질의 처리 작업을 수행한다. impala state store 는 각 데이터 노드에서 수행되는 impalad에 대한 메타데이터를 유지하는 역할을 담당한다. impalad 프로세스가 클러스터 내에 추가 또는 제거될 때 impala state store 프로세스를 통해 메타데이터가 업데이트된다. impalad : 분산 질의 엔진, impala state store : Impalad의 메타데이터 관리 [그림] - impala high-level architecture (원본출처) Presto Presto는 크게 Coordinator와 Worker로 구성된다. Coordinator는 SQL query 분석, query 계획과 Presto Worker 노드 (worker)를 관리한다. REST API를 사용하여 Worker 및 Client와 통신한다. Worker는 작업을 실행하고 데이터를 처리한다. Worker가 수행한 결과를 Coordinator를 거쳐 Client에게 전달하며 Coordinator와 마찬가지로 REST API를 사용해 통신한다. [그림] - Presto architecture (출처) 이러한 대화형 쿼리 엔진은 Hive 와는 달리 순간 최대 속도를 높이기 위해 모든 오버헤드를 제거하여, 리소스를 최대한 활용하여 쿼리를 실행한다 (이는 Hive의 단점으로 언급한 부분을 해결한다) . 그 결과, 대화형 쿼리 엔진은 MPP DB와 비교해도 손색없는 응답 시간을 얻을 수 있다. Hadoop에서는 쿼리 엔진을 목적에 따라 구분한다. 대량의 비구조화 데이터를 가공하는 무거운 배치 처리에는 높은 처리량으로 리소스를 활용할 수 있는 Hive를, 구조화 및 완성된 데이터를 대화식으로 집계를 원할 땐 지연이 적은 Impala와 Presto가 적합하다. 2019.10.22 made by jaejun.lee","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://jx2lee.github.io/categories/Hadoop/"}],"tags":[]},{"title":"[Python] 등굣길","slug":"programmers-tortoise","date":"2019-10-17T15:00:00.000Z","updated":"2020-03-30T15:06:23.526Z","comments":true,"path":"programmers-tortoise/","link":"","permalink":"https://jx2lee.github.io/programmers-tortoise/","excerpt":"등교하는데 가능한 루트의 최솟값을 구하는 문제를 풀어본다.","text":"등교하는데 가능한 루트의 최솟값을 구하는 문제를 풀어본다. 문제 설명문제 설명계속되는 폭우로 일부 지역이 물에 잠겼습니다. 물에 잠기지 않은 지역을 통해 학교를 가려고 합니다. 집에서 학교까지 가는 길은 m x n 크기의 격자모양으로 나타낼 수 있습니다. 아래 그림은 m = 4, n = 3 인 경우입니다. 가장 왼쪽 위, 즉 집이 있는 곳의 좌표는 (1, 1)로 나타내고 가장 오른쪽 아래, 즉 학교가 있는 곳의 좌표는 (m, n)으로 나타냅니다. 격자의 크기 m, n과 물이 잠긴 지역의 좌표를 담은 2차원 배열 puddles이 매개변수로 주어집니다. 집에서 학교까지 갈 수 있는 최단경로의 개수를 1,000,000,007로 나눈 나머지를 return 하도록 solution 함수를 작성해주세요. 제한사항 격자의 크기 m, n은 1 이상 100 이하인 자연수입니다. m과 n이 모두 1인 경우는 입력으로 주어지지 않습니다. 물에 잠긴 지역은 0개 이상 10개 이하입니다. 집과 학교가 물에 잠긴 경우는 입력으로 주어지지 않습니다. 입출력 예 m n puddles return 4 3 [[2, 2]] 4 입출력 예 설명 문제 접근Dynamic Programming 문제. 오랜만에 파이썬 알고리즘 문제를 풀었다. 쉬운 레벨이라 생각해 도전하였지만 역시나 구글검색행.. 우선 코드에 사용한 변수들을 살펴보자 grid : 격자 (index error를 방지하기 위해 +1 만큼 더 생성) 여기서 조심해야 할 것은, 문제에서 제공하는 m,n이 행과 열이라고 생각할 수 있는데 그 반대이다. 이 점을 명심하고 문제를 풀어야 index error를 방지하고 문제를 해결할 수 있다. 물이 고여있는 좌표에는 -1로 대체한다. 123if puddles: for x, y in puddles: grid[y][x] = -1 이제 (a,b) = (a-1, b) + (a, b-1) 식을 구현하는데 위에 언급한 것 처럼 행과 열을 조심해서 for문을 수행해야 한다. 행을 기준으로 열을 채워나가는 구조로 (1,1) 인 부분은 continue로 수정하지 않게 설정한다. 또한, 물이 있는 경우는 0으로 바꿔 횟수가 커지기 않게 방지하고 마지막으로 위 식을 작성하면 grid 변수는 원하는 대로 채워질 것이다. 12345678for j in range(1, n+1): for i in range(1, m+1): if i == j == 1: continue if grid[j][i] == -1: grid[j][i] = 0 continue grid[j][i] = (grid[j][i-1] + grid[j-1][i])%1000000007 행과 열 순서가 바꿔 있기 때문에 마지막 return 값도 [m][n]이 아닌 [n][m]으로 return 해야한다. return grid[n][m] full code는 하기와 같다 문제 해결12345678910111213141516def solution(m, n, puddles): grid = [[0] * (m+1) for _ in range(n+1)] grid[1][1] = 1 if puddles: for x, y in puddles: grid[y][x] = -1 for j in range(1, n+1): for i in range(1, m+1): if i == j == 1: continue if grid[j][i] == -1: grid[j][i] = 0 continue grid[j][i] = (grid[j][i-1] + grid[j-1][i])%1000000007 return grid[n][m] 2019.10.18 made by jaejun.lee","categories":[{"name":"Python","slug":"Python","permalink":"https://jx2lee.github.io/categories/Python/"}],"tags":[{"name":"Programmers","slug":"Programmers","permalink":"https://jx2lee.github.io/tags/Programmers/"}]},{"title":"[SQL] Print Prime Number","slug":"hackerrank-print_prime_number","date":"2019-10-14T15:00:00.000Z","updated":"2020-03-30T15:06:23.580Z","comments":true,"path":"hackerrank-print_prime_number/","link":"","permalink":"https://jx2lee.github.io/hackerrank-print_prime_number/","excerpt":"hackerrank에서 제공하는 Print Prime Number 문제를 사용자 변수를 활용해 해결하였다.","text":"hackerrank에서 제공하는 Print Prime Number 문제를 사용자 변수를 활용해 해결하였다. 문제Write a query to print all prime numbers less than or equal to 1000. Print your result on a single line, and use the ampersand () character as your separator (instead of a space). For example, the output for all prime numbers &lt;= 10 would be: 12&amp;3&amp;5&amp;7 접근1000 이하의 소수를 구하는 문제로, sql query로는 처음 풀어본다. 아래와 같은 순서로 풀어볼 수 있다. (참고한 자료는 url을 잃어버렸다. 죄송합니다) 첫 번째, prime number를 구하기 위한 num 변수를 2 이상 1000이하 까지 조회하는 부분이다. information_schema의 테이블을 이용해 num := num + 1 을 조회하면 아래와 같다. 1234567select @num1 :=@num1 + 1 as num1from information_schema.tables t1, information_schema.tables t2, (select @num1 := 1) tmp; 두 번째, 소수가 아닌 수를 걸러내기 위해 exists 문을 작성한다. div 변수를 num 변수와 같이 조회하는 문을 이용해 소수(약수는 나와 그 수 밖에 없는 특징 : floor(num / div) != num / div )를 구한다. 이때, where절에 속해야 한다. 12345678910111213141516171819where num1 &lt;= 1000and not exists ( select * from ( select @num2 :=@num2 + 1 as num2 from information_schema.tables as t1, information_schema.tables as t2, (select @num2 := 1) tmp2 limit 1000 ) t2 where floor(num1 / num2) = (num1 / num2) and num2 * num2 &lt;= num1 and num2 &gt; 1) 이제 적절히 두 sql 문을 합쳐주면 된다. 테이블명이 중복되지 않게, 이미 사용한 사용자 변수 또한 중복되지 않게 두 query를 섞어주면 문제를 해결할 수 있다. 특히, 사용자 변수 num1, num2 를 같은 것으로 실행하니 오류가 발생하였다. 앞으로 주의할 것! 사용자 변수는 모두 다르게 해결123456789101112131415161718192021222324252627282930select group_concat(num1 separator '&amp;')from ( select @num1 :=@num1 + 1 as num1 from information_schema.tables as t1, information_schema.tables as t2, (select @num1 := 1) tmp1 ) t1where num1 &lt;= 1000and not exists ( select * from ( select @num2 :=@num2 + 1 as num2 from information_schema.tables as t1, information_schema.tables as t2, (select @num2 := 1) tmp2 limit 1000 ) t2 where floor(num1 / num2) = (num1 / num2) and num2 * num2 &lt;= num1 and num2 &gt; 1); 2019.10.15 made by jaejun.lee","categories":[{"name":"SQL","slug":"SQL","permalink":"https://jx2lee.github.io/categories/SQL/"}],"tags":[{"name":"Hackerrank","slug":"Hackerrank","permalink":"https://jx2lee.github.io/tags/Hackerrank/"}]},{"title":"[SQL] Symmetric Pairs & Draw The Triangle 1","slug":"hackerrank-symmetric_pairs_draw_triangle_1","date":"2019-10-13T15:00:00.000Z","updated":"2020-03-30T15:06:23.550Z","comments":true,"path":"hackerrank-symmetric_pairs_draw_triangle_1/","link":"","permalink":"https://jx2lee.github.io/hackerrank-symmetric_pairs_draw_triangle_1/","excerpt":"hackerrank에서 제공하는 Symmetric Pairs &amp; Draw The Triangle 1 문제를 정리한다.","text":"hackerrank에서 제공하는 Symmetric Pairs &amp; Draw The Triangle 1 문제를 정리한다. Symmetric Pairs문제You are given a table, Functions, containing two columns: X and Y. Two pairs (X1, Y1) and (X2, Y2) are said to be symmetric pairs if X1 = Y2 and X2 = Y1. Write a query to output all such symmetric pairs in ascending order by the value of X. Sample Input Sample Output 12320 2020 2122 23 접근문제는 1) x = y인 짝들과 2) x != y 인 짝들의 union으로 접근하였다.우선, 1) x = y인 경우는 아래 쿼리로 표현할 수 있다. 12345678910select x, yfrom functions as f1where x = y and (select count(*) from functions where x = f1.x and y = f1.x) &gt; 1``` *count(*) &gt; 1*인 이유는 나온 갯수가 2개 이상인 짝들만 뽑아줘야 하므로 `where`절에 조건을 추가한 것이다. 이후 `2) x != y` 는 아래와 같다.```sqlselect f1.x, f1.yfrom functions as f1, functions as f2where f1.x = f2.y and f1.y = f2.x and f1.x &lt; f1.y 주목해야하는 부분은 f1.x &lt; f1.y인 부분으로, 뽑아내는 짝들의 x값이 y보다 작은 짝들만 찾아주게 되면 1) x = y인 부분은 제외할 수 있다. 이와 같은 두 쿼리를 union으로 묶어주고 마지막 order by를 통해 정렬만 하면 문제가 해결된다. 해결123456789select x, yfrom functions as f1where x = y and (select count(*) from functions where x = f1.x and y = f1.x) &gt; 1unionselect f1.x, f1.yfrom functions as f1, functions as f2where f1.x = f2.y and f1.y = f2.x and f1.x &lt; f1.yorder by x; Draw The Triangle 1문제P(R) represents a pattern drawn by Julia in R rows. The following pattern represents P(5): 12345* * * * * * * * * * * * * * * Write a query to print the pattern P(20). 접근사용자 정의 변수를 이용해 접근하였다.i라는 변수를 21로 선언하고, repeat함수를 이용해 i &gt; 0일 때까지 반복하는 쿼리를 작성하였다. 4문장으로 쉽게 풀리는 문제인데, 사용자 정의 변수에 대해 다시 한 번 생각해보자는 의미로 정리하였고 다음에는 꼭 틀리지 말자. 해결1234set @i = 21;select repeat('* ', @i := @i - 1)from information_schema.tableswhere @i &gt; 0; 2019.10.14 made by jaejun.lee","categories":[{"name":"SQL","slug":"SQL","permalink":"https://jx2lee.github.io/categories/SQL/"}],"tags":[{"name":"Hackerrank","slug":"Hackerrank","permalink":"https://jx2lee.github.io/tags/Hackerrank/"}]},{"title":"[SQL] Contest Leaderboard","slug":"hackerrank-contest_leaderboard","date":"2019-10-09T15:00:00.000Z","updated":"2020-03-30T15:06:23.583Z","comments":true,"path":"hackerrank-contest_leaderboard/","link":"","permalink":"https://jx2lee.github.io/hackerrank-contest_leaderboard/","excerpt":"hackerrank에서 제공하는 Contest Leaderboard 문제를 Group by를 활용해 해결하였다.","text":"hackerrank에서 제공하는 Contest Leaderboard 문제를 Group by를 활용해 해결하였다. 문제You did such a great job helping Julia with her last coding contest challenge that she wants you to work on this one, too! The total score of a hacker is the sum of their maximum scores for all of the challenges. Write a query to print the hacker_id, name, and total score of the hackers ordered by the descending score. If more than one hacker achieved the same total score, then sort the result by ascending hacker_id. Exclude all hackers with a total score of from your result. Input Format The following tables contain contest data: Hackers: The hacker_id is the id of the hacker, and name is the name of the hacker. Submissions: The submission_id is the id of the submission, hacker_id is the id of the hacker who made the submission, challenge_id is the id of the challenge for which the submission belongs to, and score is the score of the submission. Sample Input Hackers Table: Submissions Table: Sample Output 12345674071 Rose 19174842 Lisa 17484072 Bonnie 1004806 Angela 8926071 Frank 8580305 Kimberly 6749438 Patrick 43 Explanation Hacker 4071 submitted solutions for challenges 19797 and 49593, so the total score . Hacker 74842 submitted solutions for challenges 19797 and 63132, so the total score Hacker 84072 submitted solutions for challenges 49593 and 63132, so the total score . The total scores for hackers 4806, 26071, 80305, and 49438 can be similarly calculated. 접근우선은, challenge_id / hacker_id 별 score의 최댓값을 구하고 이를 hackers 테이블과 조인한다. 123456select h.hacker_id, h.name, sum(m.score) as total_scorefrom (select hacker_id, challenge_id, max(score) as score from submissions group by challenge_id, hacker_id) as mjoin hackers as h on h.hacker_id = m.hacker_id 이후 max score들의 total_score를 구하기 위해 hacker_id / name 을 key로 하여 group by 한다. 1234567select h.hacker_id, h.name, sum(m.score) as total_scorefrom (select hacker_id, challenge_id, max(score) as score from submissions group by challenge_id, hacker_id) as mjoin hackers as h on h.hacker_id = m.hacker_idgroup by h.hacker_id, h.name 마지막으로 문제에 따라 정렬만 하면 된다. (total_score &gt; 0인 조건도 추가) 해결123456789select h.hacker_id, h.name, sum(m.score) as total_scorefrom (select hacker_id, challenge_id, max(score) as score from submissions group by challenge_id, hacker_id) as mjoin hackers as h on h.hacker_id = m.hacker_idgroup by h.hacker_id, h.namehaving total_score &gt; 0order by total_score desc, h.hacker_id; 2019.10.10 made by jaejun.lee","categories":[{"name":"SQL","slug":"SQL","permalink":"https://jx2lee.github.io/categories/SQL/"}],"tags":[{"name":"Hackerrank","slug":"Hackerrank","permalink":"https://jx2lee.github.io/tags/Hackerrank/"}]},{"title":"[SQL] SQL Project Planning","slug":"hackerrank-project_planning","date":"2019-10-09T15:00:00.000Z","updated":"2020-03-30T15:06:23.581Z","comments":true,"path":"hackerrank-project_planning/","link":"","permalink":"https://jx2lee.github.io/hackerrank-project_planning/","excerpt":"hackerrank에서 제공하는 SQL Project Planning 문제를 정리한다.","text":"hackerrank에서 제공하는 SQL Project Planning 문제를 정리한다. 문제You are given a table, Projects, containing three columns: Task_ID, Start_Date and End_Date. It is guaranteed that the difference between the End_Date and the Start_Date is equal to 1 day for each row in the table. If the End_Date of the tasks are consecutive, then they are part of the same project. Samantha is interested in finding the total number of different projects completed. Write a query to output the start and end dates of projects listed by the number of days it took to complete the project in ascending order. If there is more than one project that have the same number of completion days, then order by the start date of the project. Sample Input Sample Output 12342015-10-28 2015-10-292015-10-30 2015-10-312015-10-13 2015-10-152015-10-01 2015-10-04 Explanation The example describes following four projects: Project 1: Tasks 1, 2 and 3 are completed on consecutive days, so these are part of the project. Thus start date of project is 2015-10-01 and end date is 2015-10-04, so it took 3 days to complete the project. Project 2: Tasks 4 and 5 are completed on consecutive days, so these are part of the project. Thus, the start date of project is 2015-10-13 and end date is 2015-10-15, so it took 2 days to complete the project. Project 3: Only task 6 is part of the project. Thus, the start date of project is 2015-10-28 and end date is 2015-10-29, so it took 1 day to complete the project. Project 4: Only task 7 is part of the project. Thus, the start date of project is 2015-10-30 and end date is 2015-10-31, so it took 1 day to complete the project. 접근프로젝트의 시작과 끝 날짜를 출력하는 문제. 테이블에는 각각의 task들이 start_date, end_date로 구성되었고 같은 프로젝트는 각 task가 이어질 수 있다. 처음 접근했을 때 join을 이용해 p1, p2 테이블로부터 p1.end_date=p2.start_date를 이용하였지만 실패하여 블로그를 참고하였다. 우선 start_date가 end_date에 포함되지 않는 날짜를 확인한다. 이는 프로젝트의 시작일 것이다 1234select start_datefrom projectswhere start_date not in(select end_date from projects); 마찬가지로 end_date가 start_date에 포함되지 않는 날짜를 확인한다. 이는 프로젝트이 끝일 것이다. 1234select end_datefrom projectswhere end_date not in(select start_date from projects); 그런 다음 각 프로젝트에 대해 (시작 날짜, 종료 날짜) 쌍을 찾아야한다. 그 전에 프로젝트의 시작 날짜와 종료 날짜를 교차시켜 모든 잠재적 쌍을 생성한다. 또한, 동일한 프로젝트의 경우 종료 날짜는 프로젝트 시작 날짜보다 큰 프로젝트의 모든 종료 날짜 중 가장 작아야한다. 123456select start_date, min(end_date)from (select start_date from projects where start_date not in (select end_date from projects)) as t1, (select end_date from projects where end_date not in (select start_date from projects)) as t2where start_date &lt; end_dategroup by start_date 마지막 문제 조건 중 프로젝트 수행 기간이 짧은 순서로, 수행 기간이 같다면 시작 날짜를 기준으로 정렬하면 해결할 수 있다. (datediff 함수 이용 - 두 날짜 데이터 차이값 생성) ADD order by datediff(min(end_date), start_date), start_date 해결1234567select start_date, min(end_date)from (select start_date from projects where start_date not in (select end_date from projects)) as t1, (select end_date from projects where end_date not in (select start_date from projects)) as t2where start_date &lt; end_dategroup by start_dateorder by datediff(min(end_date), start_date), start_date; 2019.10.10 made by jaejun.lee","categories":[{"name":"SQL","slug":"SQL","permalink":"https://jx2lee.github.io/categories/SQL/"}],"tags":[{"name":"Hackerrank","slug":"Hackerrank","permalink":"https://jx2lee.github.io/tags/Hackerrank/"}]},{"title":"[SQL] Challenges","slug":"hackerrank-challenges","date":"2019-10-06T15:00:00.000Z","updated":"2020-03-30T15:06:23.549Z","comments":true,"path":"hackerrank-challenges/","link":"","permalink":"https://jx2lee.github.io/hackerrank-challenges/","excerpt":"hackerrank에서 제공하는 Challenges 문제를 Join을 활용해 해결하였다.","text":"hackerrank에서 제공하는 Challenges 문제를 Join을 활용해 해결하였다. 문제Julia asked her students to create some coding challenges. Write a query to print the hacker_id, name, and the total number of challenges created by each student. Sort your results by the total number of challenges in descending order. If more than one student created the same number of challenges, then sort the result by hacker_id. If more than one student created the same number of challenges and the count is less than the maximum number of challenges created, then exclude those students from the result. Input Format The following tables contain challenge data: Hackers: The hacker_id is the id of the hacker, and name is the name of the hacker. Challenges: The challenge_id is the id of the challenge, and hacker_id is the id of the student who created the challenge. Sample Input 0 Hackers Table: Challenges Table: Sample Output 0 12321283 Angela 688255 Patrick 596196 Lisa 1 Sample Input 1 Hackers Table: Challenges Table: Sample Output 1 1234512299 Rose 634856 Angela 679345 Frank 480491 Patrick 381041 Lisa 1 Explanation For Sample Case 0, we can get the following details:Students and both created challenges, but the maximum number of challenges created is so these students are excluded from the result. For Sample Case 1, we can get the following details:Students and both created challenges. Because is the maximum number of challenges created, these students are included in the result. 접근group by 조건 중 1) 최댓값이 하나여야하고, 2) 최댓값 아래로 중복되는 횟수를 가지면 안된다 를 만족하는 것이 어려웠다. count 값을 OR을 이용해 만족하는 범위로 having절을 작성하여 해결할 수 있었다. 순서는 아래와 같이 풀었다. hackers 테이블과 challenges 테이블을 join 및 group by (group by key는 id / name) having 절 cnt 조건을 OR로 작성 (1. max value, 2. not duplicated) 문제 조건에 맞는 order by 추가 해결1234567891011121314151617181920212223242526select c.hacker_id, h.name, count(c.challenge_id) as cntfrom challenges as cjoin hackers as h on c.hacker_id = h.hacker_idgroup by c.hacker_id, h.namehaving cnt = (select count(c1.challenge_id) from challenges as c1 group by c1.hacker_id order by count(c1.challenge_id) desc limit 1) or cnt not in (select count(c2.challenge_id) from challenges as c2 group by c2.hacker_id having c2.hacker_id &lt;&gt; c.hacker_id)order by cnt desc, c.hacker_id; 2019.10.07 made by jaejun.lee","categories":[{"name":"SQL","slug":"SQL","permalink":"https://jx2lee.github.io/categories/SQL/"}],"tags":[{"name":"Hackerrank","slug":"Hackerrank","permalink":"https://jx2lee.github.io/tags/Hackerrank/"}]},{"title":"[SQL] Ollivander's Inventory","slug":"hackerrank-ollivanders_inventory","date":"2019-10-06T15:00:00.000Z","updated":"2020-03-30T15:06:23.576Z","comments":true,"path":"hackerrank-ollivanders_inventory/","link":"","permalink":"https://jx2lee.github.io/hackerrank-ollivanders_inventory/","excerpt":"hackerrank에서 제공하는 Ollivander&#39;s Inventory 문제를 Group by를 활용해 해결하였다.","text":"hackerrank에서 제공하는 Ollivander&#39;s Inventory 문제를 Group by를 활용해 해결하였다. 문제Harry Potter and his friends are at Ollivander’s with Ron, finally replacing Charlie’s old broken wand. Hermione decides the best way to choose is by determining the minimum number of gold galleons needed to buy each non-evil wand of high power and age. Write a query to print the id, age, coins_needed, and power of the wands that Ron’s interested in, sorted in order of descending power. If more than one wand has same power, sort the result in order of descending age. Input Format The following tables contain data on the wands in Ollivander’s inventory: Wands: The id is the id of the wand, code is the code of the wand, coins_needed is the total number of gold galleons needed to buy the wand, and power denotes the quality of the wand (the higher the power, the better the wand is). Wands_Property: The code is the code of the wand, age is the age of the wand, and is_evil denotes whether the wand is good for the dark arts. If the value of is_evil is 0, it means that the wand is not evil. The mapping between code and age is one-one, meaning that if there are two pairs, and , then and . Sample Input Wands Table: Wands_Property Table: Sample Output 12345678910119 45 1647 1012 17 9897 101 20 3688 815 40 6018 719 20 7651 611 40 7587 510 20 504 518 40 3312 320 17 5689 35 45 6020 214 40 5408 1 Explanation The data for wands of age 45 (code 1): The minimum number of galleons needed for The minimum number of galleons needed for The data for wands of age 40 (code 2): The minimum number of galleons needed for The minimum number of galleons needed for The minimum number of galleons needed for The minimum number of galleons needed for The data for wands of age 20 (code 4): The minimum number of galleons needed for The minimum number of galleons needed for The minimum number of galleons needed for The data for wands of age 17 (code 5): The minimum number of galleons needed for The minimum number of galleons needed for 접근determining the minimum number of gold galleons needed 부분을 놓쳤다. Wands 테이블에서 code/power 별 coins_needed의 최솟값을 찾은 테이블과 Wands / Wands_property 테이블을 조인하여 order by만 추가하여 쿼리를 완성하면 된다. code, power, min(coins_needed) 를 code/power 별 group by wands/wands_property 테이블과 join (wands 테이블 조인 시 code/coins_needed 일치) order by 로 power/age descending 해결123456789select w.id, p.age, w.coins_needed, w.powerfrom (select code, power, min(coins_needed) as coins_needed from wands group by code, power) as mjoin wands as w on w.code = m.code and w.coins_needed = m.coins_neededjoin wands_property as p on p.code = m.codewhere p.is_evil = 0order by m.power desc, p.age desc; 2019.10.07 made by jaejun.lee","categories":[{"name":"SQL","slug":"SQL","permalink":"https://jx2lee.github.io/categories/SQL/"}],"tags":[{"name":"Hackerrank","slug":"Hackerrank","permalink":"https://jx2lee.github.io/tags/Hackerrank/"}]},{"title":"[Python] 단어 변환","slug":"programmers-disk_controller","date":"2019-10-06T15:00:00.000Z","updated":"2020-03-30T15:06:23.545Z","comments":true,"path":"programmers-disk_controller/","link":"","permalink":"https://jx2lee.github.io/programmers-disk_controller/","excerpt":"특정 기준을 가지고 단어를 변환할 때 최소 횟수를 구하는 문제를 풀어본다.","text":"특정 기준을 가지고 단어를 변환할 때 최소 횟수를 구하는 문제를 풀어본다. 문제 설명하드디스크는 한 번에 하나의 작업만 수행할 수 있습니다. 디스크 컨트롤러를 구현하는 방법은 여러 가지가 있습니다. 가장 일반적인 방법은 요청이 들어온 순서대로 처리하는 것입니다.예를들어 123- 0ms 시점에 3ms가 소요되는 A작업 요청- 1ms 시점에 9ms가 소요되는 B작업 요청- 2ms 시점에 6ms가 소요되는 C작업 요청 와 같은 요청이 들어왔습니다. 이를 그림으로 표현하면 아래와 같습니다. 한 번에 하나의 요청만을 수행할 수 있기 때문에 각각의 작업을 요청받은 순서대로 처리하면 다음과 같이 처리 됩니다. 123- A: 3ms 시점에 작업 완료 (요청에서 종료까지 : 3ms)- B: 1ms부터 대기하다가, 3ms 시점에 작업을 시작해서 12ms 시점에 작업 완료(요청에서 종료까지 : 11ms)- C: 2ms부터 대기하다가, 12ms 시점에 작업을 시작해서 18ms 시점에 작업 완료(요청에서 종료까지 : 16ms) 이 때 각 작업의 요청부터 종료까지 걸린 시간의 평균은 10ms(= (3 + 11 + 16) / 3)가 됩니다. 하지만 A → C → B 순서대로 처리하면 123- A: 3ms 시점에 작업 완료(요청에서 종료까지 : 3ms)- C: 2ms부터 대기하다가, 3ms 시점에 작업을 시작해서 9ms 시점에 작업 완료(요청에서 종료까지 : 7ms)- B: 1ms부터 대기하다가, 9ms 시점에 작업을 시작해서 18ms 시점에 작업 완료(요청에서 종료까지 : 17ms) 이렇게 A → C → B의 순서로 처리하면 각 작업의 요청부터 종료까지 걸린 시간의 평균은 9ms(= (3 + 7 + 17) / 3)가 됩니다. 각 작업에 대해 [작업이 요청되는 시점, 작업의 소요시간]을 담은 2차원 배열 jobs가 매개변수로 주어질 때, 작업의 요청부터 종료까지 걸린 시간의 평균을 가장 줄이는 방법으로 처리하면 평균이 얼마가 되는지 return 하도록 solution 함수를 작성해주세요. (단, 소수점 이하의 수는 버립니다) 제한 사항 jobs의 길이는 1 이상 500 이하입니다. jobs의 각 행은 하나의 작업에 대한 [작업이 요청되는 시점, 작업의 소요시간] 입니다. 각 작업에 대해 작업이 요청되는 시간은 0 이상 1,000 이하입니다. 각 작업에 대해 작업의 소요시간은 1 이상 1,000 이하입니다. 하드디스크가 작업을 수행하고 있지 않을 때에는 먼저 요청이 들어온 작업부터 처리합니다. 입출력 예jobs return [[0, 3], [1, 9], [2, 6]] 9 입출력 예 설명문제에 주어진 예와 같습니다. 0ms 시점에 3ms 걸리는 작업 요청이 들어옵니다. 1ms 시점에 9ms 걸리는 작업 요청이 들어옵니다. 2ms 시점에 6ms 걸리는 작업 요청이 들어옵니다. 문제 접근heap 을 이용해야 하는 문제. 사실 이에 대한 개념이 부족하여 공부하려 했지만 방대하길래 우선 패쓰하고 블로그를 참고하였다. 시간이 된다면 heap 에 대해 글을 정리하고 우선 아래와 같은 변수를 통해 해결하였다. 변수 in_, out_ : 작업을 시작/종료한 시간 ans&#39; : 총 작업 시간 n : 총 작업의 갯수 cnt : heap 구조에서 자료가 빠져나간 횟수로 while문에 사용 wt : heap구조로 작업의 종료 시간을 담는다. 문제 해결참고 블로그 123456789101112131415161718192021import heapqdef solution(jobs): in_, out_, ans, cnt = -1, 0, 0, 0 wt = [] n = len(jobs) while cnt &lt; n: for job in jobs: if in_ &lt; job[0] &lt;= out_ : ans += (out_ - job[0]) heapq.heappush(wt, job[1]) if len(wt) &gt; 0: ans += len(wt) * wt[0] #len(wt)를 곱하는 이유는 대기열에 들어간 작업도 작업 중인 시간을 더해야하므로 wt 길이를 곱해준다. in_ = out_ out_ += heapq.heappop(wt) cnt += 1 else: out_ += 1 return ans // n 2019.10.07 made by jaejun.lee","categories":[{"name":"Python","slug":"Python","permalink":"https://jx2lee.github.io/categories/Python/"}],"tags":[{"name":"Programmers","slug":"Programmers","permalink":"https://jx2lee.github.io/tags/Programmers/"}]},{"title":"[SQL] The Report","slug":"hackerrank-the_report","date":"2019-10-03T15:00:00.000Z","updated":"2020-03-30T15:06:23.547Z","comments":true,"path":"hackerrank-the_report/","link":"","permalink":"https://jx2lee.github.io/hackerrank-the_report/","excerpt":"hackerrank에서 제공하는 The Report 문제를 Join을 활용해 해결하였다.","text":"hackerrank에서 제공하는 The Report 문제를 Join을 활용해 해결하였다. 문제You are given two tables: Students and Grades. Students contains three columns ID, Name and Marks. Grades contains the following data: Ketty gives Eve a task to generate a report containing three columns: Name, Grade and Mark. Ketty doesn’t want the NAMES of those students who received a grade lower than 8. The report must be in descending order by grade – i.e. higher grades are entered first. If there is more than one student with the same grade (8-10) assigned to them, order those particular students by their name alphabetically. Finally, if the grade is lower than 8, use “NULL” as their name and list them by their grades in descending order. If there is more than one student with the same grade (1-7) assigned to them, order those particular students by their marks in ascending order. Write a query to help Eve. Sample Input Sample Output 123456Maria 10 99Jane 9 81Julia 9 88 Scarlet 8 78NULL 7 63NULL 7 68 Note Print “NULL” as the name if the grade is less than 8. Explanation Consider the following table with the grades assigned to the students: So, the following students got 8, 9 or 10 grades: Maria (grade 10) Jane (grade 9) Julia (grade 9) Scarlet (grade 8) 접근order by 에 name이 NULL인 학생들 중 같은 Grade이면 점수를 오름차순 정렬할 때 헷갈렸다. name 정렬 후 marks로 정렬하면 끝나는 문제. 그리고 처음엔 Union으로 문제를 접근했는데 case문으로 쉽게 풀 수 있었다. 해결12345678910select case when g.grade &lt; 8 then null else s.name end, g.grade, s.marksfrom students as sjoin grades as g on s.marks between g.min_mark and g.max_markorder by g.grade desc, s.name, s.marks; 2019.10.04 made by jaejun.lee","categories":[{"name":"SQL","slug":"SQL","permalink":"https://jx2lee.github.io/categories/SQL/"}],"tags":[{"name":"Hackerrank","slug":"Hackerrank","permalink":"https://jx2lee.github.io/tags/Hackerrank/"}]},{"title":"[SQL] Weather Observation Station 20","slug":"hackerrank-weather_observation_station_20","date":"2019-10-01T15:00:00.000Z","updated":"2020-03-30T15:06:23.589Z","comments":true,"path":"hackerrank-weather_observation_station_20/","link":"","permalink":"https://jx2lee.github.io/hackerrank-weather_observation_station_20/","excerpt":"hackerrank에서 제공하는 Weather Observation Station 20 문제를 사용자 정의 변수를 활용해 해결하였다.","text":"hackerrank에서 제공하는 Weather Observation Station 20 문제를 사용자 정의 변수를 활용해 해결하였다. 문제A median is defined as a number separating the higher half of a data set from the lower half. Query the median of the Northern Latitudes (LAT_N) from STATION and round your answer to decimal places. Input Format The STATION table is described as follows: where LAT_N is the northern latitude and LONG_W is the western longitude. 접근사용자 정의 변수를 이용해 median을 구하는 문제이다. row index가 1부터 시작하며 LAT_N을 기준으로 sorting된 테이블에서, index가 @ct/2.0, @ct/2.0+1 범위일 경우 조회하는 query를 작성하였다. 여기선 @ct는 median 계산을 위해 테이블 전체 행을 뜻한다. 해결123456789set @row_id = 0;set @ct = (select count(*) from station);select round(avg(LAT_N), 4)from (select * from station order by LAT_N) as samplewhere (select @row_id := @row_id + 1) between @ct/2.0 and @ct/2.0 + 1; 2019.10.02 made by jaejun.lee","categories":[{"name":"SQL","slug":"SQL","permalink":"https://jx2lee.github.io/categories/SQL/"}],"tags":[{"name":"Hackerrank","slug":"Hackerrank","permalink":"https://jx2lee.github.io/tags/Hackerrank/"}]},{"title":"[Python] 예산","slug":"programmers-budgets","date":"2019-09-30T15:00:00.000Z","updated":"2020-03-30T15:06:23.530Z","comments":true,"path":"programmers-budgets/","link":"","permalink":"https://jx2lee.github.io/programmers-budgets/","excerpt":"정해진 총액 이하에서 가능한 한 최대 예산을 배정하는 문제를 풀어본다.","text":"정해진 총액 이하에서 가능한 한 최대 예산을 배정하는 문제를 풀어본다. 문제 설명문제 설명국가의 역할 중 하나는 여러 지방의 예산요청을 심사하여 국가의 예산을 분배하는 것입니다. 국가예산의 총액은 미리 정해져 있어서 모든 예산요청을 배정해 주기는 어려울 수도 있습니다. 그래서 정해진 총액 이하에서 가능한 한 최대의 총 예산을 다음과 같은 방법으로 배정합니다. 1231. 모든 요청이 배정될 수 있는 경우에는 요청한 금액을 그대로 배정합니다.2. 모든 요청이 배정될 수 없는 경우에는 특정한 정수 상한액을 계산하여 그 이상인 예산요청에는 모두 상한액을 배정합니다. 상한액 이하의 예산요청에 대해서는 요청한 금액을 그대로 배정합니다. 예를 들어, 전체 국가예산이 485이고 4개 지방의 예산요청이 각각 120, 110, 140, 150일 때, 상한액을 127로 잡으면 위의 요청들에 대해서 각각 120, 110, 127, 127을 배정하고 그 합이 484로 가능한 최대가 됩니다.각 지방에서 요청하는 예산이 담긴 배열 budgets과 총 예산 M이 매개변수로 주어질 때, 위의 조건을 모두 만족하는 상한액을 return 하도록 solution 함수를 작성해주세요. 제한 사항 지방의 수는 3 이상 100,000 이하인 자연수입니다. 각 지방에서 요청하는 예산은 1 이상 100,000 이하인 자연수입니다. 총 예산은 지방의 수 이상 1,000,000,000 이하인 자연수입니다. 입출력 예 budgets M return [120, 110, 140, 150] 485 127 출처 ※ 공지 - 2019년 3월 15일, 테스트케이스가 강화되었습니다. 이번 업데이트로 인해 지방의 수가 최대 10,000개에서 100,000개로 늘어났으며, 이에 따라 테스트케이스가 수정되었습니다. 이로 인해 이전에 통과하던 코드가 더 이상 통과하지 않을 수 있습니다. 문제 접근이분 탐색으로 문제를 해결할 수 있다.. left와 rigth의 중간값 mid를 구해 매 번 총액 M을 최대한 맞춘다. 만약 M보다 작으면 left +1, M보다 크면 right -1로 이분 탐색한다. 변수 설명 res : left mid right에 따른 총 예산액 left, mid, right : 최소, 최대에 따른 중간값(이분 탐색을 위해) 문제 해결틀린 코드12345678910111213141516def solution(budgets, M): sorted_budgets = sorted(budgets) res = 0 while M &gt; 0: tmp = sorted_budgets[0] if tmp &lt; M // len(sorted_budgets): res = tmp M -= tmp sorted_budgets.pop(0) else: res = M // len(sorted_budgets) M -= res sorted_budgets.pop(0) if len(sorted_budgets) == 1: return res return 0 맞은 코드참고 blog 12345678910111213141516def solution(budgets, M): left, right, tmp = 0, max(budgets), 0 while right &gt;= left: mid = (left + right) // 2 res = 0 for budget in budgets: if mid &gt; budget: res += budget else: res += mid if res &gt; M: right = mid - 1 else: answer = mid left = mid + 1 return answer 2019.10.01 made by jaejun.lee","categories":[{"name":"Python","slug":"Python","permalink":"https://jx2lee.github.io/categories/Python/"}],"tags":[{"name":"Programmers","slug":"Programmers","permalink":"https://jx2lee.github.io/tags/Programmers/"}]},{"title":"[Python] 단어 변환","slug":"programmers-convert_word","date":"2019-09-30T15:00:00.000Z","updated":"2020-03-30T15:06:23.533Z","comments":true,"path":"programmers-convert_word/","link":"","permalink":"https://jx2lee.github.io/programmers-convert_word/","excerpt":"특정 기준을 가지고 단어를 변환할 때 최소 횟수를 구하는 문제를 풀어본다.","text":"특정 기준을 가지고 단어를 변환할 때 최소 횟수를 구하는 문제를 풀어본다. 문제 설명두 개의 단어 begin, target과 단어의 집합 words가 있습니다. 아래와 같은 규칙을 이용하여 begin에서 target으로 변환하는 가장 짧은 변환 과정을 찾으려고 합니다. 121. 한 번에 한 개의 알파벳만 바꿀 수 있습니다.2. words에 있는 단어로만 변환할 수 있습니다. 예를 들어 begin이 hit, target가 cog, words가 [hot,dot,dog,lot,log,cog]라면 hit -&gt; hot -&gt; dot -&gt; dog -&gt; cog와 같이 4단계를 거쳐 변환할 수 있습니다. 두 개의 단어 begin, target과 단어의 집합 words가 매개변수로 주어질 때, 최소 몇 단계의 과정을 거쳐 begin을 target으로 변환할 수 있는지 return 하도록 solution 함수를 작성해주세요. 제한사항 각 단어는 알파벳 소문자로만 이루어져 있습니다. 각 단어의 길이는 3 이상 10 이하이며 모든 단어의 길이는 같습니다. words에는 3개 이상 50개 이하의 단어가 있으며 중복되는 단어는 없습니다. begin과 target은 같지 않습니다. 변환할 수 없는 경우에는 0를 return 합니다. 입출력 예 begin target words return hit cog [hot, dot, dog, lot, log, cog] 4 hit cog [hot, dot, dog, lot, log] 0 입출력 예 설명예제 #1문제에 나온 예와 같습니다. 예제 #2target인 cog는 words 안에 없기 때문에 변환할 수 없습니다. 문제 접근DFS/BFS 문제로 실패하여 블로그를 참고하였다. words가 빈 리스타가 될 때까지 탐색을 통해 문자 한 개만 변형된 단어를 cnt변수를 이용해 찾고, tmp를 계속해서 업데이트 해나간다. 이후 tmp안에 target word가 포함되면 while 횟수를 return하고, 아니면 answer = tmp로 계속해서 찾아나가는 방법으로 풀 수 있다. 변수 answer : 탐색을 시작하는 단어를 저장하는 변수 res : 문제의 결괏값 tmp : 변환될 단어들의 후보 cnt : 한 문자만 다른 단어를 뽑기위한 변수 (cnt == 1) 문제 해결12345678910111213141516171819202122232425def solution(begin, target, words): answer = [begin] res = 0 if not target in words: return 0 while words: for ans in answer: tmp = [] for word in words: cnt = 0 for i in range(len(ans)): if ans[i] != word[i]: cnt += 1 if cnt == 2: break if cnt == 1: tmp.append(word) words.remove(word) res += 1 if target in tmp: return res else: answer = tmp return 0 2019.10.01 made by jaejun.lee","categories":[{"name":"Python","slug":"Python","permalink":"https://jx2lee.github.io/categories/Python/"}],"tags":[{"name":"Programmers","slug":"Programmers","permalink":"https://jx2lee.github.io/tags/Programmers/"}]},{"title":"[SQL] Occupations","slug":"hackerrank-occupations","date":"2019-09-26T15:00:00.000Z","updated":"2020-03-30T15:06:23.563Z","comments":true,"path":"hackerrank-occupations/","link":"","permalink":"https://jx2lee.github.io/hackerrank-occupations/","excerpt":"hackerrank에서 제공하는 Occupations 문제를 pivot을 활용해 해결하였다.","text":"hackerrank에서 제공하는 Occupations 문제를 pivot을 활용해 해결하였다. 문제Pivot the Occupation column in OCCUPATIONS so that each Name is sorted alphabetically and displayed underneath its corresponding Occupation. The output column headers should be Doctor, Professor, Singer, and Actor, respectively. Note: Print NULL when there are no more names corresponding to an occupation. Input Format The OCCUPATIONS table is described as follows: Occupation will only contain one of the following values: Doctor, Professor, Singer or Actor. Sample Input Sample Output 123Jenny Ashley Meera JaneSamantha Christeen Priya JuliaNULL Ketty NULL Maria Explanation The first column is an alphabetically ordered list of Doctor names.The second column is an alphabetically ordered list of Professor names.The third column is an alphabetically ordered list of Singer names.The fourth column is an alphabetically ordered list of Actor names.The empty cell data for columns with less than the maximum number of names per occupation (in this case, the Professor and Actor columns) are filled with NULL values. 접근 변수 설정과(set @~) case문을 이용 set @[변수명] = [값] case 절은 위 문제 참고 from 절에 min대신 max를 해도 결과는 동일 해결123456789101112131415161718set @drow=0, @prow=0, @srow=0, @arow=0;select min(doctor), min(professor), min(singer), min(actor)from( select case occupation when 'Doctor' then @drow := @drow + 1 when 'Professor' then @prow := @prow + 1 when 'Singer' then @srow := @srow + 1 when 'Actor' then @arow := @arow + 1 end as row, if(occupation='Doctor', name, null) as doctor, if(occupation='Professor', name, null) as professor, if(occupation='Singer', name, null) as singer, if(occupation='Actor', name, null) as actor from occupations order by name) as agroup by row; 2019.09.27 made by jaejun.lee","categories":[{"name":"SQL","slug":"SQL","permalink":"https://jx2lee.github.io/categories/SQL/"}],"tags":[{"name":"Hackerrank","slug":"Hackerrank","permalink":"https://jx2lee.github.io/tags/Hackerrank/"}]},{"title":"[SQL] Type of Triangle","slug":"hackerrank-type_of_triangle","date":"2019-09-26T15:00:00.000Z","updated":"2020-03-30T15:06:23.564Z","comments":true,"path":"hackerrank-type_of_triangle/","link":"","permalink":"https://jx2lee.github.io/hackerrank-type_of_triangle/","excerpt":"hackerrank에서 제공하는 Type of Triangle 문제를 case를 활용해 해결하였다.","text":"hackerrank에서 제공하는 Type of Triangle 문제를 case를 활용해 해결하였다. 문제Write a query identifying the type of each record in the TRIANGLES table using its three side lengths. Output one of the following statements for each record in the table: Equilateral: It’s a triangle with sides of equal length. Isosceles: It’s a triangle with sides of equal length. Scalene: It’s a triangle with sides of differing lengths. Not A Triangle: The given values of A, B, and C don’t form a triangle. Input Format The TRIANGLES table is described as follows: Each row in the table denotes the lengths of each of a triangle’s three sides. Sample Input Sample Output 1234IsoscelesEquilateralScaleneNot A Triangle Explanation Values in the tuple form an Isosceles triangle, because .Values in the tuple form an Equilateral triangle, because . Values in the tuple form a Scalene triangle, because .Values in the tuple cannot form a triangle because the combined value of sides and is not larger than that of side 접근IF문을 사용하려다 select에 case::when-then을 이용하였다. 그리고 Not a triangle 조건을 먼저 주지않고 나중에 준다면(Isosceles 이후에 조건을 삽입) 결과값이 달라지는 오류가 발생한다. case문법은 아래와 같다. 123456case [column(선택)] when ~ then ~ when ~ then ~ ... ...end 해결123456789101112selectcase when a=b and b=c then \"Equilateral\" when a+b&lt;=c then \"Not A Triangle\" when a+c&lt;=b then \"Not A Triangle\" when b+c&lt;=a then \"Not A Triangle\" when a=b and a&lt;&gt;c then \"Isosceles\" when a=c and c&lt;&gt;b then \"Isosceles\" when b=c and c&lt;&gt;a then \"Isosceles\" when a&lt;&gt;b and b&lt;&gt;c then \"Scalene\"endfrom triangles; 2019.09.27 made by jaejun.lee","categories":[{"name":"SQL","slug":"SQL","permalink":"https://jx2lee.github.io/categories/SQL/"}],"tags":[{"name":"Hackerrank","slug":"Hackerrank","permalink":"https://jx2lee.github.io/tags/Hackerrank/"}]},{"title":"[Python] 단속카메라","slug":"programmers-camera","date":"2019-09-26T15:00:00.000Z","updated":"2020-03-30T15:06:23.525Z","comments":true,"path":"programmers-camera/","link":"","permalink":"https://jx2lee.github.io/programmers-camera/","excerpt":"단속카메라를 일정 조건에 맞게 최소로 설치하는 문제를 풀어본다","text":"단속카메라를 일정 조건에 맞게 최소로 설치하는 문제를 풀어본다 문제문제 설명고속도로를 이동하는 모든 차량이 고속도로를 이용하면서 단속용 카메라를 한 번은 만나도록 카메라를 설치하려고 합니다. 고속도로를 이동하는 차량의 경로 routes가 매개변수로 주어질 때, 모든 차량이 한 번은 단속용 카메라를 만나도록 하려면 최소 몇 대의 카메라를 설치해야 하는지를 return 하도록 solution 함수를 완성하세요. 제한사항 차량의 대수는 1대 이상 10,000대 이하입니다. routes에는 차량의 이동 경로가 포함되어 있으며 routes[i][0]에는 i번째 차량이 고속도로에 진입한 지점, routes[i][1]에는 i번째 차량이 고속도로에서 나간 지점이 적혀 있습니다. 차량의 진입/진출 지점에 카메라가 설치되어 있어도 카메라를 만난것으로 간주합니다. 차량의 진입 지점, 진출 지점은 -30,000 이상 30,000 이하입니다. 입출력 예 routes return [[-20,15], [-14,-5], [-18,-13], [-5,-3]] 2 입출력 예 설명 -5 지점에 카메라를 설치하면 두 번째, 네 번째 차량이 카메라를 만납니다. -15 지점에 카메라를 설치하면 첫 번째, 세 번째 차량이 카메라를 만납니다. 문제 접근 Greedy 알고리즘으로 쉽게 해결할 수 있는 문제. 입력받은 list를 sorting (도착 지점을 기준으로)하고 tmp변수와 출발지점을 비교해 작다면 해당 범위에 포함되지 않으므로 answer을 추가하며 tmp를 갱신해주면 된다. 문제 해결아래 코드로 해결하였다. (참고) 1234567891011def solution(routes): routes = sorted(routes, key=lambda x: x[1]) answer = 0 tmp = -100000000000 for route in routes: if tmp &lt; route[0]: answer += 1 tmp = route[1] return answer 2019.09.27 made by jaejun.lee","categories":[{"name":"Python","slug":"Python","permalink":"https://jx2lee.github.io/categories/Python/"}],"tags":[{"name":"Programmers","slug":"Programmers","permalink":"https://jx2lee.github.io/tags/Programmers/"}]},{"title":"[Python] 가장 먼 노드","slug":"programmers-node","date":"2019-09-26T15:00:00.000Z","updated":"2020-03-30T15:06:23.561Z","comments":true,"path":"programmers-node/","link":"","permalink":"https://jx2lee.github.io/programmers-node/","excerpt":"노드를 연결하는 그래프를 작성하고 노드 1에서 가장 멀리 떨어진 노드 갯수를 구하는 문제를 풀어본다","text":"노드를 연결하는 그래프를 작성하고 노드 1에서 가장 멀리 떨어진 노드 갯수를 구하는 문제를 풀어본다 문제문제 설명n개의 노드가 있는 그래프가 있습니다. 각 노드는 1부터 n까지 번호가 적혀있습니다. 1번 노드에서 가장 멀리 떨어진 노드의 갯수를 구하려고 합니다. 가장 멀리 떨어진 노드란 최단경로로 이동했을 때 간선의 개수가 가장 많은 노드들을 의미합니다. 노드의 개수 n, 간선에 대한 정보가 담긴 2차원 배열 vertex가 매개변수로 주어질 때, 1번 노드로부터 가장 멀리 떨어진 노드가 몇 개인지를 return 하도록 solution 함수를 작성해주세요. 제한사항 노드의 개수 n은 2 이상 20,000 이하입니다. 간선은 양방향이며 총 1개 이상 50,000개 이하의 간선이 있습니다. vertex 배열 각 행 [a, b]는 a번 노드와 b번 노드 사이에 간선이 있다는 의미입니다. 입출력 예 n vertex return 6 [[3, 6], [4, 3], [3, 2], [1, 3], [1, 2], [2, 4], [5, 2]] 3 입출력 예 설명예제의 그래프를 표현하면 아래 그림과 같고, 1번 노드에서 가장 멀리 떨어진 노드는 4,5,6번 노드입니다. 문제 접근그래프 관련 문제를 처음 풀어보았다. 어떻게 접근해야 될지를 몰라 구글에서 찾은 이 블로그를 우선 참고했다. 해결 방법의 간단한 스케치는 1) 각 노드별 인접한 노드 index 구하기, 2) queue를 이용해 방문여부(is_visit)이 False인 경우 True로 바꿔주며 distance, queue를 업데이트, 3) distance 변수를 sorting하고 max값을 count하여 return 이다. 좀 더 자세히 살펴보면 아래와 같다. 변수 설정 graph : 인접한 노드를 나타내는 변수 distance : 노드 1에서 각 노드index 까지의 거리 is_visit : 방문 여부 (모두 False로 초기화, 노드 1은 True로 바꾸고 시작) queue : 큐 변수 graph 변수 채우기 (연결된 노드 append) queue가 빈 리스트가 될 때까지 i : queue의 맨 첫 번째 index 추출 j가 graph[i]안 원소일 때 (for) if is_visit[j] : False, is_visit[j] = False queue에 j (노드 index) append distance update (+1) distance 정렬 후 첫 번째 값(max) 카운트 값 return 문제 해결123456789101112131415161718192021222324252627def solution(n, edge): graph =[ [] for _ in range(n + 1) ] distances = [ 0 for _ in range(n) ] is_visit = [False for _ in range(n)] queue = [0] is_visit[0] = True # 연결된 node append for (a, b) in edge: graph[a-1].append(b-1) graph[b-1].append(a-1) # queue를 이용한 distance 계산 while queue: i = queue.pop(0) for j in graph[i]: if is_visit[j] == False: is_visit[j] = True queue.append(j) distances[j] = distances[i] + 1 # max distance를 계산한 후 count 결과 return distances.sort(reverse=True) answer = distances.count(distances[0]) return answer 2019.09.27 made by jaejun.lee","categories":[{"name":"Python","slug":"Python","permalink":"https://jx2lee.github.io/categories/Python/"}],"tags":[{"name":"Programmers","slug":"Programmers","permalink":"https://jx2lee.github.io/tags/Programmers/"}]},{"title":"[Python] 섬 연결하기","slug":"programmers-island","date":"2019-09-25T15:00:00.000Z","updated":"2020-03-30T15:06:23.553Z","comments":true,"path":"programmers-island/","link":"","permalink":"https://jx2lee.github.io/programmers-island/","excerpt":"모든 섬을 연결할 때 최소 비용을 구하는 문제를 풀어본다","text":"모든 섬을 연결할 때 최소 비용을 구하는 문제를 풀어본다 문제n개의 섬 사이에 다리를 건설하는 비용(costs)이 주어질 때, 최소의 비용으로 모든 섬이 서로 통행 가능하도록 만들 때 필요한 최소 비용을 return 하도록 solution을 완성하세요. 다리를 여러 번 건너더라도, 도달할 수만 있으면 통행 가능하다고 봅니다. 예를 들어 A 섬과 B 섬 사이에 다리가 있고, B 섬과 C 섬 사이에 다리가 있으면 A 섬과 C 섬은 서로 통행 가능합니다. 제한사항 섬의 개수 n은 1 이상 100 이하입니다. costs의 길이는 ((n-1) * n) / 2이하입니다. 임의의 i에 대해, costs[i][0] 와 costs[i] [1]에는 다리가 연결되는 두 섬의 번호가 들어있고, costs[i] [2]에는 이 두 섬을 연결하는 다리를 건설할 때 드는 비용입니다. 같은 연결은 두 번 주어지지 않습니다. 또한 순서가 바뀌더라도 같은 연결로 봅니다. 즉 0과 1 사이를 연결하는 비용이 주어졌을 때, 1과 0의 비용이 주어지지 않습니다. 모든 섬 사이의 다리 건설 비용이 주어지지 않습니다. 이 경우, 두 섬 사이의 건설이 불가능한 것으로 봅니다. 연결할 수 없는 섬은 주어지지 않습니다. 입출력 예 n costs return 4 [[0,1,1],[0,2,2],[1,2,5],[1,3,1],[2,3,8]] 4 입출력 예 설명 costs를 그림으로 표현하면 다음과 같으며, 이때 초록색 경로로 연결하는 것이 가장 적은 비용으로 모두를 통행할 수 있도록 만드는 방법입니다. 문제 접근어려워 여러 블로그를 참고하였다. conn, answer변수를 반복문을 이용해 풀었다. 아래와 같은 로직으로 구성하였다. costs 리스트 정렬 후 변수 설정 conn : 연결된 섬, answer : 비용 conn의 길이가 n이 되지 않을 때까지 tmp : 최소 비교를 위한 변수, idx : costs의 인덱스 (훑어본 cost를 pop하기 위한 변수) costs를 돌며 (for) costs[i][0] or costs[i][1]이 conn 에 포함되는지 확인 비용 저장을 위해 tmp를 이용해 costs[i][2]값을 저장하고 idx 저장 costs를 한번 다 훑고 answer에 tmp만큼 더한다. conn에 같은 그룹의 섬들을 저장한다 (append), 중복 저장이 있을 수 있으니 set으로 저장한 수 list() idx변수를 이용해 costs pop 풀다보면 난이도가 쑥쑥 올라가는 느낌이다. 더 노력해보자. 문제 해결아래 코드로 해결하였다. 참고 1234567891011121314151617181920def solution(n, costs): costs.sort() conn=[costs[0][0]] answer = 0 while len(conn)!=n: tmp=1000000000000000 idx=0 for i in range(len(costs)): if costs[i][0] in conn or costs[i][1] in conn: if costs[i][0] in conn and costs[i][1] in conn: continue if tmp &gt; costs[i][2]: tmp=costs[i][2] idx=i answer+=tmp conn.append(costs[idx][0]) conn.append(costs[idx][1]) conn=list(set(conn)) costs.pop(idx) return answer 2019.09.26 made by jaejun.lee","categories":[{"name":"Python","slug":"Python","permalink":"https://jx2lee.github.io/categories/Python/"}],"tags":[{"name":"Programmers","slug":"Programmers","permalink":"https://jx2lee.github.io/tags/Programmers/"}]},{"title":"[Python] 네트워크","slug":"programmers-network","date":"2019-09-24T15:00:00.000Z","updated":"2020-03-30T15:06:23.565Z","comments":true,"path":"programmers-network/","link":"","permalink":"https://jx2lee.github.io/programmers-network/","excerpt":"25일부로 백준 대신 프로그래머스 문제를 풀어본다. 매일 한 문제씩, 풀다가 못풀어서 참고해 푼 문제는 모두 블로그로 남기자","text":"25일부로 백준 대신 프로그래머스 문제를 풀어본다. 매일 한 문제씩, 풀다가 못풀어서 참고해 푼 문제는 모두 블로그로 남기자 문제네트워크란 컴퓨터 상호 간에 정보를 교환할 수 있도록 연결된 형태를 의미합니다. 예를 들어, 컴퓨터 A와 컴퓨터 B가 직접적으로 연결되어있고, 컴퓨터 B와 컴퓨터 C가 직접적으로 연결되어 있을 때 컴퓨터 A와 컴퓨터 C도 간접적으로 연결되어 정보를 교환할 수 있습니다. 따라서 컴퓨터 A, B, C는 모두 같은 네트워크 상에 있다고 할 수 있습니다. 컴퓨터의 개수 n, 연결에 대한 정보가 담긴 2차원 배열 computers가 매개변수로 주어질 때, 네트워크의 개수를 return 하도록 solution 함수를 작성하시오. 제한사항 컴퓨터의 개수 n은 1 이상 200 이하인 자연수입니다. 각 컴퓨터는 0부터 n-1인 정수로 표현합니다. i번 컴퓨터와 j번 컴퓨터가 연결되어 있으면 computers[i][j]를 1로 표현합니다. computer[i][j]는 항상 1입니다. 입출력 예 n computers return 3 [[1, 1, 0], [1, 1, 0], [0, 0, 1]] 2 3 [[1, 1, 0], [1, 1, 1], [0, 1, 1]] 1 입출력 예 설명예제 #1아래와 같이 2개의 네트워크가 있습니다. 예제 #2아래와 같이 1개의 네트워크가 있습니다. 문제 접근처음에는 stack 을 이용하지 않고 문제를 풀어보았다. 하지만 아니나 다를까.. 시간초과가 났다. 결국 구글링을 통해 한 블로그를 참고해서 문제를 해결하였다. 로직은 다음과 같다. 입력받은 n을 통해 구성원이 각자인 각각의 네트워크을 tuple로 생성한다. i와 j를 roof를 통해 연결되어 있는지 확인(computers[i][j]) 연결되어 있다면, 두 컴퓨터가 속한 index를 추출한다(idx1, idx2) idx1가 idx2가 다르다면, 하나로 묶어준다(if 뒷부분) hap 변수 생성과 idx1 != idx2 부분이 이해되지 않아 정리하는데 시간이 오래걸렸다. 어쨌든, 양방향으로 연결된 네트워크는 대각선 기준 위쪽만 살펴보면 되기 때문에 아래와 같이 간단히 해결할 수 있었던 것 같다. (그리고 stack의 중요성..) 문제 해결아래 코드로 해결하였다 참고 123456789101112131415161718def solution(n, computers): res = [] for i in range(n): res.append(&#123;i&#125;) for i in range(0, n): for j in range(i+1, n): if computers[i][j] == 1: for idx,st in enumerate(res): if i in res[idx]: idx1 = idx if j in res[idx]: idx2 = idx if idx1 != idx2: hap = res[idx1] | res[idx2] res.pop(min(idx1,idx2)) res.pop(max(idx1,idx2)) res.append(hap) return len(res) 2019.09.25 made by jaejun.lee","categories":[{"name":"Python","slug":"Python","permalink":"https://jx2lee.github.io/categories/Python/"}],"tags":[{"name":"Programmers","slug":"Programmers","permalink":"https://jx2lee.github.io/tags/Programmers/"}]},{"title":"[Cloud] Install Docker","slug":"cloud-install_docker","date":"2019-09-22T15:00:00.000Z","updated":"2020-09-26T13:38:48.156Z","comments":true,"path":"cloud-install_docker/","link":"","permalink":"https://jx2lee.github.io/cloud-install_docker/","excerpt":"도커 개발 환경을 셋업하는 과정을 다룬다. docker client는 윈도우/맥에서 돌리고 docker server를 제어할 수 있지만, docker container는 linux 환경에서 만들고 실행해볼 수 있다. 따라서 docker server를 띄우기 위해서는 가상 머신이나 원격 서버가 필요하다.","text":"도커 개발 환경을 셋업하는 과정을 다룬다. docker client는 윈도우/맥에서 돌리고 docker server를 제어할 수 있지만, docker container는 linux 환경에서 만들고 실행해볼 수 있다. 따라서 docker server를 띄우기 위해서는 가상 머신이나 원격 서버가 필요하다. key word책에서 다루는 용어들에 대해 정리하면 다음과 같다. docker client 대부분의 docker worflow를 관리, 원격 docker server와 통신하는 docker 명령어 docker server docker 명렁어를 daemon 모드로 사용, 이는 리눅스 시스템을 docker server로 만들게 한다. docker client를 통해 container를 배포 / 실행 / 제거 docker image 하나 이상의 파일 시스템 계층 도커화(리눅스 컨테이너로 생성된)된 앱 실행을 위한 모든 파일들의 meta data 포함 하나의 docker image -&gt; 여러 host에 카피 가능 Name, Tag : image의 특정 realease 표시 docker container docker image에 의해 생성되는 linux container 특정 container는 단 하나, 동일 image 내 container 다중 생성 가능 atomic host(원자적 호스트) less, optimized 된 CoreOS나 아토믹 프로젝트 같은 OS의 이미지 container hosting &amp; OS 업그레이드 지원 [참고 - Docker Architecture] 아래와 같은 Docker 환경을 구성한다. docker client : CentOS docker server : docker clinet가 설치된 CentOS Docker Client리눅스 시스템에서 Docker 설치는 Client만 설치하면 Server도 함께 설치된다. Yum package를 이용해 Docker를 설치한다. old version 삭제12345678910111213141516171819[tibero@node5 ~]$ sudo yum remove docker \\&gt; docker-client \\&gt; docker-client-latest \\&gt; docker-common \\&gt; docker-latest \\&gt; docker-latest-logrotate \\&gt; docker-logrotate \\&gt; docker-engine[sudo] password for tibero: Loaded plugins: fastestmirror, langpacksNo Match for argument: dockerNo Match for argument: docker-clientNo Match for argument: docker-client-latestNo Match for argument: docker-commonNo Match for argument: docker-latestNo Match for argument: docker-latest-logrotateNo Match for argument: docker-logrotateNo Match for argument: docker-engineNo Packages marked for removal repository를 이용한 설치12345$ sudo yum install -y yum-utils \\ device-mapper-persistent-data \\ lvm2$ sudo yum-config-manager \\ --add-repo https://download.docker.com/linux/centos/docker-ce.repo Run12$ sudo systemctl enable docker # 서버 재기동 이후 자동으로 서비스 시작을 위함$ sudo systemctl start docker Docker 명령어를 sudo 없이 사용을 원하면 docker 실행권한을 가진 그룹을생성하여 권한을 부여하면 된다 $ sudo groupadd docker $ sudo gpasswd -a $USER docker Test설치가 잘 되어있는지 확인 할 겸 centos 최신 이미지를 이용해 컨테이너를 생성하고 이에 접근해보도록 한다. 1234567891011[kuber@node2 ~]$ docker run --rm -ti centos:latest /bin/bashUnable to find image 'centos:latest' locallyTrying to pull repository docker.io/library/centos ... latest: Pulling from docker.io/library/centosd8d02d457314: Pull complete Digest: sha256:307835c385f656ec2e2fec602cf093224173c51119bbebd602c53c3653a3d6ebStatus: Downloaded newer image for docker.io/centos:latest[root@0f84b088b2e6 /]# pwd/[root@0f84b088b2e6 /]# whoamiroot 2019.09.23 made by jaejun.lee","categories":[{"name":"Cloud","slug":"Cloud","permalink":"https://jx2lee.github.io/categories/Cloud/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://jx2lee.github.io/tags/Docker/"}]},{"title":"[Cloud] Docker Image","slug":"cloud-docker_image","date":"2019-09-22T15:00:00.000Z","updated":"2020-09-26T04:07:27.301Z","comments":true,"path":"cloud-docker_image/","link":"","permalink":"https://jx2lee.github.io/cloud-docker_image/","excerpt":"Docker Image에 대해 알아보자","text":"Docker Image에 대해 알아보자 Docker Image 모든 Docker Container는 Image에 기반하고 Image는 Docker로 배포하고 실행하기 위한 모든 것의 기반을 제공 Image 특징 Container를 실행하려면 docker Image가 필요한데, 이는 공개된 것을 다운로드 하거나 직접 이미지를 생성 모든 docker Image는 하나 이상의 파일 시스템 계층으로 이루어짐 파일 시스템 계층은 이미지 생성을 위해 적용되는 개별 빌드 단계마다 1:1 직접 매핑 이미지 관리를 위해 docker는 스토리지 백엔드(Stroage backend)에 크게 의존 이미지를 구성하는 파일 시스템 계층을 만들고 관리하기 위해 리눅스 파일 시스템과 통신하는 역할 AUFS, BTRFS, 디바이스-매퍼, 오버레이 등 빠른 이미지 관리를 위해 CoW(Copy-on-Write) 시스템 제공 DockerfileDockerfile은 이미지 생성에 필요한 모든 단계를 기술한 파일로, 보통 App의 소스 코드 저장의 root 디렉토리에 포함된다. 구조 설명을 위해 Node.js 기반 애플리케이션을 위한 컨테이너를 만드는 예제의 Dockerfile를 살펴본다. 123456789101112131415161718192021222324252627FROM node:11.11.0LABEL \"maintainer\"=\"anna@example.com\"LABEL \"rating\"=\"Five Stars\" \"class\"=\"First Class\"USER rootENV AP /data/appENV SCPATH /etc/supervisor/conf.dRUN apt-get -y update# The daemonsRUN apt-get -y install supervisorRUN mkdir -p /var/log/supervisor# Supervisor ConfigurationADD ./supervisord/conf.d/* $SCPATH/# Application CodeADD *.js* $AP/WORKDIR $APRUN npm installCMD [\"supervisord\", \"-n\"] 파일 각 라인은 도커에 의해 저장되는 새 이미지 계층을 만든다. 이렇게 설계한 것은 새로운 이미지를 빌드 할 때 변경된 계층만을 새로 빌드하기 위함이다. 각 라인을 차례대로 훑어본다. FROM node:11.11.0 이 부분은 특정 노드 버젼 (node.11.11.0)으로 고정한 우분투 리눅스 이미지를 제공한다는 뜻이다. 이처럼 도커 허브를 통해 노드를 위한 공식 이미지를 받을 수 있고, 일반적인 리눅스 이미지에서도 빌드 가능하다. LABEL “maintainer”=”anna@example.com“ (LABEL “rating”=”Five Stars” “class”=”First Class”) Image나 Container에 라벨 (Label)을 적용하는 기능이다. key-value형태로 메타데이터에 추가할 수 있고, 이미지 빌드 후 docker inspect 명령어로 확인이 가능하다. 1234567891011121314151617[kuber@node2 ~]$ docker inspect test/docker-node-hello[ &#123; \"Id\": \"sha256:fd425ed5d292360c4b20bb193c402e4fb939b73e07a7f7b6f600e31c9d3a63f8\", ... ... \"Image\": \"sha256:40ce3ed86b2ce0c74ba5d6de3ec99fca6982ef43355956502bf7adb62b973d05\", \"Volumes\": null, \"WorkingDir\": \"/data/app\", \"Entrypoint\": null, \"OnBuild\": [], \"Labels\": &#123; \"class\": \"First Class\", \"maintainer\": \"anna@example.com\", \"rating\": \"Five Stars\" &#125; &#125;, USER root 기본적으로docker Container 내 모든 프로세스를 root로 실행하지만, 때에 따라 USER 명령어로 특정 사용자로 지정할 수 있다. (Container가 호스트 커널 위에서 동작하므로 잠재적 보안 위협이 있을 수 있다. root 보단 특정 사용자 계정으로 실행해야 한다고 권고한다.) ** ENV AP(SCPATH) /data/app( /etc/supervisor/conf.d) ** shell 환경 변수 설정 단계 RUN ~ 파일/디렉터리 구조를 만들고 요구되는 소프트웨어를 설치하기 위한 단계로, 위 ENV 단계에서 설정한 환경변수로 간략하게 작성할 수 있다. RUN apt-get -y update RUN apt-get -y install supervisorRUN mkdir -p /var/log/supervisor (yum/apt-get update의 경우 빌드 시간이 오래걸릴 수 있다. 이렇게 Dockerfile내 명시하는 것이 아닌 업데이트가 적용된 기본 리눅스 이미지 위에 빌드할 수 있도록 하는 것이 좋다고 권장한다) ADD ./supervisord/conf.d/* $SCPATH/ (ADD .js $AP/) 로컬 파일 시스템의 파일을 Image로 카피하는 단계이다. 코드나 필요한 보조 파일들을 카피하는데 주로 쓰인다. WORKDIR $AP 작업 디렉토리를 변경하는 단계이다. 이처럼 빌드 시 변경되는 사항의 경우, Dockerfile 작성 시 최대한 뒤쪽으로 작성하기를 권장한다.왜냐면 이미지를 새로 빌드하면 처음으로 바뀐 부분부터 새로 빌드되기 때문이다. CMD [“supervisord”, “-n”] Container 안에서 실행하고자 하는 프로세스를 띄우는 명령어를 작성하는 단계이다. 커뮤니티 내에서 많은 논란이 있었지만 컨테이너 내 하나의 프로세스만 실행하는 것이 가장 좋다고 말한다. 컨테이너는 단일 기능만을 제공한다는 철학(?)에 기초한다. 123[kuber@node2 conf.d]$ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES8affdbc03b3e example/docker-node-hello:lastest \"supervisord -n\" 22 hours ago Up 26 minutes 0.0.0.0:8080-&gt;8080/tcp romantic_haibt Image Build위 Dockerfile을 기반으로 한 이미지를 빌드해보고자 한다. git을 이용해 예제 애플리케이션 저장소를 복제해온다. 12345678[kuber@node2 ~]$ git clone https://github.com/spkane/docker-node-hello.gitCloning into 'docker-node-hello'...remote: Enumerating objects: 47, done.remote: Total 47 (delta 0), reused 0 (delta 0), pack-reused 47Unpacking objects: 100% (47/47), done.[kuber@node2 ~]$ cd docker-node-hello/[kuber@node2 docker-node-hello]$ git 디렉토리를 제외한 파일들을 살펴보면 다음과 같다. 1234567891011121314[kuber@node2 docker-node-hello]$ tree -a -I .git.├── Dockerfile├── .dockerignore├── .gitignore├── index.js├── Makefile├── package.json├── README.md├── supervisord│ └── conf.d│ ├── node.conf│ └── supervisord.conf└── Vagrantfile .dockerignore: docker Image 빌드 시 도커 호스트에 업로드하고 싶지 않은 파일이나 디렉토리를 지정하는 파일 12[kuber@node2 docker-node-hello]$ cat .dockerignore .git package.json : Node.js 애플리케이션 정의 및 의존성 나열 파일 index.js : 애플리케이션 메인 소스 코드 supervisord : 애플리케이션 시작 및 모니터링이 가능한 supervisord을 위한 설정 파일이 포함된 폴더 그럼 이제 Image를 빌드해보도록 한다. 명령어에 쓰이는 자세한 옵션들은 공식문서를 참고하면 된다. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117[kuber@node2 docker-node-hello]$ docker build -t example/docker-node-hello:lastest .Sending build context to Docker daemon 15.87 kBStep 1/14 : FROM node:11.11.0Trying to pull repository docker.io/library/node ... 11.11.0: Pulling from docker.io/library/node22dbe790f715: Pull complete 0250231711a0: Pull complete 6fba9447437b: Pull complete c2b4d327b352: Pull complete 270e1baa5299: Pull complete 08ba2f9dd763: Pull complete edf54285ab13: Pull complete 4d751c169397: Pull complete Digest: sha256:065610e9b9567dfecf10f45677f4d372a864a74a67a7b2089f5f513606e28edeStatus: Downloaded newer image for docker.io/node:11.11.0 ---&gt; 9ff38e3a6d9dStep 2/14 : LABEL \"maintainer\" \"anna@example.com\" ---&gt; Running in d0c3d1590e1f ---&gt; b4855e6ce77cRemoving intermediate container d0c3d1590e1fStep 3/14 : LABEL \"rating\" \"Five Stars\" \"class\" \"First Class\" ---&gt; Running in 04a73c43a44a ---&gt; 2ebbef69d1b3Removing intermediate container 04a73c43a44aStep 4/14 : USER root ---&gt; Running in 5de276a155e2 ---&gt; 8ad740a32fddRemoving intermediate container 5de276a155e2Step 5/14 : ENV AP /data/app ---&gt; Running in aa77a9763782 ---&gt; 45925bd0fb66Removing intermediate container aa77a9763782Step 6/14 : ENV SCPATH /etc/supervisor/conf.d ---&gt; Running in 931030d51457 ---&gt; a2975676fd6dRemoving intermediate container 931030d51457Step 7/14 : RUN apt-get -y update ---&gt; Running in ecc80b77c428Ign:1 http://deb.debian.org/debian stretch InReleaseGet:2 http://deb.debian.org/debian stretch-updates InRelease [91.0 kB]Get:3 http://deb.debian.org/debian stretch Release [118 kB]Get:4 http://security.debian.org/debian-security stretch/updates InRelease [94.3 kB]Get:5 http://deb.debian.org/debian stretch-updates/main amd64 Packages [27.4 kB]Get:6 http://security.debian.org/debian-security stretch/updates/main amd64 Packages [506 kB]Get:7 http://deb.debian.org/debian stretch Release.gpg [2365 B]Get:8 http://deb.debian.org/debian stretch/main amd64 Packages [7086 kB]Fetched 7925 kB in 3s (2531 kB/s)Reading package lists... ---&gt; 37d8e3e4a4b4Removing intermediate container ecc80b77c428Step 8/14 : RUN apt-get -y install supervisor ---&gt; Running in 74da13fa61deReading package lists...Building dependency tree...Reading state information...The following additional packages will be installed: python-meld3 python-pkg-resourcesSuggested packages: python-setuptools supervisor-docThe following NEW packages will be installed: python-meld3 python-pkg-resources supervisor0 upgraded, 3 newly installed, 0 to remove and 57 not upgraded.Need to get 483 kB of archives.After this operation, 2157 kB of additional disk space will be used.Get:1 http://deb.debian.org/debian stretch/main amd64 python-pkg-resources all 33.1.1-1 [166 kB]Get:2 http://deb.debian.org/debian stretch/main amd64 python-meld3 all 1.0.2-2 [37.3 kB]Get:3 http://deb.debian.org/debian stretch/main amd64 supervisor all 3.3.1-1+deb9u1 [280 kB]debconf: delaying package configuration, since apt-utils is not installedFetched 483 kB in 0s (488 kB/s)Selecting previously unselected package python-pkg-resources.(Reading database ... 29978 files and directories currently installed.)Preparing to unpack .../python-pkg-resources_33.1.1-1_all.deb ...Unpacking python-pkg-resources (33.1.1-1) ...Selecting previously unselected package python-meld3.Preparing to unpack .../python-meld3_1.0.2-2_all.deb ...Unpacking python-meld3 (1.0.2-2) ...Selecting previously unselected package supervisor.Preparing to unpack .../supervisor_3.3.1-1+deb9u1_all.deb ...Unpacking supervisor (3.3.1-1+deb9u1) ...Setting up python-meld3 (1.0.2-2) ...Setting up python-pkg-resources (33.1.1-1) ...Setting up supervisor (3.3.1-1+deb9u1) ...invoke-rc.d: could not determine current runlevelinvoke-rc.d: policy-rc.d denied execution of start. ---&gt; d66e13271f30Removing intermediate container 74da13fa61deStep 9/14 : RUN mkdir -p /var/log/supervisor ---&gt; Running in f85e1c0267de ---&gt; 9979f514ad99Removing intermediate container f85e1c0267deStep 10/14 : ADD ./supervisord/conf.d/* $SCPATH/ ---&gt; 6db6cac86654Removing intermediate container d565d65027c7Step 11/14 : ADD *.js* $AP/ ---&gt; 3a8554a9e86dRemoving intermediate container f60af33566beStep 12/14 : WORKDIR $AP ---&gt; 7437cca985ffRemoving intermediate container 6b1e8ae1aec2Step 13/14 : RUN npm install ---&gt; Running in e1038f1c1b6anpm WARN deprecated connect@2.7.9: connect 2.x series is deprecatednpm notice created a lockfile as package-lock.json. You should commit this file.added 18 packages from 15 contributors and audited 34 packages in 3.641sfound 16 vulnerabilities (5 low, 5 moderate, 6 high) run `npm audit fix` to fix them, or `npm audit` for details ---&gt; 40ce3ed86b2cRemoving intermediate container e1038f1c1b6aStep 14/14 : CMD supervisord -n ---&gt; Running in dd4b1488e1a7 ---&gt; fd425ed5d292Removing intermediate container dd4b1488e1a7Successfully built fd425ed5d292 Run ImageImage 빌드가 성공하면 아래와 같이 Image를 실행해본다. 12345[kuber@node2 docker-node-hello]$ docker run -d -p 8080:8080 example/docker-node-hello:lastest 8affdbc03b3ec66745ef5cb9e90f5d1f71b46c93932b706317b744fbb7212371in Web browser,Hello World. Wish you were here. 위 명령은 아래와 같다. example/docker-node-hello:lastest 태그를 가진 이미지로부터 백그라운드에 실행 컨테이너로 만들고 (-d) 8080 port 를 docker 호스트의 8080 port에 매핑 (-p 8080:8080) 그럼 실제 웹이 잘 띄워졌는지 확인하기 위해 docker server의 ip를 확인하고 접속해본다. (로컬=서버이므로 해당 아이피:8080 으로 접속하면 보인다) 1Hello World. Wish you were here. 환경 변수index.js를 살펴보자. $WHO 는 Hello의 대상이 되는 앱이 사용하는 변수임을 확인할 수 있다. 1234567891011// Constantsvar DEFAULT_PORT = 8080;var DEFAULT_WHO = \"World\";var PORT = process.env.PORT || DEFAULT_PORT;var WHO = process.env.WHO || DEFAULT_WHO;// Appvar app = express();app.get('/', function (req, res) &#123; res.send('Hello ' + WHO + '. Wish you were here.\\n');&#125;); 그럼 Container를 띄울 때 환경변수를 넘겨 Hello 대상을 변경해보도록 한다. 우선 띄워져 있는 Container를 확인한 후 중지한다. 중지하는 방법은 CONTAINER ID와 NAMES를 이용하는 두 가지 방법이 있다. 1234567891011[kuber@node2 docker-node-hello]$ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES8affdbc03b3e example/docker-node-hello:lastest \"supervisord -n\" About a minute ago Up About a minute 0.0.0.0:8080-&gt;8080/tcp romantic_haibt[kuber@node2 docker-node-hello]$ docker stop 8affdbc03b3e(or docker stop romantic_haibt)8affdbc03b3e[kuber@node2 docker-node-hello]$ docker run -d -p 8080:8080 example/docker-node-hello:lastest a412191bdcdb7b55e687b44dedc0707164fff784c0e1c64f1ce11a22afe64b2a[kuber@node2 docker-node-hello]$ docker stop priceless_lovelacepriceless_lovelace[kuber@node2 docker-node-hello]$ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES run 명령어에 -e 를 추가하여 재시작한다. (WHO : Jaejun Lee) 12[kuber@node2 docker-node-hello]$ docker run -d -p 8080:8080 -e WHO=\"Jaejun Lee\" example/docker-node-hello:lastest 389f5be3ee7c2020ecacbd032a5ffa048339d0bdee66666e6dedc8e74e992e78 web으로 접속하여 확인해본다. Hello World. Wish you were here. Image 저장Image도 만들었겠다, 배포를 원하는 docker 호스트에서 쉽게 접근할 수 있는 곳에다가 저장해야 한다. 이곳을 Image 빌드와 Image 실행 사이의 명백한 핸드오프 포인트(hand-off point)라고 한다. 보통 Image를 Server에서 직접 빌드하고 실행하지 않는다. 대개 Image 저장소에서 Image를 끌어와 하나 이상의 Docker Server에 실행할 수 있게끔 배포한다. 손쉽게 이미지를 끌어오기 위해 Image를 저장하는 몇 가지 방법에 대해 살펴보자 공개 registry 공개 Image들을 저장하기 위한 Image registry를 제공한다(https://hub.docker.com/). Linux 배포판, WordPress등 다양한 Image가 존재한다. 비공개 registry Image를 인터넷을 통해 공개하지 않고 내부적으로 Docker Image를 관리하는 방법도 존재한다. registry 인증 Container Image를 저장하는 registry와의 통신은 필수적이다. 저장하는 과정에서 권한을 요구하는데, Docker는 자동화를 위해 Image 다운 요청을 받는 경우 사용자를 대신해 로그인 정보를 저장하고 이를 사용한다. default registry는 앞에서 언급한 공개 Image가 저장되어 있는 registry 이다. Docker Hub 계정 생성 후 로그인 https://hub.docker.com/에 접속하여 가입한 후 로그인을 해보도록 한다. 12345[kuber@node2 ~]$ docker loginLogin with your Docker ID to push and pull images from Docker Hub. If you don't have a Docker ID, head over to https://hub.docker.com to create one.Username: jaejunlee Password: Login Succeeded 로그인이 성공되면, ~/.docker/confing.json라는 파일이 생성되는데, 나의 로그인 정보를 캐시하기 위함이다. 1234567[kuber@node2 ~]$ cat ~/.docker/config.json &#123; \"auths\": &#123; \"https://index.docker.io/v1/\": &#123; \"auth\": \"amFlanVubGVlOndvd25zbGQ5NDg5\" &#125; &#125; 이러한 정보를 담고 있는 파일은 registry에 접근하고자 할 때 Docker는 이를 참고해 연결을 시도한다. 사용을 다 한 이후에 만약 로그아웃을 하게된다면, 파일이 비어있음을 확인할 수 있다. 123456[kuber@node2 ~]$ docker logoutRemoving login credentials for https://index.docker.io/v1/[kuber@node2 ~]$ cat ~/.docker/config.json &#123; \"auths\": &#123;&#125;&#125; registry에 Image 저장앞서 Image build 시 우리는 docker build -t example/docker-node-hello:lastest .명령어를 사용했다. 이는 공개 registry 의 example/docker-node-hello 이미지를 빌드한 것인데, 만약 로컬에서 생성한 이미지의 경우 아무 이름이나(보통 사용자/그룹 이용) 사용 가능하다. tag명령어를 통해 이미지의 태그를 변경할 수 있다. 이미지 태그명은 docker 사용자 아이디로 설정하여야 추후에 push/pull이 가능하다 12345678[kuber@node2 ~]$ docker tag test/docker-node-hello:latest jaejunlee/docker-node-hello:latest[kuber@node2 ~]$ docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEexample/docker-node-hello lastest fd425ed5d292 2 days ago 928 MBjaejunlee/docker-node-hello latest fd425ed5d292 2 days ago 928 MBtest/docker-node-hello latest fd425ed5d292 2 days ago 928 MBdocker.io/centos latest 67fa590cfc1c 5 weeks ago 202 MBdocker.io/node 11.11.0 9ff38e3a6d9d 6 months ago 904 MB 이번엔 그럼 tag를 바꾼 이미지를 push (공개 registry) 해보도록 한다. 12345678910111213141516[kuber@node2 ~]$ docker push jaejunlee/docker-node-hello-test:latestThe push refers to a repository [docker.io/jaejunlee/docker-node-hello-test]a233ae287464: Pushed 8f9ee22c1347: Pushed ce283841f218: Pushed f8fc35d38ecc: Pushed cb7a837507c0: Pushed 4ebe27287e94: Pushed abdde7643382: Pushed 909542b1bce2: Pushed 7de462056991: Pushed 3443d6cf0f1f: Pushed f3a38968d075: Pushed a327787b3c73: Pushed 5bb0785f2eee: Pushed latest: digest: sha256:98a38e1a53a9473ab1b083099d29d70d9a05d5f924533b378e70555b1f1714a3 size: 3055 Docker Hub에 접속하여 제대로 push 되었는지 확인해보도록 한다. (https://cloud.docker.com/u/jaejunlee/repository/docker/jaejunlee/docker-node-hello-test) 2019.09.23 made by jaejun.lee","categories":[{"name":"Cloud","slug":"Cloud","permalink":"https://jx2lee.github.io/categories/Cloud/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://jx2lee.github.io/tags/Docker/"}]},{"title":"[Python] 동적 인수 지정 시 None과 docstring 활용","slug":"python-better_way_20","date":"2019-09-18T15:00:00.000Z","updated":"2020-03-30T15:06:23.586Z","comments":true,"path":"python-better_way_20/","link":"","permalink":"https://jx2lee.github.io/python-better_way_20/","excerpt":"아래 동적 인수를 활용하는 함수 (이벤트 발생 시각을 포함하는 log 함수) 를 생각해보자.","text":"아래 동적 인수를 활용하는 함수 (이벤트 발생 시각을 포함하는 log 함수) 를 생각해보자. 12345678910111213from datetime import datetimeimport timedef get_log(mes, date=datetime.now()): print('%s: %s'%(date, mes))get_log('Hi there!')time.sleep(0.1)get_log('Hi again!')&gt;&gt;&gt;2019-09-19 08:28:26.737742: Hi there!2019-09-19 08:28:26.737742: Hi again! 원하는 함수 결과는 0.1초 이후 타임스탬프가 찍혀야 하는데 같지 않은가!! 이는 함수를 정의할 때 date변수가 default로 그 시간을 가져가기 때문에 원하는 결과값을 얻지 못한다. 즉, 재평가하지 않는다. 이를 해결하기 위해서는 어떻게 해야하는가? default 를 None으로 설정하고 docstring(문서화) 하자! 함수 정의 시 기본 값을 None으로 설정하고 이에 작용 원리와 변수를 나타내도록 문서화해보면 아래와 같다. 123456789101112131415161718def get_log(mes, date=None): \"\"\"Log a message with a timestamp. Args: mes : message to print date : datetime of whe the message occured. Defaults to the present time. \"\"\" date = datetime.now() if date is None else date print('%s: %s'%(date, mes))get_log('Hi there!')time.sleep(0.1)get_log('Hi again!')&gt;&gt;&gt;2019-09-19 08:35:17.079609: Hi there!2019-09-19 08:35:17.180161: Hi again! 타임스탬프가 바뀐것을 확인하였다! 이처럼 None을 사용하는 방법은 인수가 수정 가능(Mutable)할 때 중요하다고 한다. 예를 들어 아래 함수를 살펴보자 123456789101112131415161718import jsondef decode(data, default=&#123;&#125;): try: return json.loads(data) except ValueError: return default test1 = decode('hi')test1['t']=10test2 = decode('hii')test2['t1']=100print('test1: ', test1)print('test2: ', test2)&gt;&gt;&gt;test1: &#123;'t': 10, 't1': 100&#125;test2: &#123;'t': 10, 't1': 100&#125; 이 함수도 마찬가지로 get_log 함수와 같은 문제가 존재한다. 원했던 결과는 각각 test1/test2 에는 try부분이 실행되지 않기 때문에 print 되는 default 가 하나씩 찍혔어야 한다. 이처럼 문제가 발생하는 원인은 함수 인자로 설정한 dictionary가 decode 호출에서 공유하기 때문이다.이를 해결할 수 있는 것은 default 인수를 None으로 받고 문서화하면 된다. 12345678910111213141516171819202122232425def updated_decode(data, default=None): \"\"\"Load JSON data from a string. Args: data : JSON data to decode default : Value to return if decoding fails. Defaults to an empty dictionary. \"\"\" if default is None: default = &#123;&#125; try: return json.loads(data) except ValueError: return defaulttest1 = updated_decode('hi')test1['t']=10test2 = updated_decode('hii')test2['t1']=100print('test1: ', test1)print('test2: ', test2)&gt;&gt;&gt;test1: &#123;'t': 10&#125;test2: &#123;'t1': 100&#125; Summary 함수의 기본 인수는 모듈 로드 시점에 함수 정의 과정에서 딱 한 번만 평가된다. 함수의 기본 인수가 동적인 경우에는 기본값으로 None을 사용하자. 이후 docstring에 실제 기본 동작을 문서화하자 참조 : (http://www.yes24.com/Product/goods/25138160) 2019.09.19 made by jaejun.lee","categories":[{"name":"Python","slug":"Python","permalink":"https://jx2lee.github.io/categories/Python/"}],"tags":[{"name":"Python Better Way","slug":"Python-Better-Way","permalink":"https://jx2lee.github.io/tags/Python-Better-Way/"}]},{"title":"[Python] 가변 위치 인수 활용","slug":"python-better_way_18","date":"2019-09-17T15:00:00.000Z","updated":"2020-03-30T15:06:23.573Z","comments":true,"path":"python-better_way_18/","link":"","permalink":"https://jx2lee.github.io/python-better_way_18/","excerpt":"선택적 위치 인수, \\args* 불리는 star args는 함수의 호출을 더 명확하고 가독성을 높인다. 다음 아래 함수를 보자","text":"선택적 위치 인수, \\args* 불리는 star args는 함수의 호출을 더 명확하고 가독성을 높인다. 다음 아래 함수를 보자 123456789101112def get_log(mes, val): if not val: print(mes) else: val_str = ', '.join(str(x) for x in val) print('%s %s'%(mes, val_str))get_log('My numbers are', [1.2])get_log('Hi there', [])&gt;&gt;&gt;My numbers are 1.2Hi there 허나, 굳이 로그를 남길 값이 없을 때 빈 리스트를 넣어주는 것은 참으로 불편한 짓이다. 이러한 불편함을 해소하기 위해 * 기호를 마지막 파라미터에 붙이면 이 변수는 선택적이다. 다음 함수를 보자. 123456789def get_log(mes, *val): if not val: print(mes) else: val_str = ', '.join(str(x) for x in val) print('%s %s'%(mes, val_str))&gt;&gt;&gt;My numbers are [1.2]Hi there 굳이 빈 리스트를 넣어주지 않아도 함수가 알아서 작동한다. 하지만 가변 개수의 위치 변수는 다음 두 가지 문제를 가지고 있다고 한다. return 값이 항상 튜플로 반환 generator로 생성된 모든 값을 담으므로 메모리를 많이 차지하는 문제점이 있다. 아래 코드를 보자. 1234567891011def _generator(): for i in range(10): yield idef get_func(*args): print(args) it = _generator()get_func(*it)&gt;&gt;&gt;(0, 1, 2, 3, 4, 5, 6, 7, 8, 9) 위와 같이 입력 수가 적다면은 가장 좋은 방법이기도 하다. 하지만 입력 수가 많다면 위와 같이 *args 방법은 비효율적일 것이다. 나중에 함수를 고칠 때 새 위치 인수 추가 불가능 코드는 바뀔 수 있다. 하지만 star agrs를 사용한다면 코드 수정 시 아래와 같은 문제가 발생할 수 있다. 123456789101112def get_log(seq, mes, *val): if not val: print('%s: %s'%(seq, mes)) else: val_str = ', '.join(str(x) for x in val) print('%s: %s: %s'%(seq, mes, val_str))get_log(1, 'My numbers are', 1, 2)get_log('Hi there', 1, 2)&gt;&gt;&gt;1: My numbers are: 1, 2Hi there: 1: 2 첫 번째 호출과 두 번째 return 값이 다른 이유는, mes를 서로 다르게(‘My numbers are’, 1) 받았기 때문이다. 이러한 문제가 생길 가능성을 없애기 위해서는 *agrs 를 받는 함수를 확장할 때 키워드 전용(keyword-only) 인수를 사용해야한다. 이는 Better Way 21에서 다루도록 한다. Summary *args 를 이용해 함수에서 가변 개수의 인수를 사용할 수 있다 *과 generator를 함께 사용한다면 메모리가 부족할 수 있다 *args 를 이용해 만든 함수를 확장할 때 버그가 생길 수 있다 참조 : (http://www.yes24.com/Product/goods/25138160) 2019.09.18 made by jaejun.lee","categories":[{"name":"Python","slug":"Python","permalink":"https://jx2lee.github.io/categories/Python/"}],"tags":[{"name":"Python Better Way","slug":"Python-Better-Way","permalink":"https://jx2lee.github.io/tags/Python-Better-Way/"}]},{"title":"[Python] 키워드 인수 활용","slug":"python-better_way_19","date":"2019-09-17T15:00:00.000Z","updated":"2020-03-30T15:06:23.588Z","comments":true,"path":"python-better_way_19/","link":"","permalink":"https://jx2lee.github.io/python-better_way_19/","excerpt":"다른 프로그래밍 언어와 마찬가지로 함수 호출 시 인수를 위치로 전달이 가능하다. 아래 함수를 보자","text":"다른 프로그래밍 언어와 마찬가지로 함수 호출 시 인수를 위치로 전달이 가능하다. 아래 함수를 보자 12345678910111213141516def remainder(num, div): return num % divprint(remainder(20, 7))print(remainder(20, div=7))print(remainder(num=20, div=7))print(remainder(div=7, num=20))&gt;&gt;&gt;6666print(remainder(num=20, 7))&gt;&gt;&gt;SyntaxError: positional argument follows keyword argument 위 네 개 print는 제대로 작동하지만, 아래의 경우 위치인수를 키워드 인수 뒤에 배치할 경우 에러가 발생한다. 키워드 인수는 다음 세 가지 중요한 이점이 있다고 한다. 함수 호출을 명확하게 이해할 수 있다 키워드 인수를 사용하면 각각의 목적으로 어떤 parameter를 사용했는지 곧바로 명확하게 알 수 있다. default 값 설정이 가능 입력 인수를 기본 default로 설정할 수 있다. 아래 코드를 보자 1234567891011def get_rate(wgt, time, period=1): return (wgt / time) * periodwgt = 0.5time = 3flow = get_rate(wgt, time)print('%.3f kg per second' % flow)flow = get_rate(wgt, time, period=3600)print('%.3f kg per second' % flow)&gt;&gt;&gt;0.167 kg per second period 인수를 default 1로 설정하였다. 첫 번재 호출에서는 period를 사용하지 않아 자동으로 1로 입력받고, 두 번째 호출에서는 3600을 전달받아 호출하였다. 이처럼 코드가 깔끔해진다. 기존 호출 코드와 호환성을 유지-함수의 파라미터 확장이 가능 이는 딱히 예제가 필요없을 것 같아 말로 풀겠다. #2 와 비슷한 항목으로 기본 default 인수를 설정하면 파라미터 확장이 용이함을 확인할 수 있다. 이 책에서 주의할 점은 선택적 인수를 위치로 넘기면 어떤 인수인지 어려울 수 있다고 한다. 이때 가장 좋은 방법으로는 항상 키워드 이름으로 선택적 인수를 지정하고 위치 인수로는 아예 넘기지 않게 하는 것을 권장한다. Summary 함수의 인자를 위치나 키워드로 지정이 가능하다 키워드 인수를 사용하여 위치 인수만으로 이해하기 어려운 문제를 해결할 수 있다 키워드 인수에 default 값을 지정하면 확장이 가능하다 참조 : (http://www.yes24.com/Product/goods/25138160) 2019.09.18 made by jaejun.lee","categories":[{"name":"Python","slug":"Python","permalink":"https://jx2lee.github.io/categories/Python/"}],"tags":[{"name":"Python Better Way","slug":"Python-Better-Way","permalink":"https://jx2lee.github.io/tags/Python-Better-Way/"}]},{"title":"[Python] 리스트를 반환하는 대신 제네레이터","slug":"python-better_way_16","date":"2019-09-12T15:00:00.000Z","updated":"2020-03-30T15:06:23.569Z","comments":true,"path":"python-better_way_16/","link":"","permalink":"https://jx2lee.github.io/python-better_way_16/","excerpt":"결과 생성 함수에서 택할 수 있는 가장 간단한 방법은 리스트를 반환하는 것이다. 예로, 문자열에 포함된 모든 단어의 인덱스를 출력하고자 한다. append 함수를 이용해 리스트를 반환하는 코드를 작성할 수 있다.","text":"결과 생성 함수에서 택할 수 있는 가장 간단한 방법은 리스트를 반환하는 것이다. 예로, 문자열에 포함된 모든 단어의 인덱스를 출력하고자 한다. append 함수를 이용해 리스트를 반환하는 코드를 작성할 수 있다. 12345678910111213141516def get_word_index(text): result = [] if text: result.append(0) for idx, wd in enumerate(text): if wd == ' ': result.append(idx + 1) return resulttest = 'Four score and seven years ago...'result = get_word_index(test)print(result)&gt;&gt;&gt;[0, 5, 11, 15, 21, 27] 하지만 이 함수는 두 가지 문제점이 있다고 한다 코드가 복잡하고 깔끔하지 않다 새로운 결과를 생성할 때마다 append 메소드를 호출 결과 리스트를 생성하는데 한 줄, 반환하는 데도 한 줄..작성한 코드는 전체 130개 있지만 그 중에서 중요한 문자는 75개 정도이다. 이러한 함수를 효율적으로 작성하는 방법은 제네레이터(generator)를 사용하는 것이다. 실제로 실행하지 않고 바로 이터레이터(iterator)를 반환한다. 이터레이터(iterator는 제네레이터가 다음 yield 표현식으로 진행한다. 제네레이터에서 yield에 전달한 값을 이터레이터가 호출하는 쪽에서 반환한다.get_word_index함수를 제네레이터 함수로 수정하면 다음과 같다. 123456789101112def get_word_index_2(text): if text: yield 0 for idx, wd in enumerate(text): if wd == ' ': yield idx + 1test = 'Four score and seven years ago...'result = list(get_word_index(test))print(result)&gt;&gt;&gt;[0, 5, 11, 15, 21, 27] get_word_index 함수와는 다르게, index_2 함수는 return 되는 iterator를 list로 넘겨주면 된다. 반환 전 모든 결과를 리스트에 저장한다 입력이 매우 많다면 메모리 고갈 -&gt; 다운되는 원인이 된다.다음은 file에서 한 줄씩 읽어와 단어의 index를 반환하는 함수를 generator를 사용해 작성해보자 12345678910111213141516def index_file(handle): res = 0 for line in handle: yield res for wd in lune: offset += 1 if wd == ' ': yield resfrom itertools import islicewith open('./address.txt', 'r') as f: it = index_file(f) results = islice(it, 0) print(list(result))&gt;&gt;&gt;[0, 5, 11, 15, 21, 27] islice는 itertools 라이브러리에 있는 함수로, 반복 가능한 객체(iterator)를 slice하는 함수이다.위 함수 작성 시 유의사항은 반환되는 iterator에서 재사용할 수 없다는 사실을 호출하는 쪽에서 반드시 알아야 한다. 이는 Better Way 17에서 다룰 예정이다. Summary 제네레이터를 사용하는 것이 누적된 결과의 리스트를 return 하는 것보다 보기 이쁘다 generator 함수 return = yield로 전달된 값들의 집.합 generator는 모든 입출력을 메모리에 저장하지 않는다 참조 : (http://www.yes24.com/Product/goods/25138160) 2019.09.13 made by jaejun.lee","categories":[{"name":"Python","slug":"Python","permalink":"https://jx2lee.github.io/categories/Python/"}],"tags":[{"name":"Python Better Way","slug":"Python-Better-Way","permalink":"https://jx2lee.github.io/tags/Python-Better-Way/"}]},{"title":"[Python] 인수를 순회할 때는 방어적으로","slug":"python-better_way_17","date":"2019-09-12T15:00:00.000Z","updated":"2020-03-30T15:06:23.556Z","comments":true,"path":"python-better_way_17/","link":"","permalink":"https://jx2lee.github.io/python-better_way_17/","excerpt":"입력값으로 리스트를 받는 함수를 생각해보자. 이때 리스트를 여러 번 순회해야 할 때가 종종 있다.예를 들어 각 도시의 방문자 수가 list로 구성되고, 각 도시에서 전체 여행자 중 몇 퍼센트를 차지하는지 return 하는 함수를 작성해보면 다음과 같다.","text":"입력값으로 리스트를 받는 함수를 생각해보자. 이때 리스트를 여러 번 순회해야 할 때가 종종 있다.예를 들어 각 도시의 방문자 수가 list로 구성되고, 각 도시에서 전체 여행자 중 몇 퍼센트를 차지하는지 return 하는 함수를 작성해보면 다음과 같다. 12345678910111213def normalize_pop(n): tot = sum(n) result = [] for val in n: per = 100 * val / tot result.append(per) return resultv = [15, 35, 80]portions = normalize_pop(v)print(portions)&gt;&gt;&gt;[11.538461538461538, 26.923076923076923, 61.53846153846154] 위와 같이 리스트를 input으로 받지 않고 file로 받는 함수를 생각해보면 generator를 이용해 아래와 같이 작성할 수 있다. 1234567891011def read_file(path): with open(path) as f: for line in f: yield int(line)it = read_file('./numbers.txt')portions = normalize_pop(it)print(portions)# 과연 결과는?&gt;&gt;&gt;[] # ??? 껍데기만 나오는 이유는, iterator는 결과를 한 번 생성, 즉 한 바퀴를 다 돌고나면 재생성하지 않는 성질을 갖는다. 123456it = read_file('./numbers.txt')print(list(it), 'first')print(list(it), 'second') # second만 print, 이미 소진!&gt;&gt;&gt;[15, 35, 80] first[] second StopIteration이라는 에러를 뱉어낼 줄 알았지만 그러한 결과는 알려주지 않는다. 따라서 위와 같은 해결을 위한 방안은 다음과 같이 크게 ?가지로 해결할 수 있다. case1) iterator를 list로 저장 1234567891011121314def normalize_pop_2(n): n = list(n) # iterator to list! tot = sum(n) result = [] for val in n: per = 100 * val / tot result.append(per) return resultit = read_file('./numbers.txt')portions = normalize_pop_2(it)print(portions)&gt;&gt;&gt;[11.538461538461538, 26.923076923076923, 61.53846153846154] 하지만 이 방법의 경우 만약 list로 저장되는 iterator가 크다면 문제가 발생 큰 iterator -&gt; 큰 list 저장 -&gt; 메모리 고갈 -&gt; 다운! solution : 호출 때마다 새 iterator 반환..? case2) 호출 때마다 iterator 반환 123456789101112def normalize_pop_3(iter_): tot = sum(iter_()) result = [] for val in iter_(): per = 100 * val / tot result.append(per) return resultportions = normalize_pop_3(lambda: read_file('./numbers.txt'))print(portions)&gt;&gt;&gt;[11.538461538461538, 26.923076923076923, 61.53846153846154] 매번 iterator를 생성해야함 입력값을 lambda를 이용해야함위 방법은 세련되지 못한다고 책에서 말한다. 이에 마지막으로 iterator protocol을 구현한 새 컨테이너 클래스로 작성하는 것을 추천한다. Iterator protocol ? 1234567891011121314class readVisits(object): def __init__(self, path): self.path = path def __iter__(self): with open(self.path) as f: for line in f: yield int(line)v = readVisits('./numbers.txt')portions = normalize_pop(v)print(portions)&gt;&gt;&gt;[11.538461538461538, 26.923076923076923, 61.53846153846154] normalize_pop 내 sum을 위해 __iter__ 를 호출 normalize_pop 내 정규화하는 과정에서 또한 __iter__ 을 호출이 방법의 유일한 단점은 입력 데이터를 여러 번 읽어낸다는 것이다. 이 해결을 위해서는 입력값이 iterator일 경우 예외를 일으킨다. 123456789101112131415161718192021222324252627class readVisits(object): def __init__(self, path): self.path = path def __iter__(self): with open(self.path) as f: for line in f: yield int(line)def normalize_pop_final(n): if iter(n) is iter(n): raise TypeError('Must input be container') tot = sum(n) result = [] for val in n: per = 100 * val / tot result.append(per) return resultv = [15, 35, 80]normalize_pop_final(v) # no errorv = readVisits('./numbers.txt')normalize_pop_final(v) # no errorit = iter(v)normalize_pop_final(v)&gt;&gt;&gt;TypeError: Must input be container normalize_pop_final은 전체를 복사하고 싶지 않지만 입력값을 여러 번 순회해야 할 때 사용하면 좋다. Summary 입력값을 여러 번 순회하는 함수는 주의하자 iterator protocol : 컨테이너와 iterator가 내장함수 iter/next와 for 루프 및 관련 포현식과 상호 작용하는 방법을 정의한다 __iter__ 메서드를 generator로 구현하면 자신만의 iterable container type을 쉽게 정의할 수 있다 참조 : (http://www.yes24.com/Product/goods/25138160) 2019.09.13 made by jaejun.lee","categories":[{"name":"Python","slug":"Python","permalink":"https://jx2lee.github.io/categories/Python/"}],"tags":[{"name":"Python Better Way","slug":"Python-Better-Way","permalink":"https://jx2lee.github.io/tags/Python-Better-Way/"}]},{"title":"[Cloud] Kubernetes 개념 및 아키텍쳐","slug":"cloud-introduction_to_kubernetes","date":"2019-09-05T15:00:00.000Z","updated":"2020-03-30T15:06:23.590Z","comments":true,"path":"cloud-introduction_to_kubernetes/","link":"","permalink":"https://jx2lee.github.io/cloud-introduction_to_kubernetes/","excerpt":"공유 및 세미나를 위해 Kubernetes를 정리 최근 제품 관련하여 쿠버네티스 지식이 필요하여 세미나를 재시작 하여 팀원분이 발표한 자료를 바탕으로 내용을 추가/수정 하였다. (2020/01/10)","text":"공유 및 세미나를 위해 Kubernetes를 정리 최근 제품 관련하여 쿠버네티스 지식이 필요하여 세미나를 재시작 하여 팀원분이 발표한 자료를 바탕으로 내용을 추가/수정 하였다. (2020/01/10) Container애플리케이션과 그 실행에 필요한 Library, Binary, 구성 파일 등을 패키지로 묶어 배포하는 것 컨테이너로 불리는 이유는 프로세스들을 컨테이너화 하여 같은 리눅스 호스트를 쓰지만 격리되어 운영하기 때문 프로세스를 격리하는 방법으로는 리눅스에서 컨트롤 그룹 Cgroups 과 리눅스 네임스페이스를 이용한 LXC (LInuX Containers) 운영체제 레벨 가상화 별도의 운영체에서 프로세스가 실행되는 가상머신과 달리 컨테이너에서 실행하는 프로세스는 호스트의 운영체제 내부에서 실행하는 구조 훨씬 가볍고 운영체제 커널을 공유하며 시작이 빠름 운영체제 전체 부팅보다 메모리를 덜 사용 DockerDocker는 컨테이너 기술 중 하나로 여러 컴퓨터에 쉽게 이식 가능하게 하는 시스템 Docker 파일을 생성하여 *”어떤 SW를 컨테이너에 담아 구동할 것이다”* 명시하고 빌드 이후 docker image에 맞게 docker container 위에 생성 Kubernetes (K8s) 오픈소스 컨테이너 클러스터 관리 도구 Declarative Orchestration 단순 실행이 아닌 컨테이너의 실행 스케쥴 관리 컨테이너 배치, 스케일링, 운영 자동화 관리 Docker와 Kubernetes 관계 Docker : 컨테이너 운송 (빌딩 블록) Kubernetes : 컨테이너 운송을 어우르는 물류 시스템 Master &amp; Node 클러스터 전체를 관리하는 Master와 컨테이너가 배포되는 머신 (가상 or 물리적 머신) 인 Node로 Master 관리자만 접속하여 보안 설정이 필요 마스터 다운이 발생하면 클러스터 관리에 장애가 생기므로 보통 3대로 구성하여 클러스터 구성 소규모 환경에서는 마스터와 노드를 분리하지 않고 같은 서버에 구성 관리의 측면도 있지만 클러스터 전체 리소스 배분을 위해 파드를 띄울 수 있게 설정이 가능 (taint) Master component API server kubectl 요청 및 내부 모듈의 요청 처리 권한 체크를 통한 요청 허용 및 거부 실제로는 key-value로 저장된 Etcd에 저장된 데이터를 토대로 조회 RESTful API 제공 Etcd Kubernetes cluster의 DB 역할을 하는 서버로 설정값이나 cluster 상태를 저장 etcd라는 분산형 key/value 스토어 오픈소스 이용 Etcd 백업을 통해 클러스터 상태 복구가 가능 API 서버와만 통신 kube scheduler 할당이 필요한 Pod를 여러 조건(source, label)에 따라 적절한 노드에 할당해주는 모듈 kube controller-manager K8s 대부분의 object (Pod, ReplicaSet) 상태 관리 Cloud controller-manager AWS, GCE, Azure 등의 클라우드에 특화된 모듈 노드 추가 및 삭제, 로드 밸런서와 볼륨 연결 기능 Node Pod를 생성하고 네트워크와 볼륨을 설정 실제 컨테이너가 생성되는 서버 각 서버에 라벨을 붙여 사용목적에 따라 나눌 수 있음 Node component kubelet 노드에 배포되는 에이전트로 노드에 할당한 Pod 생명주기 관리 Pod 안 컨테이너 상태를 체크하고 주기적으로 Master에 전달 Master의 API서버와 통신 및 노드가 수행해야 할 명령 수행 kube-proxy kubelet이 pod를 관리한다면 kube-proxy는 Pod로 연결되는 네트워크를 관리 (네트워크 트래픽 분산) K8s ObjectKubernets는 상태를 관리하기 위한 대상을 Object라 칭하며 크게 기본 오브젝트와 컨트롤러로 구분 Pod Kubernetes의 최소 실행 단위 Kubernetes는 컨테이너를 개별적으로 배포하는 것이 아니라 Pod 단위로 배포 항상 같은 Node 위에서 실행되어야 하는 컨테이너들 Pod 내 네트워크 환경(IP, Port)과 디스크(Volume) 공유 A Container(Port 8080)와 B Container(Port 7001)가 하나의 Pod로 배포되었을때, localhost를 통해 통신이 가능 디스크를 공유하고 있기 때문에 다른 두 성격의 컨테이너를 배포할 때 타 컨테이너의 파일을 읽을 수 있음 YAML / JSON 형식으로 선언(config) Volume Container 재시작에 상관없이 파일을 영구적으로 저장해야하는 스토리지 Container의 외장 디스크라 생각하고, Pod이 기동할 때 컨테이너에서 마운트해 사용 ReplicaSet Pod를 여러 개 복제하여 관리하는 오브젝트 Pod를 생성하고 개수를 유지하려면 ReplicaSet 오브젝트를 사용해야함 복제 수, 레이블, 생성할 Pod의 템플릿 포함 직접적으로 사용하기 보단 Deployment 등 다른 오브젝트에 의해 사용되는 경우가 많음) Reference https://bcho.tistory.com/1256?category=731548 (https://www.slideshare.net/ext/devfair-kubernetes-101?qid=5ea32175-424b-4cda-b7b8-ccc96f01e7a5&amp;v=&amp;b=&amp;from_search=7 https://kubernetes.io/docs/concepts/architecture/cloud-controller/ 2019.09.06 made by jaejun.lee","categories":[{"name":"Cloud","slug":"Cloud","permalink":"https://jx2lee.github.io/categories/Cloud/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://jx2lee.github.io/tags/Kubernetes/"}]},{"title":"[Etc] Sebastian Ruder Interview","slug":"etc-sebastian_interview","date":"2019-08-18T15:00:00.000Z","updated":"2020-03-30T15:06:23.541Z","comments":true,"path":"etc-sebastian_interview/","link":"","permalink":"https://jx2lee.github.io/etc-sebastian_interview/","excerpt":"Sebastian Ruder 의 인터뷰를 정리하였다 세바스찬 루더 github : (https://github.com/sebastianruder) blog : (http://ruder.io/#open) 핫한 NLP 연구자 AYLIEN에서 리서치 사이언티스트로 일하며 현재 PhD 학생","text":"Sebastian Ruder 의 인터뷰를 정리하였다 세바스찬 루더 github : (https://github.com/sebastianruder) blog : (http://ruder.io/#open) 핫한 NLP 연구자 AYLIEN에서 리서치 사이언티스트로 일하며 현재 PhD 학생 QnANLP 초심자들에게 조언을 해줄 수 있겠는가?NLP-progress(http://nlpprogress.com/) 같은 곳에서 관심이 가는 분야를 찾아라. 만일 당신이 리서치에 관심이 있다면, 다른 사람이 하지 않은 특정한 세부 주제를 찾아보도록 노력하라. 예를 들어 감정 분석이라면 영화 리뷰 분석을 하지 말고 대화를 분석하라. 요약이라면 뉴스 기사를 요약하지 말고 의학 저널을 요약하라. 그 분야와 관련된 논문들을 읽고, 가장 첨단의 기술이 어떠한 지를 이해하라. 당신이 직접 돌려볼 수 있는 오픈 소스 구현이 있는 분야이면 더 좋다. 어떻게 이러한 작업이 돌아가는지를 잘 알게 되었을 때, 연구를 위해서 그 논문에서 당신을 놀라게한 점이 있었는지를 잘 생각해보아라. 그 모델이 어떠한 오류를 저질렀는 지를 이해하려 노력하고, 어떻게 하면 이를 줄여볼 수 있을지 생각하라. 어떠한 모델이 특정한 종류의 정보를 잘 잡아 내는 지를 측정하는 오류 절제 평가를 해보거나 합성 모델을 사용해보면 좋다. 만일 당신이 그 문제를 더 도전적으로, 혹은 현실적으로 해결해볼 수 있는 아이디어가 있다면, 데이터 셋을 만들고 현존하는 모델에 적용해보라. 그리고 다시 그 데이터 셋을 당신의 언어로 만들어보라. 그리고 그 모델이 역시 잘 작동하는 지를 확인하라. 흔히 NLP 분야가 컴퓨터 비전에 비해 뒤쳐졌다고들 한다. 현재의 상황에 대해서 어떻게 생각하는가? NLP 현업자로 뛰어들기에 좋은 시기인가?나는 지금이 NLP에 뛰어들기 좋은 시기라 생각한다. 수 년전과 비교하면 성숙기에 접어들었다고 본다. 단순히 워드 임베딩이나 기존에 나온 모델들을 사용하는 것에 제한되지 않고, 다양한 부품들로 당신만의 모델을 만들 수 있는 것이다. 이를 테면 다른 신경망 층들, 사전 학습된 표현들, 부가적인 로스의 사용 등이 있다. 또한 POS 태깅, 감정 분석 등 고전적인 문제들이 거의 해결되고 있다는 커뮤니티의 반응도 있다. 그러므로 우리는 더 어려운 문제들에서 발전을 이루어야만 한다. 이를테면 “진짜” 자연어를 이해하고 생성해내는 일반화된 모델이 있다. 이러한 문제들을 풀기 위해선 나는 새로운 사람들의 관점과 아이디어들이 필요하다고 생각한다. 덧붙여 우리는 이제 분류나 문장 라벨링 등의 작업을 꽤 높은 정확도로 수행하는 모델들을 학습할 수 있으니, 이러한 모델들을 다른 언어에 적용해 볼 수 있는 기회가 많이 있다. 만일 당신이 또 다른 언어의 사용자라면, 당신은 다른 사람들이 모델 학습과 평가에 사용할 수 있는 데이터 셋을 만드는 것 만으로도 큰 변화를 만들어 낼 수 있을 것이다. DL/ML 분야에서 많은 job 들이 석박사나 연구 경험을 요구한다. 머신 러닝을 커리어 패스로 고려하는 독자들을 위해서, 연구 경험이 반드시 필요하다고 생각하는가?나는 리서치 경험이 당신이 특정 모델에 대해서 잘 알고 있는지, 창의적인지, 새로운 해결방안을 생각해 낼 수 있을 만큼 혁신적인지를 측정하는 좋은 지표라고 생각한다. 그러나 이러한 스킬들을 익히기 위해서 PhD나 리서치 펠로우십을 취득할 필요는 없다. 능동적으로 움직이고, 흥미있는 분야에 대해서 배우고 문제를 해결하고, 모델을 개선하고, 당신의 경험을 글로 쓰는 것은 이러한 스킬들을 키우고 증명하는 좋은 방법이다. 현재의 ML 환경에서 당신은 완전히 새로운 문제를 풀도록 요구되어지지 않는다. ML이나 데이터 사이언스 대회에 참여하는 것도 비슷하게 당신이 ML 모델을 어떻게 실용적으로 적용하는지를 증명하는데 도움이 된다. 리서치 분야의 폭발적인 성장을 놓고 봤을 때, 가장 첨단의 기술들에 대해서 어떻게 따라갈 수 있는가?나는 매일 arXiv의 업데이트를 확인한다. 관련 논문들을 내 읽기 목록에 추가한 뒤에 한번에 읽는다. 제프 딘은 최근 Deep Learning Indaba에서 열 개 논문의 abstract를 읽는 것이 한 개 논문을 깊이 있게 읽는 것 보다 낫다고 한다. 왜냐면 언제든 되돌아가서 특정 논문을 깊이 있게 읽는 것은 가능하기 때문이다.”고 말했다. 그의 말에 동의하며, 최대한 넓게 읽어라. 그래서 당신이 목록화 할 수 있고 나중에 영감을 받을 수 있도록 하라. 좋은 문서 관리 시스템을 갖는 것도 핵심이다. 나는 Mendeley를 사용해왔다. 최근에는 Arxiv Sanity Preserver(http://www.arxiv-sanity.com/recommend) 를 사용한다. 어떻게 시작하게 되었는가? 특히 왜 딥 러닝과 NLP에 관심을 갖게 되었는가?고등학생 때부터 언어와 수학에 관심이 있었고, 여러 대회에 참가했었다. 내 학업을 위해서 나는 수학의 논리와 언어의 창의성을 결합하고 싶었다. 그러나 그런 분야가 존재하는 지 몰랐다. 그 떄 전산 언어학이라는, 컴퓨터 과학과 언어학이 적절하게 교차하는 분야를 접하게 되었다. 그래서 독일 대학에서 전산 언어학 학사를 땄다. 학부생 시절에 머신 러닝을 접하게 되었고, 인턴쉽과 온라인 강의들을 통해 최대한 지식을 습득했다. word2vec에 대해 들은건 학부를 마친 2015년이었다. PhD를 시작하면서 딥 러닝에 대해 알게 되었고, 이 분야에 더욱 흥미를 가지게 되었다. 산업과 리서치 중에서 리서치를 택한 이유가 있는가?졸업 이후에 스타트업에서 산업 경험을 쌓고자 했다. PhD는 내가 항상 꿈꾸던 것이었지만, 그 당시에는 심각하게 생각하지 않았다. 듀블린의 NLP 스타트업 Aylien에서 일하면서 그들은 나에게 고용이 보장된 PhD 프로그램을 소개해 주었고, 나에게 잘 들어맞는다고 생각했다. 회사에서 일하면서 동시에 연구를 하는 것은 매우 힘들었지만, 결국 나에게 돌아오는 것은 많았다. 가장 중요하게도, 내 회사와 잘 맞았다. 지금까지 연구자로 3년간 일해왔다. 이 기간동안 당신의 최애 프로젝트는 무엇이었는가?배움의 관점에서는 잘 알지 못하는 분야에 뛰어드는 것과 논문들을 읽는 것, 그리고 훌륭한 사람들과 협업하는 것이다. 이러한 맥락에서 코펜하겐 대학에서 진행했던 multi-task learning 프로젝트가 굉장히 자극이 되는 경험이었다.영향력의 관점에서는 fastai, 제레미 (fast ai 창립자)와 협업하면서 그들이 어떻게 우리의 언어 모델을 유용하게 사용하는지를 본 것이다. 당신은 매우 훌륭한 블로그를 관리했다. 기술적인 글들을 효과적으로 쓸 수 있는 팁들이 있는가?나는 내 자신이 특정한 주제에 대해서 더 잘 이해하기 위해서 블로그를 쓸 때 아주 좋았던 경험이 있다. 만일 당신이 어떤 주제에 대하여 많은 리서치를 하거나 직관을 얻고 싶다면, 포스트를 작성하는 것을 고려해보아라. 그리고 이것은 훗날 누군가의 학습을 더욱 빠르게 도와줄 것이다. 연구 논문에서는 지면이 부족하여 충분히 글로써 설명해내지 못하는 측면들이 있다. 블로그 포스팅은 기술들을 더 접근하기 쉽게 설명하는 아주 좋은 방법이다. 블로그의 좋은 점은 완벽하지 않아도 좋다는 점이다. 당신은 이를 커뮤니케이션 능력을 향상시키기 위해 사용해도 좋고, 당신의 아이디어에 대한 피드백을 얻기 위해 사용해도 좋다. 글을 쓰는 관점에서는, 가장 중요한 것은 명확하기 위해 노력해야한다는 것이다. 애매 모호하지 않고 데이터가 보여주는 것에 대해서만 써라. 만일 의심스럽다면 명확하게 no라고 말하라. 당신의 초고에 대해서도 당신의 친구들이나 동료들의 피드백을 들어라. 100% 완벽하게 만들려고 애쓰지 말아라. 그러나 만족할 만한 수준까지는 끌어올려야한다. 공개 버튼을 누를 때 걱정이되는 것은 당연한 것이니 도망치지 말라. 무언가를 퍼블리싱 한다는 것은 장기적인 관점에서 분명 가치가 있다. 딥 러닝이 어려운 분야라는 생각에 시작하기 망설이는 초보자들을 위해 해주고 싶은 말은?아무도 당신에게 넌 할 수 없어라고 말할 수 없다. 온라인 수업들을 듣고 이해하라. 기본적인 지식들에 익숙해지면 시간이 될 때마다 영감을 위해 논문들을 읽어라. 흥미로운 분야를 선택하고, 라이브러리를 선택하고 진행해보라. 당신이 의미있는 문제를 풀기 위해서는 반드시 거대한 컴퓨터가 있어야 된다는 생각을 버려라. 특히 NLP 분야에서는 라벨링 된 예시의 수가 적은 문제들이 많이 있다. 당신이 하고 있고 배우고 있는 것에 대해서 써라. 비슷한 관심사가 있는 사람들을 만나라. 커뮤니티에 참여하고, 특히 fast ai는 좀 쩐다. 트위터를 하라. 트위터는 훌륭한 ML 커뮤니티이다. 당신은 탑 연구자들로부터 이메일보다 빨리 답장을 받을 수도 있다. 멘토를 찾으라. 만일 누군가에게 조언을 구하기 위해 이메일을 쓴다면, 그들이 바쁘다는 것을 고려하라. 존중하고 다른 이들을 도와라. 칭찬을 많이 하고, 비평할 때는 조심스럽게 하라. END 2019.08.09 made by jaejun.lee","categories":[{"name":"Etc","slug":"Etc","permalink":"https://jx2lee.github.io/categories/Etc/"}],"tags":[{"name":"Review","slug":"Review","permalink":"https://jx2lee.github.io/tags/Review/"}]},{"title":"[Etc] How to Read Research Papers","slug":"etc-how_to_read_research","date":"2019-08-08T15:00:00.000Z","updated":"2020-03-30T15:06:23.527Z","comments":true,"path":"etc-how_to_read_research/","link":"","permalink":"https://jx2lee.github.io/etc-how_to_read_research/","excerpt":"Siraj Raval의 research paper를 효율적으로 보는 방법에 대해 설명한다","text":"Siraj Raval의 research paper를 효율적으로 보는 방법에 대해 설명한다 Siraj는 다행히 학위 소지자가 아니다. 즉, 학위 소지자가 아니어도 충분히 research 논문을 이해할 수 있고 접할 수 있다. Siraj는 다양한 분야의 논문을 즐겨 읽는다고 한다. Siraj는 다음과 같이 세 단계 프로세스로 논문을 이해하고 읽는다고 한다. 이 방식으로 일주일에 약 10-20편의 논문을 소화한다고 한다. 가장 중요한 것은 포기하지 않는 마음가짐이며, 이해가 되지 않는 부분은 커뮤니티에서 discussion해야 한다고 말한다. 3-PASS Approach논문을 가볍게 파악 논문 제목 / Abstract 먼저 읽기 Introduction 신중히 읽기 Section / Sub-section의 경우 타이틀만 읽고 PASS Conclusion 신중히 읽기 이외 수학적인 부분은 완전히 무시 1단계에서는 수학적인 부분은 PASS googling을 통해 다른 사람들이 리뷰한 내용을 살펴보고 내 의견과 비교해보기 (using reddit) High-level 이해 1단계보다 더 deep하게 읽기 문장을 모두 이해할 수 있도록 수식을 정확히 이해하지는 않으나 원리는 이해하고 넘어가기 소스코드가 있다면 소스 및 문서를 읽어보고 코드 실행 및 재현해보기 수식을 통해 이해 지금까지 이해한 내용을 문서로 정리하여 누군가에게 설명이 가능할 정도로 정리하기 수식을 스스로 풀어보고 완벽히 이해하기 수식의 내용을 스스로 프로그래밍하며 수식 완벽히 이해하기 summary정말 어려운 과정이지만 정말 효과적인 방법인 것 같다. 특히, 우리가 무심코 넘어가는 수식 완벽히 이해하기는 처음에는 어렵고 귀찮을 수 있지만 논문을 정확히 이해하고 이를 활용할 수 있는 지식을 쌓을 수 있을 것 같다. 이러한 프로세스도 도움이 되지만 나만의 논문 읽는 노하우를 익히고 싶다. 2019.08.09 made by jaejun.lee","categories":[{"name":"Etc","slug":"Etc","permalink":"https://jx2lee.github.io/categories/Etc/"}],"tags":[{"name":"Review","slug":"Review","permalink":"https://jx2lee.github.io/tags/Review/"}]},{"title":"[Python] 클로저가 변수 스코프와 상호 작용하는 방법","slug":"python-better_way_15","date":"2019-08-07T15:00:00.000Z","updated":"2020-03-30T15:06:23.546Z","comments":true,"path":"python-better_way_15/","link":"","permalink":"https://jx2lee.github.io/python-better_way_15/","excerpt":"클로저가 변수 스포크와 상호 작용하는 방법에 대해 알아본다.","text":"클로저가 변수 스포크와 상호 작용하는 방법에 대해 알아본다. 숫자 list를 정렬할 때 일정 숫자들을 먼저 정렬하고자 한다. 이는 사용자 인터페이스를 표현하거나, 중요 메세지 또는 예외 이벤트를 먼저 보여줄 때 유용하다. 일반적인 방법으로는 list의 sort method에 helper function을 key 변수로 넘기는 것이다. helper function의 return 값은 숫자를 정렬하는 사용된다. 다음 코드를 확인해보자. 12345678910111213def sort_priority(values, group): def helper(x): if x in group: return (0, x) return (1, x)numbers = [8, 3, 1, 2, 5, 4, 7, 6]group = &#123;2, 3, 5, 7&#125;sort_priority(numbers, group)print(numbers)&gt;&gt;&gt;[2, 3, 5, 7, 1, 4, 6, 8] 위 함수가 동작하는 이유는 다음과 같다. python은 closure를 지원, closure란 자신이 정의된 스코프에 있는 변수를 참조하는 함수다. 이 때문에 helper function이 sort_priority의 group에 접근할 수 있다. summary None을 반환하는 함수는 None 이나 다른 값이 조건식에서 False로 평가되기 때문에 쉽게 오류를 범할 수 있다. 특별한 상황을 알릴 때는 None 대신 예외를 일으키자. 참조 : (http://www.yes24.com/Product/goods/25138160) 2019.08.08 made by jaejun.lee","categories":[{"name":"Python","slug":"Python","permalink":"https://jx2lee.github.io/categories/Python/"}],"tags":[{"name":"Python Better Way","slug":"Python-Better-Way","permalink":"https://jx2lee.github.io/tags/Python-Better-Way/"}]},{"title":"[Python] None을 반환하기보다 예외 발생","slug":"python-better_way_14","date":"2019-08-05T15:00:00.000Z","updated":"2020-03-30T15:06:23.586Z","comments":true,"path":"python-better_way_14/","link":"","permalink":"https://jx2lee.github.io/python-better_way_14/","excerpt":"함수를 이용해 None을 반환하기보다는 예외를 일으키는 방법에 대해 알아본다","text":"함수를 이용해 None을 반환하기보다는 예외를 일으키는 방법에 대해 알아본다 파이썬 프로그래머들은 보통 None값에 특별한 의미를 부여하는 경우가 있다. 예를 들어 나눗셈을 수행하는 헬퍼함수를 생각해보자. 0으로 나누는 경우는 존재하지 않기 때문에 None을 반환하는 게 자연스럽다. 12345def divide(a,b): try: return a / b except ZeroDivisionError: return None 반환 값을 다음과 같이 해석할 수 있다. 123result = divide(x, y)if result is None: print('Invalid inputs') 하지만 분자가 0, 즉 나누는 숫자를 0으로 하면?이런 경우 if문으로 결과를 평가할 때 문제가 될 수 있다.조건을 None이 아닌 False로 검사할 수 있기 때문이다. 1234x, y = 0, 5result = divide(x, y)if no result: print('Invalid inputs') # wrong! 위 예는 None에 특별한 의미를 부여할 때 파이썬 코드에서 흔히 하느 실수라고 한다.이러한 실수를 방지하기 위한 방법은 두 가지로 설명한다. 반환 값을 두 개로 나눠 튜플에 담자튜플의 첫 value는 성공/실패 여부를 알려준다. 두 번째 값은 계산된 실제 결과다. 123456789def divide(a, b): try: return True, a / b except ZeroDivisionError: return False, Nonesuccess, result = divide(x, y)if no success: print('Invalid inputs') 허나 이 방법은 튜플 첫 값을 쉽게 무시할 수 있다(가령 _을 이용해 무시 가능).겉보기에느 잘못된 것 같지 않지만 그냥 None을 반환하는 것만큼 나쁘다. 123_, result = divide(x, y)if no result: print('Invalid inputs') 절대로 None을 반환하지 말자! -&gt; 호출 함수에서 예외 일으키기저자는 None을 절대로 반환하지 않는 방법을 추천한다. 즉, 호출하는 함수 내에서 예외를 일으켜ZeroDivisionError을 ValueError로 변경하는 것이다. 12345def divide(a, b): try: return a / b except ZeroDivisionError as e: raise ValueError('Invalid inputs') from e 위와 같이 함수를 정의하면 함수의 반환 값을 조건식으로 검사할 필요가 없다. 1234567x, y = 0, 10try: result = divide(x, y)except ValueError: print('Invalid inputs')else: print('Result is %.1f' % result) summary None을 반환하는 함수는 None 이나 다른 값이 조건식에서 False로 평가되기 때문에 쉽게 오류를 범할 수 있다. 특별한 상황을 알릴 때는 None 대신 예외를 일으키자. 참조 : (http://www.yes24.com/Product/goods/25138160) 2019.08.07 made by jaejun.lee","categories":[{"name":"Python","slug":"Python","permalink":"https://jx2lee.github.io/categories/Python/"}],"tags":[{"name":"Python Better Way","slug":"Python-Better-Way","permalink":"https://jx2lee.github.io/tags/Python-Better-Way/"}]},{"title":"[Database] Connect to Tibero using Python","slug":"database-connect_to_tibero_using_python","date":"2019-08-01T15:00:00.000Z","updated":"2020-09-24T14:34:37.718Z","comments":true,"path":"database-connect_to_tibero_using_python/","link":"","permalink":"https://jx2lee.github.io/database-connect_to_tibero_using_python/","excerpt":"docker image로 띄운 Python 에서 Tibero 에 접근하는 방법을 다룬다.","text":"docker image로 띄운 Python 에서 Tibero 에 접근하는 방법을 다룬다. Preliminariespython 은 이미 설치된 것을 전제iodbc Container에 tibero client가 설치되어 있다고 가정 tibero client가 없다면 서버에 있는 tibero6 복사 tbdsn.tbr 에 호스트를 참조하고자하는 Tibero ip로 변경 $TB_HOME/client/lib(또는 $TB_HOME/client/lib32) 디렉터리에 libtbodbc.so 파일이 존재하는지 확인 http://iodbc.org 에서 다운로드 tar 파일을 설치 디렉토리에 옮긴 후 make를 이용해 build 한다. 1234567$ ./configure --prefix=/app/odbc/iodbc --disable-gui$ make$ make install......$ file /app/odbc/lib/libtbodbc.so$ file /app/odbc/lib/libiodbcinst.so.2.1.18 unixODBC-develapt-get install unixodbc-dev ~/.profile12345678910111213## Tibero RDBMS 6 Client ENV ##export TB_HOME=/app/tibero6export TB_SID=tiberoexport TB_PROF_DIR=$TB_HOME/bin/profexport PATH=$TB_HOME/bin:$TB_HOME/client/bin:$PATHexport LD_LIBRARY_PATH=$TB_HOME/lib:$TB_HOME/client/lib:$LD_LIBRARY_PATHexport TB_NLS_LANG=UTF8export TBCLI_WCHAR_TYPE=UCS2## IODBC ENV ##export IODBC_HOME=/app/odbc/iodbcexport LD_LIBRARY_PATH=$IODBC_HOME/lib:$LD_LIBRARY_PATHexport PATH=$IODBC_HOME/bin:$PATH Set DSN$HOME/odbc.ini 파일을 아래와 같이 수정한다 1234567891011[ODBC Data Sources]tibero6 = Tibero6 ODBC driver[ODBC]Trace = 1TraceFile = /tmp/odbc.trace[tibero6]Driver = /app/tibero6/client/lib/libtbodbc.soDescription = Tibero6 ODBC DatasourceSID = tibero156 # tbdsn.tbr 파일에 설정한 DSN 정보User = erpPassword = xxxxx isql를 통해 접속이 되는지 확인하고 조회해본다. 1234567891011121314151617$ isql -v tibero6+---------------------------------------+| Connected! || || sql-statement || help [tablename] || quit || |+---------------------------------------+SQL&gt; select count(*) from BPRJT00T;+------------------------------------------------------+| COUNT(*) |+------------------------------------------------------+| 56125 |+------------------------------------------------------+SQLRowCount returns 11 rows fetched .py1234567891011121314151617181920import pyodbctry: # db connection dbuser = 'erp' dbpw = 'tibero' conn = pyodbc.connect('DSN=tibero6;UID=' + dbuser + ';PWD=' + dbpw) cur = conn.cursor() stmt = \"SELECT COUNT(*) FROM BPRJT00T;\" rows = cur.execute(stmt) for row in rows: print (row) # db connection close cur.close() conn.close()except Exception as ex: print(ex) 12root@ff5f929d7f7f:~# python3 test2.py (56125.0, ) .py 스크립트를 실행하면 정상적으로 연결되지만, Notebook을 이용하면 Data Source를 찾지 못한다는 에러를 뱉어낸다. 이는 추후에 해결하고 따로 포스팅 할 예정이다 2019.08.02 made by jaejun.lee","categories":[{"name":"Database","slug":"Database","permalink":"https://jx2lee.github.io/categories/Database/"}],"tags":[{"name":"Tibero","slug":"Tibero","permalink":"https://jx2lee.github.io/tags/Tibero/"}]},{"title":"[Machine Learning] Understanding Hyperparameter","slug":"ml-introduction_to_grid_search","date":"2019-07-01T15:00:00.000Z","updated":"2020-03-30T15:06:23.567Z","comments":true,"path":"ml-introduction_to_grid_search/","link":"","permalink":"https://jx2lee.github.io/ml-introduction_to_grid_search/","excerpt":"Hyperparameter 란? Wiki에 따르면 학습이 시작되기 전 설정된 변수 라고 정의 즉, 학습 이전 initialized variable 학습 parameter는 크게 Model parameter 와 Hyperparameter 로 구분","text":"Hyperparameter 란? Wiki에 따르면 학습이 시작되기 전 설정된 변수 라고 정의 즉, 학습 이전 initialized variable 학습 parameter는 크게 Model parameter 와 Hyperparameter 로 구분 Model parametersModel paramters는 ML model에 의해 학습할 데이터의 속성으로 모델이 학습됨과 동시에 학습하는 parameter를 의미한다. 예로 Weight과 Biases를 Model parameter라 부른다. Model HyperparametersModel Hyperparameters는 ML model의 전체 학습 과정을 관리하는 속성이다. Model parameter과는 다르게 학습 도중 변하지 않는것이 특징이다. Learning Rate Epochs Hidden Layers(Units) Activation functions model_parameter Hyperparameter가 왜 필요한가?Hyperparamter는 training algorithm 동작을 직접 제어, 모델 성능에 중요한 영향을 미친다. “적절한 Hyperparameter 선택은 algorithm을 빛나게 만든다” (A good choice of hyperparameters can really make an algorithm shine) 이라는 말도 있다.Hyperparameter가 중요한 이유는 예를 들어 Learning rate이 너무 낮게되면 모델 패턴을 놓칠 수 있고, 높으면 충돌이 발생할 수 있다. 즉 학습이 제대로 이루어지지 않을 수 있는 문제가 발생한다. 이에 적절한 Hyperparameter 선택은 다음과 같은 이점이 있다. 효율적인 매개 변수 공간 여러 실험(experiment)들을 손쉬운 관리 Hyperparameters 최적화 기법 (Optimisation Techniques)ML에서 최적의 Hyperparameter를 찾는 과정을 Hyperparameter optimisation이라 한다. 기법들은 크게 다음과 같다. Grid Search Random Search Bayesian Optimisation 20190702 기준으로는 Grid Search 만 다루고 나중에 시간이 된다면 나머지 기법도 정리할 예정이다 Grid SearchGrid Search는 Hyperparameter 를 찾는 가장 전통적인 방법이다. 모든 조합을 고려해 최적의 set를 찾아내는 약간 무식한 방법이다. Grid Search 는 보통 2개의 Hyperparamter 조합을 만든다. (본 게시글엔 Learning Rate / Number of Layers 로 되어있다) Grid Search는 두 개의 Hyperparameter를 이용해 모든 조합을 만들어 학습하고 Cross Validation 기술을 사용해 성능을 측정한다. grid search Grid Search는 사용하기 간편하지만 curse of dimensionality(차원의 저주)라는 문제를 안고 있다. Training data의 dimension이 높으면 그만큼 생기는 Hyperparamter 조합도 많아져 차원의 저주가 발생한다. 2019.07.02 made by jaejun.lee","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://jx2lee.github.io/categories/Machine-Learning/"}],"tags":[{"name":"Algorithm","slug":"Algorithm","permalink":"https://jx2lee.github.io/tags/Algorithm/"}]}],"categories":[{"name":"Jenkins","slug":"Jenkins","permalink":"https://jx2lee.github.io/categories/Jenkins/"},{"name":"Hadoop","slug":"Hadoop","permalink":"https://jx2lee.github.io/categories/Hadoop/"},{"name":"Cloud","slug":"Cloud","permalink":"https://jx2lee.github.io/categories/Cloud/"},{"name":"Linux","slug":"Linux","permalink":"https://jx2lee.github.io/categories/Linux/"},{"name":"TroubleShoot","slug":"TroubleShoot","permalink":"https://jx2lee.github.io/categories/TroubleShoot/"},{"name":"Database","slug":"Database","permalink":"https://jx2lee.github.io/categories/Database/"},{"name":"Python","slug":"Python","permalink":"https://jx2lee.github.io/categories/Python/"},{"name":"SQL","slug":"SQL","permalink":"https://jx2lee.github.io/categories/SQL/"},{"name":"Etc","slug":"Etc","permalink":"https://jx2lee.github.io/categories/Etc/"},{"name":"Shell","slug":"Shell","permalink":"https://jx2lee.github.io/categories/Shell/"},{"name":"BI","slug":"BI","permalink":"https://jx2lee.github.io/categories/BI/"},{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://jx2lee.github.io/categories/Machine-Learning/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://jx2lee.github.io/tags/Docker/"},{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://jx2lee.github.io/tags/Kubernetes/"},{"name":"Tibero","slug":"Tibero","permalink":"https://jx2lee.github.io/tags/Tibero/"},{"name":"Review","slug":"Review","permalink":"https://jx2lee.github.io/tags/Review/"},{"name":"Kubeflow","slug":"Kubeflow","permalink":"https://jx2lee.github.io/tags/Kubeflow/"},{"name":"Programmers","slug":"Programmers","permalink":"https://jx2lee.github.io/tags/Programmers/"},{"name":"MySQL","slug":"MySQL","permalink":"https://jx2lee.github.io/tags/MySQL/"},{"name":"Spark","slug":"Spark","permalink":"https://jx2lee.github.io/tags/Spark/"},{"name":"Hackerrank","slug":"Hackerrank","permalink":"https://jx2lee.github.io/tags/Hackerrank/"},{"name":"Python Better Way","slug":"Python-Better-Way","permalink":"https://jx2lee.github.io/tags/Python-Better-Way/"},{"name":"Algorithm","slug":"Algorithm","permalink":"https://jx2lee.github.io/tags/Algorithm/"}]}