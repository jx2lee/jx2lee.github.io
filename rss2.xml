<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"
  xmlns:atom="http://www.w3.org/2005/Atom"
  xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>go hard</title>
    <link>https://jx2lee.github.io/</link>
    
    <atom:link href="/rss2.xml" rel="self" type="application/rss+xml"/>
    
    <description></description>
    <pubDate>Sun, 11 Jul 2021 10:29:20 GMT</pubDate>
    <generator>http://hexo.io/</generator>
    
    <item>
      <title>[Java] JPA 데이터베이스 스키마 자동생성</title>
      <link>https://jx2lee.github.io/java-jpa_auto_generating_schema/</link>
      <guid>https://jx2lee.github.io/java-jpa_auto_generating_schema/</guid>
      <pubDate>Sat, 10 Jul 2021 15:00:00 GMT</pubDate>
      <description>
      
        &lt;center&gt; DB 스키마를 미리 지정하는 방법에 대해 살펴본다 🤡 &lt;/center&gt;
&lt;br&gt;
&lt;br&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<center> DB 스키마를 미리 지정하는 방법에 대해 살펴본다 🤡 </center><br><br><a id="more"></a><p><strong><span class="github-emoji" alias="sparkles" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png?v8">&#x2728;</span> Contents:</strong>  </p><ol><li><a href="#subject01">개요</a></li><li><a href="#subject02">hibernate.hbm2ddl.auto</a></li><li><a href="#reference">Reference</a></li></ol><h1 id="개요"><a href="#개요" class="headerlink" title=" 개요"></a><a name="subject01"></a> 개요</h1><ul><li>DDL 을 애플리케이션 실행 시점에 자동으로 생성</li><li>DDL 은 반드시 개발 장비에서만 사용<ul><li>운영 서버에는 절대 사용하지 않고 다듬은 후 운영 서버에 반영하는 것이 최선</li></ul></li></ul><h1 id="hibernate-hbm2ddl-auto-옵션"><a href="#hibernate-hbm2ddl-auto-옵션" class="headerlink" title=" hibernate.hbm2ddl.auto 옵션"></a><a name="subject02"></a> hibernate.hbm2ddl.auto 옵션</h1><ul><li>create: 기존테이블 삭제 후 다시 생성 (drop → create)</li><li>create-drop: create 와 동일하지만 애플리케이션 종료 이후 drop</li><li>update: 변경 부분만 반영</li><li>validate: 엔티티 &lt;-&gt; 테이블 매핑만 확인</li><li>none: 사용하지 않음<ul><li>운영 장비에는 create &amp; create-drop &amp; update 절대 사용하지 않도록 주의</li><li>개발 초기: create or update</li><li>테스트 서버: update or validate</li><li>스테이징 및 운영 서버: validate or none</li></ul></li><li><code>@Column</code> annotation 으로 제약 조건 추가가 가능<ul><li>단, DB 설정을 변경하는 것이지 JPA 실행 로직에는 전혀 영향이 없음</li></ul></li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title=" Reference"></a><a name="reference"></a> Reference</h1><ul><li><a href="https://www.inflearn.com/course/ORM-JPA-Basic/dashboard">https://www.inflearn.com/course/ORM-JPA-Basic/dashboard</a></li></ul><hr><p>made by <em>jaejun.lee</em></p>]]></content:encoded>
      
      <comments>https://jx2lee.github.io/java-jpa_auto_generating_schema/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[Java] JPA 양방향 연관관계 주인</title>
      <link>https://jx2lee.github.io/java-jpa_associated_owner/</link>
      <guid>https://jx2lee.github.io/java-jpa_associated_owner/</guid>
      <pubDate>Sat, 10 Jul 2021 15:00:00 GMT</pubDate>
      <description>
      
        &lt;center&gt; 객체 간 양방향 연관관계에서의 주인을 살펴보자 🤩 &lt;/center&gt;
&lt;br&gt;
&lt;br&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<center> 객체 간 양방향 연관관계에서의 주인을 살펴보자 🤩 </center><br><br><a id="more"></a><p><strong><span class="github-emoji" alias="sparkles" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png?v8">&#x2728;</span> Contents:</strong>  </p><ol><li><a href="#subject01">양방향 객체 연관관계</a><ul><li><a href="#subject02">mappedBy</a></li></ul></li><li><a href="#subject03">연관관계의 주인(Owner)</a></li><li><a href="#subject04">양방향 매핑시 주로 하는 실수</a></li><li><a href="#subject05">양방향 매핑 정리</a></li><li><a href="#reference">Reference</a></li></ol><h1 id="양방향-객체-연관관계"><a href="#양방향-객체-연관관계" class="headerlink" title=" 양방향 객체 연관관계"></a><a name="subject01"></a> 양방향 객체 연관관계</h1><p><img src="/image/jpa-associated-owner-1.png" alt="/image/jpa-associated-owner-1.png"></p><ul><li>양방향 연관관계는 단방향 연관관계랑 크게 다를 거 없이 <strong>한 쪽에 참조할 수 있도록 추가</strong>만 하면 된다.</li></ul><h2 id="mappedBy"><a href="#mappedBy" class="headerlink" title=" mappedBy"></a><a name="subject02"></a> mappedBy</h2><ul><li>객체와 테이블 간 연관관계를 맺는 차이를 이해하면 mappedBy 를 이해하기 쉬움</li><li>객체의 경우 양방향 연관관계 → 단방향 연관관계 2개를 포함하는 개념</li><li>객체를 양방향으로 참조하려면 단방향 연관관계를 2개를 생성해야함</li><li><code>테이블</code>의 경우 외래 키 하나로 두 테이블 연관관계를 정의 (외래키를 이용해 <em>외래키를 가지는 테이블</em>을 참조할 수 있음)</li></ul><h1 id="연관관계의-주인-Owner"><a href="#연관관계의-주인-Owner" class="headerlink" title=" 연관관계의 주인(Owner)"></a><a name="subject03"></a> 연관관계의 주인(Owner)</h1><ul><li>그럼 객체 간 연관관계 주인은 누가 가져가야 하는가?</li><li>테이블 관점에서 보면 <strong>외래키를 포함하는 객체가 관계의 주인</strong>으로 설정</li><li>즉, {} to {} 관계(ex. Many to One, One to Many) 일 경우 Many 쪽을 주인으로 가져가는 것!<ul><li>하나의 팀에는 많은 회원을 가지는 경우를 생각하면, 회원 쪽을 관계의 주인으로 가져간다.</li><li>팀 entity 에는 mappedBy 를 설정하여 가짜 매핑</li></ul></li><li><strong>관계의 주인만 외래 키를 관리할 수 있으며 주인이 아닌 쪽은 읽기만 가능</strong></li></ul><h1 id="양방향-매핑시-주로-하는-실수"><a href="#양방향-매핑시-주로-하는-실수" class="headerlink" title=" 양방향 매핑시 주로 하는 실수"></a><a name="subject04"></a> 양방향 매핑시 주로 하는 실수</h1><ul><li>주인이 되는 객체에 값을 입력하지 않는 경우</li><li>순수 객체 관계를 고려하여 양 쪽 모두 값을 세팅해야 한다. (<u>한 쪽만 설정하면 안된다!</u>)<p align="left"><img src="/image/jpa-associated-owner-2.png"></p></li><li>필요한 경우 연관관계 편의 메소드를 생성하여 사용한다. 단, <em>양방향 매핑 시 무한 루프를 조심</em></li></ul><h1 id="양방향-매핑-정리"><a href="#양방향-매핑-정리" class="headerlink" title=" 양방향 매핑 정리"></a><a name="subject05"></a> 양방향 매핑 정리</h1><ul><li>단방향 매핑으로만 될 수 있게 설계한다.</li><li>이후 반대 방향으로 조회하는 경우 필요한 기능만 추가한다.<ul><li>JPQL 에서 역방향으로 탐색할 일이 많음</li><li>이때 양방향 매핑을 설정하여 그때 처리해도 늦지 않는다.</li></ul></li><li><code>즉, 단방향 매핑만으로 객체 설계를 하자</code></li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title=" Reference"></a><a name="reference"></a> Reference</h1><ul><li><a href="https://www.inflearn.com/course/ORM-JPA-Basic/dashboard">https://www.inflearn.com/course/ORM-JPA-Basic/dashboard</a></li></ul><hr><p>made by <em>jaejun.lee</em></p>]]></content:encoded>
      
      <comments>https://jx2lee.github.io/java-jpa_associated_owner/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[Java] JPA 연관관계 매핑 살펴보기</title>
      <link>https://jx2lee.github.io/java-jpa_association_mapping/</link>
      <guid>https://jx2lee.github.io/java-jpa_association_mapping/</guid>
      <pubDate>Sat, 10 Jul 2021 15:00:00 GMT</pubDate>
      <description>
      
        &lt;center&gt; JPA 에서 제공하는 객체 간 연관관계 매핑의 종류를 살펴본다 🤪 &lt;/center&gt;
&lt;br&gt;
&lt;br&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<center> JPA 에서 제공하는 객체 간 연관관계 매핑의 종류를 살펴본다 🤪 </center><br><br><a id="more"></a><p><strong><span class="github-emoji" alias="sparkles" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png?v8">&#x2728;</span> Contents:</strong>  </p><ol><li><a href="#subject01">객체 간 매핑 시 고려사항</a></li><li><a href="#subject02">종류</a><ul><li><a href="#subject03">다대일</a></li><li><a href="#subject04">일대다</a></li><li><a href="#subject05">일대일</a></li><li><a href="#subject06">다대다</a></li></ul></li><li><a href="#reference">Reference</a></li></ol><h1 id="객체-간-매핑-시-고려사항"><a href="#객체-간-매핑-시-고려사항" class="headerlink" title=" 객체 간 매핑 시 고려사항"></a><a name="subject01"></a> 객체 간 매핑 시 고려사항</h1><ul><li>다중성</li><li>단방향 vs 양방향<ul><li>왠만하면 설계 초기에는 단방향으로</li></ul></li><li>연관관계 주인</li></ul><h1 id="종류"><a href="#종류" class="headerlink" title=" 종류"></a><a name="subject02"></a> 종류</h1><h2 id="다대일"><a href="#다대일" class="headerlink" title=" 다대일"></a><a name="subject03"></a> 다대일</h2><p><img src="/image/jpa-association-mapping-1.png" alt="/image/jpa-association-mapping-1.png"></p><ul><li>가장 많이 사용하는 연관관계</li><li><strong><em>외래키를 가진 테이블의 객체를 연관관계 주인으로 설정</em></strong></li></ul><h2 id="일대다"><a href="#일대다" class="headerlink" title=" 일대다"></a><a name="subject04"></a> 일대다</h2><p><img src="/image/jpa-association-mapping-2.png" alt="/image/jpa-association-mapping-2.png"></p><ul><li>entity 가 관리하는 외캐리(TEAM_ID) 가 다른 테이블에 존재<ul><li>이는 추가 update query 가 발생</li><li><strong><em>일대다 보단 다대일 매핑을 이용하자</em></strong></li></ul></li></ul><h2 id="일대일"><a href="#일대일" class="headerlink" title=" 일대일"></a><a name="subject05"></a> 일대일</h2><ul><li>일대일의 경우 외래키를 두 테이블 중 아무 테이블에 존재해도 됨</li><li>두 가지 경우<ul><li>주 테이블에 외래키<ul><li>주 객체가 대상 객체의 참조를 가지는 것</li><li><strong>객체지향 개발자 선호</strong></li><li>주 객체만 조회해도 대상 테이블 데이터 확인 가능 But 값이 없다면 외래키 Null</li></ul></li><li>대상 테이블에 외래키<ul><li>대상 객체가 주 객체의 참조(외래키)하는 것</li><li><strong>DBA 선호</strong></li><li>일대다 관계 변경 시 테이블 구조 유지 및 수월 <em>But 지연로딩 시 항상 즉시 로딩(lazy) 으로 설정</em></li></ul></li></ul></li></ul><h2 id="다대다"><a href="#다대다" class="headerlink" title=" 다대다"></a><a name="subject06"></a> 다대다</h2><p><img src="/image/jpa-association-mapping-3.png" alt="/image/jpa-association-mapping-3.png"></p><ul><li><strong>왠만하면 다대다는 설계하지 말자</strong></li><li>어쩔수 없이 한다면 중간에 조인 테이블을 설계하고 이를 엔티티로 승격하는 구조를 가져가자.</li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title=" Reference"></a><a name="reference"></a> Reference</h1><ul><li><a href="https://www.inflearn.com/course/ORM-JPA-Basic/dashboard">https://www.inflearn.com/course/ORM-JPA-Basic/dashboard</a></li></ul><hr><p>made by <em>jaejun.lee</em></p>]]></content:encoded>
      
      <comments>https://jx2lee.github.io/java-jpa_association_mapping/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[Java] JPA 필드 및 기본 키 매핑</title>
      <link>https://jx2lee.github.io/java-field_primary_key_mapping/</link>
      <guid>https://jx2lee.github.io/java-field_primary_key_mapping/</guid>
      <pubDate>Tue, 01 Jun 2021 15:00:00 GMT</pubDate>
      <description>
      
        &lt;center&gt; JPA 의 필드 및 기본 키 매핑에 대한 내용을 정리한다 ⊸ &lt;/center&gt;
&lt;br&gt;
&lt;br&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<center> JPA 의 필드 및 기본 키 매핑에 대한 내용을 정리한다 ⊸ </center><br><br><a id="more"></a><p><strong><span class="github-emoji" alias="sparkles" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png?v8">&#x2728;</span> Contents:</strong>  </p><ol><li><a href="#subject01">필드와 컬럼 매핑</a><ul><li><a href="#subject02">매핑 어노테이션 개요</a><ul><li><a href="#subject03">@Column</a></li><li><a href="#subject04">@Temporal</a></li><li><a href="#subject05">@Enumerated</a></li><li><a href="#subject06">@Lob</a></li><li><a href="#subject07">@Transient</a></li></ul></li></ul></li><li><a href="#subject08">기본 키 매핑</a><ul><li><a href="#subject09">자동 할당 전략</a><ul><li><a href="#subject10">IDENTITY</a></li><li><a href="#subject11">SEQUENCE</a></li><li><a href="#subject12">TABLE</a></li><li><a href="#subject13">권장 전략</a></li></ul></li></ul></li><li><a href="#reference">Reference</a></li></ol><h1 id="필드와-컬럼-매핑"><a href="#필드와-컬럼-매핑" class="headerlink" title=" 필드와 컬럼 매핑"></a><a name="subject01"></a> 필드와 컬럼 매핑</h1><h2 id="매핑-어노테이션-개요"><a href="#매핑-어노테이션-개요" class="headerlink" title=" 매핑 어노테이션 개요"></a><a name="subject02"></a> 매핑 어노테이션 개요</h2><p><img src="/image/field-primary-key-mapping-1.png" alt="/image/field-primary-key-mapping-1.png"></p><h3 id="Column"><a href="#Column" class="headerlink" title=" @Column"></a><a name="subject03"></a> <code>@Column</code></h3><p><img src="/image/field-primary-key-mapping-2.png" alt="/image/field-primary-key-mapping-2.png"></p><h3 id="Temporal"><a href="#Temporal" class="headerlink" title=" @Temporal"></a><a name="subject04"></a> <code>@Temporal</code></h3><ul><li>Java 8 이상에서는 LocalDate &amp; LocalDateTime 을 지원하므로 8 버젼 이상을 사용하는 사용자에게는 크게 중요하지 않음</li></ul><h3 id="Enumerated"><a href="#Enumerated" class="headerlink" title=" @Enumerated"></a><a name="subject05"></a> <code>@Enumerated</code></h3><ul><li>Java Enum type 매핑 시 사용</li><li>단, ORDINAL 사용 금지<ul><li>why? ORDINAL 을 사용할 경우 enum 순서를 DB 에 저장</li><li>만약에 enum 순서가 변경될 경우 DB Migration 이 필요함</li></ul></li></ul><h3 id="Lob"><a href="#Lob" class="headerlink" title=" @Lob"></a><a name="subject06"></a> <code>@Lob</code></h3><ul><li>CLOB, BLOB 타입과 매핑</li><li>String 만 CLOB, 나머지 타입은 BLOG 이 default</li></ul><h3 id="Transient"><a href="#Transient" class="headerlink" title=" @Transient"></a><a name="subject07"></a> <code>@Transient</code></h3><ul><li>필드 매핑을 원하지 않을 경우 사용</li><li>메모리 상에서 임시로 값을 사용할 때</li></ul><h1 id="기본-키-매핑"><a href="#기본-키-매핑" class="headerlink" title=" 기본 키 매핑"></a><a name="subject08"></a> 기본 키 매핑</h1><ul><li>두 가지 annotation 존재</li><li><code>@Id</code> : 직접 할당</li><li><code>@GeneratedValue</code> : 자동 할당</li></ul><h2 id="자동-할당-전략"><a href="#자동-할당-전략" class="headerlink" title=" 자동 할당 전략"></a><a name="subject09"></a> 자동 할당 전략</h2><h3 id="IDENTITY"><a href="#IDENTITY" class="headerlink" title=" IDENTITY"></a><a name="subject10"></a> IDENTITY</h3><ul><li>데이터베이스에 위임하는 전략 (MYSQL)<ul><li>주로 MySQL, PostgreSQL, SQL Server, DB2 에서 사용</li><li>JPA 는 트랜잭션 커밋 이후 insert query 를 날리는데, auto_increment 의 경우 데이터베이스에 저장된 이후에 id 값을 알 수 있다.</li><li><strong>조금 특이하게 em.persist 시점에 insert query 가 발생한다.</strong></li></ul></li></ul><h3 id="SEQUENCE"><a href="#SEQUENCE" class="headerlink" title=" SEQUENCE"></a><a name="subject11"></a> SEQUENCE</h3><ul><li>데이터베이스의 sequence object 이용하는 전략 (ORACLE)</li><li><code>@SequenceGenerator</code></li><li>allocationSize<ul><li>sequence 한 번 호출에 증가하는 수로 sequence 값을 1씩 증가하도록 설정하면 반드시 1로 설정한다.</li><li>성능 최적화를 위해 JPA 에서는 50 을 default 로 가져간다.<ul><li>why? sequence 를 DB 에 호출하는 횟수를 줄이기 위함인데, 너무 큰 숫자를 두게되면 메모리에 저장되어 있기 때문에 웹서버가 다운 시 비어있는 숫자가 발생한다.</li></ul></li></ul></li><li>속성값<ul><li><img src="/image/field-primary-key-mapping-3.png" alt="/image/field-primary-key-mapping-3.png"></li></ul></li></ul><h3 id="TABLE"><a href="#TABLE" class="headerlink" title=" TABLE"></a><a name="subject12"></a> TABLE</h3><ul><li>키 생성용 테이블 이용하는 전략</li><li><code>@TableGenerator</code></li><li>데이터베이스의 sequence 를 흉내내는 전략으로 모든 데이터베이스에 적용할 수 있지만 성능의 문제가 발생할 수 있음</li><li>속성값<ul><li><img src="/image/field-primary-key-mapping-4.png" alt="/image/field-primary-key-mapping-4.png"></li></ul></li></ul><h3 id="권장-전략"><a href="#권장-전략" class="headerlink" title=" 권장 전략"></a><a name="subject13"></a> 권장 전략</h3><ul><li>데이터베이스 입장에서 기본키는 null 이면 안되며 유일 및 변경되면 안됨</li><li>위 조건을 만족하는 자연키(nature key? 심지어 주민등록번호라도 기본키로 사용하면 안된다. 물론 현재에는 불가하지만)</li></ul><blockquote><p>answer: <strong><em>Long + 대체키(UUID) + 키 생성</em></strong></p></blockquote><h1 id="Reference"><a href="#Reference" class="headerlink" title=" Reference"></a><a name="reference"></a> Reference</h1><ul><li><a href="https://www.inflearn.com/course/ORM-JPA-Basic/dashboard">https://www.inflearn.com/course/ORM-JPA-Basic/dashboard</a></li></ul><hr><p>made by <em>jaejun.lee</em></p>]]></content:encoded>
      
      <comments>https://jx2lee.github.io/java-field_primary_key_mapping/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[Java] JPA 영속성 관리</title>
      <link>https://jx2lee.github.io/java-jpa_persistence_context/</link>
      <guid>https://jx2lee.github.io/java-jpa_persistence_context/</guid>
      <pubDate>Sun, 30 May 2021 15:00:00 GMT</pubDate>
      <description>
      
        &lt;center&gt; 영한님 강의를 들으며 정리한 부분을 공유한다 🏄‍ &lt;/center&gt;
&lt;br&gt;
&lt;br&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<center> 영한님 강의를 들으며 정리한 부분을 공유한다 🏄‍ </center><br><br><a id="more"></a><p><strong><span class="github-emoji" alias="sparkles" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png?v8">&#x2728;</span> Contents:</strong>  </p><ol><li><a href="#subject01">JPA</a></li><li><a href="#subject02">JPA 핵심</a></li><li><a href="#subject03">Entity 생명 주기</a></li><li><a href="#subject04">영속성 컨텍스트 feature</a></li><li><a href="#subject05">flush</a></li><li><a href="#reference">Reference</a></li></ol><h1 id="JPA"><a href="#JPA" class="headerlink" title=" JPA"></a><a name="subject01"></a> JPA</h1><blockquote><p><em>JPA(Java Persistence API, JPA)는 자바 플랫폼 SE와 자바 플랫폼 EE를 사용하는 응용프로그램에서 관계형 데이터베이스의 관리를 표현하는 자바 API이다. (by wiki)</em></p></blockquote><ul><li>ORM<ul><li>Object Relational mapping (객체 관계 매핑)</li><li>객체는 객체대로, RDB 는 RDB 대로 설계하는 것이 목적</li><li>객체와 RDB 사이에 존재하는 기술로 대부분의 언어에는 ORM 기술이 존재</li></ul></li><li>JPA 는 Hibernate (ORM framework 중 하나) 기반 Java ORM 표준<ul><li>애플리케이션과 JDBC 사이에서 동작</li><li>특정 DB 에 종속 X</li><li>Java Interface 로 구현되어 있음</li></ul></li></ul><p><img src="/image/persistence-context-1.png" alt="/image/persistence-context-1.png"></p><center>영한님 강의자료</center><h1 id="JPA-핵심"><a href="#JPA-핵심" class="headerlink" title=" JPA 핵심"></a><a name="subject02"></a> JPA 핵심</h1><ul><li>ORM</li><li>영속성 컨텍스트<ul><li>Entity 를 영구 저장하는 환경</li><li>논리적인 개념으로 눈에 보이지 않고 EntityManger 를 통해 영속성 컨텍스트에 접근할 수 있다.<ul><li>EntityManger 와 영속성 컨텍스트가 1:1 매핑</li><li>but, Spring 에서는 N:1</li></ul></li><li><code>EniityManager.persist(entity)</code></li><li>EntityManagerFactory &amp; EntityManager<ul><li><img src="/image/persistence-context-2.png" alt="/image/persistence-context-2.png"></li></ul></li></ul></li></ul><h1 id="Entity-생명-주기"><a href="#Entity-생명-주기" class="headerlink" title=" Entity 생명 주기"></a><a name="subject03"></a> Entity 생명 주기</h1><ul><li>비영속: 전혀 관계가 없음</li><li>영속: 영속성 컨텍스트에 관리되는 상태<ul><li><code>em.persist</code></li><li><img src="/image/persistence-context-3.png" alt="/image/persistence-context-3.png"></li><li><img src="/image/persistence-context-4.png" alt="/image/persistence-context-4.png"> </li></ul></li><li>준영속: 관리되었다 분리된 상태 (더이상 관여 없음 → 영속에서 비영속으로 변경)<ul><li>관리되지 않고 있기 때문에 영속성 컨텍스트가 제공하는 기능을 수행하지 못한다.</li><li><code>em.detach</code> &amp; <code>em.clear</code> &amp; <code>em.close</code></li></ul></li><li>삭제: 삭제 상태<ul><li><code>em.remove</code></li></ul></li></ul><h1 id="영속성-컨텍스트-feature"><a href="#영속성-컨텍스트-feature" class="headerlink" title=" 영속성 컨텍스트 feature"></a><a name="subject04"></a> 영속성 컨텍스트 feature</h1><ul><li>1차 캐시</li><li>동일성(identity) 보장  <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Member a = em.find(Member.class, <span class="string">&quot;member1&quot;</span>);</span><br><span class="line">Member b = em.find(Member.class, <span class="string">&quot;member1&quot;</span>);</span><br><span class="line">System.out.println(a == b) <span class="comment">// true</span></span><br></pre></td></tr></table></figure></li><li>쓰기 지연(transactional write-behind)<ul><li><img src="/image/persistence-context-5.png" alt="/image/persistence-context-5.png"></li><li>commit 시 쓰기 지연 SQL 저장소에 있는 update query 가 DB 로 전송 및 수행</li></ul></li><li>변경 감지(dirty checking)<ul><li>1차 캐시에 snapshot 이 존재하여 entity 와 비교해 변경된 부분을 감지하여 update query 를 쓰기 지연 SQL 저장소에 저장</li><li><img src="/image/persistence-context-6.png" alt="/image/persistence-context-6.png"></li></ul></li><li>지연 로딩(lazy loading)</li></ul><h1 id="flush"><a href="#flush" class="headerlink" title=" flush"></a><a name="subject05"></a> flush</h1><ul><li>영속성 컨텍스트의 변경내용을 DB에 반영 (<code>동기화</code>)</li><li>em.flush 가 호출되면 쓰기 지연 SQL 저장소의 udpate query 를 몽땅 DB 에 반영</li><li>flush 방법 3가지<ul><li>em.flush</li><li>Transaction commit</li><li>JPQL 실행<ul><li>why? 영속된 Entity 를 만약에 호출하는 경우 이는 DB 에 반영되어 있지 않기 때문에 내부적으로 JPQL 이 호출되면 flush 를 통해 영속성 컨텍스트의 Entity 를 DB 에 저장</li></ul></li></ul></li><li>flush 는 영속성 컨텍스트를 비우지 않는다.</li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title=" Reference"></a><a name="reference"></a> Reference</h1><ul><li><a href="https://www.inflearn.com/course/ORM-JPA-Basic/dashboard">https://www.inflearn.com/course/ORM-JPA-Basic/dashboard</a></li></ul><hr><p>made by <em>jaejun.lee</em></p>]]></content:encoded>
      
      <comments>https://jx2lee.github.io/java-jpa_persistence_context/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[Java] Singleton with java</title>
      <link>https://jx2lee.github.io/java-singleton/</link>
      <guid>https://jx2lee.github.io/java-singleton/</guid>
      <pubDate>Wed, 19 May 2021 15:00:00 GMT</pubDate>
      <description>
      
        &lt;center&gt; Singleton 패턴에 대한 내용을 살펴본다 🏠 &lt;/center&gt;

&lt;br&gt;
&lt;br&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<center> Singleton 패턴에 대한 내용을 살펴본다 🏠 </center><br><br><a id="more"></a><p><strong><span class="github-emoji" alias="sparkles" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png?v8">&#x2728;</span> Contents:</strong>  </p><ol><li><a href="#subject01">Singleton?</a></li><li><a href="#subject02">Why use Singleton?</a></li><li><a href="#subject03">Singleton 구현 종류</a><ul><li><a href="#subject04">Eager initialization</a></li><li><a href="#subject05">Static block initialization</a></li><li><a href="#subject06">Lazy initialization</a></li><li><a href="#subject07">Thread safe initialization</a></li><li><a href="#subject08">Double-Checked Locking</a></li><li><a href="#subject09">Bill Pugh Solution</a></li></ul></li><li><a href="#reference">Reference</a></li></ol><h1 id="Singleton"><a href="#Singleton" class="headerlink" title=" Singleton?"></a><a name="subject01"></a> Singleton?</h1><p>class 의 Instance 를 하나만 생성하고 어디서든 그 인스턴스를 참조할 수 있도록 하는 패턴으로, 생성자를 여러 번 호출하더라도 실제로 생성하는 객체는 <code>단 하나</code></p><h1 id="Why-use-Singleton"><a href="#Why-use-Singleton" class="headerlink" title=" Why use Singleton?"></a><a name="subject02"></a> Why use Singleton?</h1><ol><li>고정된 메모리 영역을 가지고 하나의 Instance 만 사용하기 때문에 메모리 낭비 방지</li><li><code>전역</code> 이므로 다른 class 의 instance 들이 데이터를 공유하기 수월</li><li>DBCP(Database Connection Pool)과 같이 객체를 여러 개 생성해야 하는 경우 많이 사용</li></ol><h2 id="Singleton-구현-종류"><a href="#Singleton-구현-종류" class="headerlink" title=" Singleton 구현 종류"></a><a name="subject03"></a> Singleton 구현 종류</h2><p>점점 발전된 형태의 singleton 구현 방법을 소개한다. (차근차근 효율적인 구조 제시)</p><h2 id="Eager-initialization"><a href="#Eager-initialization" class="headerlink" title=" Eager initialization"></a><a name="subject04"></a> Eager initialization</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Eager initialization</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SingletonService</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> SingletonService instance = <span class="keyword">new</span> SingletonService();</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="title">SingletonService</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> SingletonService <span class="title">getInstance</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> instance;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>가장 빠르게 구현이 가능함</li><li>빠르다<ul><li><em>클라이언트에서 사용하지 않더라도 static 이므로 인스턴스가 항상 생성됨</em></li><li><em>예외 처리가 불가능</em></li></ul></li></ul><h2 id="Static-block-initialization"><a href="#Static-block-initialization" class="headerlink" title=" Static block initialization"></a><a name="subject05"></a> Static block initialization</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Static block initialization</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SingletonService</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> SingletonService instance;</span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="title">SingletonService</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">static</span>&#123;</span><br><span class="line">        <span class="keyword">try</span>&#123;</span><br><span class="line">            instance = <span class="keyword">new</span> SingletonService();</span><br><span class="line">        &#125;<span class="keyword">catch</span>(Exception e)&#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> RuntimeException(<span class="string">&quot;싱글톤 객체 생성 오류&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> SingletonService <span class="title">getInstance</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> instance;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><ul><li>Eager initialization 과 유사하지만 예외 처리가 가능하다.<ul><li><em>여전히 초기화가 바로 발생</em></li></ul></li></ul><h2 id="Lazy-initialization"><a href="#Lazy-initialization" class="headerlink" title=" Lazy initialization"></a><a name="subject06"></a> Lazy initialization</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Lazy initialization</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SingletonService</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> SingletonService instance;</span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="title">SingletonService</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> SingletonService <span class="title">getInstance</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(Objects.isNull(instance)) &#123;</span><br><span class="line">            instance = <span class="keyword">new</span> SingletonService();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> instance;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">logic</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;singleton 객체 로직 호출&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>getInstance() 호출에 초기화 진행</li><li>Eager &amp; static block initializaion 단점을 보완<ul><li><em>not thread safe!</em></li></ul></li></ul><h2 id="Thread-safe-initialization"><a href="#Thread-safe-initialization" class="headerlink" title=" Thread safe initialization"></a><a name="subject07"></a> Thread safe initialization</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Thread safe initialization</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SingletonService</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> SingletonService instance;</span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="title">SingletonService</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">synchronized</span> SingletonService <span class="title">getInstance</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(instance == <span class="keyword">null</span>) &#123;</span><br><span class="line">            instance = <span class="keyword">new</span> SingletonService();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> instance;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">logic</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;singleton 객체 로직 호출&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>동시의 thread 가 접근하지 못하게 <code>synchronized</code></li><li>thread 접근 → 나머지 thread 대기<ul><li><em>하나의 thread 만 접근 가능하므로 성능 저하</em></li></ul></li></ul><h2 id="Double-Checked-Locking"><a href="#Double-Checked-Locking" class="headerlink" title=" Double-Checked Locking"></a><a name="subject08"></a> Double-Checked Locking</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Double-Checked Locking</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SingletonService</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> SingletonService instance;</span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="title">SingletonService</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> SingletonService <span class="title">getInstance</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(Objects.isNull(instance)) &#123;</span><br><span class="line">            <span class="keyword">synchronized</span> (SingletonService.class) &#123;</span><br><span class="line">                <span class="keyword">if</span>(Objects.isNull(instance)) &#123;</span><br><span class="line">                    instance = <span class="keyword">new</span> SingletonService();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> instance;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">logic</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;singleton 객체 로직 호출&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>Thread safe initialization 의 성능을 개선</li><li>twice null check<ul><li><em>코드가 복잡</em></li></ul></li></ul><h2 id="Bill-Pugh-Solution"><a href="#Bill-Pugh-Solution" class="headerlink" title=" Bill Pugh Solution"></a><a name="subject09"></a> Bill Pugh Solution</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Bill Pugh Solution</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SingletonService</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="title">SingletonService</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">SingletonServiceHelper</span></span>&#123;</span><br><span class="line">        <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> SingletonService INSTANCE = <span class="keyword">new</span> SingletonService();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> SingletonService <span class="title">getInstance</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> SingletonServiceHelper.INSTANCE;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">logic</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;singleton 객체 로직 호출&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>helper 함수를 이용해 static final 로 인스턴스 생성</li><li>Double Checked 비해 간단</li><li>Lazy Loading &amp; Thread safe</li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title=" Reference"></a><a name="reference"></a> Reference</h1><ul><li><a href="https://www.youtube.com/watch?v=C6CczyrkYXU">https://www.youtube.com/watch?v=C6CczyrkYXU</a></li></ul><hr><p>made by <em>jaejun.lee</em></p>]]></content:encoded>
      
      <comments>https://jx2lee.github.io/java-singleton/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[Kafka] 모든 broker 다운 시 복구 시나리오</title>
      <link>https://jx2lee.github.io/kafka-restore_broker/</link>
      <guid>https://jx2lee.github.io/kafka-restore_broker/</guid>
      <pubDate>Fri, 14 May 2021 15:00:00 GMT</pubDate>
      <description>
      
        &lt;center&gt; Kafka 는 partition replication 기능을 제공하여 하나의 broker 가 다운되도 topic 을 유지할 수 있는데 최악의 상황인 전 broker 가 다운되었을 때 복구 전략이 무엇인지 확인한다 😌 &lt;/center&gt;
&lt;br&gt;
&lt;br&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<center> Kafka 는 partition replication 기능을 제공하여 하나의 broker 가 다운되도 topic 을 유지할 수 있는데 최악의 상황인 전 broker 가 다운되었을 때 복구 전략이 무엇인지 확인한다 😌 </center><br><br><a id="more"></a><p><strong><span class="github-emoji" alias="sparkles" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png?v8">&#x2728;</span> Contents:</strong>  </p><ol><li><a href="#subject01">그s럴 일은 업겠지만 대비해야지</a><ul><li><a href="#subject02">마지막으로 장애가 난 리더가 살아나길 기다린다</a></li><li><a href="#subject03">장애 상관없이 먼저 복구된 리더 또는 팔로워(브로커) 가 자동으로 리더가 된다</a></li></ul></li><li><a href="#subject04">그럼 무슨 전략이 좋을까요?</a></li><li><a href="#reference">Reference</a></li></ol><h1 id="그럴-일은-업겠지만-대비해야지"><a href="#그럴-일은-업겠지만-대비해야지" class="headerlink" title=" 그럴 일은 업겠지만 대비해야지"></a><a name="subject01"></a> 그럴 일은 업겠지만 대비해야지</h1><p>재수가 없다면 모든 broker 장애가 발생할 수 있다. (딱히 들 수 있는 얘가 없지만..) 이럴 때 kafka 에서 제공하는 복구 전략이 무엇인 지 살펴본다.</p><p>크게 복구 전략은 2가지로 다음과 같다.</p><ul><li>마지막으로 장애가 난 리더(브로커)가 살아나길 기다린다.</li><li>장애 상관없이 먼저 복구된 리더 또는 팔로워(브로커) 가 자동으로 리더가 된다.</li></ul><p>각 전략은 장단점이 존재한다.</p><h2 id="마지막으로-장애가-난-리더가-살아나길-기다린다"><a href="#마지막으로-장애가-난-리더가-살아나길-기다린다" class="headerlink" title=" 마지막으로 장애가 난 리더가 살아나길 기다린다"></a><a name="subject02"></a> 마지막으로 장애가 난 리더가 살아나길 기다린다</h2><ul><li>3개 broker 로 예를 들때, broker01 → broker02 → broker03 순으로 장애가 낫다고 가정하자</li><li>그럼 가장 최근 장애가 난 broker03 의 파티션이 손실이 가장 적다.</li><li>이렇게 메시지 손실이 가장 적은 broker 가 기동되기를 기다리는 전략</li><li>장점: 메세지 손실이 적다</li><li>단점: 장애시간이 길어질 수 있다.(만약 broker03 의 해결이 늦어지면 결국 해당 토픽은 정상화되기까지 시간이 오래 걸린다.)</li></ul><h2 id="장애-상관없이-먼저-복구된-리더-또는-팔로워-브로커-가-자동으로-리더가-된다"><a href="#장애-상관없이-먼저-복구된-리더-또는-팔로워-브로커-가-자동으로-리더가-된다" class="headerlink" title=" 장애 상관없이 먼저 복구된 리더 또는 팔로워(브로커) 가 자동으로 리더가 된다."></a><a name="subject03"></a> 장애 상관없이 먼저 복구된 리더 또는 팔로워(브로커) 가 자동으로 리더가 된다.</h2><ul><li>최근이건 나중이건 먼저 살아나는 브로커의 파티션이(리더 or 팔로워) 자동으로 리더로 승격하는 전략</li><li>장점: 장애시간을 단축할 수 있다.</li><li>단점: 메세지 손실이 발생할 수 있다.<ul><li>만약 모든 브로커가 동시 다운이 된 것이 아니라면</li><li>먼저 다운된 브로커가 모든 브로커 장애 이후 복구가 되면 리더로 승격되어 나머지 복구되는 팔로워들과 싱크 작업을 진행하고</li><li>이는 가장 최근에 장애 난 브로커의 리더 파티션의 메세지도 동기화 되기 때문에</li><li>메세지 손실이 발생한다.</li></ul></li></ul><h1 id="그럼-무슨-전략이-좋을까요"><a href="#그럼-무슨-전략이-좋을까요" class="headerlink" title=" 그럼 무슨 전략이 좋을까요?"></a><a name="subject04"></a> 그럼 무슨 전략이 좋을까요?</h1><p>정답은 없다. 단, kafka 0.11.0.0 이하 버전에서는 두 번째 전략을 default 로 가져가지만 이후 버젼에서는 첫 번째 전략을 default 를 가져간다. 물론, 해당 전략을 변경할 수 있다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#server.properties</span></span><br><span class="line">unclean.leader.election.enable=&#123;true_or_false&#125;</span><br></pre></td></tr></table></figure><ul><li>false: 첫 번째 전략</li><li>true: 두 번재 전략</li></ul><p>현재 내가 맡고 있는 데이터플랫폼의 경우 전략은 <code>false</code> 이다. 로그를 수집하는 영역에서 메세지 손실을 최소화 하는게 목표이고 최대한 kafka(0.10.2) 에서 권장하는 옵션(<code>default=false</code>)을 따라간 것으로 보인다. Kafka 의 고가용성 &amp; 손실 최소화를 위해 첫 번째 전략으로 가져가는 게 맞다고 본다. (물론, 저장하지 않고 크게 중요하지 않은 데이터라면 두 번째 전략을 가져가면 될 것 같고 <code>상황에 따라 다르다</code> 가 정답인 것 같다.)</p><h1 id="Reference"><a href="#Reference" class="headerlink" title=" Reference"></a><a name="reference"></a> Reference</h1><ul><li>카프카, 데이터플랫폼의 최강자</li></ul><hr><p>made by <em>jaejun.lee</em></p>]]></content:encoded>
      
      <comments>https://jx2lee.github.io/kafka-restore_broker/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[Kafka] Zookeeper Cluster 세팅</title>
      <link>https://jx2lee.github.io/kafka-set_zookeeper_cluster/</link>
      <guid>https://jx2lee.github.io/kafka-set_zookeeper_cluster/</guid>
      <pubDate>Mon, 10 May 2021 15:00:00 GMT</pubDate>
      <description>
      
        &lt;center&gt; NHN Cloud 인스턴스를 이용해 zookeeper 클러스터(3-node)를 구축한다 🐒 &lt;/center&gt;
&lt;br&gt;
&lt;br&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<center> NHN Cloud 인스턴스를 이용해 zookeeper 클러스터(3-node)를 구축한다 🐒 </center><br><br><a id="more"></a><p><strong><span class="github-emoji" alias="sparkles" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png?v8">&#x2728;</span> Contents:</strong>  </p><ol><li><a href="#subject01">패키지 설정 및 binary 설정</a></li><li><a href="#subject02">Zookeeper service 실행</a></li><li><a href="#reference">Reference</a></li></ol><h1 id="패키지-설정-및-binary-설정"><a href="#패키지-설정-및-binary-설정" class="headerlink" title=" 패키지 설정 및 binary 설정"></a><a name="subject01"></a> 패키지 설정 및 binary 설정</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ sudo yum install -y java-1.8.0-openjdk</span><br><span class="line">$ wget https://archive.apache.org/dist/zookeeper/zookeeper-3.4.10/zookeeper-3.4.10.tar.gz</span><br></pre></td></tr></table></figure><ul><li><p>symbolic link</p><ul><li><p>계정의 default home 에 생성</p></li><li><p><code>ln -s /app/zookeeper-3.4.10 zookeeper</code></p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[centos@zookeeper-1 apps]$ ll</span><br><span class="line">합계 0</span><br><span class="line">lrwxrwxrwx 1 centos centos 21  5월  7 14:52 zookeeper -&gt; /app/zookeeper-3.4.10</span><br></pre></td></tr></table></figure></li></ul></li><li><p>data 경로 지정</p><ul><li><p>/data</p></li><li><p><code>myid</code> 파일을 생성해 각 id 번호를 작성</p></li><li><p>해당 서버를 이미지로 만들어 2개 인스턴스를 생성할 계획이기 때문에 나는 우선 1까지만 생성하였다.(<em>1번 인스턴스 이미지를 생성할 예정</em>) 3개의 서버를 동시에 진행하는 분들은 각 서버에 맞게 myid 파일을 1,2,3 순으로 작성한다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[centos@zookeeper-1 data]$ <span class="built_in">pwd</span></span><br><span class="line">/data</span><br><span class="line">[centos@zookeeper-1 data]$ cat myid</span><br><span class="line">1</span><br></pre></td></tr></table></figure></li></ul></li><li><p>zoo.cfg 설정</p><ul><li><p>binary 로 설치한 경우 <code>conf/zoo_sample.cfg</code> 를 수정한다.</p></li><li><p><code>vi conf/zoo.cfg</code></p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">[centos@zookeeper-1 zookeeper]$ cat conf/zoo.cfg</span><br><span class="line"><span class="comment"># The number of milliseconds of each tick</span></span><br><span class="line"><span class="comment"># 기본 측정 시간</span></span><br><span class="line">tickTime=2000</span><br><span class="line"><span class="comment"># The number of ticks that the initial</span></span><br><span class="line"><span class="comment"># synchronization phase can take</span></span><br><span class="line"><span class="comment"># 팔로워가 리더와 초기에 연결하는 시간에 대한 타임아웃 시간</span></span><br><span class="line">initLimit=10</span><br><span class="line"><span class="comment"># The number of ticks that can pass between</span></span><br><span class="line"><span class="comment"># sending a request and getting an acknowledgement</span></span><br><span class="line"><span class="comment"># 팔로워가 리더와 동기화 하는 시간에 대한 타임 아웃 티켓 수(주키퍼 데이터가 크면 더 늘려야함)</span></span><br><span class="line">syncLimit=5</span><br><span class="line"><span class="comment"># the directory where the snapshot is stored.</span></span><br><span class="line"><span class="comment"># do not use /tmp for storage, /tmp here is just</span></span><br><span class="line"><span class="comment"># example sakes.</span></span><br><span class="line"><span class="comment"># 트랜잭션 로그와 스냅샷이 저장되는 경로</span></span><br><span class="line">dataDir=/data</span><br><span class="line"><span class="comment"># the port at which the clients will connect</span></span><br><span class="line"><span class="comment"># TCP port</span></span><br><span class="line">clientPort=2181</span><br><span class="line"><span class="comment"># 주키퍼 앙상블 구성을 위한 서버 설정, 각 넘버는 myid 값과 일치</span></span><br><span class="line">server.1=zookeeper-1:2888:3888</span><br><span class="line">server.2=zookeeper-2:2888:3888</span><br><span class="line">server.3=zookeeper-3:2888:3888</span><br></pre></td></tr></table></figure></li></ul></li><li><p>systemd 등록</p><ul><li><p>binary 로 실행할 수 있지만 systemd 로 관리하기 위해 service 등록</p></li><li><p><code>sudo vi /etc/systemd/system/zookeeper-server.service</code></p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[Unit]</span><br><span class="line">Description=zookeeper-server</span><br><span class="line">After=network.target</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">Type=forking</span><br><span class="line">User=root</span><br><span class="line">Group=root</span><br><span class="line">SyslogIdentifier=zookeeper-server</span><br><span class="line">WorkingDirectory=/home/centos/apps/zookeeper</span><br><span class="line">Restart=always</span><br><span class="line">RestartSec=0s</span><br><span class="line">ExecStart=/home/centos/apps/zookeeper/bin/zkServer.sh start</span><br><span class="line">ExecStop=/home/centos/apps/zookeeper/bin/zkServer.sh stop</span><br></pre></td></tr></table></figure></li></ul></li></ul><h1 id="Zookeeper-service-실행"><a href="#Zookeeper-service-실행" class="headerlink" title=" Zookeeper service 실행"></a><a name="subject02"></a> Zookeeper service 실행</h1><p>각 서버(3대)에서 zookeeper 서비스를 systemd 로 실행한다. 아래와 같이 3개 서비스 상태를 확인했을 때 초록불이면 성공이다.</p><p><img src="/image/kafka-set_zookeeper_cluster-1.png" alt="/image/kafka-set_zookeeper_cluster-1.png"></p><h1 id="Reference"><a href="#Reference" class="headerlink" title=" Reference"></a><a name="reference"></a> Reference</h1><ul><li>카프카, 데이터플랫폼의 최강자</li></ul><hr><p>made by <em>jaejun.lee</em></p>]]></content:encoded>
      
      <comments>https://jx2lee.github.io/kafka-set_zookeeper_cluster/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[Kafka] Topic &amp; Partition</title>
      <link>https://jx2lee.github.io/kafka-topic_and_partition/</link>
      <guid>https://jx2lee.github.io/kafka-topic_and_partition/</guid>
      <pubDate>Mon, 10 May 2021 15:00:00 GMT</pubDate>
      <description>
      
        &lt;center&gt; Kafka 데이터 모델의 핵심 요소인 topic 과 partition 에 대해 살펴보자 〽 &lt;/center&gt;
&lt;br&gt;
&lt;br&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<center> Kafka 데이터 모델의 핵심 요소인 topic 과 partition 에 대해 살펴보자 〽 </center><br><br><a id="more"></a><p><strong><span class="github-emoji" alias="sparkles" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png?v8">&#x2728;</span> Contents:</strong>  </p><ol><li><a href="#subject01">Topic</a></li><li><a href="#subject02">Partition</a><ul><li><a href="#subject03">그럼 Partition 은 무작정 늘리는 게 좋을까요?</a></li><li><a href="#subject03">Topic 의 적절한 Partition 수는..</a></li></ul></li><li><a href="#reference">Reference</a></li></ol><ul><li>Topic: 메세지를 consumer 가 나눠가질 수 있도록 논리적 단위로 묶은 개념</li><li>Partition: Topic 을 구성하는 데잍 저장소로 수평 확장이 가능한 단위</li></ul><h1 id="Topic"><a href="#Topic" class="headerlink" title=" Topic"></a><a name="subject01"></a> Topic</h1><ul><li>기본적으로 Topic 에 메세지를 저장</li><li>mail 시스템과 함께 생각해보면 다음과 같다.<ul><li>메일을 보내려면 메일 수신자, 즉 이메일 주소가 필요하다.</li><li>이메일 주소 없이 메일을 발송할 수 있는가? → 없다!</li><li>이메일 주소 == Kafka Topic 이라고 생각하면 이해하기 쉽다.</li></ul></li><li>메세지를 구분하기 위한 단위로 Topic 을 사용</li><li>Topic 명에 prefix 를 통해 메세지 특성에 따라 분류 가능함</li></ul><h1 id="Partition"><a href="#Partition" class="headerlink" title=" Partition"></a><a name="subject02"></a> Partition</h1><ul><li>Topic 을 분할한 것</li><li>왜 분할을 해야 하나?<ul><li>메시징 큐 시스템의 제약조건은 <code>순서가 보장된다.</code></li><li>1개 메세지를 보내는데 1초가 걸린다면, 4개 메세지를 파티션 1개로 유지하며 전송하면 총 4초가 걸림 (<code>순서 보장</code>)</li><li>따라서 효율적인 메세지 전송을 위해 topic 의 partition 을 4개로 구성하면 병렬 처리로 메세지를 전송할 수 있음</li><li>ex. 메세지 1 → partition 1, 메세지 2 → partition 2 … 메세지 4 → partition 4</li></ul></li></ul><h2 id="그럼-Partition-은-무작정-늘리는-게-좋을까요"><a href="#그럼-Partition-은-무작정-늘리는-게-좋을까요" class="headerlink" title=" 그럼 Partition 은 무작정 늘리는 게 좋을까요?"></a><a name="subject03"></a> 그럼 Partition 은 무작정 늘리는 게 좋을까요?</h2><p>라는 질문이 나올 수 있다. 무작정 늘리는 거는 단어부터 거슬린다. 즉, 무조건 partition 을 늘린다고 만능은 아니다. partition 이 늘어나면 발생할 수 있는 문제를 살펴보면 다음과 같다.</p><ul><li>File Handler (리소스 낭비)<ul><li>각 partition 은 broker 의 디렉토리와 매핑, 저장되는 데이터 마다 2개의 파일 (index data + 실제 데이터)이 존재</li><li>모든 디렉토리의 파일들에 대해 file handler 오픈</li><li><strong>결국 리소스 낭비로 이어지게 될 수 있다</strong></li></ul></li><li>장애 복구 시간 증가<ul><li>kafka 는 가용성 확보를 위해 replication 지원</li><li>각 partition 마다 replication 이 동작하여 leader 와 follower 로 구분</li><li>replication 은 topic 을 복제하는 것이 아닌 <code>partition 을 복제하는 것</code></li><li>장애 복구 시나리오는 다음과 같다.<ul><li>broker 다운 → 해당 broker 의 leader partition 이용 불가능 → follower 중 leader 로 선출 → 클라이언트 요청을 새롭게 leader 가 처리</li><li>장애 처리는 controller 로 지정된 broker 가 진행</li><li>controller 는 kafka 클러스터 중 하나만 존재하며 만약 해당 controller 가 다운되면 나머지 broker 중 하나가 controller 역할을 대신 수행한다.</li><li><em>만약 broker 에 1000개 partition 과 2개 replication 이 다운되면 해당 broker 의 1000개 partition 을 사용할 수 없다. partition leader 를 다른 broker 로 이동해야 하는데, 각 partition 별 leader 선출 시간이 2ms 이라면 총 5초가 소요되며 장애시간은 5초 이상이 될 수 있다.</em></li><li><em>만약 해당 broker 가 controller 라면..? 최악이다. 자동으로 fail over 를 시작하지만 초기화 하는 동안 zookeeper 에서 모든 partition 의 데이터를 읽어야 한다. 10,000 개 partition 이 존재하면 20초 이상 장애 상황이 발생한다.</em></li></ul></li></ul></li></ul><blockquote><p><em>replication 에 대해 학습하려면 다음 url 을 추천한다. (<a href="https://www.popit.kr/kafka-%EC%9A%B4%EC%98%81%EC%9E%90%EA%B0%80-%EB%A7%90%ED%95%98%EB%8A%94-topic-replication/">https://www.popit.kr/kafka-운영자가-말하는-topic-replication/</a>)</em></p></blockquote><p>이처럼 partition 이 많다고 만능은 아니다. 그러면 최적의 partition 수는 어떻게 설정해야 할까?</p><h2 id="Topic-의-적절한-Partition-수는"><a href="#Topic-의-적절한-Partition-수는" class="headerlink" title=" Topic 의 적절한 Partition 수는.."></a><a name="subject04"></a> Topic 의 적절한 Partition 수는..</h2><ul><li><code>원하는 목표 처리량의 기준을 잡는다.</code></li><li>다음 예를 확인해본다.<ul><li>4개의 producer 가 초당 10개 메세지를 보낸다면</li><li><strong>초당 40개 메세지를 처리해야한다.</strong></li><li>partition 1개당 초당 10개를 처리한다고 하면 partition 수를 4개로 늘려 목표 처리량을 달성할 수 있도록 partition 수를 조정한다.</li><li>producer 말고 consumer 의 상황도 고려해야 한다.</li><li>8개 consumer 가 초당 5개 메세지를 소비한다면 partition 수를 consumer 수에 맞게 8개로 설정해야한다. (<code>producer 의 상황만 고려하지 않고 partition 을 최대로 맞출 수 있는 갯수로 맞춘다</code>)</li></ul></li><li>partition 증가는 아무때나 변경 가능하지만 축소는 삭제 말고는 방법이 없기 때문에 초기 설계가 굉장히 중요하다.<ul><li><strong>축소를 원하면 topic 을 삭제하고 재 생성해야한다.</strong></li></ul></li><li>1개 broker 당 최대 partition 수는 2000 개로 권장하고 있다.</li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title=" Reference"></a><a name="reference"></a> Reference</h1><ul><li>카프카, 데이터플랫폼의 최강자</li></ul><hr><p>made by <em>jaejun.lee</em></p>]]></content:encoded>
      
      <comments>https://jx2lee.github.io/kafka-topic_and_partition/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[Kafka] Kafka 디자인 특징</title>
      <link>https://jx2lee.github.io/kafka-kafka_design_pattern/</link>
      <guid>https://jx2lee.github.io/kafka-kafka_design_pattern/</guid>
      <pubDate>Sun, 09 May 2021 15:00:00 GMT</pubDate>
      <description>
      
        &lt;center&gt; Kafka 디자인 특징을 살펴본다. 🖼 &lt;/center&gt;
&lt;br&gt;
&lt;br&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<center> Kafka 디자인 특징을 살펴본다. 🖼 </center><br><br><a id="more"></a><p><strong><span class="github-emoji" alias="sparkles" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png?v8">&#x2728;</span> Contents:</strong>  </p><ol><li><a href="#subject01">분산 시스템</a></li><li><a href="#subject02">페이지 캐시</a></li><li><a href="#subject03">배치 전송 처리</a></li><li><a href="#reference">Reference</a></li></ol><h1 id="분산-시스템"><a href="#분산-시스템" class="headerlink" title=" 분산 시스템"></a><a name="subject01"></a> 분산 시스템</h1><ul><li>분산 시스템 정의: 네트워크로 이루어진 컴퓨터들의 그룹<ul><li>시스템 전체가 공통의 목표</li><li>단일 시스템 보다 더 많은 성능을 낼 수 있음</li><li>서버 장애 시 다른 서버가 일을 대신 처리</li><li>시스템 확장 용이 (축소도 가능)</li></ul></li><li>Kafka broker 수 측정은 해당 시스템을 잘 파악하여 유동적으로 늘려야 함</li><li>링크드인은 사용량이 가장 높은 클러스터의 경우 60개 broker 를 운영한다고 함 (apache kafka 공식 Document)</li></ul><h1 id="페이지-캐시"><a href="#페이지-캐시" class="headerlink" title=" 페이지 캐시"></a><a name="subject02"></a> 페이지 캐시</h1><p>Kafka 는 메세지 처리량을 높이기 위해 페이지 캐싱을 이용하였다.</p><ul><li>OS 관점에서 물리적 메모리에 애플리케이션이 사용하고 남은 잔여 메모리 부분을 페이지 캐시로 유지해 OS 성능을 향상시킴</li><li>빠른 액세스를 위해 kafka 는 <code>페이지 캐시</code> 를 이용하도록 설계<ul><li>페이지 캐시에 대한 내용은 위키피디아 참고</li></ul></li><li>kafka broker 의 디스크를 SATA 로 사용해도 무방하다고 한다 (공식 Document)</li><li>JVM Heap 여유를 위해 kafka broker 는 5GB 힙 메모리면 충분하다고 한다.<ul><li>여유 메모리는 페이지 캐시로 사용하길 권장한다.</li><li>추가로 kafka broker 내 다른 java application 을 실행하지 않는 것을 권장한다.</li></ul></li></ul><p><img src="/image/kafka-kafka_design_pattern-1.png" alt="https://needjarvis.tistory.com/602"></p><blockquote><p>페이지 캐시를 검색하던 중 버퍼캐시와 함께 설명한 부분이 딱 와닿았다. 버퍼 캐시는 파일 시스템의 메타 데이터 관련 블록들을 저장한 캐시이며, 페이지 캐시는 파일의 내용을 저장하고 있는 캐시이다. (<a href="https://brunch.co.kr/@alden/25"><em>https://brunch.co.kr/@alden/25</em></a>)</p></blockquote><h1 id="배치-전송-처리"><a href="#배치-전송-처리" class="headerlink" title=" 배치 전송 처리"></a><a name="subject03"></a> 배치 전송 처리</h1><p>Kafka 는 메시지를 1개 마다 전송할 경우 발생하는 네트워크 오버헤드를 방지하기 위해 <strong>작은 메세지들을 묶어 한 번에 전송</strong>하는 <code>배치 전송 처리</code> 기능을 지원한다.</p><p><img src="/image/kafka-kafka_design_pattern-2.png" alt="/image/kafka-kafka_design_pattern-2.png"> </p><h1 id="Reference"><a href="#Reference" class="headerlink" title=" Reference"></a><a name="reference"></a> Reference</h1><ul><li>카프카, 데이터 플랫폼의 최강자</li><li><a href="https://needjarvis.tistory.com/602">https://needjarvis.tistory.com/602</a></li></ul><hr><p>made by <em>jaejun.lee</em></p>]]></content:encoded>
      
      <comments>https://jx2lee.github.io/kafka-kafka_design_pattern/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[ElasticSearch] Shard 와 Segment</title>
      <link>https://jx2lee.github.io/es-shard_segment/</link>
      <guid>https://jx2lee.github.io/es-shard_segment/</guid>
      <pubDate>Wed, 05 May 2021 15:00:00 GMT</pubDate>
      <description>
      
        &lt;center&gt;shard 는 elasticsearch 의 가장 기본적인 개념으로 머릿속에 저장하기 위해 기록을 남긴다. 💾&lt;/center&gt;
&lt;br&gt;
&lt;br&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<center>shard 는 elasticsearch 의 가장 기본적인 개념으로 머릿속에 저장하기 위해 기록을 남긴다. 💾</center><br><br><a id="more"></a><p><strong><span class="github-emoji" alias="sparkles" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png?v8">&#x2728;</span> Contents:</strong>  </p><ol><li><a href="#subject01">정의</a><ul><li><a href="#subject02">Shard</a></li><li><a href="#subject03">Segment</a></li></ul></li><li><a href="#subject04">데이터가 저장되는 방식</a></li><li><a href="#subject05">documnet 가 upadte 되는 과정</a><ul><li><a href="#subject06">그럼 계속해서 segment 를 생성할텐데..?</a></li></ul></li><li><a href="#reference">Reference</a></li></ol><h1 id="정의"><a href="#정의" class="headerlink" title=" 정의"></a><a name="subject01"></a> 정의</h1><p><img align="left" src="/image/es-shard_segment-1.png"><br><br><br><br><br><br></p><h2 id="Shard"><a href="#Shard" class="headerlink" title=" Shard"></a><a name="subject02"></a> Shard</h2><ul><li>index 에 저장되는 논리적인 공간</li><li>하나의 Index 는 다수의 shard 를 가지고 있음</li><li>shard 는 1개 이상의 segment 로 구성<ul><li>shard 마다 segment 수는 상이할 수 있음</li></ul></li></ul><h2 id="Segment"><a href="#Segment" class="headerlink" title=" Segment"></a><a name="subject03"></a> Segment</h2><ul><li>shard 의 데이터들이 가지고 있는 물리적인 파일</li><li>불변(immutable)</li></ul><h1 id="데이터가-저장되는-방식"><a href="#데이터가-저장되는-방식" class="headerlink" title=" 데이터가 저장되는 방식"></a><a name="subject04"></a> 데이터가 저장되는 방식</h1><p><img src="/image/es-shard_segment-2.png" alt="shard"></p><ul><li>index 를 shard 로 나누고 데이터 노드에 shard 를 할당</li><li>각 shard 에 문서를 저장</li><li>shard: primary shard &amp; replica shard<ul><li>primary shard: 원본 문서</li><li>replica shard: 원본 문서가 유실되지 않기 위해 복사한 문서</li><li>replica shard 를 통해 데이터 안정성 보장</li><li>primary shard 는 최초 index 생성 시 개수를 결정하는데 이는 설정 이후 변경이 불가하다. 따라서 신중하게 결정해야함</li></ul></li><li>해시 알고리즘을 통해 문서는 shard 들에 분산 저장되고 실제는 segment 라는 실제 물리적 파일에 저장</li><li>색인(indexing)된 문서는 먼저 memory buffer cache 에 저장<ul><li>이후 refresh 를 통해 디스크에 segment 단위로 문서가 저장되고 이때부터 검색이 가능하다.</li></ul></li></ul><h1 id="documnet-가-upadte-되는-과정"><a href="#documnet-가-upadte-되는-과정" class="headerlink" title=" documnet 가 upadte 되는 과정"></a><a name="subject05"></a> documnet 가 upadte 되는 과정</h1><ol><li>document 업데이트를 시도하면 elasticsearch 는 새로운 segment 에 업데이트 된 document 를 저장</li><li>기존 데이터는 쓰이지 않게 불용 처리 (바로 삭제는 하지 않음)</li><li>이후 데이터 조회 시 update 된 document 를 검색하여 기존 불용처리된 document 는 검색되지 않음</li></ol><h2 id="그럼-계속해서-segment-를-생성할텐데"><a href="#그럼-계속해서-segment-를-생성할텐데" class="headerlink" title=" 그럼 계속해서 segment 를 생성할텐데..?"></a><a name="subject06"></a> 그럼 계속해서 segment 를 생성할텐데..?</h2><p>이와 같이 생각할 수 있다. 업데이트 혹은 삭제 시 불용처리만 진행되면 기존 생성된 segment 들이 쌓여 시스템에 영향을 줄 수 있다. 이러한 단점을 보완하기 위해 elasticsearch 는 background 에서 segment merge 작업을 진행한다.</p><ol><li>ID 1,2,3 document 색인</li><li>ID 3인 document 를 삭제하면 해당 문서가 삭제되었다는 flag 를 기록하고 불용 처리</li><li>ID 4인 document 색인</li><li>색인이 된다고 merge 작업이 일어나는 것은 아니지만 예제를 쉽게 설명하기 위해 색인 작업을 추가하였다. 백그라운드에서 merge 작업이 진행될 때 ID 3인 문서는 삭제 flag 가 존재하기 때문에 해당 문서는 제외하고 1,2 문서만 하나의 segment 로 병합한다.</li></ol><p>이러한 segment merge 작업은 하나의 큰 segment 로 합치는 작업이 무수히 많이 발생한다. 또한, merge 시 삭제 flag 가 존재하는 데이터는 실제로 디스크에서 삭제한다. segment 병합을 통해 검색 시 파일 개수가 줄어들기 때문에 적은 비용으로 빠르게 응답할 수 있다.</p><blockquote><p><em>그렇다면 해당 segment 크기가 커지게 되면 파일 개수가 적지만 응답할 때 그만큼 비용이 더 들 수 있지 않을까? 이 부분은 서치 혹은 저자에게 물어봐야겠다.</em></p></blockquote><h1 id="Reference"><a href="#Reference" class="headerlink" title=" Reference"></a><a name="reference"></a> Reference</h1><ul><li>기초부터 배우는 ElasticSearch 노하우</li></ul><hr><p>made by <em>jaejun.lee</em></p>]]></content:encoded>
      
      <comments>https://jx2lee.github.io/es-shard_segment/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[Kafka] kafka 설치 및 간단한 테스트</title>
      <link>https://jx2lee.github.io/kafka-install_and_simple_test/</link>
      <guid>https://jx2lee.github.io/kafka-install_and_simple_test/</guid>
      <pubDate>Wed, 05 May 2021 15:00:00 GMT</pubDate>
      <description>
      
        &lt;center&gt;운영중인 플랫폼에 사용하는 Kafka 를 설치하고 테스트 한 기록을 남긴다.🔑&lt;/center&gt;
&lt;br&gt;
&lt;br&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<center>운영중인 플랫폼에 사용하는 Kafka 를 설치하고 테스트 한 기록을 남긴다.🔑</center><br><br><a id="more"></a><p><strong><span class="github-emoji" alias="sparkles" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png?v8">&#x2728;</span> Contents:</strong>  </p><ol><li><a href="#subject01">용어 정리</a></li><li><a href="#subject02">Download Binary</a></li><li><a href="#subject03">Zookeeper?</a><ul><li><a href="#subject04">Run Zookeeper</a></li></ul></li><li><a href="#subject05">Run Kafka Broker</a><ul><li><a href="#subject06">단일 노드 다중 브로커 클러스터 구동</a></li></ul></li><li><a href="#subject07">Create Topic</a><ul><li><a href="#subject08">Topic 리스트 확인</a></li><li><a href="#subject09">Topic 설정 확인</a></li></ul></li><li><a href="#subject10">Producer &amp; Consumer 테스트</a><ul><li><a href="#subject11">Start Producer</a></li><li><a href="#subject12">Start Consumer</a></li></ul></li><li><a href="#subject13">Consumer lag 확인</a><ul><li><a href="#subject14">Run console Producer &amp; Consumer</a></li><li><a href="#subject15">Consumer list 확인</a></li><li><a href="#subject16">Lag 확인 (주기: 1초)</a></li></ul></li><li><a href="#subject17">Log</a></li><li><a href="#reference">Reference</a></li></ol><ul><li>version: 1.1.0</li><li>One broker Cluster</li></ul><h1 id="용어-정리"><a href="#용어-정리" class="headerlink" title=" 용어 정리"></a><a name="subject01"></a> 용어 정리</h1><ul><li><strong>Kafka</strong>: 아파치 프로젝트 애플리케이션 이름. kafka cluster 라고도 부름</li><li><strong>Broker</strong>: 카프카 애플리케이션이 설치되어 있는 서버 혹은 노드</li><li><strong>Topic</strong>: producer 와 consumer 들이 보낸 자신들의 메세지를 구분하기 위한 네임. topic 이라는 개념을 통해 다수의 producer 와 consumer 들이 보낸 메세지를 구분하기 위해 사용</li><li><strong>Partition</strong>: 병렬처리가 가능하게 메세지를 나눈 단위 (많은 양의 메세지 처리를 위해서는 partition 증가)</li><li><strong>Produdcer &amp; Consumer</strong>: 단어 그 자체로 메세지를 생산하여 broker 의 topic 으로 보내는 서버 또는 애플리케이션 &amp; broker 의 topic 으로 저장된 메세지를 가져오는 서버 또는 애플리케이션</li></ul><h1 id="Download-Binary"><a href="#Download-Binary" class="headerlink" title=" Download Binary"></a><a name="subject02"></a> Download Binary</h1><p><code>wget https://archive.apache.org/dist/kafka/1.1.0/kafka_2.11-1.1.0.tgz</code></p><h2 id="Zookeeper"><a href="#Zookeeper" class="headerlink" title=" Zookeeper?"></a><a name="subject03"></a> Zookeeper?</h2><p>Zookeeper 에 대한 간략한 설명은 다음과 같다.</p><ul><li>분산 application 을 위한 coordination system</li><li>kafka broker 와 consumer 간 인터페이스 역할</li><li>file system 과 유사한 형태의 계층화된 공유 namespace data 등록을 통한 분산 프로세스 조율</li><li>file system 과의 차이점<ul><li>znode(zookeeper component) 제한된 데이터만 저장</li><li>상태 정보, 설정 정보, 위치 정보 등 kafka producer &lt;-&gt; consumer 간 조율에 필요한 데이터만 저장</li></ul></li></ul><h2 id="Run-Zookeeper"><a href="#Run-Zookeeper" class="headerlink" title=" Run Zookeeper"></a><a name="subject04"></a> Run Zookeeper</h2><p><code>bin/zookeeper-server-start.sh -daemon config/zookeeper.properties</code></p><ul><li>-daemon 옵션을 통해 백그라운드 실행</li></ul><h1 id="Run-Kafka-Broker"><a href="#Run-Kafka-Broker" class="headerlink" title=" Run Kafka Broker"></a><a name="subject05"></a> Run Kafka Broker</h1><p><code>config/server.properties</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">############################# Server Basics #############################</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># The id of the broker. This must be set to a unique integer for each broker.</span></span><br><span class="line">broker.id=0</span><br><span class="line"></span><br><span class="line"><span class="comment">############################# Socket Server Settings #############################</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># The address the socket server listens on. It will get the value returned from</span></span><br><span class="line"><span class="comment"># java.net.InetAddress.getCanonicalHostName() if not configured.</span></span><br><span class="line"><span class="comment">#   FORMAT:</span></span><br><span class="line"><span class="comment">#     listeners = listener_name://host_name:port</span></span><br><span class="line"><span class="comment">#   EXAMPLE:</span></span><br><span class="line"><span class="comment">#     listeners = PLAINTEXT://your.host.name:9092</span></span><br><span class="line">listeners=PLAINTEXT://:9092</span><br><span class="line"></span><br><span class="line"><span class="comment"># Hostname and port the broker will advertise to producers and consumers. If not set,</span></span><br><span class="line"><span class="comment"># it uses the value for &quot;listeners&quot; if configured.  Otherwise, it will use the value</span></span><br><span class="line"><span class="comment"># returned from java.net.InetAddress.getCanonicalHostName().</span></span><br><span class="line">advertised.listeners=PLAINTEXT://&#123;외부에서 접근할 수 있는 Ip&#125;:9092</span><br></pre></td></tr></table></figure><p><code>bin/kafka-server-start.sh -daemon config/server.properties</code></p><ul><li>daemon 옵션을 통해 백그라운드 실행</li></ul><h2 id="단일-노드-다중-브로커-클러스터-구동"><a href="#단일-노드-다중-브로커-클러스터-구동" class="headerlink" title=" 단일 노드 다중 브로커 클러스터 구동"></a><a name="subject06"></a> 단일 노드 다중 브로커 클러스터 구동</h2><ul><li>zookeeper 기동은 단일 노드 단일 브로커 클러스터와 동일</li><li>kafka server 기동 시 <a href="http://server.properties"><code>server.properties</code></a><ul><li>하기 3개 value 를 다르게 설정하여 해당 config 를 읽어들여 기동<ul><li>brokerid</li><li>listeners<ul><li>ex<em>) listeners=PLAINTEXT://:9092(or 9093 or 9094)</em></li></ul></li><li>log.dir</li></ul></li></ul></li><li>이후 콘솔로 pub/sub 프로세스 실행 시 producer 에 broker-list 인자는 위 kafka server properties.listeners.port 를 확인하여 나열<ul><li><em>ex) bin/kafka-console-producer.sh –broker-list localhost:9092,localhost:9093 –topic 20210324-night</em></li></ul></li></ul><h1 id="Create-Topic"><a href="#Create-Topic" class="headerlink" title=" Create Topic"></a><a name="subject07"></a> Create Topic</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[centos@singlenode kafka_2.11-1.1.0]$ bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic kafka-test</span><br><span class="line">Created topic <span class="string">&quot;kafka-test&quot;</span>.</span><br></pre></td></tr></table></figure><h2 id="Topic-리스트-확인"><a href="#Topic-리스트-확인" class="headerlink" title=" Topic 리스트 확인"></a><a name="subject08"></a> Topic 리스트 확인</h2><p><code>kafka-topics.sh --list --zookeeper &#123;ip:port&#125;</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[centos@singlenode kafka_2.11-1.1.0]$ bin/kafka-topics.sh --list --zookeeper localhost:2181</span><br><span class="line">kafka-test</span><br></pre></td></tr></table></figure><h2 id="Topic-설정-확인"><a href="#Topic-설정-확인" class="headerlink" title=" Topic 설정 확인"></a><a name="subject09"></a> Topic 설정 확인</h2><p><code>kafka-topics.sh --describe &#123;topic_name&#125; --zookeeper &#123;ip:port&#125;</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[centos@singlenode kafka_2.11-1.1.0]$ bin/kafka-topics.sh --describe kafka-test --zookeeper localhost:2181</span><br><span class="line">Topic:kafka-testPartitionCount:1ReplicationFactor:1Configs:</span><br><span class="line">Topic: kafka-testPartition: 0Leader: 0Replicas: 0Isr: 0</span><br></pre></td></tr></table></figure><h1 id="Producer-amp-Consumer-테스트"><a href="#Producer-amp-Consumer-테스트" class="headerlink" title=" Producer &amp; Consumer 테스트"></a><a name="subject10"></a> Producer &amp; Consumer 테스트</h1><h2 id="Start-Producer"><a href="#Start-Producer" class="headerlink" title=" Start Producer"></a><a name="subject11"></a> Start Producer</h2><p><code>kafka-console-producer.sh --topic &#123;topic_name&#125; --broker-list &#123;broker_ip:broker_port&#125;</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[centos@singlenode kafka_2.11-1.1.0]$ bin/kafka-console-producer.sh --topic kafka-test --broker-list localhost:9092</span><br><span class="line">&gt;ls</span><br></pre></td></tr></table></figure><h2 id="Start-Consumer"><a href="#Start-Consumer" class="headerlink" title=" Start Consumer"></a><a name="subject12"></a> Start Consumer</h2><p><code>kafka-console-consumer.sh --topic &#123;topic_name&#125; --zookeeper &#123;zk_ip:zk_port&#125;</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[centos@singlenode kafka_2.11-1.1.0]$ bin/kafka-console-consumer.sh --topic kafka-test --zookeeper localhost:2181</span><br><span class="line">ls</span><br></pre></td></tr></table></figure><ul><li>Consumer 콘솔창에서 ls 텍스트 수신 확인</li></ul><h1 id="Consumer-lag-확인"><a href="#Consumer-lag-확인" class="headerlink" title=" Consumer lag 확인"></a><a name="subject13"></a> Consumer lag 확인</h1><p>컨슈머가 읽어들어야 할 데이터(producer 가 보낸 데이터 - consumer 가 받은 데이터) 양을 나타내는 lag 을 커맨드로 확인한다.</p><h1 id="Run-console-Producer-amp-Consumer"><a href="#Run-console-Producer-amp-Consumer" class="headerlink" title=" Run console Producer &amp; Consumer"></a><a name="subject14"></a> Run console Producer &amp; Consumer</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#producer</span></span><br><span class="line">[centos@singlenode bin]$ ./kafka-console-producer.sh --topic kafka-test --broker-list localhost:9092</span><br><span class="line"><span class="comment">#consumer</span></span><br><span class="line">[centos@singlenode kafka_2.11-1.1.0]$ bin/kafka-console-consumer.sh --topic kafka-test --bootstrap-server localhost:9092</span><br></pre></td></tr></table></figure><h2 id="Consumer-list-확인"><a href="#Consumer-list-확인" class="headerlink" title=" Consumer list 확인"></a><a name="subject15"></a> Consumer list 확인</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[centos@singlenode kafka_2.11-1.1.0]$ bin/kafka-consumer-groups.sh --list --bootstrap-server localhost:9092</span><br><span class="line">Note: This will not show information about old Zookeeper-based consumers.</span><br><span class="line">console-consumer-62348</span><br></pre></td></tr></table></figure><h2 id="Lag-확인-주기-1초"><a href="#Lag-확인-주기-1초" class="headerlink" title=" Lag 확인 (주기: 1초)"></a><a name="subject16"></a> Lag 확인 (주기: 1초)</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">Every 1.0s: bin/kafka-consumer-groups.sh --describe --bootstrap-server localhost:9092 --group console-consumer-62348                                                                                                                                                                                  Mon Mar 22 16:08:42 2021</span><br><span class="line"></span><br><span class="line">Note: This will not show information about old Zookeeper-based consumers.</span><br><span class="line"></span><br><span class="line">TOPIC           PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG             CONSUMER-ID                                     HOST            CLIENT-ID</span><br><span class="line">kafka-test0          210             210             0               consumer-1-bea6ae07-f72f-4496-b3fc-e429a0f4679d /192.168.0.4    consumer-1</span><br><span class="line"><span class="comment"># 현재 Lag 이 없는데, producer 에서 많은 메세지를 생성해서 enter 를 치면 LAG 값이 상승하는 시점이 존재함</span></span><br><span class="line"></span><br><span class="line">Every 1.0s: bin/kafka-consumer-groups.sh --describe --bootstrap-server localhost:9092 --group console-consumer-62348                                                                                                                                                                                  Mon Mar 22 16:09:48 2021</span><br><span class="line"></span><br><span class="line">Note: This will not show information about old Zookeeper-based consumers.</span><br><span class="line"></span><br><span class="line">TOPIC           PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG             CONSUMER-ID                                     HOST            CLIENT-ID</span><br><span class="line">kafka-test0          222             242             20              consumer-1-bea6ae07-f72f-4496-b3fc-e429a0f4679d /192.168.0.4    consumer-1</span><br><span class="line"><span class="comment"># 이후 produceer 가 생성한 메세지를 consumer 가 가져오면서 LAG 이 0이 되는 시점(생상된 모든 메세지 sub 완료) 이 존재</span></span><br><span class="line">Every 1.0s: bin/kafka-consumer-groups.sh --describe --bootstrap-server localhost:9092 --group console-consumer-62348                                                                                                                                                                                  Mon Mar 22 16:09:48 2021</span><br><span class="line"></span><br><span class="line">Note: This will not show information about old Zookeeper-based consumers.</span><br><span class="line"></span><br><span class="line">TOPIC           PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG             CONSUMER-ID                                     HOST            CLIENT-ID</span><br><span class="line">kafka-test0          222             242              0              consumer-1-bea6ae07-f72f-4496-b3fc-e429a0f4679d /192.168.0.4    consumer-1</span><br></pre></td></tr></table></figure><h1 id="Log"><a href="#Log" class="headerlink" title=" Log"></a><a name="subject17"></a> Log</h1><ul><li><p><code>logs/controller.log</code></p><ul><li><p>Kafka 클러스터의 노드들 중에서 하나가 controller 로 작동하는데 controller node 에서 발생한 로그</p><ul><li><p>kafka 노드의 구동과 종료 관련된 로그</p></li><li><p>controller 가 각각의 노드의 broker 와 통신하는 로그</p></li><li><p>어떤 노드가 controller 가 될지 결정하는 로그</p><ul><li>예시 : [2019-08-05 13:20:27,944] DEBUG [Controller id=1] Broker 3 has been elected as the controller, so stopping the election process. (kafka.controller.KafkaController)</li></ul></li></ul></li><li><p>Topic 생성에 대한 로그</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[2021-03-22 16:38:36,346] INFO [Controller id=0] New topics: [Set(kafka-test-04)], deleted topics: [Set()], new partition replica assignment [Map(kafka-test-04-0 -&gt; Vector(0))] (kafka.controller.KafkaController)</span><br><span class="line">[2021-03-22 16:38:36,346] INFO [Controller id=0] New partition creation callback <span class="keyword">for</span> kafka-test-04-0 (kafka.controller.KafkaController)</span><br></pre></td></tr></table></figure></li></ul></li><li><p><code>logs/server.log</code></p><ul><li>노드(주로 broker)에서 발생한 로그</li><li>Topic 들의 각 Partition 에 대한 leader election 에 대한 로그</li></ul></li><li><p><code>logs/zookeeper.out</code> : zookeeper 서버에 대한 로그</p></li><li><p><code>logs/state-change.log</code> : Topic 들의 각각의 Partition 들에 대한 상태 변화 관련 로그</p></li><li><p><code>logs/kafka-authorizer.log</code> : 권한이 부여되는 모든 request 와 관련 user name 을 기록한 로그</p></li><li><p><code>logs/log-cleaner.log</code> : kafka.log.LogCleaner 클래스가 남기는 로그로 보이는데, 메시지 압축과 kafka 가 생상한 로그를 관리하는 로그</p></li><li><p><code>logs/kafka-request.log</code> : kafka.server.KafkaApis 클래스가 남기는 로그로 보이는데, kafka 로 들어온 요청에 대한 정보를 남기는 로그</p></li><li><p><code>logs/kafkaServer.out</code> : kafka.server.KafkaServer 클래스가 남기는 로그로 보이는데, kafka 브로커를 실행할 때 관련 config 및 상태에 대한 정보가 남는다. 정확히 어떤 로그를 남기는지는 확인이 필요하다.</p></li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title=" Reference"></a><a name="reference"></a> Reference</h1><ul><li><a href="https://velog.io/@hanblueblue/Kafka-%EC%84%A4%EC%B9%98%EC%8B%A4%ED%96%89-%EB%B0%8F-%ED%85%8C%EC%8A%A4%ED%8A%B8">https://velog.io/@hanblueblue/Kafka-설치실행-및-테스트</a></li><li><a href="https://devidea.tistory.com/55">https://devidea.tistory.com/55</a></li><li><a href="https://community.cloudera.com/t5/Support-Questions/What-is-the-use-of-zookeeper-out/td-p/218537">https://community.cloudera.com/t5/Support-Questions/What-is-the-use-of-zookeeper-out/td-p/218537</a></li><li><a href="https://godway1225.wordpress.com/2019/08/29/kafka-%EB%A1%9C%EA%B7%B8-%EC%A2%85%EB%A5%98-%EB%B0%8F-%EB%A1%9C%EA%B7%B8-%EC%83%98%ED%94%8C%EC%97%90-%EB%8C%80%ED%95%9C-%EC%84%A4%EB%AA%85/">https://godway1225.wordpress.com/2019/08/29/kafka-로그-종류-및-로그-샘플에-대한-설명/</a></li><li><a href="https://docs.confluent.io/platform/current/kafka/authorization.html">https://docs.confluent.io/platform/current/kafka/authorization.html</a></li></ul><hr><p>made by <em>jaejun.lee</em></p>]]></content:encoded>
      
      <comments>https://jx2lee.github.io/kafka-install_and_simple_test/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[Network] ngrok?</title>
      <link>https://jx2lee.github.io/network-introduction_to_ngrok/</link>
      <guid>https://jx2lee.github.io/network-introduction_to_ngrok/</guid>
      <pubDate>Fri, 30 Apr 2021 15:00:00 GMT</pubDate>
      <description>
      
        &lt;center&gt;tunnel program 인 ngrok 을 간단히 살펴보자 🔨&lt;/center&gt;
&lt;br&gt;
&lt;br&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<center>tunnel program 인 ngrok 을 간단히 살펴보자 🔨</center><br><br><a id="more"></a><p><strong><span class="github-emoji" alias="sparkles" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png?v8">&#x2728;</span> Contents:</strong>  </p><ol><li><a href="#subject01">ngork?</a></li><li><a href="#subject02">How to use</a><ul><li><a href="#subject03">Download binary</a></li><li><a href="#subject04">http 서버를 외부에 연결하고자 할 때 (port: 8080)</a></li></ul></li><li><a href="#reference">Reference</a></li></ol><h1 id="ngrok"><a href="#ngrok" class="headerlink" title=" ngrok?"></a><a name="subject01"></a> ngrok?</h1><ul><li>Secure Tunnerls to localhost</li><li>외부에서 로컬에 접속 가능하게 하는 tunnel program</li></ul><p><img src="/image/network-introduction_to_ngrok-1.png" alt="ngrok"></p><h1 id="How-to-use"><a href="#How-to-use" class="headerlink" title=" How to use"></a><a name="subject02"></a> How to use</h1><h2 id="Download-binary"><a href="#Download-binary" class="headerlink" title=" Download binary"></a><a name="subject03"></a> Download binary</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-darwin-amd64.zip</span><br><span class="line">&gt; mv ngrok /usr/<span class="built_in">local</span>/bin</span><br></pre></td></tr></table></figure><h2 id="http-서버를-외부에-연결하고자-할-때-port-8080"><a href="#http-서버를-외부에-연결하고자-할-때-port-8080" class="headerlink" title=" http 서버를 외부에 연결하고자 할 때 (port: 8080)"></a><a name="subject04"></a> http 서버를 외부에 연결하고자 할 때 (port: 8080)</h2><ul><li>spring web server 테스트</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">&gt; ngrok http 8080</span><br><span class="line">ngrok by @inconshreveable                                                                                                                                       (Ctrl+C to quit)</span><br><span class="line"></span><br><span class="line">Session Status                online</span><br><span class="line">Session Expires               1 hour, 58 minutes</span><br><span class="line">Version                       2.3.38</span><br><span class="line">Region                        United States (us)</span><br><span class="line">Web Interface                 http://127.0.0.1:4040</span><br><span class="line">Forwarding                    http://7791735535f7.ngrok.io -&gt; http://localhost:8080</span><br><span class="line">Forwarding                    https://7791735535f7.ngrok.io -&gt; http://localhost:8080</span><br><span class="line"></span><br><span class="line">Connections                   ttl     opn     rt1     rt5     p50     p90</span><br><span class="line">                              2       2       0.01    0.01    0.00    0.00</span><br><span class="line"></span><br><span class="line">HTTP Requests</span><br><span class="line">-------------</span><br></pre></td></tr></table></figure><ul><li>Forwarding 으로 포워딩 된 주소로 웹브라우저 접속</li></ul><p><img src="/image/network-introduction_to_ngrok-2.png" alt="웹브라우저 확인"></p><h1 id="Reference"><a href="#Reference" class="headerlink" title=" Reference"></a><a name="reference"></a> Reference</h1><ul><li><a href="https://ngrok.com/">https://ngrok.com/</a></li><li><a href="https://subicura.com/2017/11/22/mac-os-development-environment-setup.html">https://subicura.com/2017/11/22/mac-os-development-environment-setup.html</a></li></ul><hr><p>made by <em>jaejun.lee</em></p>]]></content:encoded>
      
      <comments>https://jx2lee.github.io/network-introduction_to_ngrok/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[ElasticSearch] 하나의 서버에서 여러 ES node 실행</title>
      <link>https://jx2lee.github.io/es-run_cluster_in_node/</link>
      <guid>https://jx2lee.github.io/es-run_cluster_in_node/</guid>
      <pubDate>Mon, 15 Mar 2021 15:00:00 GMT</pubDate>
      <description>
      
        &lt;p&gt;Elasticsearch의 노드들은 클라이언트와의 통신을 위한 http 포트(9200-9299), 노드 간의 데이터 교환을 위한 tcp 포트 (9300-9399) 총 2 종류의 네트워크 통신을 사용한다. 일반적으로 1개의 물리 서버마다 하나의 노드를 실행하는 것을 권장하고 있다. 테스트를 위해 하나의 서버에서 2개 elasticsearch 노드를 실행해본다.&lt;/p&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p>Elasticsearch의 노드들은 클라이언트와의 통신을 위한 http 포트(9200-9299), 노드 간의 데이터 교환을 위한 tcp 포트 (9300-9399) 총 2 종류의 네트워크 통신을 사용한다. 일반적으로 1개의 물리 서버마다 하나의 노드를 실행하는 것을 권장하고 있다. 테스트를 위해 하나의 서버에서 2개 elasticsearch 노드를 실행해본다.</p><a id="more"></a><p><strong><span class="github-emoji" alias="sparkles" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png?v8">&#x2728;</span> Contents:</strong>  </p><ol><li><a href="#subject01">config/elasticsearch.yml</a></li><li><a href="#subject02">Run node-1</a></li><li><a href="#subject03">Run node-2</a></li><li><a href="#reference">Reference</a></li></ol><h1 id="config-elasticsearch-yml"><a href="#config-elasticsearch-yml" class="headerlink" title=" config/elasticsearch.yml"></a><a name="subject01"></a> config/elasticsearch.yml</h1><p>config 파일에 <code>node.max_local_storage_nodes</code> 값을 es node 수로 설정한다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#elasticsearch.yml</span></span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line">node.max_local_storage_nodes: 3</span><br></pre></td></tr></table></figure><blockquote><blockquote><p>node.max_local_storage_nodes<br>데이터 경로는 다른 클러스터의 node 혹은 여러 node에서 공유 할 수 있다. <strong>그러나 동일한 데이터 경로를 사용하여 하나의 Elasticsearch node 만 실행하는 것이 좋다.</strong> 이 설정은 7.x에서 더 이상 사용되지 않으며 버전 8.0에서 사용 불가하다. 기본적으로 Elasticsearch는 둘 이상의 node가 동일한 데이터 경로를 공유하지 못하도록 구성된다. 둘 이상의 노드를 허용하려면 (예 : 개발 머신에서) node.max_local_storage_nodes 를 1보다 큰 정수로 설정한다. 동일한 데이터 디렉토리에서 다른 노드 유형 (예 : 마스터, 데이터)을 실행하면, 이로 인해 예기치 않은 데이터 손실이 발생할 수 있다.</p></blockquote></blockquote><h1 id="Run-node-1"><a href="#Run-node-1" class="headerlink" title=" Run node-1"></a><a name="subject02"></a> Run node-1</h1><p><code>es-cluster-1</code> 클러스터에 <code>node-1</code> 노드를 구동한다.</p><ul><li><code>bin/elasticsearch -Ecluster.name=es-cluster-1 -Enode.name=node-1</code></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">[centos@singlenode elasticsearch-7.10.2]$ bin/elasticsearch -Ecluster.name=es-cluster-1 -Enode.name=node-1</span><br><span class="line">[2021-03-04T17:27:20,414][INFO ][o.e.n.Node               ] [node-1] version[7.10.2], pid[16662], build[default/tar/747e1cc71def077253878a59143c1f785afa92b9/2021-01-13T00:42:12.435326Z], OS[Linux/3.10.0-862.14.4.el7.x86_64/amd64], JVM[Red Hat, Inc./OpenJDK 64-Bit Server VM/1.8.0_275/25.275-b01]</span><br><span class="line">[2021-03-04T17:27:20,416][INFO ][o.e.n.Node               ] [node-1] JVM home [/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.275.b01-0.el7_9.x86_64/jre], using bundled JDK [<span class="literal">false</span>]</span><br><span class="line">[2021-03-04T17:27:20,416][INFO ][o.e.n.Node               ] [node-1] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=<span class="literal">true</span>, -Dfile.encoding=UTF-8, -Djna.nosys=<span class="literal">true</span>, -XX:-OmitStackTraceInFastThrow, -Dio.netty.noUnsafe=<span class="literal">true</span>, -Dio.netty.noKeySetOptimization=<span class="literal">true</span>, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=<span class="literal">false</span>, -Dlog4j2.disable.jmx=<span class="literal">true</span>, -Djava.locale.providers=SPI,JRE, -Xms1g, -Xmx1g, -XX:+UseConcMarkSweepGC, -XX:CMSInitiatingOccupancyFraction=75, -XX:+UseCMSInitiatingOccupancyOnly, -Djava.io.tmpdir=/tmp/elasticsearch-3044858611283555949, -XX:+HeapDumpOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -XX:+PrintGCDetails, -XX:+PrintGCDateStamps, -XX:+PrintTenuringDistribution, -XX:+PrintGCApplicationStoppedTime, -Xloggc:logs/gc.log, -XX:+UseGCLogFileRotation, -XX:NumberOfGCLogFiles=32, -XX:GCLogFileSize=64m, -XX:MaxDirectMemorySize=536870912, -Des.path.home=/app/elasticsearch-7.10.2, -Des.path.conf=/app/elasticsearch-7.10.2/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=<span class="literal">true</span>]</span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line">[2021-03-04T17:27:23,567][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-watcher]</span><br><span class="line">[2021-03-04T17:27:23,567][INFO ][o.e.p.PluginsService     ] [node-1] no plugins loaded</span><br><span class="line">[2021-03-04T17:27:23,627][INFO ][o.e.e.NodeEnvironment    ] [node-1] using [1] data paths, mounts [[/ (rootfs)]], net usable_space [93.7gb], net total_space [99.9gb], types [rootfs]</span><br><span class="line">[2021-03-04T17:27:23,627][INFO ][o.e.e.NodeEnvironment    ] [node-1] heap size [990.7mb], compressed ordinary object pointers [<span class="literal">true</span>]</span><br><span class="line">[2021-03-04T17:27:23,763][INFO ][o.e.n.Node               ] [node-1] node name [node-1], node ID [nHPEGErISlGQO-7OiBVLbw], cluster name [es-cluster-1], roles [transform, master, remote_cluster_client, data, ml, data_content, data_hot, data_warm, data_cold, ingest]</span><br><span class="line">[2021-03-04T17:27:27,914][INFO ][o.e.x.m.p.l.CppLogMessageHandler] [node-1] [controller/16827] [Main.cc@114] controller (64 bit): Version 7.10.2 (Build 40a3af639d4698) Copyright (c) 2021 Elasticsearch BV</span><br><span class="line">[2021-03-04T17:27:28,769][INFO ][o.e.x.s.a.s.FileRolesStore] [node-1] parsed [0] roles from file [/app/elasticsearch-7.10.2/config/roles.yml]</span><br><span class="line">[2021-03-04T17:27:30,276][INFO ][o.e.t.NettyAllocator     ] [node-1] creating NettyAllocator with the following configs: [name=unpooled, suggested_max_allocation_size=1mb, factors=&#123;es.unsafe.use_unpooled_allocator=null, g1gc_enabled=<span class="literal">false</span>, g1gc_region_size=0b, heap_size=990.7mb&#125;]</span><br><span class="line">[2021-03-04T17:27:30,387][INFO ][o.e.d.DiscoveryModule    ] [node-1] using discovery <span class="built_in">type</span> [zen] and seed hosts providers [settings]</span><br><span class="line">[2021-03-04T17:27:31,102][WARN ][o.e.g.DanglingIndicesState] [node-1] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually</span><br><span class="line">[2021-03-04T17:27:31,660][INFO ][o.e.n.Node               ] [node-1] initialized</span><br><span class="line">[2021-03-04T17:27:31,661][INFO ][o.e.n.Node               ] [node-1] starting ...</span><br><span class="line">[2021-03-04T17:27:31,820][INFO ][o.e.t.TransportService   ] [node-1] publish_address &#123;127.0.0.1:9300&#125;, bound_addresses &#123;[::1]:9300&#125;, &#123;127.0.0.1:9300&#125;</span><br><span class="line">[2021-03-04T17:27:32,210][WARN ][o.e.b.BootstrapChecks    ] [node-1] the default discovery settings are unsuitable <span class="keyword">for</span> production use; at least one of [discovery.seed_hosts, discovery.seed_providers, cluster.initial_master_nodes] must be configured</span><br><span class="line">[2021-03-04T17:27:32,212][INFO ][o.e.c.c.Coordinator      ] [node-1] cluster UUID [DmY4hGLYRra7H50T-RSoqw]</span><br><span class="line">[2021-03-04T17:27:32,225][INFO ][o.e.c.c.ClusterBootstrapService] [node-1] no discovery configuration found, will perform best-effort cluster bootstrapping after [3s] unless existing master is discovered</span><br><span class="line">[2021-03-04T17:27:32,385][INFO ][o.e.c.s.MasterService    ] [node-1] elected-as-master ([1] nodes joined)[&#123;node-1&#125;&#123;nHPEGErISlGQO-7OiBVLbw&#125;&#123;9xoo0XVVSMu0LphMhu9QNw&#125;&#123;127.0.0.1&#125;&#123;127.0.0.1:9300&#125;&#123;cdhilmrstw&#125;&#123;ml.machine_memory=3973890048, xpack.installed=<span class="literal">true</span>, transform.node=<span class="literal">true</span>, ml.max_open_jobs=20&#125; elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 16, version: 243, delta: master node changed &#123;previous [], current [&#123;node-1&#125;&#123;nHPEGErISlGQO-7OiBVLbw&#125;&#123;9xoo0XVVSMu0LphMhu9QNw&#125;&#123;127.0.0.1&#125;&#123;127.0.0.1:9300&#125;&#123;cdhilmrstw&#125;&#123;ml.machine_memory=3973890048, xpack.installed=<span class="literal">true</span>, transform.node=<span class="literal">true</span>, ml.max_open_jobs=20&#125;]&#125;</span><br><span class="line">[2021-03-04T17:27:32,462][INFO ][o.e.c.s.ClusterApplierService] [node-1] master node changed &#123;previous [], current [&#123;node-1&#125;&#123;nHPEGErISlGQO-7OiBVLbw&#125;&#123;9xoo0XVVSMu0LphMhu9QNw&#125;&#123;127.0.0.1&#125;&#123;127.0.0.1:9300&#125;&#123;cdhilmrstw&#125;&#123;ml.machine_memory=3973890048, xpack.installed=<span class="literal">true</span>, transform.node=<span class="literal">true</span>, ml.max_open_jobs=20&#125;]&#125;, term: 16, version: 243, reason: Publication&#123;term=16, version=243&#125;</span><br><span class="line">[2021-03-04T17:27:32,512][INFO ][o.e.h.AbstractHttpServerTransport] [node-1] publish_address &#123;127.0.0.1:9200&#125;, bound_addresses &#123;[::1]:9200&#125;, &#123;127.0.0.1:9200&#125;</span><br><span class="line">[2021-03-04T17:27:32,513][INFO ][o.e.n.Node               ] [node-1] started</span><br><span class="line">[2021-03-04T17:27:32,867][INFO ][o.e.l.LicenseService     ] [node-1] license [25e67c23-b75a-4b40-a9ee-78e3ddab64c3] mode [basic] - valid</span><br><span class="line">[2021-03-04T17:27:32,869][INFO ][o.e.x.s.s.SecurityStatusChangeListener] [node-1] Active license is now [BASIC]; Security is disabled</span><br><span class="line">[2021-03-04T17:27:32,874][INFO ][o.e.g.GatewayService     ] [node-1] recovered [8] indices into cluster_state</span><br><span class="line">[2021-03-04T17:27:33,816][INFO ][o.e.c.r.a.AllocationService] [node-1] Cluster health status changed from [RED] to [YELLOW] (reason: [shards started [[ilm-history-3-000001][0], [basketball][0], [.kibana-event-log-7.10.2-000001][0]]]).</span><br></pre></td></tr></table></figure><h1 id="Run-node-2"><a href="#Run-node-2" class="headerlink" title=" Run node-2"></a><a name="subject03"></a> Run node-2</h1><p>같은 서버에서<code>es-cluster-1</code> 클러스터의 <code>node-1</code> 노드를 구동한다.</p><ul><li><code>bin/elasticsearch -Ecluster.name=es-cluster-1 -Enode.name=node-2</code></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">[centos@singlenode elasticsearch-7.10.2]$ bin/elasticsearch -Ecluster.name=es-cluster-1 -Enode.name=node-2</span><br><span class="line">[2021-03-04T17:29:35,358][INFO ][o.e.n.Node               ] [node-2] version[7.10.2], pid[16990], build[default/tar/747e1cc71def077253878a59143c1f785afa92b9/2021-01-13T00:42:12.435326Z], OS[Linux/3.10.0-862.14.4.el7.x86_64/amd64], JVM[Red Hat, Inc./OpenJDK 64-Bit Server VM/1.8.0_275/25.275-b01]</span><br><span class="line">[2021-03-04T17:29:35,359][INFO ][o.e.n.Node               ] [node-2] JVM home [/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.275.b01-0.el7_9.x86_64/jre], using bundled JDK [<span class="literal">false</span>]</span><br><span class="line">[2021-03-04T17:29:35,360][INFO ][o.e.n.Node               ] [node-2] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=<span class="literal">true</span>, -Dfile.encoding=UTF-8, -Djna.nosys=<span class="literal">true</span>, -XX:-OmitStackTraceInFastThrow, -Dio.netty.noUnsafe=<span class="literal">true</span>, -Dio.netty.noKeySetOptimization=<span class="literal">true</span>, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=<span class="literal">false</span>, -Dlog4j2.disable.jmx=<span class="literal">true</span>, -Djava.locale.providers=SPI,JRE, -Xms1g, -Xmx1g, -XX:+UseConcMarkSweepGC, -XX:CMSInitiatingOccupancyFraction=75, -XX:+UseCMSInitiatingOccupancyOnly, -Djava.io.tmpdir=/tmp/elasticsearch-4970032312008421325, -XX:+HeapDumpOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -XX:+PrintGCDetails, -XX:+PrintGCDateStamps, -XX:+PrintTenuringDistribution, -XX:+PrintGCApplicationStoppedTime, -Xloggc:logs/gc.log, -XX:+UseGCLogFileRotation, -XX:NumberOfGCLogFiles=32, -XX:GCLogFileSize=64m, -XX:MaxDirectMemorySize=536870912, -Des.path.home=/app/elasticsearch-7.10.2, -Des.path.conf=/app/elasticsearch-7.10.2/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=<span class="literal">true</span>]</span><br><span class="line">[2021-03-04T17:29:38,449][INFO ][o.e.p.PluginsService     ] [node-2] loaded module [aggs-matrix-stats]</span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line">[2021-03-04T17:29:38,460][INFO ][o.e.p.PluginsService     ] [node-2] no plugins loaded</span><br><span class="line">[2021-03-04T17:29:38,522][INFO ][o.e.e.NodeEnvironment    ] [node-2] using [1] data paths, mounts [[/ (rootfs)]], net usable_space [93.7gb], net total_space [99.9gb], types [rootfs]</span><br><span class="line">[2021-03-04T17:29:38,522][INFO ][o.e.e.NodeEnvironment    ] [node-2] heap size [990.7mb], compressed ordinary object pointers [<span class="literal">true</span>]</span><br><span class="line">[2021-03-04T17:29:38,655][INFO ][o.e.n.Node               ] [node-2] node name [node-2], node ID [Xye_inmsQf2pVylm4m65hA], cluster name [es-cluster-1], roles [transform, master, remote_cluster_client, data, ml, data_content, data_hot, data_warm, data_cold, ingest]</span><br><span class="line">[2021-03-04T17:29:42,816][INFO ][o.e.x.m.p.l.CppLogMessageHandler] [node-2] [controller/17156] [Main.cc@114] controller (64 bit): Version 7.10.2 (Build 40a3af639d4698) Copyright (c) 2021 Elasticsearch BV</span><br><span class="line">[2021-03-04T17:29:43,668][INFO ][o.e.x.s.a.s.FileRolesStore] [node-2] parsed [0] roles from file [/app/elasticsearch-7.10.2/config/roles.yml]</span><br><span class="line">[2021-03-04T17:29:45,109][INFO ][o.e.t.NettyAllocator     ] [node-2] creating NettyAllocator with the following configs: [name=unpooled, suggested_max_allocation_size=1mb, factors=&#123;es.unsafe.use_unpooled_allocator=null, g1gc_enabled=<span class="literal">false</span>, g1gc_region_size=0b, heap_size=990.7mb&#125;]</span><br><span class="line">[2021-03-04T17:29:45,203][INFO ][o.e.d.DiscoveryModule    ] [node-2] using discovery <span class="built_in">type</span> [zen] and seed hosts providers [settings]</span><br><span class="line">[2021-03-04T17:29:45,906][WARN ][o.e.g.DanglingIndicesState] [node-2] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually</span><br><span class="line">[2021-03-04T17:29:46,453][INFO ][o.e.n.Node               ] [node-2] initialized</span><br><span class="line">[2021-03-04T17:29:46,454][INFO ][o.e.n.Node               ] [node-2] starting ...</span><br><span class="line">[2021-03-04T17:29:46,608][INFO ][o.e.t.TransportService   ] [node-2] publish_address &#123;127.0.0.1:9301&#125;, bound_addresses &#123;[::1]:9301&#125;, &#123;127.0.0.1:9301&#125;</span><br><span class="line">[2021-03-04T17:29:46,970][WARN ][o.e.b.BootstrapChecks    ] [node-2] the default discovery settings are unsuitable <span class="keyword">for</span> production use; at least one of [discovery.seed_hosts, discovery.seed_providers, cluster.initial_master_nodes] must be configured</span><br><span class="line">[2021-03-04T17:29:46,972][INFO ][o.e.c.c.Coordinator      ] [node-2] cluster UUID [DmY4hGLYRra7H50T-RSoqw]</span><br><span class="line">[2021-03-04T17:29:46,984][INFO ][o.e.c.c.ClusterBootstrapService] [node-2] no discovery configuration found, will perform best-effort cluster bootstrapping after [3s] unless existing master is discovered</span><br><span class="line">[2021-03-04T17:29:47,415][INFO ][o.e.c.s.ClusterApplierService] [node-2] master node changed &#123;previous [], current [&#123;node-1&#125;&#123;nHPEGErISlGQO-7OiBVLbw&#125;&#123;9xoo0XVVSMu0LphMhu9QNw&#125;&#123;127.0.0.1&#125;&#123;127.0.0.1:9300&#125;&#123;cdhilmrstw&#125;&#123;ml.machine_memory=3973890048, ml.max_open_jobs=20, xpack.installed=<span class="literal">true</span>, transform.node=<span class="literal">true</span>&#125;]&#125;, added &#123;&#123;node-1&#125;&#123;nHPEGErISlGQO-7OiBVLbw&#125;&#123;9xoo0XVVSMu0LphMhu9QNw&#125;&#123;127.0.0.1&#125;&#123;127.0.0.1:9300&#125;&#123;cdhilmrstw&#125;&#123;ml.machine_memory=3973890048, ml.max_open_jobs=20, xpack.installed=<span class="literal">true</span>, transform.node=<span class="literal">true</span>&#125;&#125;, term: 16, version: 257, reason: ApplyCommitRequest&#123;term=16, version=257, sourceNode=&#123;node-1&#125;&#123;nHPEGErISlGQO-7OiBVLbw&#125;&#123;9xoo0XVVSMu0LphMhu9QNw&#125;&#123;127.0.0.1&#125;&#123;127.0.0.1:9300&#125;&#123;cdhilmrstw&#125;&#123;ml.machine_memory=3973890048, ml.max_open_jobs=20, xpack.installed=<span class="literal">true</span>, transform.node=<span class="literal">true</span>&#125;&#125;</span><br><span class="line">[2021-03-04T17:29:47,642][INFO ][o.e.x.s.a.TokenService   ] [node-2] refresh keys</span><br><span class="line">[2021-03-04T17:29:47,972][INFO ][o.e.x.s.a.TokenService   ] [node-2] refreshed keys</span><br><span class="line">[2021-03-04T17:29:48,013][INFO ][o.e.l.LicenseService     ] [node-2] license [25e67c23-b75a-4b40-a9ee-78e3ddab64c3] mode [basic] - valid</span><br><span class="line">[2021-03-04T17:29:48,016][INFO ][o.e.x.s.s.SecurityStatusChangeListener] [node-2] Active license is now [BASIC]; Security is disabled</span><br><span class="line">[2021-03-04T17:29:48,048][INFO ][o.e.h.AbstractHttpServerTransport] [node-2] publish_address &#123;127.0.0.1:9201&#125;, bound_addresses &#123;[::1]:9201&#125;, &#123;127.0.0.1:9201&#125;</span><br><span class="line">[2021-03-04T17:29:48,049][INFO ][o.e.n.Node               ] [node-2] started</span><br></pre></td></tr></table></figure><ul><li><p>로그를 확인해보면 Cluster UUID(<em>DmY4hGLYRra7H50T-RSoqw</em>) 가 같음을 확인할 수 있다.</p></li><li><p>클러스터에 조인하면 기존 실행한 node-1 에는 다음과 같이 node 가 조인되었음을 확인할 수 있다.</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[2021-03-04T17:29:47,317][INFO ][o.e.c.r.a.AllocationService] [node-1] updating number_of_replicas to [1] <span class="keyword">for</span> indices [.apm-agent-configuration, .kibana_task_manager_1, ilm-history-3-000001, .kibana_1, .apm-custom-link, .kibana-event-log-7.10.2-000001]</span><br><span class="line">[2021-03-04T17:29:47,319][INFO ][o.e.c.s.MasterService    ] [node-1] node-join[&#123;node-2&#125;&#123;Xye_inmsQf2pVylm4m65hA&#125;&#123;fACpCFV-Q56QZk6anPgfEg&#125;&#123;127.0.0.1&#125;&#123;127.0.0.1:9301&#125;&#123;cdhilmrstw&#125;&#123;ml.machine_memory=3973890048, ml.max_open_jobs=20, xpack.installed=<span class="literal">true</span>, transform.node=<span class="literal">true</span>&#125; join existing leader], term: 16, version: 257, delta: added &#123;&#123;node-2&#125;&#123;Xye_inmsQf2pVylm4m65hA&#125;&#123;fACpCFV-Q56QZk6anPgfEg&#125;&#123;127.0.0.1&#125;&#123;127.0.0.1:9301&#125;&#123;cdhilmrstw&#125;&#123;ml.machine_memory=3973890048, ml.max_open_jobs=20, xpack.installed=<span class="literal">true</span>, transform.node=<span class="literal">true</span>&#125;&#125;</span><br><span class="line">[2021-03-04T17:29:48,022][INFO ][o.e.c.s.ClusterApplierService] [node-1] added &#123;&#123;node-2&#125;&#123;Xye_inmsQf2pVylm4m65hA&#125;&#123;fACpCFV-Q56QZk6anPgfEg&#125;&#123;127.0.0.1&#125;&#123;127.0.0.1:9301&#125;&#123;cdhilmrstw&#125;&#123;ml.machine_memory=3973890048, ml.max_open_jobs=20, xpack.installed=<span class="literal">true</span>, transform.node=<span class="literal">true</span>&#125;&#125;, term: 16, version: 257, reason: Publication&#123;term=16, version=257&#125;</span><br><span class="line">[2021-03-04T17:29:50,120][INFO ][o.e.c.r.a.AllocationService] [node-1] Cluster health status changed from [YELLOW] to [GREEN] (reason: [shards started [[ilm-history-3-000001][0]]]).</span><br></pre></td></tr></table></figure></li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title=" Reference"></a><a name="reference"></a> Reference</h1><ul><li><a href="http://kangmyounghun.blogspot.com/2019/08/nodemaxlocalstoragenodes.html">http://kangmyounghun.blogspot.com/2019/08/nodemaxlocalstoragenodes.html</a></li><li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-node.html#max-local-storage-nodes">https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-node.html#max-local-storage-nodes</a></li></ul><hr><p>made by <em>jaejun.lee</em></p>]]></content:encoded>
      
      <comments>https://jx2lee.github.io/es-run_cluster_in_node/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[Linux] switch root 시 cannot open session: 모듈을 알 수 없음 Error</title>
      <link>https://jx2lee.github.io/linux-switch_root_error/</link>
      <guid>https://jx2lee.github.io/linux-switch_root_error/</guid>
      <pubDate>Mon, 15 Mar 2021 15:00:00 GMT</pubDate>
      <description>
      
        &lt;center&gt;인스턴스 사용 중 switch root 에러가 발생하여 이를 해결하는 과정을 다뤄본다.&lt;/center&gt;
&lt;br&gt;
&lt;br&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<center>인스턴스 사용 중 switch root 에러가 발생하여 이를 해결하는 과정을 다뤄본다.</center><br><br><a id="more"></a><p><strong><span class="github-emoji" alias="sparkles" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png?v8">&#x2728;</span> Contents:</strong>  </p><ol><li><a href="#subject01">check Log</a></li><li><a href="#subject02">원인</a></li><li><a href="#subject03">Solution</a></li><li><a href="#reference">Reference</a></li></ol><h1 id="check-Log"><a href="#check-Log" class="headerlink" title=" check Log"></a><a name="subject01"></a> check Log</h1><ul><li>path: /var/log/messsages<ul><li>CentOS 7</li></ul></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Mar  9 20:51:52 singlenode sshd[13968]: pam_unix(sshd:session): session opened <span class="keyword">for</span> user centos by (uid=0)</span><br><span class="line">Mar  9 20:51:53 singlenode su: PAM unable to dlopen(/usr/lib64/security/pam_lists.so): /usr/lib64/security/pam_lists.so: 동적 오브젝트 파일을 열 수 없습니다: 그런 파일이나 디렉터리가 없습니다</span><br><span class="line">Mar  9 20:51:53 singlenode su: PAM adding faulty module: /usr/lib64/security/pam_lists.so</span><br><span class="line">Mar  9 20:51:54 singlenode su: pam_unix(su:session): session opened <span class="keyword">for</span> user root by centos(uid=1000)</span><br></pre></td></tr></table></figure><h1 id="원인"><a href="#원인" class="headerlink" title=" 원인"></a><a name="subject02"></a> 원인</h1><ul><li><p>Log 상에서 <code>/usr/lib64/security/pam_lists.so</code> 파일이 존재하지 않는다고 한다.</p></li><li><p>pam_lists.so 를 필요로 하는 파일을 찾아보던 중</p><ul><li><p>/etc/pam.d/su 파일을 확인 (맨 마지막 라인)</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#%PAM-1.0</span></span><br><span class="line">auth            sufficient      pam_rootok.so</span><br><span class="line"><span class="comment"># Uncomment the following line to implicitly trust users in the &quot;wheel&quot; group.</span></span><br><span class="line"><span class="comment">#auth           sufficient      pam_wheel.so trust use_uid</span></span><br><span class="line"><span class="comment"># Uncomment the following line to require a user to be in the &quot;wheel&quot; group.</span></span><br><span class="line"><span class="comment">#auth           required        pam_wheel.so use_uid</span></span><br><span class="line">auth            substack        system-auth</span><br><span class="line">auth            include         postlogin</span><br><span class="line">account         sufficient      pam_succeed_if.so uid = 0 use_uid quiet</span><br><span class="line">account         include         system-auth</span><br><span class="line">password        include         system-auth</span><br><span class="line">session         include         system-auth</span><br><span class="line">session         include         postlogin</span><br><span class="line">session         optional        pam_xauth.so</span><br><span class="line">session         required        pam_lists.so</span><br></pre></td></tr></table></figure></li></ul></li></ul><h1 id="Solution"><a href="#Solution" class="headerlink" title=" Solution"></a><a name="subject03"></a> Solution</h1><ul><li><p>/etc/pam.d/su 파일 마지막 라인 pam_lists.so → pam_limits.so 로 변경</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">session         required        pam_limits.so</span><br></pre></td></tr></table></figure></li><li><p>이후 root 로 스위치 해보자</p><ul><li><p>su root</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[centos@singlenode pam.d]$ su root</span><br><span class="line">암호:</span><br><span class="line">[root@singlenode pam.d]<span class="comment"># Success!</span></span><br></pre></td></tr></table></figure></li></ul></li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title=" Reference"></a><a name="reference"></a> Reference</h1><ul><li><a href="https://m.blog.naver.com/PostView.nhn?blogId=ddakawar&logNo=60191982338&proxyReferer=https:%2F%2Fwww.google.com%2F">https://m.blog.naver.com/PostView.nhn?blogId=ddakawar&amp;logNo=60191982338&amp;proxyReferer=https:%2F%2Fwww.google.com%2F</a></li></ul><hr><p>made by <em>jaejun.lee</em></p>]]></content:encoded>
      
      <comments>https://jx2lee.github.io/linux-switch_root_error/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[Network] HAProxy 간단히 살펴보기</title>
      <link>https://jx2lee.github.io/network-introduction_to_haproxy/</link>
      <guid>https://jx2lee.github.io/network-introduction_to_haproxy/</guid>
      <pubDate>Mon, 15 Mar 2021 15:00:00 GMT</pubDate>
      <description>
      
        &lt;center&gt;&lt;span class=&quot;github-emoji&quot; alias=&quot;fire&quot; style=&quot;&quot; fallback-src=&quot;https://github.githubassets.com/images/icons/emoji/unicode/1f525.png?v8&quot;&gt;&amp;#x1f525;&lt;/span&gt;&lt;/center&gt;
&lt;br&gt;
&lt;br&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<center><span class="github-emoji" alias="fire" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f525.png?v8">&#x1f525;</span></center><br><br><a id="more"></a><p><strong><span class="github-emoji" alias="sparkles" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png?v8">&#x2728;</span> Contents:</strong>  </p><ol><li><a href="#subject01">정의</a></li><li><a href="#subject02">기능 및 특징</a></li><li><a href="#subject03">동작 방식</a></li><li><a href="#reference">Reference</a></li></ol><h1 id="정의"><a href="#정의" class="headerlink" title=" 정의"></a><a name="subject01"></a> 정의</h1><p>HAProxy는 기존의 하드웨어 스위치를 대체하는 <strong>소프트웨어 로드 밸런서</strong></p><p><img src="/image/haproxy-def.png" alt="그림1"></p><h1 id="기능-및-특징"><a href="#기능-및-특징" class="headerlink" title=" 기능 및 특징"></a><a name="subject02"></a> 기능 및 특징</h1><ul><li>네트워크 스위치에서 제공하는 L4, L7 기능 및 로드 밸런서 기능 제공</li><li>설치가 쉽고 또한 환경 설정도 어렵지 않으므로 서비스 이중화를 빠르게 구성이 가능</li></ul><blockquote><p><code>로드 밸런싱</code>?<br>부하 분산을 위해서 가상(virtual) IP를 통해 여러 서버에 접속하도록 분배하는 기능</p></blockquote><h1 id="동작-방식"><a href="#동작-방식" class="headerlink" title=" 동작 방식"></a><a name="subject03"></a> 동작 방식</h1><ul><li>HAProxy는 기본적으로 reverse proxy 형태로 동작<ul><li>우리가 브라우저에서 사용하는 proxy는 클라이언트 앞에서 처리하는 기능으로, forward proxy라 일컫음</li><li>reverse proxy의 역할을 간단히 설명하면, 실제 서버 요청에 대해서 서버 앞 단에 존재하면서, 서버로 들어오는 요청을 대신 받아서 서버에 전달하고 요청한 곳에 그 결과를 다시 전달하는 것이다.</li></ul></li></ul><p><strong>동작 방식은 하기 프로세스를 가짐</strong></p><ol><li>최초 접근 시 서버에 요청 전달</li><li>응답 시 쿠키(cookie)에 서버 정보 추가 후 반환</li><li>재요청 시 proxy에서 쿠키 정보 확인 &gt; 최초 요청 서버로 전달</li><li>다시 접근 시 쿠키 추가 없이 전달 &gt; 클라이언트에 쿠키 정보가 계속 존재함(쿠키 재사용)</li></ol><p><img src="/image/haproxy-process.png" alt="그림2"></p><h1 id="Reference"><a href="#Reference" class="headerlink" title=" Reference"></a><a name="reference"></a> Reference</h1><ul><li><a href="https://d2.naver.com/helloworld/284659">https://d2.naver.com/helloworld/284659</a></li></ul><hr><p>made by <em>jaejun.lee</em></p>]]></content:encoded>
      
      <comments>https://jx2lee.github.io/network-introduction_to_haproxy/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[Network] REST API 간단히 살펴보기</title>
      <link>https://jx2lee.github.io/network-introduction_to_restapi/</link>
      <guid>https://jx2lee.github.io/network-introduction_to_restapi/</guid>
      <pubDate>Mon, 15 Mar 2021 15:00:00 GMT</pubDate>
      <description>
      
        &lt;center&gt;&lt;span class=&quot;github-emoji&quot; alias=&quot;fire&quot; style=&quot;&quot; fallback-src=&quot;https://github.githubassets.com/images/icons/emoji/unicode/1f525.png?v8&quot;&gt;&amp;#x1f525;&lt;/span&gt;&lt;/center&gt;
&lt;br&gt;
&lt;br&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<center><span class="github-emoji" alias="fire" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f525.png?v8">&#x1f525;</span></center><br><br><a id="more"></a><p><strong><span class="github-emoji" alias="sparkles" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png?v8">&#x2728;</span> Contents:</strong>  </p><ol><li><a href="#subject01">REST?</a></li><li><a href="#subject02">구성요소</a></li><li><a href="#subject03">REST API?</a></li><li><a href="#subject03">RESTful?</a></li><li><a href="#reference">Reference</a></li></ol><h1 id="REST"><a href="#REST" class="headerlink" title=" REST?"></a><a name="subject01"></a> REST?</h1><h2 id="Representational-State-Transfer"><a href="#Representational-State-Transfer" class="headerlink" title="Representational State Transfer"></a>Representational State Transfer</h2><ul><li>자원의 이름(표현)으로 구분하여 해당 자원의 상태(정보)를 주고 받는 모든 것</li><li>resouce 의 representation 에 의한 state 전달!<ul><li>resource: SW가 관리할 수 있는 모든 것 (ex. 문서, 그림, 데이터 등등)</li><li>representation: resource 를 표현하기 위한 이름 (ex. 학생 정보가 resource 인 경우 <em>student</em> 가 representation)</li></ul></li><li>Web + HTTP 프로토콜 활용</li><li>Client &lt;-&gt; Server 통신 방법 중 하나</li></ul><h2 id="HTTP-URI-Uniform-Resource-Identifier-를-통해-Resource를-명시-HTTP-Method-POST-GET-PUT-DELETE-를-통해-해당-resource에-대한-CRUD-Operation을-적용하는-것"><a href="#HTTP-URI-Uniform-Resource-Identifier-를-통해-Resource를-명시-HTTP-Method-POST-GET-PUT-DELETE-를-통해-해당-resource에-대한-CRUD-Operation을-적용하는-것" class="headerlink" title="HTTP URI(Uniform Resource Identifier)를 통해 Resource를 명시, HTTP Method(POST, GET, PUT, DELETE)를 통해 해당 resource에 대한 CRUD Operation을 적용하는 것"></a>HTTP URI(Uniform Resource Identifier)를 통해 Resource를 명시, HTTP Method(POST, GET, PUT, DELETE)를 통해 해당 resource에 대한 CRUD Operation을 적용하는 것</h2><h2 id="구성요소"><a href="#구성요소" class="headerlink" title=" 구성요소"></a><a name="subject02"></a> 구성요소</h2><ul><li>Resource: URI<ul><li>모든 자원에 대한 ID 는 고유 (in server)</li><li>URI를 이용해 server 에 resource 상태(정보) 를 요청</li></ul></li><li>Verb: HTTP Method<ul><li>GET/PUT/POST/DELETE</li></ul></li><li>Representation<ul><li>요청에 적절한 응답</li><li>resource 종류: JSON/XML/TEXT/RSS<ul><li>이중 JSON 과 XML을 이용해 데이터를 주고 받음 (주)</li></ul></li></ul></li></ul><h1 id="REST-API"><a href="#REST-API" class="headerlink" title=" REST API?"></a><a name="subject03"></a> REST API?</h1><h2 id="REST-기반으로-서비스-API를-구현한-것"><a href="#REST-기반으로-서비스-API를-구현한-것" class="headerlink" title="REST 기반으로 서비스 API를 구현한 것"></a>REST 기반으로 서비스 API를 구현한 것</h2><ul><li><code>API:</code>데이터와 기능의 집합을 제공하여 컴퓨터 프로그램간 상호작용을 촉진하며, 서로 정보를 교환가능 하도록 하는 것</li></ul><p><img src="https://api.zestard.com/wp-content/uploads/2015/12/What-is-Rest-API-02-1.jpg" alt="https://api.zestard.com/wp-content/uploads/2015/12/What-is-Rest-API-02-1.jpg"></p><h1 id="RESTful"><a href="#RESTful" class="headerlink" title=" RESTful?"></a><a name="subject04"></a> RESTful?</h1><h2 id="REST-원리를-잘-따르면-RESTful-하다"><a href="#REST-원리를-잘-따르면-RESTful-하다" class="headerlink" title="REST 원리를 잘 따르면 RESTful 하다"></a>REST 원리를 잘 따르면 RESTful 하다</h2><h2 id="RESTful은-일반적으로-REST라는-아키텍처를-구현하는-웹-서비스를-나타내기-위해-사용되는-용어"><a href="#RESTful은-일반적으로-REST라는-아키텍처를-구현하는-웹-서비스를-나타내기-위해-사용되는-용어" class="headerlink" title="RESTful은 일반적으로 REST라는 아키텍처를 구현하는 웹 서비스를 나타내기 위해 사용되는 용어"></a>RESTful은 일반적으로 REST라는 아키텍처를 구현하는 웹 서비스를 나타내기 위해 사용되는 용어</h2><h1 id="Reference"><a href="#Reference" class="headerlink" title=" Reference"></a><a name="reference"></a> Reference</h1><ul><li><a href="https://gmlwjd9405.github.io/2018/09/21/rest-and-restful.html">https://gmlwjd9405.github.io/2018/09/21/rest-and-restful.html</a></li><li><a href="https://www.zestard.com/blog/rest-api-benefits/">https://www.zestard.com/blog/rest-api-benefits/</a></li></ul><hr><p>made by <em>jaejun.lee</em></p>]]></content:encoded>
      
      <comments>https://jx2lee.github.io/network-introduction_to_restapi/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[Kubernetes] Pod 삭제 이벤트 마슬러보기</title>
      <link>https://jx2lee.github.io/cloud-events_at_pod_deletion/</link>
      <guid>https://jx2lee.github.io/cloud-events_at_pod_deletion/</guid>
      <pubDate>Wed, 17 Feb 2021 15:00:00 GMT</pubDate>
      <description>
      
        &lt;center&gt; Kubernetes 의 Pod 가 삭제되는 과정을 살펴본다. &lt;/center&gt;
&lt;br&gt;
&lt;br&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<center> Kubernetes 의 Pod 가 삭제되는 과정을 살펴본다. </center><br><br><a id="more"></a><p><strong><span class="github-emoji" alias="sparkles" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png?v8">&#x2728;</span> Contents:</strong>  </p><ol><li><a href="#subject01">Overview</a></li><li><a href="#subject02">Events</a><ul><li><a href="#subject03">A event</a></li><li><a href="#subject04">B event</a></li></ul></li><li><a href="#subject05">Parallel events</a></li><li><a href="#reference">Reference</a></li></ol><h1 id="Overview"><a href="#Overview" class="headerlink" title=" Overview"></a><a name="subject01"></a> Overview</h1><p>Kubernetes 클러스터의 구성 요소는 여러 시스템에서 별도의 프로세스로 실행된다. 하나의 큰 monolithic 프로세스의 일부가 아니다. 모든 구성 요소가 클러스터 상태와 관련하여 동일한 페이지에 있는 데는 시간이 걸린다. Pod 가 삭제 될 때 클러스터 전체에서 어떤 일이 발생하는지 살펴본다.</p><h1 id="Events"><a href="#Events" class="headerlink" title=" Events"></a><a name="subject02"></a> Events</h1><p>pod 삭제 요청이 API 서버에 수신되면, 먼저 etcd 에서 상태를 수정 한 다음 삭제를 알린다 (이러한 noti 를 알리는 요소들은 Kubelet 및 Endpoints Controller - <em>kube-controller-manager</em> 가 있다.) <strong>병렬</strong>로 발생하는 두 가지 이벤트(<code>A</code> 와 <code>B</code>로 표시)를 그림으로 보면 다음과 같다.</p><p><img src="/image/events-when-pod-is-deleted.png" alt="events-when-pod-is-deleted.png"></p><h2 id="A-event"><a href="#A-event" class="headerlink" title=" A event"></a><a name="subject03"></a> A event</h2><p>etcd 에서 상태 수정 후 API server 를 통해 Kubelet 에게 포드를 종료해야한다는 알림을 전송하면 종료 시퀀스를 시작한다. (사전 중지 후크 실행-<em>pre-stop-hook</em>, SIGTERM 전송, 잠시 후 컨테이너가 종료되지 않은 경우 강제 종료). 앱이 클라이언트 요청 수신을 즉시 중단하여 SIGTERM에 응답하는 경우, 연결을 시도하는 모든 클라이언트는 연결 거부 오류를 수신한다. API 서버에서 Kubelet으로의 직접 경로로 인해 포드가 삭제 된 시간부터이 문제가 발생하는 데 걸리는 시간은 비교적 짧다.</p><ul><li><code>A1</code>: API server 가 해당 Pod 가 기동되는 Node 의 Kubelet 에게 종료 요청</li><li><code>A2</code>: Kubelet 이 종료 시퀀스 시작<ul><li>A2-(1): pre-stop-hook</li><li>A2-(2): SIGTERM 전송</li><li>A2-(3): 종료되지 않은 경우 강제로 종료 수행</li></ul></li></ul><h2 id="B-event"><a href="#B-event" class="headerlink" title=" B event"></a><a name="subject04"></a> B event</h2><p>pod로 이어지는 이벤트는 iptables 규칙을 제거한다(<code>B4</code>). Endpoints 컨트롤러 (Kubernetes Control Plane의 Controller Manager)는 삭제 알림(<code>B1</code>)을 받으면 포드가 속한 모든 서비스의 endpoint 를 제거한다(<code>B2</code>). API 서버는 REST 요청을 전송하여 Endpoints API 객체를 수정한다.</p><p>그런 다음 API 서버는 Endpoints 개체를 보고있는 모든 node의 kube-proxy 에게 알린다(<code>B3</code>). 각 kube-proxy 는 해당 노드의 iptables 규칙을 업데이트 하므로 앱에 새로운 연결이 종료 Pod 로 전달되지 않습니다. 여기서 iptables 규칙을 제거해도 기존 연결에 영향을 주지 않는다. 다른 Pod 에 연결된 클라이언트는 기존 연결을 통해 Pod 에 추가 요청을 보낼 수 있다. (삭제되지 않는 다른 Pod 에는 영향을 주지 않는다는 이야기)</p><ul><li><code>B1</code>: API server 는 controller manager 에게 삭제 알림을 전달</li><li><code>B2</code>: controller manager 는 삭제될 pod 가 속한 모든 서비스의 endpoints 삭제</li><li><code>B3</code>: endpoint 객체 수정을 위해 API server 는 각 node 의 kube-proxy 에게 수정을 요청</li><li><code>B4</code>: 삭제될 Pod 가 속한 node 의 kube-proxy 가 삭제될 Pod 관련 iptables 를 삭제</li></ul><h1 id="Parallel-events"><a href="#Parallel-events" class="headerlink" title=" Parallel events"></a><a name="subject05"></a> Parallel events</h1><p>이 두 가지 이벤트(<code>A</code>, <code>B</code>)는 모두 병렬로 수행한다. Pod 에서 앱을 종료하는 데 걸리는 시간은 iptables 규칙을 업데이트 하는 시간보다 짧은데, 이는 iptables 규칙을 업데이트 하는 체인이 길기 때문이다.</p><ul><li>API 서버에 새 요청을 보내고 Endpoints 컨트롤러에 도달 한 다음, API 서버가 또 이를 알려야 한다<ul><li>API server 를 여러 번 거쳐가야 함</li></ul></li><li>모든 노드에서 iptables 규칙이 업데이트 되기 전 SIGTERM 신호가 전송 될 가능성이 높다</li></ul><p><img src="/image/timeline-events.png" alt="timeline-events.png"></p><blockquote><p><strong>위 그림만 생각하면 Pod deletion 프로세스는 쉽게 이해할 수 있을 것 같다.</strong></p></blockquote><h1 id="Reference"><a href="#Reference" class="headerlink" title=" Reference"></a><a name="reference"></a> Reference</h1><ul><li><a href="https://freecontent.manning.com/handling-client-requests-properly-with-kubernetes/">https://freecontent.manning.com/handling-client-requests-properly-with-kubernetes/</a></li></ul><hr><p>made by <em>jaejun.lee</em></p>]]></content:encoded>
      
      <comments>https://jx2lee.github.io/cloud-events_at_pod_deletion/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[Hadoop] Hadoop Client 환경 구성 시 발생한 에러 정리</title>
      <link>https://jx2lee.github.io/hadoop-troubleshoot_hadoop_client/</link>
      <guid>https://jx2lee.github.io/hadoop-troubleshoot_hadoop_client/</guid>
      <pubDate>Sat, 09 Jan 2021 15:00:00 GMT</pubDate>
      <description>
      
        &lt;center&gt;Hadoop Client 환경 세팅 중 발생한 문제를 공유한다.&lt;/center&gt;
&lt;br&gt;
&lt;br&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<center>Hadoop Client 환경 세팅 중 발생한 문제를 공유한다.</center><br><br><a id="more"></a><p><strong><span class="github-emoji" alias="sparkles" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png?v8">&#x2728;</span> Contents:</strong>  </p><ol><li><a href="#subject02">hadoop client 에서 hdfs dfs 명령어 수행 시 connection-pending remote 에러 발생</a><ul><li><a href="#subject03">Log</a></li><li><a href="#subject04">Cause</a></li><li><a href="#subject05">Resolve</a></li></ul></li><li><a href="#subject06">hdfs dfs 명령어 수행 시 Unable to load native-hadoop library for your platform 에러</a><ul><li><a href="#subject07">Log</a></li><li><a href="#subject08">Cause</a></li><li><a href="#subject09">Resolve</a></li></ul></li></ol><h1 id="hadoop-client-에서-hdfs-dfs-명령어-수행-시-connection-pending-remote-에러-발생"><a href="#hadoop-client-에서-hdfs-dfs-명령어-수행-시-connection-pending-remote-에러-발생" class="headerlink" title=" hadoop client 에서 hdfs dfs 명령어 수행 시 connection-pending remote 에러 발생"></a><a name="subject02"></a> hadoop client 에서 hdfs dfs 명령어 수행 시 connection-pending remote 에러 발생</h1><h2 id="Log"><a href="#Log" class="headerlink" title=" Log"></a><a name="subject03"></a> Log</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">21/01/08 17:58:35 WARN impl.BlockReaderFactory: I/O error constructing remote block reader.</span><br><span class="line">org.apache.hadoop.net.ConnectTimeoutException: 60000 millis timeout <span class="keyword">while</span> waiting <span class="keyword">for</span> channel to be ready <span class="keyword">for</span> connect. ch : java.nio.channels.SocketChannel[connection-pending remote=/192.168.0.11:50010]</span><br><span class="line">at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:535)</span><br><span class="line">at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:2902)</span><br><span class="line">at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:825)</span><br><span class="line">at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:750)</span><br><span class="line">at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:387)</span><br><span class="line">at org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:734)</span><br><span class="line">at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:681)</span><br><span class="line">at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:971)</span><br><span class="line">at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:1029)</span><br><span class="line">at java.io.DataInputStream.read(DataInputStream.java:100)</span><br><span class="line">at org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:93)</span><br><span class="line">at org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:67)</span><br><span class="line">at org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:128)</span><br><span class="line">at org.apache.hadoop.fs.shell.Display<span class="variable">$Cat</span>.printToStdout(Display.java:107)</span><br><span class="line">at org.apache.hadoop.fs.shell.Display<span class="variable">$Cat</span>.processPath(Display.java:102)</span><br><span class="line">at org.apache.hadoop.fs.shell.Command.processPaths(Command.java:327)</span><br><span class="line">at org.apache.hadoop.fs.shell.Command.processPathArgument(Command.java:299)</span><br><span class="line">at org.apache.hadoop.fs.shell.Command.processArgument(Command.java:281)</span><br><span class="line">at org.apache.hadoop.fs.shell.Command.processArguments(Command.java:265)</span><br><span class="line">at org.apache.hadoop.fs.shell.FsCommand.processRawArguments(FsCommand.java:119)</span><br><span class="line">at org.apache.hadoop.fs.shell.Command.run(Command.java:175)</span><br><span class="line">at org.apache.hadoop.fs.FsShell.run(FsShell.java:317)</span><br><span class="line">at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)</span><br><span class="line">at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:90)</span><br><span class="line">at org.apache.hadoop.fs.FsShell.main(FsShell.java:380)</span><br></pre></td></tr></table></figure><h2 id="Cause"><a href="#Cause" class="headerlink" title=" Cause"></a><a name="subject04"></a> Cause</h2><ul><li>Wrong configuration <code>hdfs-site.xml</code></li><li>Not included <code>Namenode hostname</code> in <code>/etc/hosts</code></li></ul><h2 id="Resolve"><a href="#Resolve" class="headerlink" title=" Resolve"></a><a name="subject05"></a> Resolve</h2><ul><li><p>Edit <code>hdfs-site.xml</code></p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.client.use.datanode.hostname&lt;/name&gt;</span><br><span class="line">&lt;value&gt;<span class="literal">true</span>&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><ul><li>set <strong>dfs.client.use.datanode.hostname</strong> to <strong>true</strong></li></ul></li><li><p>Edit <code>/etc/hosts</code></p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">133.186.241.218 hadoop-test.novalocal</span><br></pre></td></tr></table></figure><ul><li>Add <strong>IP hostname</strong></li></ul></li></ul><h1 id="hdfs-dfs-명령어-수행-시-Unable-to-load-native-hadoop-library-for-your-platform-에러"><a href="#hdfs-dfs-명령어-수행-시-Unable-to-load-native-hadoop-library-for-your-platform-에러" class="headerlink" title=" hdfs dfs 명령어 수행 시 Unable to load native-hadoop library for your platform 에러"></a><a name="subject06"></a> hdfs dfs 명령어 수행 시 Unable to load native-hadoop library for your platform 에러</h1><h2 id="Log-1"><a href="#Log-1" class="headerlink" title=" Log"></a><a name="subject07"></a> Log</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">21/01/08 12:44:55 WARN util.NativeCodeLoader: Unable to load native-hadoop library <span class="keyword">for</span> your platform... using builtin-java classes <span class="built_in">where</span> applicable</span><br></pre></td></tr></table></figure><h2 id="Cause-1"><a href="#Cause-1" class="headerlink" title=" Cause"></a><a name="subject08"></a> Cause</h2><ul><li>Not existed <code>HADOOP_OPTS</code> variable</li></ul><h2 id="Resolve-1"><a href="#Resolve-1" class="headerlink" title=" Resolve"></a><a name="subject09"></a> Resolve</h2><ul><li><p>Add below line in <code>.bashrc</code> or <code>.zshrc</code></p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#.bashrc, or .zshrc</span></span><br><span class="line"><span class="built_in">export</span> HADOOP_OPTS=<span class="string">&quot;<span class="variable">$HADOOP_OPTS</span> -Djava.library.path=<span class="variable">$HADOOP_HOME</span>/lib/native&quot;</span></span><br></pre></td></tr></table></figure></li></ul><hr><p>made by <em>jaejun.lee</em></p>]]></content:encoded>
      
      <comments>https://jx2lee.github.io/hadoop-troubleshoot_hadoop_client/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[Python] Python 환경 구축 with pyenv</title>
      <link>https://jx2lee.github.io/python-pyenv/</link>
      <guid>https://jx2lee.github.io/python-pyenv/</guid>
      <pubDate>Sat, 09 Jan 2021 15:00:00 GMT</pubDate>
      <description>
      
        &lt;center&gt;MacOS 에서 pyenv 를 이용해 Python 개발 환경을 구축한다.&lt;/center&gt;
&lt;br&gt;
&lt;br&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<center>MacOS 에서 pyenv 를 이용해 Python 개발 환경을 구축한다.</center><br><br><a id="more"></a><p><strong><span class="github-emoji" alias="sparkles" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png?v8">&#x2728;</span> Contents:</strong>  </p><ol><li><a href="#subject01">Install brew</a></li><li><a href="#subject02">Install pyenv</a></li><li><a href="#subject03">Set env</a><ul><li><a href="#subject04">export PATH</a></li><li><a href="#subject05">가 설치 패키지 (권장)</a></li></ul></li><li><a href="#subject06">Install Python)</a><ul><li><a href="#subject07">global python 설정 후 가상환경 세팅 (with virtualenvwrapper))</a></li><li><a href="#subject08">Set config)</a></li><li><a href="#reference">Reference</a></li></ul></li></ol><h1 id="Install-brew"><a href="#Install-brew" class="headerlink" title=" Install brew"></a><a name="subject01"></a> Install brew</h1><p><code>$ ruby -e &quot;$(curl -fsSL [https://raw.githubusercontent.com/Homebrew/install/master/install](https://raw.githubusercontent.com/Homebrew/install/master/install))&quot;</code></p><blockquote><p><em>이미 존재하면 homebrew 설치는 생략한다.</em></p></blockquote><h1 id="Install-pyenv"><a href="#Install-pyenv" class="headerlink" title=" Install pyenv"></a><a name="subject02"></a> Install pyenv</h1><p><code>$ brew install pyenv</code></p><h1 id="Set-env"><a href="#Set-env" class="headerlink" title=" Set env"></a><a name="subject03"></a> Set env</h1><h2 id="export-PATH"><a href="#export-PATH" class="headerlink" title=" export PATH"></a><a name="subject04"></a> export PATH</h2><p><code>$ echo &#39;eval &quot;$(pyenv init -)&quot;&#39; &gt;&gt; ~/.zshrc</code></p><ul><li><p>PATH 환경 변수를 관리하기 위해 pyenv를 시작하고 환경에 나타나는 첫 번째 버전과 달리 실행할 Python 버전을 삽입할 수 있다고 한다.</p><ul><li><p>환경변수 세팅을 안하고 진행을 해보니, 나중에 virtualenvwrapper 패키지 사용 시 virtual path not found error 를 반환하므로 꼭 추가하시길 바랍니다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">&gt; mkvirtualenv t</span><br><span class="line">ERROR: virtualenvwrapper could not find virtualenv <span class="keyword">in</span> your path</span><br><span class="line"><span class="comment"># after set eval command..</span></span><br><span class="line">❯ mkvirtualenv t</span><br><span class="line">created virtual environment CPython3.7.9.final.0-64 <span class="keyword">in</span> 220ms</span><br><span class="line">  creator CPython3Posix(dest=/Users/nhn/workspace/programming/python/.virtualenvs/t, clear=False, no_vcs_ignore=False, global=False)</span><br><span class="line">  seeder FromAppData(download=False, pip=bundle, setuptools=bundle, wheel=bundle, via=copy, app_data_dir=/Users/nhn/Library/Application Support/virtualenv)</span><br><span class="line">    added seed packages: pip==20.3.1, setuptools==51.0.0, wheel==0.36.1</span><br><span class="line">  activators BashActivator,CShellActivator,FishActivator,PowerShellActivator,PythonActivator,XonshActivator</span><br><span class="line">virtualenvwrapper.user_scripts creating /Users/nhn/workspace/programming/python/.virtualenvs/t/bin/predeactivate</span><br><span class="line">virtualenvwrapper.user_scripts creating /Users/nhn/workspace/programming/python/.virtualenvs/t/bin/postdeactivate</span><br><span class="line">virtualenvwrapper.user_scripts creating /Users/nhn/workspace/programming/python/.virtualenvs/t/bin/preactivate</span><br><span class="line">virtualenvwrapper.user_scripts creating /Users/nhn/workspace/programming/python/.virtualenvs/t/bin/postactivate</span><br><span class="line">virtualenvwrapper.user_scripts creating /Users/nhn/workspace/programming/python/.virtualenvs/t/bin/get_env_details</span><br></pre></td></tr></table></figure></li></ul></li><li><p>bash 환경일 경우 <code>.bashrc</code> 에 삽입하도록 한다.</p></li></ul><h2 id="추가-설치-패키지-권장"><a href="#추가-설치-패키지-권장" class="headerlink" title=" 추가 설치 패키지 (권장)"></a><a name="subject05"></a> 추가 설치 패키지 (권장)</h2><p><code>$ brew install zlib sqlite</code></p><ul><li><p>zlib 압축 알고리즘과 SQLite 데이터베이스는 pyenv의 종속된 패키지</p></li><li><p>올바르게 구성되지 않을 경우 빌드 문제를 일으키는 경우가 많다고 한다. 설치 후 아래와 같은 환경 변수를 적용한다.</p>  <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ export LDFLAGS=<span class="string">&quot;-L/usr/local/opt/zlib/lib -L/usr/local/opt/sqlite/lib&quot;</span></span><br><span class="line">$ export CPPFLAGS=<span class="string">&quot;-I/usr/local/opt/zlib/include -I/usr/local/opt/sqlite/include&quot;</span></span><br></pre></td></tr></table></figure></li></ul><h1 id="Install-Python"><a href="#Install-Python" class="headerlink" title=" Install Python"></a><a name="subject06"></a> Install Python</h1><p><code>$ pyenv install —list</code> 명령어로 설치 가능한 리스트를 확인하고 설치를 진행한다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">&gt; pyenv install --list | grep 3.7.</span><br><span class="line">  3.7.0</span><br><span class="line">  3.7-dev</span><br><span class="line">  3.7.1</span><br><span class="line">  3.7.2</span><br><span class="line">  3.7.3</span><br><span class="line">  3.7.4</span><br><span class="line">  3.7.5</span><br><span class="line">  3.7.6</span><br><span class="line">  3.7.7</span><br><span class="line">  3.7.8</span><br><span class="line">  3.7.9</span><br><span class="line">  miniconda-3.7.0</span><br><span class="line">  miniconda3-3.7.0</span><br><span class="line">  stackless-3.7.5</span><br><span class="line">&gt; pyenv install 3.7.9</span><br></pre></td></tr></table></figure><h2 id="global-python-설정-후-가상환경-세팅-with-virtualenvwrapper"><a href="#global-python-설정-후-가상환경-세팅-with-virtualenvwrapper" class="headerlink" title=" global python 설정 후 가상환경 세팅 (with virtualenvwrapper)"></a><a name="subject07"></a> global python 설정 후 가상환경 세팅 (with virtualenvwrapper)</h2><p><code>$ pyenv global 3.7.9</code></p><ul><li>python 버젼을 3.7.9 로 세팅</li></ul><p><code>$ $(pyenv which python3) -m pip install virtualenvwrapper</code></p><ul><li>global python 버젼으로 virtualenvwrapper 패키지 설치</li></ul><h2 id="Set-config"><a href="#Set-config" class="headerlink" title=" Set config"></a><a name="subject08"></a> Set config</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">echo</span> <span class="string">&#x27;export WORKON_HOME=~/workspace/programming/python/.virtualenvs&#x27;</span> &gt;&gt; ~/.zshrc</span><br><span class="line">$ <span class="built_in">echo</span> <span class="string">&#x27;mkdir -p $WORKON_HOME&#x27;</span> &gt;&gt; ~/.zshrc</span><br><span class="line">$ <span class="built_in">echo</span> <span class="string">&#x27;. ~/.pyenv/versions/3.7.9/bin/virtualenvwrapper.sh&#x27;</span> &gt;&gt; ~/.zshrc</span><br><span class="line">$ <span class="built_in">source</span> ~/.zshrc</span><br></pre></td></tr></table></figure><p>이후 <code>mkvirtualenv</code> 커맨드를 통해 가상환경을 생성하고 작업을 진행하면 된다.</p><h1 id="Reference"><a href="#Reference" class="headerlink" title=" Reference"></a><a name="reference"></a> Reference</h1><ul><li><a href="https://opensource.com/article/19/6/python-virtual-environments-mac">https://opensource.com/article/19/6/python-virtual-environments-mac*</a></li></ul><hr><p>made by <em>jaejun.lee</em></p>]]></content:encoded>
      
      <comments>https://jx2lee.github.io/python-pyenv/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[Jenkins] Jenkins 설치 on Instance</title>
      <link>https://jx2lee.github.io/jenkins-install_jenkins/</link>
      <guid>https://jx2lee.github.io/jenkins-install_jenkins/</guid>
      <pubDate>Fri, 08 Jan 2021 15:00:00 GMT</pubDate>
      <description>
      
        &lt;p&gt;Docker 로 젠킨스 환경을 세팅하려고 했는데..&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://hub.docker.com/_/jenkins&quot;&gt;https://hub.docker.com/_/jenkins&lt;/a&gt; 을 살펴보면 더 이상 지원을 안하는 것으로 파악된다. 지속적으로 업데이트되는 부분을 함께 포함하기 위해 과감히(?) docker 환경을 접고 jenkins 서버를 구축하고 환경을 설정해보도록 한다.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Docker 로 설치하는 방법도 정리하였다.&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;/blockquote&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p>Docker 로 젠킨스 환경을 세팅하려고 했는데..</p><p><a href="https://hub.docker.com/_/jenkins">https://hub.docker.com/_/jenkins</a> 을 살펴보면 더 이상 지원을 안하는 것으로 파악된다. 지속적으로 업데이트되는 부분을 함께 포함하기 위해 과감히(?) docker 환경을 접고 jenkins 서버를 구축하고 환경을 설정해보도록 한다.</p><blockquote><p>Docker 로 설치하는 방법도 정리하였다.<br><br><br><br></p></blockquote><a id="more"></a><p><strong><span class="github-emoji" alias="sparkles" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png?v8">&#x2728;</span> Contents:</strong>  </p><ol><li><a href="#subject01">Install Java</a></li><li><a href="#subject02">Install Maven</a><ul><li><a href="#subject03">.bashrc</a></li></ul></li><li><a href="#subject04">Install Git</a></li><li><a href="#subject05">Install Jenkins</a><ul><li><a href="#subject06">Edit sysconfig (jenkins config)</a></li></ul></li><li><a href="#subject07">on Docker..</a></li><li><a href="#reference">Reference</a></li></ol><h1 id="Install-Java"><a href="#Install-Java" class="headerlink" title=" Install Java"></a><a name="subject01"></a> Install Java</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">$ yum list java-1.8.0-openjdk-devel*</span><br><span class="line">base                                                                                | 3.6 kB  00:00:00</span><br><span class="line">epel/x86_64/metalink                                                                | 7.8 kB  00:00:00</span><br><span class="line">epel                                                                                | 4.7 kB  00:00:00</span><br><span class="line">extras                                                                              | 2.9 kB  00:00:00</span><br><span class="line">updates                                                                             | 2.9 kB  00:00:00</span><br><span class="line">(1/7): base/7/x86_64/group_gz                                                       | 153 kB  00:00:00</span><br><span class="line">(2/7): extras/7/x86_64/primary_db                                                   | 222 kB  00:00:00</span><br><span class="line">(3/7): updates/7/x86_64/primary_db                                                  | 4.7 MB  00:00:00</span><br><span class="line">(4/7): epel/x86_64/group_gz                                                         |  95 kB  00:00:00</span><br><span class="line">(5/7): base/7/x86_64/primary_db                                                     | 6.1 MB  00:00:00</span><br><span class="line">(6/7): epel/x86_64/primary_db                                                       | 6.9 MB  00:00:01</span><br><span class="line">(7/7): epel/x86_64/updateinfo                                                       | 1.0 MB  00:00:07</span><br><span class="line">Available Packages</span><br><span class="line">java-1.8.0-openjdk-devel.i686                         1:1.8.0.275.b01-0.el7_9                       updates</span><br><span class="line">java-1.8.0-openjdk-devel.x86_64                       1:1.8.0.275.b01-0.el7_9                       updates</span><br><span class="line"></span><br><span class="line">$ yum install -y java-1.8.0-openjdk-devel.x86_64</span><br></pre></td></tr></table></figure><ul><li><p>이후 JAVA 환경변수 등록을 위해 실제 주소를 알아내고 이를 .bashrc 에 추가한다.</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ readlink -f /usr/bin/javac</span><br><span class="line">/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.275.b01-0.el7_9.x86_64/bin/javac</span><br><span class="line"><span class="comment">#JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.275.b01-0.el7_9.x86_64</span></span><br></pre></td></tr></table></figure></li></ul><h1 id="Install-Maven"><a href="#Install-Maven" class="headerlink" title=" Install Maven"></a><a name="subject02"></a> Install Maven</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ sudo mkdir -p /app &amp;&amp; sudo chmod 777 /app</span><br><span class="line">$ wget https://downloads.apache.org/maven/maven-3/3.6.3/binaries/apache-maven-3.6.3-bin.tar.gz -P /app</span><br><span class="line">$ tar -xvzf apache-maven-3.6.3-bin.tar.gz</span><br></pre></td></tr></table></figure><h2 id="bashrc"><a href="#bashrc" class="headerlink" title=" .bashrc"></a><a name="subject03"></a> <code>.bashrc</code></h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># JAVA_HOME</span></span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.275.b01-0.el7_9.x86_64</span><br><span class="line"></span><br><span class="line"><span class="comment"># MAVEN_HOME</span></span><br><span class="line"><span class="built_in">export</span> MAVEN_HOME=/app/apache-maven-3.6.3</span><br><span class="line"></span><br><span class="line"><span class="comment"># PATH</span></span><br><span class="line">PATH=<span class="variable">$PATH</span>:<span class="variable">$JAVA_HOME</span>/bin:<span class="variable">$MAVEN_HOME</span>/bin</span><br><span class="line"><span class="built_in">export</span> PATH</span><br></pre></td></tr></table></figure><h1 id="Install-Git"><a href="#Install-Git" class="headerlink" title=" Install Git"></a><a name="subject04"></a> Install Git</h1><p><code>sudo yum install -y git</code></p><h1 id="Install-Jenkins"><a href="#Install-Jenkins" class="headerlink" title=" Install Jenkins"></a><a name="subject05"></a> Install Jenkins</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ sudo wget -O /etc/yum.repos.d/jenkins.repo https://pkg.jenkins.io/redhat-stable/jenkins.repo</span><br><span class="line">$ sudo rpm --import https://pkg.jenkins.io/redhat-stable/jenkins.io.key</span><br><span class="line">$ sudo yum install -y jenkins</span><br><span class="line">$ sudo systemctl <span class="built_in">enable</span> jenkins</span><br></pre></td></tr></table></figure><h1 id="Edit-sysconfig-jenkins-config"><a href="#Edit-sysconfig-jenkins-config" class="headerlink" title=" Edit sysconfig (jenkins config)"></a><a name="subject06"></a> Edit sysconfig (jenkins config)</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">$ sudo cat /etc/sysconfig/jenkins</span><br><span class="line"><span class="comment">## Path:        Development/Jenkins</span></span><br><span class="line"><span class="comment">## Description: Jenkins Automation Server</span></span><br><span class="line"><span class="comment">## Type:        string</span></span><br><span class="line"><span class="comment">## Default:     &quot;/var/lib/jenkins&quot;</span></span><br><span class="line"><span class="comment">## ServiceRestart: jenkins</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Directory where Jenkins store its configuration and working</span></span><br><span class="line"><span class="comment"># files (checkouts, build reports, artifacts, ...).</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line">JENKINS_HOME=<span class="string">&quot;/jenkins_home&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## Type:        string</span></span><br><span class="line"><span class="comment">## Default:     &quot;&quot;</span></span><br><span class="line"><span class="comment">## ServiceRestart: jenkins</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Java executable to run Jenkins</span></span><br><span class="line"><span class="comment"># When left empty, we&#x27;ll try to find the suitable Java.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"></span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line">JENKINS_USER=<span class="string">&quot;jenkins&quot;</span></span><br><span class="line"></span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line">JENKINS_PORT=<span class="string">&quot;9090&quot;</span></span><br><span class="line"></span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line"><span class="comment">## Type:        string</span></span><br><span class="line"><span class="comment">## Default:     &quot;&quot;</span></span><br><span class="line"><span class="comment">## ServiceRestart: jenkins</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Pass arbitrary arguments to Jenkins.</span></span><br><span class="line"><span class="comment"># Full option list: java -jar jenkins.war --help</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line">JENKINS_ARGS=<span class="string">&quot;&quot;</span></span><br></pre></td></tr></table></figure><ul><li>이후 서비스를 실행<ul><li><code>sudo systemctl start jenkins</code></li></ul></li></ul><h1 id="On-Docker"><a href="#On-Docker" class="headerlink" title=" On Docker.."></a><a name="subject07"></a> On Docker..</h1><ul><li><p>Pull Jenkins Image</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ docker pull jenkins/jenkins:lts</span><br><span class="line">$ docker images</span><br><span class="line">REPOSITORY        TAG       IMAGE ID       CREATED       SIZE</span><br><span class="line">jenkins/jenkins   lts       1920bf702d7d   4 weeks ago   713MB</span><br></pre></td></tr></table></figure></li><li><p>Run Container</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">docker run -d -p 8080:8080 --name jenkins_test \</span><br><span class="line">                -v /home/centos/jenkins_home:/var/jenkins_home \</span><br><span class="line">                -v /var/run/docker.sock:/var/run/docker.sock -u root jenkins/jenkins:lts</span><br></pre></td></tr></table></figure><ul><li>/home/centos/jenkins_home 호스트 디렉토리를 container /var/jenkins_home 으로 mount</li><li>/var/run/docker.sock 파일을 마운트하는 이유는 일반 user 도 해당 디렉토리를 넘나들 수 있도록 설정하기 위함</li></ul></li><li><p>Add Outbound rule</p><ul><li>본인은 토스트 클라우드를 사용중이므로 사용중인 클라우드의 보안그룹에 outbound rule 추가 (8080포트)</li></ul></li><li><p>Verify initial admin password</p><ul><li>컨테이너 안으로 들어가 확인해도(<em>ex. docker exec -ti {container_name} sh</em>) 되지만 docker exec 에 cat 만 전달하여 <code>initialAdminPassword</code> 내용을 확인한다<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ docker <span class="built_in">exec</span> -it jenkins_test cat /var/jenkins_home/secrets/initialAdminPassword</span><br><span class="line">75c9d1c35b674306a2ed9f114f49240f</span><br></pre></td></tr></table></figure></li></ul></li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title=" Reference"></a><a name="reference"></a> Reference</h1><ul><li><a href="https://blog.jiniworld.me/88">https://blog.jiniworld.me/88</a></li><li><a href="https://velog.io/@wimes/Jenkins%EB%A5%BC-%EC%9D%B4%EC%9A%A9%ED%95%B4-Docker%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8-%EB%B9%8C%EB%93%9C%ED%95%B4%EB%B3%B4%EA%B8%B0">https://velog.io/@wimes/Jenkins를-이용해-Docker프로젝트-빌드해보기</a></li></ul><hr><p>made by <em>jaejun.lee</em></p>]]></content:encoded>
      
      <comments>https://jx2lee.github.io/jenkins-install_jenkins/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[Node.js] NVM 설치</title>
      <link>https://jx2lee.github.io/nodejs-install_nvm/</link>
      <guid>https://jx2lee.github.io/nodejs-install_nvm/</guid>
      <pubDate>Fri, 08 Jan 2021 15:00:00 GMT</pubDate>
      <description>
      
        &lt;center&gt;node.js 버전 매니저인 NVM 을 설치하고 살펴본다.&lt;/center&gt;
&lt;br&gt;
&lt;br&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<center>node.js 버전 매니저인 NVM 을 설치하고 살펴본다.</center><br><br><a id="more"></a><p><strong><span class="github-emoji" alias="sparkles" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png?v8">&#x2728;</span> Contents:</strong>  </p><ol><li><a href="#subject01">NVM?</a></li><li><a href="#subject02">Install NVM</a><ul><li><a href="#subject03">.bashrc (or .zsh)</a></li><li><a href="#subject04">Installing a specific version of Nodejs</a></li></ul></li><li><a href="#subject05">Commands</a></li><li><a href="#reference">Reference</a></li></ol><h1 id="NVM"><a href="#NVM" class="headerlink" title=" NVM?"></a><a name="subject01"></a> NVM?</h1><ul><li>node js 버전 매니저로 시스템에 여러 개의 nodejs 를 설치하고 사용할 버전을 쉽게 전환할 수록 도와주는 shell script</li><li>rvm(Ruby Version Manager) 와 비슷한 역할을 수행</li></ul><h1 id="Install-NVM"><a href="#Install-NVM" class="headerlink" title=" Install NVM"></a><a name="subject02"></a> Install NVM</h1><p><code>curl -o- https://raw.githubusercontent.com/creationix/nvm/v0.33.8/install.sh | bash</code></p><h2 id="bashrc-or-zsh"><a href="#bashrc-or-zsh" class="headerlink" title=" .bashrc (or .zsh)"></a><a name="subject03"></a> .bashrc (or .zsh)</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ vi ~/.zshrc</span><br><span class="line"><span class="comment"># Nodejs</span></span><br><span class="line"><span class="built_in">export</span> NVM_DIR=<span class="string">&quot;<span class="variable">$HOME</span>/.nvm&quot;</span></span><br><span class="line">[ -s <span class="string">&quot;<span class="variable">$NVM_DIR</span>/nvm.sh&quot;</span> ] &amp;&amp; \. <span class="string">&quot;<span class="variable">$NVM_DIR</span>/nvm.sh&quot;</span>  <span class="comment"># This loads nvm</span></span><br></pre></td></tr></table></figure><h1 id="Installing-a-specific-version-of-Nodejs"><a href="#Installing-a-specific-version-of-Nodejs" class="headerlink" title=" Installing a specific version of Nodejs"></a><a name="subject04"></a> Installing a specific version of Nodejs</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">$ nvm install 13.1.0</span><br><span class="line">$ nvm ls</span><br><span class="line">-&gt;      v13.1.0</span><br><span class="line">         system</span><br><span class="line">default -&gt; 13.1.0 (-&gt; v13.1.0)</span><br><span class="line">node -&gt; stable (-&gt; v13.1.0) (default)</span><br><span class="line">stable -&gt; 13.1 (-&gt; v13.1.0) (default)</span><br><span class="line">iojs -&gt; N/A (default)</span><br><span class="line">lts/* -&gt; lts/fermium (-&gt; N/A)</span><br><span class="line">lts/argon -&gt; v4.9.1 (-&gt; N/A)</span><br><span class="line">lts/boron -&gt; v6.17.1 (-&gt; N/A)</span><br><span class="line">lts/carbon -&gt; v8.17.0 (-&gt; N/A)</span><br><span class="line">lts/dubnium -&gt; v10.23.1 (-&gt; N/A)</span><br><span class="line">lts/erbium -&gt; v12.20.1 (-&gt; N/A)</span><br><span class="line">lts/fermium -&gt; v14.15.4 (-&gt; N/A)</span><br></pre></td></tr></table></figure><h1 id="Commands"><a href="#Commands" class="headerlink" title=" Commands"></a><a name="subject05"></a> Commands</h1><ul><li>search node version: <code>nvm ls-remote</code></li><li>select specific node version: <code>nvm use &#123;node-version&#125;</code></li><li>set default version:  <code>nvm alias default &#123;node-version&#125;</code></li><li>delete version: <code>nvm uninstall &#123;node-version&#125;</code></li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title=" Reference"></a><a name="reference"></a> Reference</h1><ul><li>lesstif.com/javascript/nvm-node-version-manager-nodejs-82214944.html</li></ul><hr><p>made by <em>jaejun.lee</em></p>]]></content:encoded>
      
      <comments>https://jx2lee.github.io/nodejs-install_nvm/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[Hadoop] Single Node Cluster 설치</title>
      <link>https://jx2lee.github.io/hadoop-install_singloenode_cluster/</link>
      <guid>https://jx2lee.github.io/hadoop-install_singloenode_cluster/</guid>
      <pubDate>Thu, 07 Jan 2021 15:00:00 GMT</pubDate>
      <description>
      
        &lt;p&gt;CentOS 7 환경에서 Single Node Hadoop Clsuter 를 설치한다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;version&lt;ul&gt;
&lt;li&gt;java: 1.8.0&lt;/li&gt;
&lt;li&gt;hadoop: 2.10.1&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p>CentOS 7 환경에서 Single Node Hadoop Clsuter 를 설치한다.</p><ul><li>version<ul><li>java: 1.8.0</li><li>hadoop: 2.10.1</li></ul></li></ul><a id="more"></a><p><strong><span class="github-emoji" alias="sparkles" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png?v8">&#x2728;</span> Contents:</strong>  </p><ol><li><a href="#subject01">Install Java</a></li><li><a href="#subject02">SSH setting</a></li><li><a href="#subject03">Install Hadoop</a></li><li><a href="#subject04">Configuring Hadoop</a><ul><li><a href="#subject05">$HADOOP_HOME/etc/hadoop/core-site.xml</a></li><li><a href="#subject06">$HADOOP/etc/hadoop/hdfs-site.xml</a></li><li><a href="#subject07">$HADOOP_HOME/etc/hadoop/mapred-site.xml</a></li><li><a href="#subject08">$HADOOP_HOME/etc/hadoop/yarn-site.xml</a></li></ul></li><li><a href="#subject09">Running Hadoop</a></li><li><a href="#subject10">Before starting the Cluster, we need to format the Hadoop NN in our local system</a></li><li><a href="#subject11">Start NameNode daemon and DataNode daemon</a><blockquote><p><a href="#reference">Reference</a></p></blockquote></li></ol><h1 id="Install-Java"><a href="#Install-Java" class="headerlink" title=" Install Java"></a><a name="subject01"></a> Install Java</h1><p><code>$ yum install -y java-1.8.0-openjdk</code></p><h1 id="SSH-setting"><a href="#SSH-setting" class="headerlink" title=" SSH setting"></a><a name="subject02"></a> SSH setting</h1><p><a href="http://localhost">localhost</a> ssh 접속을 위해 아래와 같은 작업을 수행한다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ ssh-keygen -t rsa -P <span class="string">&#x27;&#x27;</span></span><br><span class="line">$ cat <span class="variable">$HOME</span>/.ssh/id_rsa.pub  &gt;&gt; <span class="variable">$HOME</span>/.ssh/authorized_keys</span><br><span class="line">$ sudo vi /etc/ssh/sshd_config</span><br><span class="line"><span class="comment">#PasswordAuthentication yes</span></span><br><span class="line">$ sudo systemctl restart sshd</span><br><span class="line">$ ssh localhost <span class="comment">#Test</span></span><br></pre></td></tr></table></figure><h1 id="Install-Hadoop"><a href="#Install-Hadoop" class="headerlink" title=" Install Hadoop"></a><a name="subject03"></a> Install Hadoop</h1><ul><li>binary 다운로드 및 환경변수 설정</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ wget https://downloads.apache.org/hadoop/common/hadoop-2.10.1/hadoop-2.10.1.tar.gz</span><br><span class="line">$ tar -xvzf hadoop-2.10.1.tar.gz</span><br><span class="line">$ vi ~/.bashrc</span><br><span class="line"><span class="comment"># Hadoop env</span></span><br><span class="line"><span class="built_in">export</span> HADOOP_HOME=/app/hadoop-2.10.1</span><br><span class="line"><span class="built_in">export</span> HADOOP_COMMON_LIB_NATIVE_DIR=<span class="variable">$HADOOP_HOME</span>/lib/native</span><br><span class="line"><span class="built_in">export</span> HADOOP_OPTS=<span class="string">&quot;-Djava.library.path=<span class="variable">$HADOOP_HOME</span>/lib&quot;</span></span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.275.b01-0.el7_9.x86_64</span><br><span class="line">PATH=<span class="variable">$PATH</span>:<span class="variable">$JAVA_HOME</span>/bin:<span class="variable">$HADOOP_HOME</span>/bin:<span class="variable">$HADOOP_HOME</span>/sbin</span><br><span class="line">$ <span class="built_in">source</span> ~/.bashrc</span><br></pre></td></tr></table></figure><h1 id="Configuring-Hadoop"><a href="#Configuring-Hadoop" class="headerlink" title=" Configuring Hadoop"></a><a name="subject04"></a> Configuring Hadoop</h1><h3 id="HADOOP-HOME-etc-hadoop-core-site-xml"><a href="#HADOOP-HOME-etc-hadoop-core-site-xml" class="headerlink" title=" $HADOOP_HOME/etc/hadoop/core-site.xml"></a><a name="subject05"></a> <code>$HADOOP_HOME/etc/hadoop/core-site.xml</code></h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span></span><br><span class="line"><span class="comment">&lt;!--</span></span><br><span class="line"><span class="comment">  Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span></span><br><span class="line"><span class="comment">  you may not use this file except in compliance with the License.</span></span><br><span class="line"><span class="comment">  You may obtain a copy of the License at</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">    http://www.apache.org/licenses/LICENSE-2.0</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">  Unless required by applicable law or agreed to in writing, software</span></span><br><span class="line"><span class="comment">  distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span></span><br><span class="line"><span class="comment">  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span></span><br><span class="line"><span class="comment">  See the License for the specific language governing permissions and</span></span><br><span class="line"><span class="comment">  limitations under the License. See accompanying LICENSE file.</span></span><br><span class="line"><span class="comment">--&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- Put site-specific property overrides in this file. --&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://hadoop-test.novalocal:8020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li>fs.defaultFS value format: <code>hdfs://&#123;hostname&#125;:&#123;port&#125;</code></li></ul><h3 id="HADOOP-etc-hadoop-hdfs-site-xml"><a href="#HADOOP-etc-hadoop-hdfs-site-xml" class="headerlink" title=" $HADOOP/etc/hadoop/hdfs-site.xml"></a><a name="subject06"></a> <code>$HADOOP/etc/hadoop/hdfs-site.xml</code></h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span></span><br><span class="line"><span class="comment">&lt;!--</span></span><br><span class="line"><span class="comment">  Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span></span><br><span class="line"><span class="comment">  you may not use this file except in compliance with the License.</span></span><br><span class="line"><span class="comment">  You may obtain a copy of the License at</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">    http://www.apache.org/licenses/LICENSE-2.0</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">  Unless required by applicable law or agreed to in writing, software</span></span><br><span class="line"><span class="comment">  distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span></span><br><span class="line"><span class="comment">  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span></span><br><span class="line"><span class="comment">  See the License for the specific language governing permissions and</span></span><br><span class="line"><span class="comment">  limitations under the License. See accompanying LICENSE file.</span></span><br><span class="line"><span class="comment">--&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- Put site-specific property overrides in this file. --&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/hdata/name<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">value</span>&gt;</span>/hdata/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li>이후 namenode/datanode directory 를 생성한다.</li></ul><h3 id="HADOOP-HOME-etc-hadoop-mapred-site-xml"><a href="#HADOOP-HOME-etc-hadoop-mapred-site-xml" class="headerlink" title=" $HADOOP_HOME/etc/hadoop/mapred-site.xml"></a><a name="subject07"></a> <code>$HADOOP_HOME/etc/hadoop/mapred-site.xml</code></h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=&quot;1.0&quot;?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span></span><br><span class="line"><span class="comment">&lt;!--</span></span><br><span class="line"><span class="comment">  Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span></span><br><span class="line"><span class="comment">  you may not use this file except in compliance with the License.</span></span><br><span class="line"><span class="comment">  You may obtain a copy of the License at</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">    http://www.apache.org/licenses/LICENSE-2.0</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">  Unless required by applicable law or agreed to in writing, software</span></span><br><span class="line"><span class="comment">  distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span></span><br><span class="line"><span class="comment">  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span></span><br><span class="line"><span class="comment">  See the License for the specific language governing permissions and</span></span><br><span class="line"><span class="comment">  limitations under the License. See accompanying LICENSE file.</span></span><br><span class="line"><span class="comment">--&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- Put site-specific property overrides in this file. --&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapred.job.tracker<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>localhost:9001<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="HADOOP-HOME-etc-hadoop-yarn-site-xml"><a href="#HADOOP-HOME-etc-hadoop-yarn-site-xml" class="headerlink" title=" $HADOOP_HOME/etc/hadoop/yarn-site.xml"></a><a name="subject08"></a> <code>$HADOOP_HOME/etc/hadoop/yarn-site.xml</code></h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=&quot;1.0&quot;?&gt;</span></span><br><span class="line"><span class="comment">&lt;!--</span></span><br><span class="line"><span class="comment">  Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span></span><br><span class="line"><span class="comment">  you may not use this file except in compliance with the License.</span></span><br><span class="line"><span class="comment">  You may obtain a copy of the License at</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">    http://www.apache.org/licenses/LICENSE-2.0</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">  Unless required by applicable law or agreed to in writing, software</span></span><br><span class="line"><span class="comment">  distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span></span><br><span class="line"><span class="comment">  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span></span><br><span class="line"><span class="comment">  See the License for the specific language governing permissions and</span></span><br><span class="line"><span class="comment">  limitations under the License. See accompanying LICENSE file.</span></span><br><span class="line"><span class="comment">--&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h2 id="Running-Hadoop"><a href="#Running-Hadoop" class="headerlink" title=" Running Hadoop"></a><a name="subject09"></a> Running Hadoop</h2><h3 id="Before-starting-the-Cluster-we-need-to-format-the-Hadoop-NN-in-our-local-system"><a href="#Before-starting-the-Cluster-we-need-to-format-the-Hadoop-NN-in-our-local-system" class="headerlink" title=" Before starting the Cluster, we need to format the Hadoop NN in our local system"></a><a name="subject10"></a> Before starting the Cluster, we need to format the Hadoop NN in our local system</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="variable">$HADOOP_HOME</span>/bin/hadoop namenode -format</span><br></pre></td></tr></table></figure><ul><li><p>이후 namenode path 로 설정한 폴더에 뭔가가 생김을 확인할 수 있다.</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ tree</span><br><span class="line">/hdata/name/</span><br><span class="line">└── current</span><br><span class="line">    ├── fsimage_0000000000000000000</span><br><span class="line">    ├── fsimage_0000000000000000000.md5</span><br><span class="line">    ├── seen_txid</span><br><span class="line">    └── VERSION</span><br><span class="line"></span><br><span class="line">1 directory, 4 files</span><br></pre></td></tr></table></figure></li></ul><h3 id="Start-NameNode-daemon-and-DataNode-daemon"><a href="#Start-NameNode-daemon-and-DataNode-daemon" class="headerlink" title=" Start NameNode daemon and DataNode daemon"></a><a name="subject10"></a> Start NameNode daemon and DataNode daemon</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="variable">$HADOOP_HOME</span>/sbin/start-dfs.sh</span><br><span class="line">$ <span class="variable">$HADOOP_HOME</span>/sbin/start-yarn.sh</span><br><span class="line">$ jps</span><br><span class="line">12960 Jps</span><br><span class="line">10161 NodeManager</span><br><span class="line">12625 SecondaryNameNode</span><br><span class="line">10051 ResourceManager</span><br><span class="line">12435 DataNode</span><br><span class="line">12287 NameNode</span><br></pre></td></tr></table></figure><h1 id="Reference"><a href="#Reference" class="headerlink" title=" Reference"></a><a name="reference"></a> Reference</h1><ul><li><a href="https://helei.pro/doc/Setup-Hadoop-2.7.3-(single-node)-on-AWS-EC2-Ubuntu-AMI.pdf">https://helei.pro/doc/Setup-Hadoop-2.7.3-(single-node)-on-AWS-EC2-Ubuntu-AMI.pdf</a></li><li><a href="https://www.tecmint.com/install-hadoop-single-node-on-centos-7/">https://www.tecmint.com/install-hadoop-single-node-on-centos-7/</a></li></ul><hr><p>made by <em>jaejun.lee</em></p>]]></content:encoded>
      
      <comments>https://jx2lee.github.io/hadoop-install_singloenode_cluster/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[Cloud] Docker 설치 On CentOS7</title>
      <link>https://jx2lee.github.io/cloud-install_docker_on_centos/</link>
      <guid>https://jx2lee.github.io/cloud-install_docker_on_centos/</guid>
      <pubDate>Wed, 06 Jan 2021 15:00:00 GMT</pubDate>
      <description>
      
        &lt;p&gt;CentOS 7 환경에서 Docker 설치 과정을 다룬다.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;CentOS Linux release 7.5.1804 (Core)&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p>CentOS 7 환경에서 Docker 설치 과정을 다룬다.</p><blockquote><p><em>CentOS Linux release 7.5.1804 (Core)</em></p></blockquote><a id="more"></a><p><strong><span class="github-emoji" alias="sparkles" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png?v8">&#x2728;</span> Contents:</strong>  </p><ol><li><a href="#subject01">이전 설치된 Docker 삭제</a></li><li><a href="#subject02">yum-utils 패키지 설치 및 Docker repo 추가</a></li><li><a href="#subject03">Docker 설치 (with yum)</a><blockquote><p><a href="#reference">Referenece</a></p></blockquote></li></ol><h1 id="이전-설치된-Docker-삭제"><a href="#이전-설치된-Docker-삭제" class="headerlink" title=" 이전 설치된 Docker 삭제"></a><a name="subject01"></a> 이전 설치된 Docker 삭제</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ sudo yum remove docker \</span><br><span class="line">                  docker-client \</span><br><span class="line">                  docker-client-latest \</span><br><span class="line">                  docker-common \</span><br><span class="line">                  docker-latest \</span><br><span class="line">                  docker-latest-logrotate \</span><br><span class="line">                  docker-logrotate \</span><br><span class="line">                  docker-engine</span><br></pre></td></tr></table></figure><h1 id="yum-utils-패키지-설치-및-Docker-repo-추가"><a href="#yum-utils-패키지-설치-및-Docker-repo-추가" class="headerlink" title=" yum-utils 패키지 설치 및 Docker repo 추가"></a><a name="subject02"></a> yum-utils 패키지 설치 및 Docker repo 추가</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ sudo yum install -y yum-utils</span><br><span class="line">$ sudo yum-config-manager \</span><br><span class="line">    --add-repo https://download.docker.com/linux/centos/docker-ce.repo</span><br></pre></td></tr></table></figure><h1 id="Docker-설치-with-yum"><a href="#Docker-설치-with-yum" class="headerlink" title=" Docker 설치 (with yum)"></a><a name="subject03"></a> Docker 설치 (with yum)</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">$ yum install -y docker-ce</span><br><span class="line">$ systemctl <span class="built_in">enable</span> docker</span><br><span class="line">Created symlink from /etc/systemd/system/multi-user.target.wants/docker.service to /usr/lib/systemd/system/docker.service.</span><br><span class="line">$ systemctl status docker</span><br><span class="line">● docker.service - Docker Application Container Engine</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/docker.service; enabled; vendor preset: disabled)</span><br><span class="line">   Active: active (running) since Mon 2021-01-04 13:19:54 KST; 1s ago</span><br><span class="line">     Docs: https://docs.docker.com</span><br><span class="line"> Main PID: 11889 (dockerd)</span><br><span class="line">    Tasks: 13</span><br><span class="line">   Memory: 47.3M</span><br><span class="line">   CGroup: /system.slice/docker.service</span><br><span class="line">           └─11889 /usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock</span><br><span class="line"></span><br><span class="line">Jan 04 13:19:54 test-centos.novalocal dockerd[11889]: time=<span class="string">&quot;2021-01-04T13:19:54.036061899+09:00&quot;</span> leve...rpc</span><br><span class="line">Jan 04 13:19:54 test-centos.novalocal dockerd[11889]: time=<span class="string">&quot;2021-01-04T13:19:54.036084134+09:00&quot;</span> leve...rpc</span><br><span class="line">Jan 04 13:19:54 test-centos.novalocal dockerd[11889]: time=<span class="string">&quot;2021-01-04T13:19:54.036094510+09:00&quot;</span> leve...rpc</span><br><span class="line">Jan 04 13:19:54 test-centos.novalocal dockerd[11889]: time=<span class="string">&quot;2021-01-04T13:19:54.066676014+09:00&quot;</span> leve...t.<span class="string">&quot;</span></span><br><span class="line"><span class="string">Jan 04 13:19:54 test-centos.novalocal dockerd[11889]: time=&quot;</span>2021-01-04T13:19:54.197938418+09:00<span class="string">&quot; leve...ss&quot;</span></span><br><span class="line">Jan 04 13:19:54 test-centos.novalocal dockerd[11889]: time=<span class="string">&quot;2021-01-04T13:19:54.247887261+09:00&quot;</span> leve...e.<span class="string">&quot;</span></span><br><span class="line"><span class="string">Jan 04 13:19:54 test-centos.novalocal dockerd[11889]: time=&quot;</span>2021-01-04T13:19:54.275547502+09:00<span class="string">&quot; leve...0.1</span></span><br><span class="line"><span class="string">Jan 04 13:19:54 test-centos.novalocal dockerd[11889]: time=&quot;</span>2021-01-04T13:19:54.275733738+09:00<span class="string">&quot; leve...on&quot;</span></span><br><span class="line">Jan 04 13:19:54 test-centos.novalocal systemd[1]: Started Docker Application Container Engine.</span><br><span class="line">Jan 04 13:19:54 test-centos.novalocal dockerd[11889]: time=<span class="string">&quot;2021-01-04T13:19:54.303691925+09:00&quot;</span> leve...ck<span class="string">&quot;</span></span><br><span class="line"><span class="string">Hint: Some lines were ellipsized, use -l to show in full.</span></span><br><span class="line"><span class="string">$ docker ps</span></span><br><span class="line"><span class="string">CONTAINER ID   IMAGE     COMMAND   CREATED   STATUS    PORTS     NAMES</span></span><br></pre></td></tr></table></figure><h1 id="Reference"><a href="#Reference" class="headerlink" title=" Reference"></a><a name="reference"></a> Reference</h1><ul><li><a href="https://docs.docker.com/engine/install/centos/">https://docs.docker.com/engine/install/centos/</a></li></ul><hr><p>made by <em>jaejun.lee</em></p>]]></content:encoded>
      
      <comments>https://jx2lee.github.io/cloud-install_docker_on_centos/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[Cloud] 자체 튜닝 Docker 이미지를 만들어 보자구요.</title>
      <link>https://jx2lee.github.io/cloud-docker_image_test/</link>
      <guid>https://jx2lee.github.io/cloud-docker_image_test/</guid>
      <pubDate>Tue, 27 Oct 2020 15:00:00 GMT</pubDate>
      <description>
      
        &lt;p&gt;급히 나주 출장을 갈 예정인데 무슨 클라우드에서 제공하는 서비스인데 서버 1개로 구축을 해달라는 황당한 요청이 있었다. 우선 설치를 하고 봐야 하니.. 각 파드 resource 를 최저로 주고 테스트 하던 도중 우리 제품 바이너리에 수정사항이 발생하였다. 이슈를 올려서 해도 되지만 어차피 검수 목적으로 캡쳐만 뜨는 설치건이라 &lt;strong&gt;바이너리를 수정하여 덮어 씌우는 자체 튜닝&lt;/strong&gt; 과정을 살펴본다.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;역시 GOD정희님 최고&lt;/p&gt;
&lt;/blockquote&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p>급히 나주 출장을 갈 예정인데 무슨 클라우드에서 제공하는 서비스인데 서버 1개로 구축을 해달라는 황당한 요청이 있었다. 우선 설치를 하고 봐야 하니.. 각 파드 resource 를 최저로 주고 테스트 하던 도중 우리 제품 바이너리에 수정사항이 발생하였다. 이슈를 올려서 해도 되지만 어차피 검수 목적으로 캡쳐만 뜨는 설치건이라 <strong>바이너리를 수정하여 덮어 씌우는 자체 튜닝</strong> 과정을 살펴본다.</p><blockquote><p>역시 GOD정희님 최고</p></blockquote><a id="more"></a><p><strong><span class="github-emoji" alias="sparkles" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png?v8">&#x2728;</span> Contents:</strong>  </p><ol><li><a href="#subject01">문제 발생</a></li><li><a href="#subject02">바이너리 수정 후 docker image 디렉토리 생성하기</a></li><li><a href="#subject03">빌드하자!</a></li></ol><h1 id="문제-발생"><a href="#문제-발생" class="headerlink" title=" 문제 발생"></a><a name="subject01"></a> 문제 발생</h1><p>JEUS 를 통해 두 개 서버를 생성하는 스크립트에서, ThreadPool min/max 값이 잘못 설정되어 배포하는 문제가 발생하였다. 이 때문에 3개 중 하나의 서비스가 동작하지 않는 문제가 발생하였고, 이를 해결하기 위해 install script 내 modify command 를 추가하여 min 값을 수정하였다. 근데.. 당장 내일 나오는 바이너리로 나갈 필요도 없고 서버 스펙도 충분하지 못한 상황에서 이전 버젼으로 나가는 게 베스트 일 것 같아 자체적으로 Docker 이미지를 튜닝해보고자 한다. </p><h1 id="바이너리-수정-후-docker-image-디렉토리-생성하기"><a href="#바이너리-수정-후-docker-image-디렉토리-생성하기" class="headerlink" title=" 바이너리 수정 후 docker image 디렉토리 생성하기"></a><a name="subject02"></a> 바이너리 수정 후 docker image 디렉토리 생성하기</h1><p>위 문제 발생에서 해결한 바이너리를 다시 똑같은 바이너리로 압축한 뒤 docker 이미지 빌드를 위한 곳에 위치한다. 그리고 Dockerfile 를 생성하여 다음과 같이 작성한다.</p><figure class="highlight docker"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">FROM</span> hyperdata8.<span class="number">3</span>_hd_v8.<span class="number">3.2</span>:<span class="number">20201016</span>_v1</span><br><span class="line"><span class="keyword">COPY</span><span class="bash"> HyperData_8.3_r874bb4-20201016030033_v8.3.2.tar.gz /deploy_src/src/hyperdata/HyperData_8.3_r874bb4-20201016030033_v8.3.2.tar.gz</span></span><br></pre></td></tr></table></figure><ul><li>기존 이미지를 base 로 FROM 으로 호출</li><li>굉장히 쉬웠다. 해당 수정한 바이너리 파일을 이미지 내 원래 있던 디렉토리에 같은 이름으로 작성만 하면 Docker build 시 최종 바이너리는 내가 수정한 바이너리로 압축을 해제할 것이다. Dockerfile 에 순서는 무관하다는 걸 오늘 깨달았고 나의 무지함 또는 깨달았다. Docker image build 에 대해 더 살펴볼 필요가 있어보인다.</li></ul><h1 id="빌드하자"><a href="#빌드하자" class="headerlink" title=" 빌드하자!"></a><a name="subject03"></a> 빌드하자!</h1><p>해당 디렉토리 안에서 다음과 같은 커맨드로 빌드한다.  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker build -t &#123;기존_이미지명&#125;:&#123;태그_변경_아무거나_하세요&#125; .</span><br></pre></td></tr></table></figure><ul><li>마지막 줄 <strong>.(점)</strong> 중요하다. 물론 디렉토리를 지정할 수 있는 옵션이 있지만 주지 않을 경우 <strong>점</strong>을 통해 빌드 디렉토리를 인식한다.</li><li>태그명의 경우 이전 태그명에서 나는 고객사 영문 줄임만 붙였다. <del>양아취</del></li></ul><p><strong>docker images</strong> 커맨드로 생성이 잘 되었는지 확인한다. 나의 경우 이렇게 새로운 이미지를 만들고 테스트를 해보니, 내가 수정한 스크립트가 정상적으로 반영되었고 모든게 Dockerfile 두 줄로 끝나버렸다. 조금은 허무하지만 Container 이미지 빌드 과정에 대해 관심을 가질 수 있는 계기가 되었다!</p><blockquote><p>공부할 게 산더미다.</p></blockquote><hr><p>made by <em>jaejun.lee</em></p>]]></content:encoded>
      
      <comments>https://jx2lee.github.io/cloud-docker_image_test/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[Cloud] cri-o 컨테이너 런타임 사용 시 docker registry 세팅</title>
      <link>https://jx2lee.github.io/cloud-crio_registry_setting/</link>
      <guid>https://jx2lee.github.io/cloud-crio_registry_setting/</guid>
      <pubDate>Fri, 02 Oct 2020 15:00:00 GMT</pubDate>
      <description>
      
        &lt;p&gt;kubernetes 버젼이 올라감에 따라 &lt;em&gt;(1.15.3 → 1.17.6)&lt;/em&gt; 컨테이너 런타임을 cri-o 로 변경하였다. 이미지 저장소는 그대로 docker 사용하는데 이때 docker registry 와 연동하여 image 를 관리하는 방법을 살펴본다.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Updata Note&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;2020.10.29 : config 추가 수정&lt;/li&gt;
&lt;/ul&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p>kubernetes 버젼이 올라감에 따라 <em>(1.15.3 → 1.17.6)</em> 컨테이너 런타임을 cri-o 로 변경하였다. 이미지 저장소는 그대로 docker 사용하는데 이때 docker registry 와 연동하여 image 를 관리하는 방법을 살펴본다.</p><p><strong>Updata Note</strong></p><ul><li>2020.10.29 : config 추가 수정</li></ul><a id="more"></a><p><strong><span class="github-emoji" alias="sparkles" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png?v8">&#x2728;</span> Contents:</strong>  </p><ol><li><a href="#subject01">config 수정</a></li><li><a href="#subject02">cri-o 재기동</a></li><li><a href="#subject03">테스트 해보자!</a></li></ol><h1 id="config-수정"><a href="#config-수정" class="headerlink" title=" config 수정"></a><a name="subject01"></a> config 수정</h1><p><strong>/etc/crio/crio.conf</strong> 내 중간에서 마지막 부분 사이 <em>insecure_registries</em> 에 docker registry endpoint 를 추가한다.  </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># List of registries to skip TLS verification for pulling images. Please</span></span><br><span class="line"><span class="comment"># consider configuring the registries via /etc/containers/registries.conf before</span></span><br><span class="line"><span class="comment"># changing them here.</span></span><br><span class="line">insecure_registries = [<span class="string">&quot;192.168.179.185:5000&quot;</span>, <span class="string">&quot;192.168.179.189:5000&quot;</span>]</span><br></pre></td></tr></table></figure><ul><li>첫 번째 endpoint 는 기존, 뒤에 추가한 endpoint 가 새로 테스트할 docker registry endpoint 이다.</li></ul><blockquote><p><em>추가적으로 config 을 다음과 같이 수정해야한다.</em></p><ul><li><strong>registries = [“{registry}:{port}” , “docker.io”]</strong></li><li><strong>plugin_dirs : “/opt/cni/bin” 추가</strong></li><li>만약 폐쇄망 환경일 경우, <strong>pause_image 부분 앞에 registry enpoint를 추가</strong>해야한다.</li></ul></blockquote><h1 id="cri-o-재기동"><a href="#cri-o-재기동" class="headerlink" title=" cri-o 재기동"></a><a name="subject02"></a> cri-o 재기동</h1><p>cri-o 서비스를 재기동한다.  </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ systemctl restart crio</span><br></pre></td></tr></table></figure><h1 id="테스트-해보자"><a href="#테스트-해보자" class="headerlink" title=" 테스트 해보자!"></a><a name="subject03"></a> 테스트 해보자!</h1><p>crictl 로 추가한 docker registry 와 연동이 잘 되었는지 테스트 해보자!  </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ crictl pull 192.168.179.189:5000/hyperdata8.3_tb:20200717_v3</span><br><span class="line">Image is up to date <span class="keyword">for</span> 192.168.179.189:5000/hyperdata8.3_tb@sha256:85e0f94a90a26090488b929e3ef9475b2cab35ad425effdf5e79a7f2f7029a06</span><br><span class="line">$ crictl images |grep hyperdata8.3_tb</span><br><span class="line">192.168.179.189:5000/hyperdata8.3_tb                                                    20200717_v3                                                        76063ea49aadd       3.1GB</span><br></pre></td></tr></table></figure><p>같은 방법으로 push 하면 정상적으로 작동하는 것을 확인할 수 있다.  </p><p>컨테이너 런타임을 관리하는 놈이 <strong>crio</strong> 이고, crictl config 는 <strong>/etc/crio/crio.conf</strong> 이며 docker registry 와 연동을 위해 해당 config 를 수정하고 crio service 를 재 시작한다. 이전에는 컨테이너 런타임, 이미지 관리를 모두 docker 에서 관리하여 편하긴 했는데.. cri-o에 대한 개념을 정리해볼 필요가 있다.  </p><hr><p>made by <em>jaejun.lee</em></p>]]></content:encoded>
      
      <comments>https://jx2lee.github.io/cloud-crio_registry_setting/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[Linux] Timezone 변경</title>
      <link>https://jx2lee.github.io/linux-change_timezone/</link>
      <guid>https://jx2lee.github.io/linux-change_timezone/</guid>
      <pubDate>Fri, 02 Oct 2020 15:00:00 GMT</pubDate>
      <description>
      
        &lt;center&gt; tzdata 패키지를 설치하고 기존 UTC timezone 을 KST 로 변경하는 과정을 살펴본다.&lt;/center&gt;
&lt;br&gt;
&lt;br&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<center> tzdata 패키지를 설치하고 기존 UTC timezone 을 KST 로 변경하는 과정을 살펴본다.</center><br><br><a id="more"></a><p><strong><span class="github-emoji" alias="sparkles" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png?v8">&#x2728;</span> Contents:</strong>  </p><ol><li><a href="#subject01">패키지 설치</a></li><li><a href="#subject02">Symbolic link 설정 및 패키지 재설정</a></li><li><a href="#reference">Reference</a></li></ol><h1 id="패키지-설치"><a href="#패키지-설치" class="headerlink" title=" 패키지 설치"></a><a name="subject01"></a> 패키지 설치</h1><p>Ubuntu 패키지 매니저인 apt 를 이용하여 tzdata 를 설치한다.  </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ apt-get install tzdata</span><br></pre></td></tr></table></figure><h1 id="Symbolic-link-설정-및-패키지-재설정"><a href="#Symbolic-link-설정-및-패키지-재설정" class="headerlink" title=" Symbolic link 설정 및 패키지 재설정"></a><a name="subject02"></a> Symbolic link 설정 및 패키지 재설정</h1><p>default timezone UTC 에서 KST 로 바꾸기 위해 link 를 생성한다.  </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ ln -sf /usr/share/zoneinfo/Asia/Seoul /etc/localtime</span><br><span class="line">$ date</span><br><span class="line">Fri Sep 18 13:19:40 KST 2020</span><br></pre></td></tr></table></figure><p>이후 재 설정을 위해 아래 커맨드를 실행하여 timezone 이 KST 로 적용하였는지 확인한다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ dpkg-reconfigure tzdata</span><br></pre></td></tr></table></figure><p>끝!</p><h1 id="Reference"><a href="#Reference" class="headerlink" title=" Reference"></a><a name="reference"></a> Reference</h1><ul><li><a href="https://www.lesstif.com/lpt/ubuntu-linux-timezone-setting-61899162.html">https://www.lesstif.com/lpt/ubuntu-linux-timezone-setting-61899162.html</a></li></ul><hr><p>made by <em>jaejun.lee</em></p>]]></content:encoded>
      
      <comments>https://jx2lee.github.io/linux-change_timezone/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[Linux] ssh 비밀번호 묻지마!</title>
      <link>https://jx2lee.github.io/linux-ssh_nopasswd/</link>
      <guid>https://jx2lee.github.io/linux-ssh_nopasswd/</guid>
      <pubDate>Fri, 02 Oct 2020 15:00:00 GMT</pubDate>
      <description>
      
        &lt;center&gt; Kubernetes 노드 관리를 위해 각 노드별 접근 시 비밀번호 없이 편하게 접속하는 방법을 정리하였다.&lt;/center&gt;
&lt;br&gt;
&lt;br&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<center> Kubernetes 노드 관리를 위해 각 노드별 접근 시 비밀번호 없이 편하게 접속하는 방법을 정리하였다.</center><br><br><a id="more"></a><p><strong><span class="github-emoji" alias="sparkles" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png?v8">&#x2728;</span> Contents:</strong>  </p><ol><li><a href="#subject01">Key 생성</a></li><li><a href="#subject02">Key 복사</a></li><li><a href="#reference">Reference</a></li></ol><h1 id="Key-생성"><a href="#Key-생성" class="headerlink" title=" Key 생성"></a><a name="subject01"></a> Key 생성</h1><p>이미 A 서버에 ssh Key 가 존재한다면 이 부분은 패스해도 좋다. 만약 처음 세팅하는거라면, 아래 커맨드를 통해 ssh Key 를 생성하자.  </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="comment"># A 서버에서 실행</span></span><br><span class="line">$ ssh-keygen</span><br></pre></td></tr></table></figure><h1 id="Key-복사"><a href="#Key-복사" class="headerlink" title=" Key 복사"></a><a name="subject02"></a> Key 복사</h1><p>생성한 Key 를 <strong>ssh-copy-id</strong> 를 이용해 B 서버에 등록한다.  </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ ssh-copy-id -i ~/.ssh/id_rsa.pub [user]@[ip]</span><br><span class="line">$ <span class="comment"># user/ip는 당연 B 서버 정보를 입력</span></span><br><span class="line">$ <span class="comment"># 위 커맨드 이후 비밀번호를 물어볼텐데 잘 입력하자.</span></span><br></pre></td></tr></table></figure><p>이후 A 서버 <em>(host: k8s-master)</em> 에서 B 서버 <em>(host: k8s-node1)</em> 로 원격접속 해보자.  </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">$ ssh k8s-node1</span><br><span class="line">Welcome to Ubuntu 18.04.3 LTS (GNU/Linux 4.15.0-112-generic x86_64)</span><br><span class="line"></span><br><span class="line"> * Documentation:  https://help.ubuntu.com</span><br><span class="line"> * Management:     https://landscape.canonical.com</span><br><span class="line"> * Support:        https://ubuntu.com/advantage</span><br><span class="line"></span><br><span class="line">  System information as of Sat Oct  3 23:03:28 KST 2020</span><br><span class="line"></span><br><span class="line">  System load:    0.34              Users logged <span class="keyword">in</span>:        1</span><br><span class="line">  Usage of /home: 0.2% of 19.56GB   IP address <span class="keyword">for</span> enp6s0:  192.168.179.173</span><br><span class="line">  Memory usage:   16%               IP address <span class="keyword">for</span> docker0: 172.17.0.1</span><br><span class="line">  Swap usage:     0%                IP address <span class="keyword">for</span> tunl0:   10.244.36.64</span><br><span class="line">  Processes:      332</span><br><span class="line"></span><br><span class="line"> * Kubernetes 1.19 is out! Get it <span class="keyword">in</span> one <span class="built_in">command</span> with:</span><br><span class="line"></span><br><span class="line">     sudo snap install microk8s --channel=1.19 --classic</span><br><span class="line"></span><br><span class="line">   https://microk8s.io/ has docs and details.</span><br><span class="line"></span><br><span class="line"> * Canonical Livepatch is available <span class="keyword">for</span> installation.</span><br><span class="line">   - Reduce system reboots and improve kernel security. Activate at:</span><br><span class="line">     https://ubuntu.com/livepatch</span><br><span class="line"></span><br><span class="line">92 packages can be updated.</span><br><span class="line">1 update is a security update.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">*** System restart required ***</span><br><span class="line">Last login: Mon Sep 28 17:41:53 2020 from 192.168.188.101</span><br></pre></td></tr></table></figure><p>끝!</p><h1 id="Reference"><a href="#Reference" class="headerlink" title=" Reference"></a><a name="reference"></a> Reference</h1><ul><li>[<a href="https://itzone.tistory.com/694]https://itzone.tistory.com/694">https://itzone.tistory.com/694]https://itzone.tistory.com/694</a>)</li></ul><hr><p>made by <em>jaejun.lee</em></p>]]></content:encoded>
      
      <comments>https://jx2lee.github.io/linux-ssh_nopasswd/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[TroubleShoot] PyTorch 1.5.0+cu101 버젼 설치 시 에러</title>
      <link>https://jx2lee.github.io/troubleshoot-torch_error/</link>
      <guid>https://jx2lee.github.io/troubleshoot-torch_error/</guid>
      <pubDate>Fri, 02 Oct 2020 15:00:00 GMT</pubDate>
      <description>
      
        &lt;p&gt;pytorch 관련 프로젝트를 clone 하여 해보던 중에, requirements.txt 내 torch==1.5.0+cu101 부분에서 에러가 발생하였다. 에러를 피하고 제대로 설치해보자!&lt;/p&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p>pytorch 관련 프로젝트를 clone 하여 해보던 중에, requirements.txt 내 torch==1.5.0+cu101 부분에서 에러가 발생하였다. 에러를 피하고 제대로 설치해보자!</p><a id="more"></a><p><strong><span class="github-emoji" alias="sparkles" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png?v8">&#x2728;</span> Contents:</strong>  </p><ol><li><a href="#subject01">error 메시지</a></li><li><a href="#subject02">해결은 은근 쉽네요?</a></li><li><a href="#reference">Reference</a></li></ol><h1 id="error-메시지"><a href="#error-메시지" class="headerlink" title=" error 메시지"></a><a name="subject01"></a> error 메시지</h1><p><strong>pip install -r requirements.txt</strong> 로 관련 패키지 설치 시 아래와 같은 에러를 맞이하였다.  </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ERROR: Could not find a version that satisfies the requirement torch==1.5.0+cu101 (from -r requirements.txt (line 59)) (from versions: 0.1.2, 0.1.2.post1, 0.1.2.post2, 0.3.1, 0.4.0, 0.4.1, 1.0.0, 1.0.1, 1.0.1.post2, 1.1.0, 1.2.0, 1.3.0, 1.3.1, 1.4.0, 1.5.0, 1.5.1, 1.6.0)</span><br><span class="line">ERROR: No matching distribution found <span class="keyword">for</span> torch==1.5.0+cu101 (from -r requirements.txt (line 59))</span><br></pre></td></tr></table></figure><h1 id="해결은-은근-쉽네요"><a href="#해결은-은근-쉽네요" class="headerlink" title=" 해결은 은근 쉽네요?"></a><a name="subject02"></a> 해결은 은근 쉽네요?</h1><p>별 거 없었다. pip install 시 -f 옵션을 이용해 stable version 을 찾아가 직접 다운로드 할 수 있다.  </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ pip install torch==1.5.0+cu101 torchvision==0.6.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html</span><br></pre></td></tr></table></figure><ul><li><strong>f 옵션:</strong> html 파일의 URL 또는 경로를 이용해 패키지를 설치</li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title=" Reference"></a><a name="reference"></a> Reference</h1><ul><li><a href="https://huvso.github.io/2020/07/02/python-pip-option.html">https://huvso.github.io/2020/07/02/python-pip-option.html</a></li></ul><hr><p>made by <em>jaejun.lee</em></p>]]></content:encoded>
      
      <comments>https://jx2lee.github.io/troubleshoot-torch_error/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[Cloud] Kubernetes 아키텍처에 대해</title>
      <link>https://jx2lee.github.io/cloud-kubernetes_chapter_02/</link>
      <guid>https://jx2lee.github.io/cloud-kubernetes_chapter_02/</guid>
      <pubDate>Sun, 27 Sep 2020 15:00:00 GMT</pubDate>
      <description>
      
        &lt;p&gt;신입 직원 교육자료를 위해 작성한 Kubernetes 소개 자료이다. 많은 블로그를 참고하여 작성하였고, 이번 장에는 Kubernetes 아키텍쳐를 간략히 소개한다.&lt;/p&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p>신입 직원 교육자료를 위해 작성한 Kubernetes 소개 자료이다. 많은 블로그를 참고하여 작성하였고, 이번 장에는 Kubernetes 아키텍쳐를 간략히 소개한다.</p><a id="more"></a><p><strong>Contents:</strong>  </p><ol><li><a href="#subject01">Architecture</a></li><li><a href="#subject02">Control Plane (Master)</a><ul><li><a href="#subject03">특징 및 기능</a></li><li><a href="#subject04">Component in Control Plane Node</a></li></ul></li><li><a href="#subject05">Worker Node</a><ul><li><a href="#subject06">특징 및 기능</a></li><li><a href="#subject07">Component in Worker Node</a></li></ul></li><li><a href="#subject08">Object</a><ul><li><a href="#subject09">Basic Object</a></li><li><a href="#subject10">Controller</a></li></ul></li><li><a href="#reference">Reference</a></li></ol><h1 id="Architecture"><a href="#Architecture" class="headerlink" title=" Architecture"></a><a name="subject01"></a> Architecture</h1><p><img src="https://v1-17.docs.kubernetes.io/images/docs/components-of-kubernetes.png" alt="kubernetes Architecture ver1.17"></p><center><U>[그림] kubernetes architecture ver1.17</U></center><p>클러스터를 관리하는 <strong>Controlplane</strong> 와 컨테이너가 배포되는 머신 <em>(가상머신이거나 실제 서버)</em> 인 <strong>Worker Node</strong>로 구성한다. Control Plane <em>(한글 번역 시 Master 라고 나와있는데 Control Plane == Master 라고 생각하면 된다.)</em> 과 Worker Node 를 확인하고 Kubernetes 내에서의 Object 를 살펴본다.</p><h2 id="Control-Plane-Master"><a href="#Control-Plane-Master" class="headerlink" title=" Control Plane (Master)"></a><a name="subject02"></a> Control Plane <em>(Master)</em></h2><h3 id="특징-및-기능"><a href="#특징-및-기능" class="headerlink" title=" 특징 및 기능"></a><a name="subject03"></a> 특징 및 기능</h3><ul><li>관리자만 접속하여 보안 설정이 필요하다.</li><li>Conrol Plane Node 다운이 발생하면 클러스터 관리에 장애가 생기므로 보통 3대로 구성하여 클러스터를 구성한다.<ul><li>홀수대로 구성하는 이유는 컨테이너 배포에 대한 voting 를 수월하게 하기 위함이라고 한다.</li></ul></li><li>소규모 환경에서는 Control Plane 과 Worker Node 를 분리하지 않고 같은 서버에 구성한다. 즉, Control Plane 이면서 동시에 컨테이너를 띄운다고 생각하면 된다.</li></ul><h3 id="Component-in-Control-Plane-Node"><a href="#Component-in-Control-Plane-Node" class="headerlink" title=" Component in Control Plane Node"></a><a name="subject04"></a> Component in Control Plane Node</h3><p><U>API server</U>:  </p><ul><li>모든 컴포넌트 간 통신의 메카이다.</li><li>kubectl 요청 및 내부 모듈의 요청을 처리한다. kubectl 명령어는 Control Plane 에서만 가능하다. <em>(물론 설치는 모든 노드에서 수행한다.)</em></li><li>권한 체크를 통해 요청을 허용하거나 거부한다.</li><li>Etcd 를 기반으로 필요한 데이터를 조회한다.</li><li>RESTful API 제공한다.</li></ul><p><U>Etcd</U>:  </p><ul><li>분산형 key/value 오픈소스 Storage.</li><li>Kubernetes cluster의 DB 역할을 하는 서버로 설정값이나 cluster 상태를 저장한다.</li><li>Etcd 백업을 통해 클러스터 상태 복구가 가능하다. <em>(ex. 오늘 컨트롤 플레인이 사망했다. 이를 복구할 수 있는 방법은 백업한 Etcd 스냅샷을 기반으로 클러스터에 조인할 수 있다.)</em></li></ul><p><U>kube-scheduler</U>:  </p><ul><li>할당이 필요한 Pod를 여러 조건(source, label)에 따라 적절한 노드에 할당하는 역할을 한다. </li></ul><p><U>kube-controller-manager</U>:  </p><ul><li><del>Kubernetes 의 Object (Pod, ReplicaSet, Deployment 등) 상태들을 관리한다.</del></li><li>Kubernetes 의 Controller 실행을 담당한다.</li></ul><p><U>cloud-controller-manager</U>:  </p><ul><li>오픈 클라우드 서비스와 연결하여 관리하는 controller manger 라고 생각하면 된다.</li><li>AWS, GCE, Azure 등의 클라우드 특화되어 있다고 한다!</li></ul><blockquote><p>이번 자료를 준비하면서 안 사실은 api-server, etcd, kube-scheduler, kube-controller-manager 요놈이 container 단위로 실행하여 관리하는 줄 알았는데 막상 보니 아니었다. <code>ps -ef |grep &#123;each_component&#125;</code> 를 수행하면 다음과 같다.  </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">root@k8s-node1:~<span class="comment"># ps -ef|grep -E &quot;kube-apiserver|etcd|kube-controller-manager|kube-scheduler&quot;</span></span><br><span class="line">root      3368  3311  1 Aug31 ?        10:17:51 kube-apiserver --advertise-address=192.&gt; 168.179.185 --allow-privileged=<span class="literal">true</span> --authorization-mode=Node,RBAC ...</span><br><span class="line">root      3584  3554  1 Aug31 ?        05:58:22 etcd --advertise-client-urls=https://192.&gt; 168.179.173:2379 --cert-file=/etc/kubernetes/pki/etcd/server.crt ...</span><br><span class="line">root      3677  3636  0 Aug31 ?        00:20:07 kube-controller-manager &gt; --allocate-node-cidrs=<span class="literal">true</span> --authentication-kubeconfig=/etc/kubernetes/&gt; controller-manager.conf ... </span><br><span class="line">root      3678  3626  0 Aug31 ?        00:38:15 kube-scheduler --bind-address=127.0.0.1 &gt; --kubeconfig=/etc/kubernetes/scheduler.conf --leader-elect=<span class="literal">true</span></span><br></pre></td></tr></table></figure></blockquote><h2 id="Worker-Node"><a href="#Worker-Node" class="headerlink" title=" Worker Node"></a><a name="subject05"></a> Worker Node</h2><h3 id="특징-및-기능-1"><a href="#특징-및-기능-1" class="headerlink" title=" 특징 및 기능"></a><a name="subject06"></a> 특징 및 기능</h3><ul><li>Pod를 생성하고 네트워크와 볼륨을 설정합니다. kubelet 이라는 kubernetes Agent 를 통해 “어떤 조건을 만족하는 Pod 를 띄워!” 라는 임무를 받는다.</li><li>실제 컨테이너를 생성하는 서버이다. 노예라고 생각하면 편하다.</li><li>각 서버에 라벨을 붙여 사용목적에 따라 나눌 수 있다. 예를들어, GPU 노드로만 사용하여 이 리소스를 사용하는 서비스만 배포하고자 하면 yaml 작성 시 관련 label 을 설정한다. </li></ul><h3 id="Component-in-Worker-Node"><a href="#Component-in-Worker-Node" class="headerlink" title=" Component in Worker Node"></a><a name="subject07"></a> Component in Worker Node</h3><p><U>kubelet</U>:  </p><ul><li>클러스터 내 모든 노드로 배포되는 Agent 이다. Control Plane 에도 배포하지만 Control Plane 에 작성하면 너무 없어보일까 Worker Node 에 작성하였다.</li><li>노드에 할당한 Pod 생명주기를 관리하다.</li><li>Pod 안 컨테이너 상태를 체크하고 주기적으로 Master에 전달한다.</li><li>Control Plane 의 API서버와 통신을 하고 임무를 받게되면 노드가 수행해야 할 임무를 수행한다.</li></ul><p><U>kube-proxy</U>:  </p><ul><li>Kubernetes 내부 별도의 가상 네트워크를 설정하고 관리하는데, 이러한 가상 네트워크가 동작할 수 있게 하는 역할을 한다.</li><li>호스트의 <em>(각 노드의)</em> 네트워크 규칙을 관리하고 connection forwarding 을 수행한다.</li><li><a href="https://arisu1000.tistory.com/27839">참고자료</a></li></ul><h2 id="Object"><a href="#Object" class="headerlink" title=" Object"></a><a name="subject08"></a> Object</h2><p>Kubernets는 상태를 관리하기 위한 대상을 Object라 칭하며 크게 Basic Object(기본 오브젝트)와 Controller(컨트롤러)로 구분한다. </p><h3 id="Basic-Object"><a href="#Basic-Object" class="headerlink" title=" Basic Object"></a><a name="subject09"></a> Basic Object</h3><p>Basic Object 는 Namespace, Pod, Service, Volume 4가지가 존재한다.</p><p><U>Namespace</U>:  </p><ul><li>Kubernetes cluster 내 논리적인 구분 단위이다. Pod, Service 등을 namespace 별로 생성하고 관리할 수 있으며, user 권한 역시 namespace 별 부여<del>(고구려)</del>할 수 있다.</li><li>예를 들어 서비스 테스트를 위해 세 개의 namepsace 를 생성하여 각각 테스트가 가능하다. 대신, 논리적으로 분리했기 때문에 두 namepsace 간 파드의 통신이 가능하다. 따라서 높은 수준의 분리를 원하면 클러스터 자체를 분리하는 것을 권장한다.</li><li>namespace 내 object 명은 유일해야하지만 전체 namespace 내 object 명은 유일하지 않아도 된다. 예를 들어, A namespace 내 Pod 이름을 test 로 배포하면 B namespace 내 Pod 이름을 test 로 배포할 수 있다.</li><li>예시: kubernetes cluster 내 namespace 리스트를 확인한다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">root@k8s-master:~<span class="comment"># kubectl get namespace</span></span><br><span class="line"></span><br><span class="line">NAME               STATUS   AGE</span><br><span class="line">anonymous          Active   23d</span><br><span class="line">default            Active   191d</span><br><span class="line">hyperdata          Active   170d</span><br><span class="line">istio-system       Active   23d</span><br><span class="line">kfserving-system   Active   23d</span><br><span class="line">knative-serving    Active   23d</span><br><span class="line">kube-node-lease    Active   191d</span><br><span class="line">kube-public        Active   191d</span><br><span class="line">kube-system        Active   191d</span><br><span class="line">kubeflow           Active   23d</span><br><span class="line">metallb-system     Active   171d</span><br><span class="line">nps                Active   191d</span><br><span class="line">rook-ceph          Active   24d</span><br><span class="line">tibero             Active   7d23h</span><br></pre></td></tr></table></figure><p><U>Pod</U>:  </p><ul><li>Kubernetes의 최소 실행 단위이다.</li><li>Kubernetes는 컨테이너를 개별적으로 배포하는 것이 아니라 Pod 단위로 배포한다. 즉, 여러 개 컨테이너가 하나의 Pod 로 배포할 수도 있고 하나의 컨테이너를 하나의 Pod 로 배포할 수 있다. <em>(후자의 경우 Pod concept 과 맞지 않아 지양해야 할 부분이다)</em></li><li>Pod 내 네트워크 환경(IP, Port)과 디스크(Volume) 공유한다.<blockquote><p>A Container(Port 8080)와 B Container(Port 7001)가 하나의 Pod로 배포되었을때, localhost:{각 포트}를 통해 두 컨테이너 간 통신이 가능하다.</p></blockquote></li><li>디스크를 공유하고 있기 때문에 다른 두 성격의 컨테이너를 배포할 때 타 컨테이너의 파일을 읽을 수 있다.</li><li>YAML / JSON 형식으로 선언(config)하는데, 주로 선언할 때는 YAML 을 주로 사용한다.</li></ul><p><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fp1EBb%2FbtqFzG1OW7a%2FjeUxSY32E5zpaBEE3mEB1k%2Fimg.png" alt="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fp1EBb%2FbtqFzG1OW7a%2FjeUxSY32E5zpaBEE3mEB1k%2Fimg.png"></p><p><U>Service</U>:  </p><ul><li>Pod 는 Controller 에 의해 관리되는데, 만약 노드 내 장애가 발생하여 다른 노드에 Pod 를 재 배포하게되면 Pod 의 IP가 변경된다. 이렇게 Pod IP가 변경되는 것을 방지하고자 IP를 특정하기 위해 사용하는 것이 Service다.</li><li>label 과 label selector 를 이용하여 같은 서비스로 묶을 파드를 정의할 수 있고, 같은 서비스로 묶이면 고정 주소를 이용해 묶인 파드간 통신을 원활히 할 수 있다.</li><li>Service Type<ul><li>ClusterIP: kubernetes cluster 내에서만 사용가능한 IP로 외부에서는 접근이 불가하다.</li><li>NodePort: cluster 내 모든 노드의 지정된 포트를 할당하는 방식이다. 예를 들어, A 노드에 떠있는 a 파드로 접근하고 싶을 때 a 파드를 NodePort 서비스로 묶어주면 B 노드 IP:Port 로 접근할 수 있다.</li><li>LoadBalancer: NodePort 의 확장으로 서비스 IP, 등 외부로 노출하고자 하는 특정 IP 를 이용하여 외부에서 특정 IP를 통해 접근할 수 있는 Service Type 이다.</li><li>ExternalName: 외부 서비스를 Kubernetes 내부에 호출하고자 할 때 사용하는 Service Type 이다. <em>(사용을 안해봤다..)</em></li></ul></li></ul><p><U>Volume</U>  </p><ul><li>Container 재시작에 상관없이 파일을 영구적으로 저장해야하는 스토리지이다. Pod 내 Container 들은 해당 디스크를 공유하여 사용할 수 있다.</li><li>종류:<ul><li>emptyDir: Pod 와 함께 생성하고 사라지는 임시 볼륨이다.</li><li>hostPath: 해당 노드의 디스크 경로를 Pod 에 마운트하여 사용하는 방식이다. 해당 노드 디스크 경로가 존재하지는 꼭 확인이 필요하다.</li></ul></li><li>Kubernetes 에서는 emptyDir, hostPath 이외에 PV(PersistentVolumne) 와 PVC(PersistentVolumnClaim) 라는 추상화한 개념을 사용하고 있다. Pod 와 별도의 생명주기를 가지고 있어 해당 볼륨을 물고 있는 Pod 가 죽더라도 PV 또는 PVC 를 지우지 않는 한 데이터는 소멸하지 않는다. 쉽게 말해 노트북 디스크를 효율적으로 사용하기 위해 외장 하드를 이용한다고 생각하자. <ul><li>PV: 클러스터 내에서 자원으로 다뤄지는 볼륨 자체를 의미한다. 하나의 저장소로 생각하자.</li><li>PVC: 사용자가 PV에 요청하는 것입니다. 얼만큼, 읽고쓰기가 가능한 지 spec 을 정의하여 배포하면 그에 맞는 PV가 생성된다.</li></ul></li></ul><p><img src="https://t1.daumcdn.net/cfile/tistory/9997504A5B1D3F0F05" alt="https://t1.daumcdn.net/cfile/tistory/9997504A5B1D3F0F05"></p><h3 id="Controller"><a href="#Controller" class="headerlink" title=" Controller"></a><a name="subject10"></a> Controller</h3><ul><li>Controller 는 다중 Pod 를 생성하고 관리하는 역할로 클러스터 내에서 replication handling, rollout 등의 기능을 제공한다. 예를 들어, 클러스터 내 한 Node 가 다운되었을 경우 Controller 는 다른 노드에 해당 파드를 배포(스케쥴링) 함으로써 관리한다. 굉장히 많은 Controller 가 있는데 그 중에서 ReplicaSet, Deployment, DaemonSet 을 살펴본다.</li></ul><p><U>ReplicaSet</U>  </p><ul><li>Pod 를 복제하여 생성하고 이를 지속적으로 유지하는 Controller 이다. 실행하고자 하는 Pod 의 가용성을 보장하기 위해 원하는 갯수만큼 설정하여 배포하면, 그 갯수만큼 Pod 가 실행되게끔 관리하는 Controller 이다.</li><li>직접적으로 사용하기 보단 Deployment 등 다른 오브젝트에 의해 사용되는 경우가 많다.</li><li>예시: nginx 이미지를 기반으로 하는 컨테이너 <em>(이 예시는 1 container == 1 pod)</em> 를 spec.replicas value 만큼 유지하세요.</li></ul><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ReplicaSet</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">test-replicaset</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">test-replicaset</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">test-replicaset</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">test-replicaset</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">nginx</span></span><br><span class="line">        <span class="attr">ports:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">80</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">3</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">test-replicaset</span></span><br></pre></td></tr></table></figure><p><U>Deployment</U>  </p><ul><li>Kubernetes 에서 상태가 없는(stateless) 앱을 배포할 때 가장 기본적인 컨트롤러, 이전 버젼에서는 ReplicaSet 이 담당했지만 이제는 Deployment 가 담당한다.</li><li>Pod 개수를 유지하는 것 뿐 아니라 무중단 배포를 위한 방식 <em>(blue/green deployment, canary deployment, rolling deployment)</em> 을 제공한다. ReplicaSet 과 비교하면 Pod 개수 유지는 같지만 추가적으로 무중단 배포 방식을 설정할 수 있다.</li><li>deployment 배포 시 업데이트 방식을 설정할 수 있다. <em>(조대협님 블로그에 따르면 가장 많이 사용하는 배포 방식이며, 따로 설정하지 않으면 rolling 배포를 수행한다. <a href="https://bcho.tistory.com/1266">참고</a>)</em></li></ul><blockquote><p>무중단 배포방식: <a href="https://perfectacle.github.io/2019/04/21/non-stop-deployment/">참고자료</a></p></blockquote><p><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FmVW7g%2FbtqFNFa7u1E%2FRTURWaMRdcLNpDzxj3GAwk%2Fimg.png" alt="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FmVW7g%2FbtqFNFa7u1E%2FRTURWaMRdcLNpDzxj3GAwk%2Fimg.png"></p><p><U>DaemonSet</U>  </p><ul><li>클러스터 전체에 Pod를 띄워주는 Controller 이다. Daemonset 을 배포하면 항상 그 Pod 가 전체 노드에 배포한다.</li><li>새롭게 노드가 추가되었을때 자동으로 Pod 를 배포한다.</li><li>반대로 노드가 클러스터에서 빠졌을 때 그 노드에 있던 포드는 그대로 사라지고 다른 곳으로 옮겨가서 배포하진 않는다.<ul><li>이러한 특징 때문에 보통 로그수집기를 실행하거나 모니터링용 데몬등 클러스터 전체에 항상 실행시켜 두어야 할 때 사용한다.</li></ul></li></ul><p><img src="https://blog.kakaocdn.net/dn/VxcCs/btqEsYpkShG/bmYxdIKPS6ELBe1asDwFT0/img.png" alt="https://blog.kakaocdn.net/dn/VxcCs/btqEsYpkShG/bmYxdIKPS6ELBe1asDwFT0/img.png"></p><p><U>StatefulSet</U>  </p><ul><li>앞서 살펴본 Controller 와 달리 상태를 가지고 있는 Pod 들을 관리하는 Controller 로, 볼륨을 사용해 특정 데이터를 기록해두고 Pod 가 재시작 해도 유지할 수 있다.</li><li>여러 Pod 를 배포할 때 순서를 지정하여 순서대로 배포할 수 있다.</li><li>Pod 이름을 연속성 있게 설정할 수 있다. <em>(name-number 와 같이 표현)</em></li><li>각 Pod 에 대한 PVC 를 설정할 수 있는 장점이 있다.</li></ul><p><img src="https://i2.wp.com/www.kylezsembery.com/wp-content/uploads/2020/06/StatefulSet.png?resize=691%2C376&ssl=1" alt="https://i2.wp.com/www.kylezsembery.com/wp-content/uploads/2020/06/StatefulSet.png?resize=691%2C376&amp;ssl=1"></p><h1 id="Reference"><a href="#Reference" class="headerlink" title=" Reference"></a><a name="reference"></a> Reference</h1><ul><li><a href="https://ooeunz.tistory.com/118">https://ooeunz.tistory.com/118</a></li><li><a href="https://www.slideshare.net/lahuman1/kubernetes-object">https://www.slideshare.net/lahuman1/kubernetes-object</a></li><li><a href="https://bcho.tistory.com/1259">https://bcho.tistory.com/1259</a></li><li><a href="https://blog.2dal.com/2018/04/30/kubernetes-02-replicaset/">https://blog.2dal.com/2018/04/30/kubernetes-02-replicaset/</a></li><li><a href="https://perfectacle.github.io/2019/04/21/non-stop-deployment/">https://perfectacle.github.io/2019/04/21/non-stop-deployment/</a></li><li><a href="https://arisu1000.tistory.com/27834">https://arisu1000.tistory.com/27834</a></li><li><a href="https://www.joinc.co.kr/w/man/12/kubernetes/overview">https://www.joinc.co.kr/w/man/12/kubernetes/overview</a></li></ul>]]></content:encoded>
      
      <comments>https://jx2lee.github.io/cloud-kubernetes_chapter_02/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[Cloud] Kubernetes 소개</title>
      <link>https://jx2lee.github.io/cloud-kubernetes_chapter_01/</link>
      <guid>https://jx2lee.github.io/cloud-kubernetes_chapter_01/</guid>
      <pubDate>Wed, 23 Sep 2020 15:00:00 GMT</pubDate>
      <description>
      
        &lt;center&gt; 신입 직원 교육자료를 위해 작성한 Kubernetes 소개 자료로 이번 장에는 배경을 시작으로 특징까지 살펴본다.&lt;/center&gt;
&lt;br&gt;
&lt;br&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<center> 신입 직원 교육자료를 위해 작성한 Kubernetes 소개 자료로 이번 장에는 배경을 시작으로 특징까지 살펴본다.</center><br><br><a id="more"></a><p><strong><span class="github-emoji" alias="sparkles" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png?v8">&#x2728;</span> Contents:</strong>  </p><ol><li><a href="#subject01">배경</a></li><li><a href="#subject02">그럼 왜 Kubernetes 가 떴을까?</a></li><li><a href="#subject03">Kubernetes ?</a></li><li><a href="#subject04">Kubernetes 특징</a></li><li><a href="#reference">Reference</a></li></ol><h1 id="배경"><a href="#배경" class="headerlink" title=" 배경"></a><a name="subject01"></a> 배경</h1><p><img src="https://d33wubrfki0l68.cloudfront.net/26a177ede4d7b032362289c6fccd448fc4a91174/eb693/images/docs/container_evolution.svg" alt="kubernetes.io"></p><center><U>[그림] 배포 환경의 변화</U></center><p>컨테이너 기반의 환경은 서비스 배포에 장점이 있고 마이크로 서비스 아키텍쳐(MSA)에 부합한 듯 싶지만, 가상 머신으로도 충분히 패키징이 가능하고 로컬의 개발환경을 동기화 시키는 일은 vagrant 로도 충분하였다.</p><blockquote><ul><li>MSA: 소프트웨어 개발 기법 중 하나로 전체 어플리케이션을 특정 목적을 가진 어플리케이션 단위로 나눈 것, 단위 어플리케이션 간 약한 결합도와 강한 응집도를 목표로 한다.</li><li>vargrant: 간소화한 VM 관리 서비스로 가상 머신과 로컬간의 동기화 서비스도 제공한다.</li></ul></blockquote><p>그리고 결정적으로 컨테이너를 운용하기 위한 관리 환경이 성숙하지 않았다. Mesos DC/OS, Docker Swarm, Kubernetes 등 다양한 환경이 나오기는 하였지만 기능적으로 부족하며 어떤 플랫폼이 대세라고 정해지지 않았다.</p><blockquote><ul><li>Mesos DC/OS: Apache Mesos 분산 시스템 커널을 기반으로하는 오픈 소스, <a href="http://www.kb-sys.co.kr/file/KBSYS_DCOS.pdf">참고자료</a></li><li>Docker Swarm: 도커가 공식적으로 개발한 Container Orchestration </li></ul></blockquote><p>그럼에도 컨테이너 환경으로 바뀐 이유가 무엇일까? 두 꼭지로 살펴보면 다음과 같다.</p><p><strong>MSA의 발전:</strong><br>MSA(Micro Service Architecture)가 단순 개념에서 발전하기 시작하며 이를 구현하기 위한 다양한 인프라 플랫폼들이 등장하기 시작했다.</p><p><img src="https://miro.medium.com/max/1050/0*kbXK4udgYykDHfgh.png" alt="Micro Service Architecture 예시"></p><center><U>[그림] Micro Service Architecture 예시- 전체 애플리케이션을 특정 단위 애플리케이션으로 분리한 모습을 확인할 수 있다.</U></center><p>또한, 서비스 크기가 작아지며 CPU 1~2 Core 로도 운영할 수 있는 서비스들이 등장하면서 이를 VM 환경에서 서비스 하기엔 리소스 낭비가 심해졌다.</p><blockquote><ul><li>VM 환경을 위해서 필요한 이미지 크기가 크고, 다양한 서비스를 VM으로 관리 배포하기에는 속도 등에서 효율적이지 못하다.</li></ul></blockquote><p><strong>솔루션 발전 및 DevOps 안착:</strong><br>서비스 배포 방식도 VM 혹은 컨테이너 단위로 배포하는 피닉스 서버 패턴이 등장하였고 이를 구현하기 위한 Spinnaker와 같은 솔루션이 등장하였다. 또한, 지능형 라우팅과 분산 트렌젝션 로그 추적을 하는 기능들이 Envoy 라는 솔루션으로 나오고 이를 중앙 통제하기 위한 Istio.io 와 같은 Service Mesh 솔루션 까지 주목을 받기 시작하였다.</p><blockquote><ul><li>Phoenix Server Patern: SW 테스트를 위해 변경한 설정을 현 서버에 적용하는 것이 아닌 OS설치부터 SW 설치-설정 변경까지 다시 &gt; 반복하는 패턴을 의미한다.</li><li>Envoy: 대형 MSA의 단일 Application과 Service를 위해 설계된 고성능 분산 c++프록시</li><li>Istio.io: Envoy proxy를 사용하며 이를 컨트롤 해주는 Control Plane의 오픈소스 솔루션, <a href="https://gruuuuu.github.io/cloud/service-mesh-istio/#">참고자료</a></li></ul></blockquote><p>DevOps 개념도 나온지 오래되었지만, 운영을 DevOps라는 이름으로 바꾼 것일뿐 실제적인 변화가 없는 팀들이 많았다고 한다. 또는 DevOps라는 이름 아래 개발팀이 개발과 운영 역할을 병행해서 하는 사례가 오히려 많았다.</p><p>이런 DevOps의 개념도 근래 들어 개발팀이 개발과 시스템에 대한 배포/운영을 담당한다면, DevOps 팀은 개발팀이 이를 편리하게 사용할 수 있게끔 플랫폼과 자동화를 하는데 목표를 두는 역할로 구분지어졌다.</p><h1 id="그럼-왜-Kubernetes-가-떴을까"><a href="#그럼-왜-Kubernetes-가-떴을까" class="headerlink" title=" 그럼 왜 Kubernetes 가 떴을까?"></a><a name="subject02"></a> 그럼 왜 Kubernetes 가 떴을까?</h1><p>컨테이너 운용 환경은 오픈소스에 의해 표준없이 혼돈상태였는데, 2017년 말을 기점으로 Kubernetes가 de-facto 표준으로 자리잡았다. 아래 트랜드 그래프를 보면 알 수 있듯 kubernetes의 트랜드가 지속적으로 올라가서 가장 높은 것을 확인할 수 있다. 추가로 github repo star 통계에 대해서도 2015 년을 기점으로 크게 증가하여 2018년 말에는 엄청 큰 차이를 보이고 있다.</p><p><img src="https://t1.daumcdn.net/cfile/tistory/990BE94D5AFEF4A20F" alt="Container Trends"></p><center><U>[그림] Container Orchestration Trends</U></center>  <p><img src="https://www.percona.com/community-blog/wp-content/uploads/2019/07/kubernetes-growth.jpg" alt="Container solution Trend about github repo start"></p><center><U>[그림] github repo star for Container Orchestration</U></center><h1 id="Kubernetes"><a href="#Kubernetes" class="headerlink" title=" Kubernetes ?"></a><a name="subject03"></a> Kubernetes ?</h1><ul><li>컨테이너 운영환경 중 가장 널리 사용되는 솔루션이다. <em>(약어로 k8s, 8의 의미는 k와 s사이 알파벳 갯수)</em></li><li>구글의 내부 컨테이너 서비스를 Borg라고 하는데 이 구조를 오픈소스화 한 것이다.</li><li>GO 언어로 구현하였다. <em><a href="https://golang.org">golang</a></em></li><li>벤더나 플랫폼에 종속되지 않기 때문에 <strong>Public Cloud</strong> <em>(Google, Amazone, MS Azure)</em> 와 <strong>Private CLoud 및 베어메탈 환경</strong> <em>(가상화 환경을 사용하지 않는 일반 서버 하드웨어, like 깡통)</em> 에서 구축 가능<ul><li>이러한 이유로 Hybrid Cloud 환경 가능 <em>(Private + Public)</em></li></ul></li><li>다양한 Container Runtime 제공<ul><li>Docker 또한 Container Runitme 으로 사용가능하지만, CRI-O 가 현재 Kubernetes 를 위한 표준 Container Runtime 으로 자리잡고 있다.</li><li>관련 자료<ul><li><a href="http://www.opennaru.com/kubernetes/cri-o/">http://www.opennaru.com/kubernetes/cri-o/</a></li><li><a href="https://www.samsungsds.com/global/ko/support/insights/docker.html">https://www.samsungsds.com/global/ko/support/insights/docker.html</a></li><li><a href="https://bcho.tistory.com/1353">https://bcho.tistory.com/1353</a></li></ul></li></ul></li></ul><h1 id="Kubernetes-특징"><a href="#Kubernetes-특징" class="headerlink" title=" Kubernetes 특징"></a><a name="subject04"></a> Kubernetes 특징</h1><ul><li>상태관리 : 상태를 선언하고 선언한 상태를 계속해서 유지한다. 노드가 죽거나 Container 응답이 없을 경우, 자동으로 새로운 Container 를 실행하거나 자동으로 특정 상태에 도달하지 못한 Conatiner 를 중지하는 등 <strong>선언한 상태를 계속해서 유지</strong>한다.</li><li>스케줄링 : 어떤 노드에 Container 를 실행할지 고민하지 않아도 Kubernetes 조건에 맞는 노드를 찾아서 Conatiner 를 배치한다. <em>(특정 노드에 실행하게 조건 설정도 가능하다)</em></li><li>클러스터 : 가상 네트워크를 통해 통신하기 때문에 하나의 서버처럼 관리할 수 있다.</li><li>서비스 디스커버리 : 서로 다른 서비스를 쉽게 찾고 통신할 수 있습니다.</li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title=" Reference"></a><a name="reference"></a> Reference</h1><ul><li><a href="https://bcho.tistory.com/1255">쿠버네티스 #1 - 소개</a></li><li><a href="https://ooeunz.tistory.com/84">[Kubernetes] 쿠버네티스의 등장 배경</a></li><li><a href="https://medium.com/@shaul1991/%EC%B4%88%EB%B3%B4%EA%B0%9C%EB%B0%9C%EC%9E%90-%EC%9D%BC%EC%A7%80-%EB%8C%80%EC%84%B8-msa-%EB%84%88-%EB%AD%90%EB%8B%88-efba5cfafdeb">[초보개발자 일지] 대세 MSA? 너 뭐니?</a></li></ul><hr><p>made by <em>jaejun.lee</em></p>]]></content:encoded>
      
      <comments>https://jx2lee.github.io/cloud-kubernetes_chapter_01/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[Database] 일 단위 적재 프로세스 훑어보기</title>
      <link>https://jx2lee.github.io/database-daily_batch_process/</link>
      <guid>https://jx2lee.github.io/database-daily_batch_process/</guid>
      <pubDate>Wed, 09 Sep 2020 15:00:00 GMT</pubDate>
      <description>
      
        &lt;p&gt;프로젝트 시범과제 중 &lt;strong&gt;[데이터 마트 구축]&lt;/strong&gt;을 수행하면서 고객요건에 맞는 테이블을 생성하고 갱신하는 프로세스를 경험하였다. 별 어려운 내용은 없지만 내 머릿속에 저장히기 위해 글로 남겨놓는다. &lt;em&gt;(도움을 주신 갓정희님께 감사의 인사를)&lt;/em&gt;&lt;/p&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p>프로젝트 시범과제 중 <strong>[데이터 마트 구축]</strong>을 수행하면서 고객요건에 맞는 테이블을 생성하고 갱신하는 프로세스를 경험하였다. 별 어려운 내용은 없지만 내 머릿속에 저장히기 위해 글로 남겨놓는다. <em>(도움을 주신 갓정희님께 감사의 인사를)</em></p><a id="more"></a><p><strong><span class="github-emoji" alias="sparkles" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png?v8">&#x2728;</span> Contents:</strong></p><ol><li><a href="#createuser">유저 생성</a>  </li><li><a href="#createtable">TARGET 테이블 생성</a>  </li><li><a href="#update">주 단위 데이터 마감</a>  </li><li><a href="#reference">Reference</a>  </li></ol><h1 id="유저-생성"><a href="#유저-생성" class="headerlink" title=" 유저 생성"></a><a name="createuser"></a> 유저 생성</h1><p>해당 테이블을 관리하는 계정을 생성하였다.</p><ul><li>user/pw: mart_20/<strong>***</strong></li><li>role: connect, resource</li><li>User create query<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--Create &quot;MART_20&quot; User</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">USER</span> MART_20 <span class="keyword">IDENTIFIED</span> <span class="keyword">BY</span> <span class="string">&#x27;*******&#x27;</span>;</span><br><span class="line"><span class="keyword">GRANT</span> <span class="keyword">CONNECT</span>, <span class="keyword">RESOURCE</span> <span class="keyword">TO</span> MART_20;</span><br></pre></td></tr></table></figure></li></ul><h1 id="TARGET-테이블-생성"><a href="#TARGET-테이블-생성" class="headerlink" title=" TARGET 테이블 생성"></a><a name="createtable"></a> TARGET 테이블 생성</h1><p>고객이 사용하던 SQL 쿼리를 살펴보면, 총 12개 Table 및 View 를 조합하여 네 덩어리 SELECT 조회 결과를 UNION ALL 한다. 운영자가 수정하는 부분은 날짜 칼럼을 입력하게끔 쿼리를 작성하여 이를 EXCEL 로 다운받아 요청자에게 전달하는 방식이다. 우리는 이 쿼리의 결과물을 미리 테이블로 정의하고, 일 적재를 통해 고객이 쉽게 우리 제품을 이용해 분석하고 싶을 때 분석 가능하게 환경을 구성할 예정이다. 하지만 요청자를 위해 작성된 Query는 다음과 같은 제약사항이 있다. </p><ul><li>Primary Key 로 설정한 SEQ 는 추후 중복이 발생할 수 있다.<ul><li>수집을 하다보면 언젠가 UNION ALL 전의 한 덩어리에서의 SEQ 칼럼과 나머지 세 덩어리에서의 SEQ 와 동일해질 수 있다. 서로 다르게 SEQ Value 관리하기 때문이다.</li><li>이를 해결할 방법으로 UNION ALL 전의 네 덩어리를 적재 테이블로 각각 관리하면 된다. 이후 생성한 네 개 테이블을 UNION ALL 한 VIEW 를 바라보면 되는데, 이는 1) 관리 포인트가 많아지고  2) View를 사용하기 대문에 성능 이슈가 발생할 수 있다.</li><li>그럼에도 나는 하나의 테이블로 관리하고자 한다.</li></ul></li><li>Source Table 에서 수정이 발생하면 이에 대한 Target Source 처리가 필요하다. 즉, 이전 날짜의 데이터에 대한 수정이 발생하면 Target Source 에 대한 반영이 필요하다.<ul><li>이는 주 또는 월 단위 DEL_FLAG 변수를 활용해 삭제 또는 추가 여부를 확인하는 <strong>배치 작업</strong> 으로 반영할 수 있다. 단, Update period 는 협의가 필요한 상황!</li><li><code>HABITAUL_PRACTICE_TEMP</code> 테이블을 생성하여 마감 처리를 진행할 것이다.</li></ul></li></ul><p>테이블 <em>Create Query</em> 는 아래와 같다.</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> MART_20.HABITAUL_PRACTICE_ANALYSIS (</span><br><span class="line">SEQ <span class="built_in">NUMBER</span>(<span class="number">22</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span> PRIMARY <span class="keyword">KEY</span>,</span><br><span class="line">DOMAIN_SEQ <span class="built_in">NUMBER</span>(<span class="number">22</span>),</span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line"><span class="string">&quot;위변조 통장&quot;</span> <span class="built_in">NUMBER</span>,</span><br><span class="line"><span class="string">&quot;위변조 기타&quot;</span> <span class="built_in">NUMBER</span>,</span><br><span class="line">DEL_FLAG <span class="built_in">VARCHAR</span>(<span class="number">2</span>) <span class="keyword">DEFAULT</span> <span class="string">&#x27;N&#x27;</span></span><br><span class="line">)</span><br><span class="line">;</span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">INDEX</span> MART_20.HABITAUL_PRACTICE_ANALYSIS_IDX_01 <span class="keyword">ON</span> MART_20.HABITAUL_PRACTICE_ANALYSIS (</span><br><span class="line">TFD_DATE,</span><br><span class="line">DOMAIN_LINK,</span><br><span class="line">DEL_FLAG</span><br><span class="line">)</span><br><span class="line">;</span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> MART_20.HABITAUL_PRACTICE_TEMP (</span><br><span class="line">SEQ <span class="built_in">NUMBER</span>(<span class="number">22</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span> PRIMARY <span class="keyword">KEY</span>,</span><br><span class="line">DOMAIN_SEQ <span class="built_in">NUMBER</span>(<span class="number">22</span>),</span><br><span class="line">...</span><br><span class="line">...<span class="string">&quot;위변조 통장&quot;</span> <span class="built_in">NUMBER</span>,</span><br><span class="line"><span class="string">&quot;위변조 기타&quot;</span> <span class="built_in">NUMBER</span></span><br><span class="line">)</span><br><span class="line">;</span><br></pre></td></tr></table></figure><ul><li>PK: SEQ</li><li>INDEX: DOMAIN_LINK, TFD_DATE, DEL_FLAG</li><li>TEMP 테이블과 원본 테이블의 차이점은 <strong>DEL_FLAG</strong> 칼럼의 여부이다.</li></ul><h1 id="주-단위-데이터-마감"><a href="#주-단위-데이터-마감" class="headerlink" title=" 주 단위 데이터 마감"></a><a name="update"></a> 주 단위 데이터 마감</h1><p>운영진에서 수정된 데이터를 반영하기 위해 주 단위 데이터를 마감할 것이다. 아래와 같은 순서로 진행하는데, 이는 실시간 수정 반영이 어려워(구조상) 고객와 협의하여 진행하였다. <em>(마감 주기, 즉 Update 반영을 한 주 전까지만 반영)</em></p><h2 id="1-현재-기준-이-전-데이터-조회-후-저장"><a href="#1-현재-기준-이-전-데이터-조회-후-저장" class="headerlink" title="1) 현재 기준 이 전 데이터 조회 후 저장"></a>1) 현재 기준 이 전 데이터 조회 후 저장</h2><p><em>데이터 주 단위 갱신 마감을 위한 조회 Query</em>는 다음과 같다.</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> *</span><br><span class="line">   <span class="keyword">FROM</span> ( <span class="keyword">SELECT</span> SEQ,</span><br><span class="line">                 DOMAIN_SEQ,</span><br><span class="line">                 DOMAIN_LINK,</span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line">                 <span class="keyword">CASE</span> <span class="keyword">WHEN</span> <span class="keyword">INSTR</span>(ILLE_EXPR,</span><br><span class="line">                 <span class="string">&#x27;0209&#x27;</span>) &gt; <span class="number">0</span> <span class="keyword">THEN</span> <span class="number">1</span> <span class="keyword">END</span> <span class="keyword">AS</span> <span class="string">&quot;위변조 기타&quot;</span></span><br><span class="line">            <span class="keyword">FROM</span> (        <span class="keyword">SELECT</span> A.SEQ,</span><br><span class="line">                          <span class="keyword">DECODE</span>(WEB_CACHE_URL,<span class="literal">NULL</span>,A.DOMAIN_SEQ,<span class="number">100000001</span>) <span class="keyword">AS</span> DOMAIN_SEQ,</span><br><span class="line">                          <span class="keyword">DECODE</span>(WEB_CACHE_URL,<span class="literal">NULL</span>,B.DOMAIN_LINK,<span class="string">&#x27;webcache.googleusercontent.com&#x27;</span>) <span class="keyword">AS</span> DOMAIN_LINK,</span><br><span class="line">                          <span class="keyword">DECODE</span>(WEB_CACHE_URL,<span class="literal">NULL</span>,F.DOMAIN_NAME,<span class="string">&#x27;구글웹캐시&#x27;</span>) <span class="keyword">AS</span> DOMAIN_NAME,</span><br><span class="line">                          F.DOMAIN_CODE2,</span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line">                          WM_CONCAT (E.EXPR_CATEGORY || E.EXPR_SUB_CATE) ILLE_EXPR</span><br><span class="line">                     <span class="keyword">FROM</span> PIRST_19.DETECTED_URL@DS16 A <span class="keyword">INNER</span> <span class="keyword">JOIN</span> PIRST_19.DOMAIN_MASTER@DS16 B <span class="keyword">ON</span> A.DOMAIN_SEQ = B.DOMAIN_SEQ </span><br><span class="line">                          <span class="keyword">INNER</span> <span class="keyword">JOIN</span> PIRST_19.DOMAIN_INFORMATION@DS16 F <span class="keyword">ON</span> A.DOMAIN_SEQ = F.DOMAIN_SEQ </span><br><span class="line">                          <span class="keyword">LEFT</span> <span class="keyword">OUTER</span> <span class="keyword">JOIN</span> PIRST_19.KEYWORD_MASTER@DS16 C <span class="keyword">ON</span> A.KEYWORD_SEQ = C.KEYWORD_SEQ </span><br><span class="line">                          <span class="keyword">LEFT</span> <span class="keyword">OUTER</span> <span class="keyword">JOIN</span> PIRST_19.A_ILLEGAL_TRADER_MST@DS16 D <span class="keyword">ON</span> A.SEQ = D.URL_SEQ </span><br><span class="line">                          <span class="keyword">LEFT</span> <span class="keyword">OUTER</span> <span class="keyword">JOIN</span> PIRST_19.DETECTED_URL_EXPR@DS16 E <span class="keyword">ON</span> A.SEQ = E.URL_SEQ <span class="keyword">AND</span> E.EXPR_USE = <span class="string">&#x27;Y&#x27;</span> </span><br><span class="line">                          <span class="keyword">LEFT</span> <span class="keyword">OUTER</span> <span class="keyword">JOIN</span> (<span class="keyword">SELECT</span> * </span><br><span class="line">                                           <span class="keyword">FROM</span> PIRST_19.DETECTED_IMG@DS16</span><br><span class="line">                                           <span class="keyword">WHERE</span> <span class="keyword">ROWID</span> <span class="keyword">IN</span> (<span class="keyword">SELECT</span> <span class="keyword">MAX</span>(<span class="keyword">ROWID</span>) <span class="keyword">FROM</span> PIRST_19.DETECTED_IMG@DS16</span><br><span class="line">                                           <span class="keyword">GROUP</span> <span class="keyword">BY</span> DETECTED_SEQ)) G <span class="keyword">ON</span> A.SEQ = G.DETECTED_SEQ</span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line">                      <span class="keyword">AND</span> C.KEYWORD_REG_USER_ID != <span class="string">&#x27;covid19&#x27;</span></span><br><span class="line">                    <span class="keyword">GROUP</span> <span class="keyword">BY</span> A.SEQ,</span><br><span class="line">                             <span class="keyword">DECODE</span>(WEB_CACHE_URL,<span class="literal">NULL</span>,A.DOMAIN_SEQ,<span class="number">100000001</span>),</span><br><span class="line">                             <span class="keyword">DECODE</span>(WEB_CACHE_URL,<span class="literal">NULL</span>,B.DOMAIN_LINK,<span class="string">&#x27;webcache.googleusercontent.com&#x27;</span>),</span><br><span class="line">                             <span class="keyword">DECODE</span>(WEB_CACHE_URL,<span class="literal">NULL</span>,F.DOMAIN_NAME,<span class="string">&#x27;구글웹캐시&#x27;</span>),</span><br><span class="line">                             F.DOMAIN_CODE2,</span><br><span class="line">                             F.DOMAIN_CODE3,            </span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line">                             D.URL_SEQ,</span><br><span class="line">                             A.POST_DATE</span><br><span class="line">                  ) A <span class="keyword">LEFT</span> <span class="keyword">OUTER</span> <span class="keyword">JOIN</span> PIRST_19.A_ILLEGAL_TRADER_MST@DS16 B <span class="keyword">ON</span> A.TRADER_SEQ = B.TRADER_SEQ</span><br><span class="line">             <span class="keyword">AND</span> A.URL_SEQ = B.URL_SEQ)</span><br><span class="line"><span class="comment">--한 덩어리 끝</span></span><br><span class="line"> <span class="keyword">UNION</span> <span class="keyword">ALL</span></span><br><span class="line"> <span class="keyword">SELECT</span> A.MANUAL_SEARCH_SEQ,</span><br><span class="line">        A.DETECTED_DOMAIN_SEQ,</span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line">        <span class="keyword">DECODE</span>(<span class="keyword">SUBSTR</span>(D.exposure_type, <span class="keyword">INSTR</span>(D.exposure_type, <span class="string">&#x27;C09&#x27;</span>),<span class="number">3</span>),<span class="string">&#x27;C09&#x27;</span>,<span class="number">1</span>) <span class="keyword">as</span> SIXTEEN     </span><br><span class="line">   <span class="keyword">FROM</span> PIRST_19.MANUAL_DETECTED_URL@DS16 A,</span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line">        PIRST_19.A_ILLEGAL_TRADER_MST@DS16 E</span><br><span class="line">  <span class="keyword">WHERE</span> A.DETECTED_DOMAIN_SEQ = B.DOMAIN_SEQ</span><br><span class="line">    <span class="keyword">AND</span> A.DETECTED_DOMAIN_SEQ = C.DOMAIN_SEQ</span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line">  <span class="keyword">UNION</span> <span class="keyword">ALL</span></span><br><span class="line"><span class="comment">--두 덩어리 끝</span></span><br><span class="line"> <span class="keyword">SELECT</span> A.MANUAL_SEARCH_SEQ,</span><br><span class="line">        A.DETECTED_DOMAIN_SEQ,</span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line">        <span class="keyword">DECODE</span>(<span class="keyword">SUBSTR</span>(D.exposure_type, <span class="keyword">INSTR</span>(D.exposure_type, <span class="string">&#x27;C08&#x27;</span>),<span class="number">3</span>),<span class="string">&#x27;C08&#x27;</span>,<span class="number">1</span>) <span class="keyword">as</span> FIFTEEN,</span><br><span class="line">        <span class="keyword">DECODE</span>(<span class="keyword">SUBSTR</span>(D.exposure_type, <span class="keyword">INSTR</span>(D.exposure_type, <span class="string">&#x27;C09&#x27;</span>),<span class="number">3</span>),<span class="string">&#x27;C09&#x27;</span>,<span class="number">1</span>) <span class="keyword">as</span> SIXTEEN     </span><br><span class="line">  <span class="keyword">FROM</span> PIRST_19.MANUAL_CACHE_URL@DS16 A,</span><br><span class="line">       PIRST_19.DOMAIN_MASTER@DS16 B,</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line"> <span class="keyword">WHERE</span> A.DETECTED_DOMAIN_SEQ = B.DOMAIN_SEQ</span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line">   <span class="keyword">AND</span> A.MBER_ID != <span class="string">&#x27;itno_cmbok&#x27;</span></span><br><span class="line"><span class="comment">----세 덩어리 끝 </span></span><br><span class="line">  <span class="keyword">UNION</span> <span class="keyword">ALL</span></span><br><span class="line"> <span class="keyword">SELECT</span> A.MANUAL_SEARCH_SEQ,</span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line">        <span class="keyword">DECODE</span>(<span class="keyword">SUBSTR</span>(D.exposure_type, <span class="keyword">INSTR</span>(D.exposure_type, <span class="string">&#x27;C08&#x27;</span>),<span class="number">3</span>),<span class="string">&#x27;C08&#x27;</span>,<span class="number">1</span>) <span class="keyword">as</span> FIFTEEN,</span><br><span class="line">        <span class="keyword">DECODE</span>(<span class="keyword">SUBSTR</span>(D.exposure_type, <span class="keyword">INSTR</span>(D.exposure_type, <span class="string">&#x27;C09&#x27;</span>),<span class="number">3</span>),<span class="string">&#x27;C09&#x27;</span>,<span class="number">1</span>) <span class="keyword">as</span> SIXTEEN     </span><br><span class="line">   <span class="keyword">FROM</span> PIRST_19.MANUAL_IMAGE_URL@DS16 A,</span><br><span class="line">        PIRST_19.DOMAIN_MASTER@DS16 B,</span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line">  <span class="keyword">WHERE</span> A.DETECTED_DOMAIN_SEQ = B.DOMAIN_SEQ</span><br><span class="line">    <span class="keyword">AND</span> A.DETECTED_DOMAIN_SEQ = C.DOMAIN_SEQ</span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line"><span class="comment">--네 덩어리 끝</span></span><br><span class="line">;</span><br></pre></td></tr></table></figure><ul><li>TFD_DATE 를 SYSDATE(현재 시간) 기준 7일 기준으로 검색한다. 즉, 이전 일주일 치 데이터를 조회한다.<ul><li>20.09.03~20.09.09: 3993건</li></ul></li><li>결과물을 <code>HABITUAL_TEMP</code> 테이블에 저장한다.</li></ul><h2 id="2-현재-기준-일주일-전-데이터의-DEL-FLAG-변경"><a href="#2-현재-기준-일주일-전-데이터의-DEL-FLAG-변경" class="headerlink" title="2) 현재 기준 일주일 전 데이터의 DEL_FLAG 변경"></a>2) 현재 기준 일주일 전 데이터의 DEL_FLAG 변경</h2><p><em><code>HABITUAL_BUYER</code> 에 위 기간 데이터의 DEL_FLAG 변경 Query</em> 는 다음과 같다.</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">UPDATE</span> HABITUAL_BUYER</span><br><span class="line"><span class="keyword">SET</span> DEL_FLAG=<span class="string">&#x27;Y&#x27;</span></span><br><span class="line"><span class="keyword">WHERE</span> TFD_DATE &gt;= <span class="keyword">TO_DATE</span>(TO_CHAR(<span class="keyword">SYSDATE</span>, <span class="string">&#x27;YYYYMMDD&#x27;</span>),<span class="string">&#x27;YYYYMMDD&#x27;</span>) - <span class="number">7</span></span><br><span class="line">       <span class="keyword">AND</span> TFD_DATE &lt; <span class="keyword">TO_DATE</span>(TO_CHAR(<span class="keyword">SYSDATE</span>, <span class="string">&#x27;YYYYMMDD&#x27;</span>),<span class="string">&#x27;YYYYMMDD&#x27;</span>)</span><br></pre></td></tr></table></figure><h2 id="3-TEMP-테이블과-TARGET-테이블-간-Merge"><a href="#3-TEMP-테이블과-TARGET-테이블-간-Merge" class="headerlink" title="3) TEMP 테이블과 TARGET 테이블 간 Merge"></a>3) TEMP 테이블과 TARGET 테이블 간 Merge</h2><p><em>두 테이블 간 (<code>HABITUAL_BUYER</code> // <code>HABITUAL_TEMP</code>) Merge Ouery</em>는 다음과 같다.</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">MERGE</span> <span class="keyword">INTO</span> (<span class="keyword">SELECT</span> *</span><br><span class="line"><span class="keyword">FROM</span> HABITUAL_BUYER</span><br><span class="line"> <span class="keyword">WHERE</span> TFD_DATE &gt;= <span class="keyword">TO_DATE</span>(TO_CHAR(<span class="keyword">SYSDATE</span>, <span class="string">&#x27;YYYYMMDD&#x27;</span>),<span class="string">&#x27;YYYYMMDD&#x27;</span>) - <span class="number">7</span></span><br><span class="line">   <span class="keyword">AND</span> TFD_DATE &lt; <span class="keyword">TO_DATE</span>(TO_CHAR(<span class="keyword">SYSDATE</span>, <span class="string">&#x27;YYYYMMDD&#x27;</span>),<span class="string">&#x27;YYYYMMDD&#x27;</span>)</span><br><span class="line">) <span class="keyword">AS</span> A</span><br><span class="line"><span class="keyword">USING</span> HABITUAL_TEMP <span class="keyword">AS</span> B</span><br><span class="line"><span class="keyword">ON</span> (A.SEQ = B.SEQ)</span><br><span class="line"><span class="keyword">WHEN</span> <span class="keyword">MATCHED</span> <span class="keyword">THEN</span></span><br><span class="line"><span class="keyword">UPDATE</span> <span class="keyword">SET</span></span><br><span class="line">A.DEL_FLAG = <span class="string">&#x27;N&#x27;</span></span><br><span class="line"><span class="keyword">WHEN</span> <span class="keyword">NOT</span> <span class="keyword">MATCHED</span> <span class="keyword">THEN</span></span><br><span class="line"><span class="keyword">INSERT</span> (A.SEQ,A.DOMAIN_SEQ ,A.DOMAIN_LINK ,A.DOMAIN_NAME ,A.CODE_NM2 ,A.CODE_NM3,A.DETECTED_CRAWL_COUNT,A.DETECTED_LINK,A.DETECTED_COUNT ,A.TRUE_DETECTION_COUNT ,A.FALSE_DETECTION_COUNT ,A.DETECTED_NEW_EXPOSURE_COUNT ,A.DETECTED_RE_EXPOSURE,A.DETECTED_RE_EXPOSURE_COUNT ,A.DOMAIN_CATEGORY01,A.DOMAIN_CATEGORY02,A.SEARCH_COL_TYPE,A.DOMAIN_GROUP,A.DETECTED_DEPTH,A.DOMAIN_COUNTRY_CODE,A.DETECTED_STATUS,A.DETECTED_GROUP,A.DETECTED_TYPE,A.DETECTED_CHECK_TYPE,A.KEYWORD_SEQ,A.KEYWORD_VALUE,A.INDI_DOMAIN_SEQ,A.WEB_CACHE_URL,A.DETECTED_TIME ,A.TFD_DATE ,A.DEL_DONE_DATE ,A.UPDATOR,A.REQUEST_ORDINAL,A.DETECTED_POST_TYPE,A.GATHERING_TYPE,A.IMG_DETECTION_TYPE,A.POST_DATE ,A.REFERENCE,A.TRADER_TYPE,A.<span class="string">&quot;판매자ID&quot;</span>,A.<span class="string">&quot;이메일&quot;</span>,A.<span class="string">&quot;카카오톡&quot;</span>,A.<span class="string">&quot;네이트온&quot;</span>,A.<span class="string">&quot;MSN메신저&quot;</span>,A.<span class="string">&quot;스카이프&quot;</span>,A.<span class="string">&quot;위챗&quot;</span>,A.QQ,A.<span class="string">&quot;텔레그램&quot;</span>,A.<span class="string">&quot;기타1&quot;</span>,A.<span class="string">&quot;기타2&quot;</span>,A.<span class="string">&quot;핸드폰번호&quot;</span>,A.<span class="string">&quot;일반번호&quot;</span>,A.<span class="string">&quot;거래 개인정보DB&quot;</span> ,A.<span class="string">&quot;거래 통장&quot;</span> ,A.<span class="string">&quot;거래 ID판매&quot;</span> ,A.<span class="string">&quot;거래 아이핀&quot;</span> ,A.<span class="string">&quot;거래 대포폰&quot;</span> ,A.<span class="string">&quot;거래 해킹&quot;</span> ,A.<span class="string">&quot;거래 기타&quot;</span> ,A.<span class="string">&quot;위변조 증명서&quot;</span> ,A.<span class="string">&quot;위변조 성적표&quot;</span> ,A.<span class="string">&quot;위변조 신분증&quot;</span> ,A.<span class="string">&quot;위변조 자격증&quot;</span> ,A.<span class="string">&quot;위변조 여권&quot;</span> ,A.<span class="string">&quot;위변조 기록부&quot;</span> ,A.<span class="string">&quot;위변조 내역서&quot;</span> ,A.<span class="string">&quot;위변조 통장&quot;</span> ,A.<span class="string">&quot;위변조 기타&quot;</span> )</span><br><span class="line"><span class="keyword">VALUES</span> (B.SEQ,B.DOMAIN_SEQ ,B.DOMAIN_LINK ,B.DOMAIN_NAME ,B.CODE_NM2 ,B.CODE_NM3,B.DETECTED_CRAWL_COUNT,B.DETECTED_LINK,B.DETECTED_COUNT ,B.TRUE_DETECTION_COUNT ,B.FALSE_DETECTION_COUNT ,B.DETECTED_NEW_EXPOSURE_COUNT ,B.DETECTED_RE_EXPOSURE,B.DETECTED_RE_EXPOSURE_COUNT ,B.DOMAIN_CATEGORY01,B.DOMAIN_CATEGORY02,B.SEARCH_COL_TYPE,B.DOMAIN_GROUP,B.DETECTED_DEPTH,B.DOMAIN_COUNTRY_CODE,B.DETECTED_STATUS,B.DETECTED_GROUP,B.DETECTED_TYPE,B.DETECTED_CHECK_TYPE,B.KEYWORD_SEQ,B.KEYWORD_VALUE,B.INDI_DOMAIN_SEQ,B.WEB_CACHE_URL,B.DETECTED_TIME ,B.TFD_DATE ,B.DEL_DONE_DATE ,B.UPDATOR,B.REQUEST_ORDINAL,B.DETECTED_POST_TYPE,B.GATHERING_TYPE,B.IMG_DETECTION_TYPE,B.POST_DATE ,B.REFERENCE,B.TRADER_TYPE,B.<span class="string">&quot;판매자ID&quot;</span>,B.<span class="string">&quot;이메일&quot;</span>,B.<span class="string">&quot;카카오톡&quot;</span>,B.<span class="string">&quot;네이트온&quot;</span>,B.<span class="string">&quot;MSN메신저&quot;</span>,B.<span class="string">&quot;스카이프&quot;</span>,B.<span class="string">&quot;위챗&quot;</span>,B.QQ,B.<span class="string">&quot;텔레그램&quot;</span>,B.<span class="string">&quot;기타1&quot;</span>,B.<span class="string">&quot;기타2&quot;</span>,B.<span class="string">&quot;핸드폰번호&quot;</span>,B.<span class="string">&quot;일반번호&quot;</span>,B.<span class="string">&quot;거래 개인정보DB&quot;</span> ,B.<span class="string">&quot;거래 통장&quot;</span> ,B.<span class="string">&quot;거래 ID판매&quot;</span> ,B.<span class="string">&quot;거래 아이핀&quot;</span> ,B.<span class="string">&quot;거래 대포폰&quot;</span> ,B.<span class="string">&quot;거래 해킹&quot;</span> ,B.<span class="string">&quot;거래 기타&quot;</span> ,B.<span class="string">&quot;위변조 증명서&quot;</span> ,B.<span class="string">&quot;위변조 성적표&quot;</span> ,B.<span class="string">&quot;위변조 신분증&quot;</span> ,B.<span class="string">&quot;위변조 자격증&quot;</span> ,B.<span class="string">&quot;위변조 여권&quot;</span> ,B.<span class="string">&quot;위변조 기록부&quot;</span> ,B.<span class="string">&quot;위변조 내역서&quot;</span> ,B.<span class="string">&quot;위변조 통장&quot;</span> ,B.<span class="string">&quot;위변조 기타&quot;</span> )</span><br></pre></td></tr></table></figure><ul><li>SEQ 기준으로 데이터가 존재하면 DEL_FLAG를 N 으로 수정한다.</li><li>만약 기존 적재된 SEQ가 없는 데이터가 있다면, 새롭게 INSERT (삭제될 경우도 있지만 추가될 경우도 존재) 한다.</li></ul><p>각 테이블 조회해보면 FLAG가 변경된 값을 확인할 수 있다.</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--20.09.03~20.09.09 새로 수집한 데이터 조회</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="keyword">COUNT</span>(*)</span><br><span class="line"><span class="keyword">FROM</span> MART_20.HABITUAL_TEMP</span><br><span class="line">;</span><br><span class="line"><span class="comment">--3993</span></span><br><span class="line"></span><br><span class="line"><span class="comment">--BUYER 테이블에서 일주일 전 데이터 FLAG Y로 수정 후 COUNT</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="keyword">COUNT</span>(*)</span><br><span class="line"><span class="keyword">FROM</span> MART_20.HABITUAL_BUYER</span><br><span class="line"><span class="keyword">WHERE</span> DEL_FLAG=<span class="string">&#x27;Y&#x27;</span></span><br><span class="line">;</span><br><span class="line"><span class="comment">--3993</span></span><br><span class="line"></span><br><span class="line"><span class="comment">--MERGE 이후 COUNT</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="keyword">COUNT</span>(*)</span><br><span class="line"><span class="keyword">FROM</span> MART_20.HABITUAL_BUYER</span><br><span class="line"><span class="keyword">WHERE</span> DEL_FLAG=<span class="string">&#x27;Y&#x27;</span></span><br><span class="line">;</span><br><span class="line"><span class="comment">--0, 왜냐면 수정된 데이터가 없으므로.. </span></span><br></pre></td></tr></table></figure><p>데이터 주 단위 마감 순서를 한 줄로 요약하면, 마감 기준 치 데이터를 조회 후 TEMP 테이블에 저장하고 TARGET 테이블(TEMP 테이블에 저장한 데이터의 같은 날짜)의 DEL_FLAG 를 Y로 변경하여 TEMP 테이블과 Merge 한다.  </p><p>직접 수행하면서 느낀점은 Merge 로 인한 성능 이슈가 발생할 수 있는데, 이를 어떻게 처리할 수 있는지는 더 연구해봐야겠다는 생각이 들었다. 배치 작업을 수행하면서 울팀 선배님이 많은 도움을 주셨다. 앞으로 나도 내 후배에게 많은 도움을 줄 수 있는 선배로 거듭나길.. </p><h1 id="Reference"><a href="#Reference" class="headerlink" title=" Reference"></a><a name="reference"></a> Reference</h1><ol><li><a href="https://kookyungmin.github.io/db/2018/07/30/oracle_32/">https://kookyungmin.github.io/db/2018/07/30/oracle_32/</a></li><li><a href="https://jennylee4517.github.io/sql/oracle-1%EC%9D%BC%EC%B0%A8/">https://jennylee4517.github.io/sql/oracle-1일차/</a></li><li><a href="https://m.blog.naver.com/PostView.nhn?blogId=ojini21c&logNo=221193420479&proxyReferer=https:%2F%2Fwww.google.com%2F">https://m.blog.naver.com/PostView.nhn?blogId=ojini21c&amp;logNo=221193420479&amp;proxyReferer=https:%2F%2Fwww.google.com%2F</a></li></ol><hr><p>made by <em>jaejun.lee</em></p>]]></content:encoded>
      
      <comments>https://jx2lee.github.io/database-daily_batch_process/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[Rook Ceph] Rook ceph dashboard 사용하기</title>
      <link>https://jx2lee.github.io/cloud-export_rook_ceph_dashboard/</link>
      <guid>https://jx2lee.github.io/cloud-export_rook_ceph_dashboard/</guid>
      <pubDate>Wed, 02 Sep 2020 15:00:00 GMT</pubDate>
      <description>
      
        &lt;p&gt;Toolbox POD 로 접근하여 ceph cluster 를 확인하곤 한다. alias 로 바로 접근할 수 있게 설정하였지만, 좀 더 직관적으로 보기 위해 dashboard 를 배포해 보자! &lt;/p&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p>Toolbox POD 로 접근하여 ceph cluster 를 확인하곤 한다. alias 로 바로 접근할 수 있게 설정하였지만, 좀 더 직관적으로 보기 위해 dashboard 를 배포해 보자! </p><a id="more"></a><p><strong><span class="github-emoji" alias="sparkles" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png?v8">&#x2728;</span> Contents:</strong>  </p><ol><li><a href="#checksvc">rook-ceph service 확인</a>  </li><li><a href="#makesvc">rook-ceph-mgr-dashboard-external-http 서비스 생성</a>  </li><li><a href="#connect">dashboard 접속</a>  </li><li><a href="#ref">Reference</a></li></ol><h1 id="rook-ceph-Service-확인"><a href="#rook-ceph-Service-확인" class="headerlink" title=" rook-ceph Service 확인"></a><a name="checksvc"></a> rook-ceph Service 확인</h1><p><code>kubectl get service -n rook-ceph</code> 명령어로 기동중인 서비스를 확인한다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">root@k8s-master:~<span class="comment"># kubectl get service -n rook-ceph</span></span><br><span class="line">NAME                                    TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)             AGE</span><br><span class="line">csi-cephfsplugin-metrics                ClusterIP   10.96.205.129   &lt;none&gt;        8080/TCP,8081/TCP   2d17h</span><br><span class="line">csi-rbdplugin-metrics                   ClusterIP   10.96.158.126   &lt;none&gt;        8080/TCP,8081/TCP   2d17h</span><br><span class="line">rook-ceph-mgr                           ClusterIP   10.96.133.224   &lt;none&gt;        9283/TCP            2d17h</span><br><span class="line">rook-ceph-mgr-dashboard                 ClusterIP   10.96.37.245    &lt;none&gt;        8443/TCP            2d17h</span><br><span class="line">rook-ceph-mon-a                         ClusterIP   10.96.59.113    &lt;none&gt;        6789/TCP,3300/TCP   2d17h</span><br><span class="line">rook-ceph-mon-b                         ClusterIP   10.96.26.250    &lt;none&gt;        6789/TCP,3300/TCP   2d17h</span><br><span class="line">rook-ceph-mon-c                         ClusterIP   10.96.235.59    &lt;none&gt;        6789/TCP,3300/TCP   2d17h</span><br></pre></td></tr></table></figure><p><code>rook-ceph-mgr-dashboard</code> 서비스가 8443 포트로 통신 가능하게 설정되어 있다. 외부로 dashboard 를 노출하는 방법은 <em>1) rook-ceph-mgr 서비스 type 바꾸기</em> 와 <em>2) external service</em> 가 있다. 이번 글에서는 2번 방법을 이용해 dashboard 를 사용해보겠다.</p><h1 id="rook-ceph-mgr-dashboard-external-http-서비스-생성하기"><a href="#rook-ceph-mgr-dashboard-external-http-서비스-생성하기" class="headerlink" title=" rook-ceph-mgr-dashboard-external-http 서비스 생성하기"></a><a name="makesvc"></a> rook-ceph-mgr-dashboard-external-http 서비스 생성하기</h1><p>다음과 같은 yaml 를 작성한다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">root@k8s-master:~/hypercloud-rook-ceph-master/deploy<span class="comment"># cat dashboard-external-http.yaml</span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: rook-ceph-mgr-dashboard-external-http</span><br><span class="line">  namespace: rook-ceph</span><br><span class="line">  labels:</span><br><span class="line">    app: rook-ceph-mgr</span><br><span class="line">    rook_cluster: rook-ceph</span><br><span class="line">spec:</span><br><span class="line">  ports:</span><br><span class="line">  - name: dashboard</span><br><span class="line">    port: 8443</span><br><span class="line">    protocol: TCP</span><br><span class="line">    targetPort: 8443</span><br><span class="line">  selector:</span><br><span class="line">    app: rook-ceph-mgr</span><br><span class="line">    rook_cluster: rook-ceph</span><br><span class="line">  sessionAffinity: None</span><br><span class="line">  <span class="built_in">type</span>: NodePort</span><br></pre></td></tr></table></figure><ul><li>Service 타입은 <code>NodePort</code> 로 설정한다. 이외 외부로 노출하고 싶다면 <code>LoadBalancer</code> 로 변경한다. <em>(로드밸런서 경우 퍼블릭 IP가 존재해야한다.)</em></li><li>rook-ceph-mgr Port 가 만약 나와 다른 번호(예. 7775) 로 설정되어 있다면, 위 yaml 에서도 같은 포트로 설정해야한다.</li></ul><p>생성한 yaml 을 이용해 <code>kubectl apply -f dashboard-external-http.yaml</code> 커맨드로 service 를 생성한다. 이후 접속하기 위한 초기 admin 비밀번호 확인을 위해 다음과 같은 커맨드를 수행한다.</p><p><code>kubectl -n rook-ceph get secret rook-ceph-dashboard-password -o jsonpath=&quot;&#123;[&#39;data&#39;][&#39;password&#39;]&#125;&quot; | base64 --decode &amp;&amp; echo</code></p><h1 id="dashboard-접속"><a href="#dashboard-접속" class="headerlink" title=" dashboard 접속"></a><a name="connect"></a> dashboard 접속</h1><p>클러스터 노드 중 하나를 골라 <code>https://[노드ip]:[port]</code> 로 접속한다.</p><ul><li>ID/PW:  <code>admin/[바로 위 커맨드 수행 결과]</code></li><li>최초 접속 후 설정에 들어가 PW를 바꾸도록 하자.</li></ul><p>dashboard UI 를 통해 ceph cluster 상태를 직관적으로 볼 수 있어 편리하긴 하다.  그래도 조금 더 쿠버네티스와 친해지기 위해 커맨드로 ceph cluster 상태를 확인하는 것이 더 나을듯 하다!</p><h1 id="Reference"><a href="#Reference" class="headerlink" title=" Reference"></a><a name="ref"></a> Reference</h1><ul><li><a href="https://github.com/rook/rook/blob/master/Documentation/ceph-dashboard.md">https://github.com/rook/rook/blob/master/Documentation/ceph-dashboard.md</a></li></ul><hr><p>made by <em>jaejun.lee</em></p>]]></content:encoded>
      
      <comments>https://jx2lee.github.io/cloud-export_rook_ceph_dashboard/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[Python] 단순 키워드 매칭 모듈</title>
      <link>https://jx2lee.github.io/python-keyword_match/</link>
      <guid>https://jx2lee.github.io/python-keyword_match/</guid>
      <pubDate>Wed, 19 Aug 2020 15:00:00 GMT</pubDate>
      <description>
      
        &lt;p&gt;프로젝트 시범 과제 중 하나인 “게시판 및 회원가입 여부 판단”을 위해 파이썬으로 모듈을 하나 작성했다. 알고리즘 자체는 정말 단순한데, 해당 게시판과 회원가입 여부에 대한 키워드를 통해 있는지 없는지만 확인하여 판단한다. 모듈 작성 배경과 간단한 테스트를 수행한다.&lt;/p&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p>프로젝트 시범 과제 중 하나인 “게시판 및 회원가입 여부 판단”을 위해 파이썬으로 모듈을 하나 작성했다. 알고리즘 자체는 정말 단순한데, 해당 게시판과 회원가입 여부에 대한 키워드를 통해 있는지 없는지만 확인하여 판단한다. 모듈 작성 배경과 간단한 테스트를 수행한다.</p><a id="more"></a><p><strong><span class="github-emoji" alias="sparkles" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png?v8">&#x2728;</span> Contents:</strong>  </p><ol><li><a href="#background">모듈 작생 배경</a></li><li><a href="#howto">어떻게 작성할까?</a></li><li><a href="#test">테스트 결과는요?</a></li><li><a href="#review">어땠나요?</a></li></ol><h1 id="모듈-작성-배경"><a href="#모듈-작성-배경" class="headerlink" title=" 모듈 작성 배경"></a><a name="background"></a> 모듈 작성 배경</h1><p>원래 키워드 매칭 관련 시범과제는 주 사업자 영역인 크롤러에서 수행하기로 하였지만, 빅데이터 플랫폼을 구축하면서 빅데이터 플랫폼 안에서 수행해야 그림이 이쁘지 않을까? 라는 의견이 나와 우리쪽에서 수행하기로 결정하였다. 관련하여 연구소에 문의했지만, 키워드 포함 여부에 대한 연구는 이루어지지 않아 우리 팀에서 맡아 진행하기로 협의하였다.  </p><p>구글링을 통해서 많은 키워드 매칭 자료를 찾았지만, 시범과제 성격이 크기 때문에 거창한 알고리즘을 쓰지 않기로 하였다. 단순 해당 여부에 대한 키워드를 받아 이 키워드가 있는지? 없는지? 만 판단하는 쪽으로 시범과제를 수행하기로 했다.  </p><p>우리는 총 2개 DB table 에서 데이터를 받기로 했다. 첫 번째 는 <em>URL / URL 내 텍스트 / timestamp</em> 칼럼으로 이루어진 테이블이다. 두 번째 는 <em>keyword type / keyword value (type: 해당 프로젝트에서는 회원가입 과 게시판)</em> 으로 이루어진 테이블이다. 위 테이블 데이터는 우리 제품 python API 를 이용해 dataframe 으로 가져올 수 있는 상황이다.  </p><blockquote><p><em>모듈 작성 배경: 프로젝트 시범과제 중 키워드 매칭을 Pyhthon 으로 수행하기 위해 작성!</em></p></blockquote><h1 id="어떻게-작성할까"><a href="#어떻게-작성할까" class="headerlink" title=" 어떻게 작성할까?"></a><a name="howto"></a> 어떻게 작성할까?</h1><p>사실 막막했다. 아니, 대충대충 짜면 금방 짤 수 있는 부분이지만 Python 역량을 키워볼 겸 모듈화로 진행하기로 결정했다. 그러던 중, <a href="https://github.com/vi3k6i5/flashtext">FlashText</a> 알고리즘을 찾았고 이 패키지를 참고하고자 했다. 본 패키지는 <strong>string/list/dictionary/file</strong> 로 데이터를 읽어와 해당 키워드 사전을 만들고, 변환할 수 있는 기능을 제공한다. 하지만 내가 수행해야 하는 것은 <strong>Pandas Dataframe</strong> 형태의 데이터를 다뤄야 하기 때문에, FlashText <strong>KeywordProcessor</strong> 클래스를 차용하여 작성하였다.  </p><p>관련 모듈 사용법은 <a href="https://github.com/jx2lee/keywordmatch">Github</a>에 올려두었다. 참고하길 바라며, 프로젝트 테스트를 위해 <strong>금융인지, 주택인지?</strong> 에 대한 키워드 매칭 결과를 확인해본다.  </p><blockquote><p><em>어떻게? FlashText 모듈 기반으로 작성!</em></p></blockquote><h1 id="테스트-결과는요"><a href="#테스트-결과는요" class="headerlink" title=" 테스트 결과는요?"></a><a name="test"></a> 테스트 결과는요?</h1><p><a href="https://github.com/jx2lee/keywordmatch">Github</a>에 <strong>example.py</strong> 로 작성하였다. 규칙으로는 test 할 수 있는 document 등을 작성해야지만 그건 추후에 시간이 남는다면 진행하도록 한다. <strong>exmaple.py</strong> 는 다음과 같고 각 줄 마다 짧은 설명을 달도록 하겠다.  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">from</span> keywordmatch <span class="keyword">import</span> MatchingProcessor</span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;example.pickle&quot;</span>, <span class="string">&quot;rb&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        test_df = pickle.load(f)</span><br><span class="line">    test_df_keyword = pd.DataFrame(&#123;<span class="string">&#x27;타입&#x27;</span>:[<span class="string">&#x27;금융&#x27;</span>, <span class="string">&#x27;주택&#x27;</span>, <span class="string">&#x27;금융&#x27;</span>, <span class="string">&#x27;주택&#x27;</span>, <span class="string">&#x27;금융&#x27;</span>, <span class="string">&#x27;주택&#x27;</span>],</span><br><span class="line">                                    <span class="string">&#x27;키워드&#x27;</span>:[<span class="string">&#x27;은행&#x27;</span>, <span class="string">&#x27;중랑구&#x27;</span>, <span class="string">&#x27;송금&#x27;</span>, <span class="string">&#x27;부산&#x27;</span>, <span class="string">&#x27;출금&#x27;</span>, <span class="string">&#x27;경남&#x27;</span>]&#125;)</span><br><span class="line"></span><br><span class="line">    instance = MatchingProcessor(test_df, <span class="string">&#x27;기사내용&#x27;</span>, [<span class="string">&#x27;주택&#x27;</span>, <span class="string">&#x27;금융&#x27;</span>])</span><br><span class="line">    instance.set_logger(<span class="string">&#x27;t1&#x27;</span>, is_file=<span class="literal">False</span>)</span><br><span class="line">    instance.add_column()</span><br><span class="line">    instance.get_keyword_processor(test_df_keyword, <span class="string">&#x27;타입&#x27;</span>, <span class="string">&#x27;키워드&#x27;</span>)</span><br><span class="line">    result = instance.is_keyword()</span><br><span class="line"></span><br><span class="line">    instance._data[<span class="string">&#x27;수집시간&#x27;</span>] = datetime(<span class="number">2020</span>, <span class="number">8</span>, <span class="number">19</span>).strftime(<span class="string">&#x27;%Y-%m-%d&#x27;</span>)</span><br><span class="line">    tibero = &#123;<span class="string">&#x27;ip&#x27;</span>: <span class="string">&#x27;192.168.179.166&#x27;</span>,</span><br><span class="line">              <span class="string">&#x27;port&#x27;</span>: <span class="string">&#x27;8629&#x27;</span>,</span><br><span class="line">              <span class="string">&#x27;sid&#x27;</span>: <span class="string">&#x27;tibero&#x27;</span>,</span><br><span class="line">              <span class="string">&#x27;id_pw&#x27;</span>: [<span class="string">&#x27;tibero&#x27;</span>, <span class="string">&#x27;tmax&#x27;</span>],</span><br><span class="line">              <span class="string">&#x27;output_columns&#x27;</span>: [<span class="string">&#x27;기사제목&#x27;</span>, <span class="string">&#x27;기사내용&#x27;</span>, <span class="string">&#x27;수집시간&#x27;</span>],</span><br><span class="line">              <span class="string">&#x27;table&#x27;</span>: <span class="string">&#x27;CRAWLER_DATA&#x27;</span>,</span><br><span class="line">              <span class="string">&#x27;table_columns&#x27;</span>: [<span class="string">&#x27;DETECTED_LINK&#x27;</span>, <span class="string">&#x27;DETECTED_CONTENTS&#x27;</span>, <span class="string">&#x27;DETECTED_TIME&#x27;</span>]&#125;</span><br><span class="line">    instance.save_output_database(jar_file=<span class="string">&#x27;/Users/jj/python/coding-test/tibero6-jdbc.jar&#x27;</span>, db_info=tibero)</span><br></pre></td></tr></table></figure><ul><li><strong>Line 1-4</strong>: 패키지 import 부분, 설치한 패키지는 <em>Jaydebeapi, pandas</em></li><li><strong>Line 7-10</strong>: 첫 번째 테이블과 두 번째 테이블 데이터를 DataFrame 으로 로드</li><li><strong>Line 12-16</strong>: <strong>MatchingProcessor</strong> 클래스로 생성한 객체로 테스트</li><li><strong>Line 18</strong>: 테스트를 위한 TimeStamp 열 추가</li><li><strong>Line 19-25</strong>: DB info 를 dictionary 형태로 표현</li><li><strong>Line 26</strong>: 해당 DB 로 Insert 함수 실행</li></ul><p><strong>결과는 다음과 같다:</strong>  </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">~/python/keywordmatch master</span><br><span class="line">keyword-matching ❯ python example.py</span><br><span class="line">[2020-08-20 16:29:16,632][INFO] Finished Adding Columns: [<span class="string">&#x27;주택&#x27;</span>, <span class="string">&#x27;금융&#x27;</span>]</span><br><span class="line">[2020-08-20 16:29:16,636][INFO] Finished Setting Keyword_processor: &#123;<span class="string">&#x27;중랑구&#x27;</span>: <span class="string">&#x27;주택&#x27;</span>, <span class="string">&#x27;부산&#x27;</span>: <span class="string">&#x27;주택&#x27;</span>, <span class="string">&#x27;경남&#x27;</span>: <span class="string">&#x27;주택&#x27;</span>, <span class="string">&#x27;은행&#x27;</span>: <span class="string">&#x27;금융&#x27;</span>, <span class="string">&#x27;송금&#x27;</span>: <span class="string">&#x27;금융&#x27;</span>, <span class="string">&#x27;출금&#x27;</span>: <span class="string">&#x27;금융&#x27;</span>&#125;</span><br><span class="line">[2020-08-20 16:29:16,636][INFO] Start Keyword Match.</span><br><span class="line">100%|███████████████████████████████████████████████████████████████████████████████████████| 8468/8468 [00:05&lt;00:00, 1530.62it/s]</span><br><span class="line">[2020-08-20 16:29:22,200][INFO] Finished Keyword Match.</span><br><span class="line">[2020-08-20 16:29:23,781][INFO] Connected Tibero: 192.168.179.166:8629:tibero</span><br><span class="line">[2020-08-20 16:29:23,781][INFO] Started Creating SQL dump.</span><br><span class="line">100%|██████████████████████████████████████████████████████████████████████████████████████| 8468/8468 [00:00&lt;00:00, 50532.56it/s]</span><br><span class="line">[2020-08-20 16:29:23,964][INFO] Finished Creating SQL dump. dump size: 8468</span><br><span class="line">[2020-08-20 16:29:23,964][INFO] Started pushing data. SQL Query: INSERT INTO CRAWLER_DATA VALUES (?,?,?)</span><br><span class="line">[2020-08-20 16:29:25,305][INFO] Finished pushing data.</span><br><span class="line">[2020-08-20 16:29:25,305][INFO] Disconnected Tibero.</span><br></pre></td></tr></table></figure><h1 id="어땠나요"><a href="#어땠나요" class="headerlink" title=" 어땠나요?"></a><a name="review"></a> 어땠나요?</h1><p>키워드 매칭이 제대로 수행되었는지는 아무도 모른다. 나는 이번 프로젝트를 통해 <strong>Python</strong> 을 좀 더 살펴볼 수 있는 기회였고, 좀 더 다양한 모듈을 작성할 수 있는 역량을 확보한 것으로 만족한다. 물론 프로젝트에는 이 모듈을 들고 들어갈 생각이다. 왜? 결과가 제대로 나왔는지는 고객도 모르기 때문이다. 물론 좀 더 결과를 향상시킬 수 있는 방안을 생각하고 추가해보도록 노력하겠지만.. 10월 말 목표인 이 프로젝트에서<em>(아 물론 한 달 정도 딜레이 될 예정)</em> 튜닝은 필요할 지 모르겠다. 아, 추가적으로 필요한 부분은 아마 이러한 키워드 매칭을 배치성으로 수행할지는 직접 들어가봐야 알 것 같다.  </p><p>하여튼, Python을 더 알아보고 싶은 마음이 생겼다.</p><blockquote><p><em>결론: Python 을 알게되면서 점점 빠져들었다.</em> </p></blockquote><hr><p>made by <em>jaejun.lee</em></p>]]></content:encoded>
      
      <comments>https://jx2lee.github.io/python-keyword_match/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[SQL] SQL 함수 정리</title>
      <link>https://jx2lee.github.io/sql-function/</link>
      <guid>https://jx2lee.github.io/sql-function/</guid>
      <pubDate>Tue, 21 Jul 2020 15:00:00 GMT</pubDate>
      <description>
      
        &lt;p&gt;프로젝트 참여 중 데이터 마트 구축을 위한 SQL 쿼리를 분석하면서, 관련된 지식을 내 머릿속에 쌓기(?)위해 정리한다. 필요 시 업데이트 할 예정&lt;/p&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p>프로젝트 참여 중 데이터 마트 구축을 위한 SQL 쿼리를 분석하면서, 관련된 지식을 내 머릿속에 쌓기(?)위해 정리한다. 필요 시 업데이트 할 예정</p><a id="more"></a><h1 id="DECODE"><a href="#DECODE" class="headerlink" title="DECODE"></a>DECODE</h1><p>표준 SQL 함수는 아니지만 잘 사용하면 편한 오라클 함수, <code>CASE WHEN</code> 구문을 권장하기도 하지만 이왕 나왔으니 공부해보자.<br><code>DECODE</code> 는 Programming 에서의 <code>if else</code> 와 같은 역할을 한다. 사용방법은 다음과 같다.  </p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">DECODE(&#123;column&#125;, &#123;condition_01&#125;, &#123;return_01&#125;, &#123;condition_02&#125;, &#123;return_02&#125;, &#123;condition_03&#125;, &#123;return_03&#125;, ...)</span><br><span class="line"></span><br><span class="line"><span class="comment">--example</span></span><br><span class="line"></span><br><span class="line">DECODE(WEB_CACHE_URL,NULL,A.DOMAIN_SEQ,100000001)</span><br></pre></td></tr></table></figure><ul><li><code>WEB_CACHE_URL</code> 칼럼이 만약 NULL 이라면 A 테이블 <code>DOMAIN_SEQ</code> 대입</li><li>만약 NULL이 아니면 100000001 대입</li><li>결론: <code>WEB_CACHE_URL</code> 칼럼이 만약 NULL 이라면 A 테이블 <code>DOMAIN_SEQ</code> 값을 대입하고 아니면 100000001 대입</li></ul><h1 id="CASE-WHEN"><a href="#CASE-WHEN" class="headerlink" title="CASE WHEN"></a>CASE WHEN</h1><p><code>DECODE</code> 대신 <code>CASE WHEN</code> 을 권장한다니 알아보도록 한다. <code>DECODE</code> 랑 같이 <code>if else</code> 와 같은 역할을 하며 <code>CASE WHEN</code> 이 가독성이 더 좋다고들 한다. 사용방법은 다음과 같다.  </p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">CASE WHEN &#123;condition_01&#125; THEN &#123;return_01&#125;</span><br><span class="line"> WHEN &#123;condition_02&#125; THEN &#123;return_02&#125;</span><br><span class="line"> ...</span><br><span class="line"> ELSE &#123;return_other&#125;</span><br><span class="line"><span class="keyword">END</span></span><br><span class="line"></span><br><span class="line"><span class="comment">--example</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">CASE</span> <span class="keyword">WHEN</span> ( <span class="keyword">SELECT</span> CODE_NM</span><br><span class="line">              <span class="keyword">FROM</span> COMTCCMMNDETAILCODE</span><br><span class="line">             <span class="keyword">WHERE</span> CODE = D.DOMAIN_CODE2) <span class="keyword">IS</span> <span class="keyword">NOT</span> <span class="literal">NULL</span> <span class="keyword">OR</span> ( <span class="keyword">SELECT</span> CODE_NM</span><br><span class="line">                                                             <span class="keyword">FROM</span> COMTCCMMNDETAILCODE</span><br><span class="line">                                                            <span class="keyword">WHERE</span> CODE = D.DOMAIN_CODE2) != <span class="string">&#x27;&#x27;</span> <span class="keyword">THEN</span> ( <span class="keyword">SELECT</span> CODE_NM</span><br><span class="line">                                                                                                        <span class="keyword">FROM</span> COMTCCMMNDETAILCODE</span><br><span class="line">                                                                                                       <span class="keyword">WHERE</span> CODE = D.DOMAIN_CODE2) <span class="keyword">ELSE</span> <span class="literal">NULL</span> <span class="keyword">END</span> <span class="keyword">AS</span> CODE_NM2</span><br></pre></td></tr></table></figure><ul><li>condition<ul><li>COMTCCMMNDETAILCODE 테이블에서 <code>CODE</code>가 D 테이블 <code>DOMATIN_CODE2</code> 와 같을때, <code>CODE_NM</code> 이 NULL 이 아니거나 <em>(or)</em></li><li>COMTCCMMNDETAILCODE 테이블에서 <code>CODE</code>가 D 테이블 <code>DOMATIN_CODE2</code> 와 같을때, <code>CODE_NM</code> 이 ‘’ 이 아니라면</li></ul></li><li>return: COMTCCMMNDETAILCODE 테이블에서 <code>CODE</code>가 D 테이블 <code>DOMATIN_CODE2</code> 와 같을때 <code>CODE_NM</code> 대입</li><li>ELSE: NULL 대입</li><li>결론: <code>CODE_NM</code> 값이 있으면 <code>CODE_NM</code>, 없으면 NULL</li></ul><h1 id="CAST"><a href="#CAST" class="headerlink" title="CAST"></a>CAST</h1><p>칼럼 형변환에 사용하는 함수로 UNION(UNION ALL) 사용 시 칼럼 형태를 일치시킬 때 사용할 수 있다. 사용방법은 다음과 같다.  </p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">CAST(&#123;column&#125; as &#123;data_type&#123;length&#125;&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment">--exmaple</span></span><br><span class="line"></span><br><span class="line">CAST(A.DETECTED_LINK AS NVARCHAR(4000)) AS DETECTED_LINK</span><br></pre></td></tr></table></figure><ul><li>A 테이블 <code>DETECTED_LINK</code> 칼럼을 <em>NVARCHAR</em> length 4000 형태로 변환</li></ul><h1 id="INSTR"><a href="#INSTR" class="headerlink" title="INSTR"></a>INSTR</h1><p>문자열에 특정 문자열(substring)을 검색해서 위치를 return(만약 없다면 0) 하는 함수로 사용방법은 다음과 같다.  </p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">INSTR(&#123;string or column&#125; &#123;substring&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment">--example</span></span><br><span class="line"></span><br><span class="line">INSTR(D.exposure_type, &#x27;S01&#x27;)</span><br></pre></td></tr></table></figure><ul><li>D 테이블 <code>EXPOSURE_TYPE</code> 칼럼 내 S01 문자열 위치를 반환</li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li><a href="https://gent.tistory.com/227">https://gent.tistory.com/227</a></li><li><a href="https://gent.tistory.com/311">https://gent.tistory.com/311</a></li><li><a href="http://www.incodom.kr/SQL/INSTR%2CINSTRB">http://www.incodom.kr/SQL/INSTR%2CINSTRB</a></li></ul><hr><p>made by <em>jaejun.lee</em></p>]]></content:encoded>
      
      <comments>https://jx2lee.github.io/sql-function/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[Cloud] Etcd 백업을 위한 CronJob 생성</title>
      <link>https://jx2lee.github.io/cloud-etcd_cronjob/</link>
      <guid>https://jx2lee.github.io/cloud-etcd_cronjob/</guid>
      <pubDate>Tue, 16 Jun 2020 15:00:00 GMT</pubDate>
      <description>
      
        &lt;p&gt;최근 Control Plane 노드 복구 때문에 Etcd 백업에 대한 중요성을 깨달았다. 매 특정 시간 스냅샷을 찍어내는 CronJob 을 배포하는 과정을 다룬다.&lt;/p&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p>최근 Control Plane 노드 복구 때문에 Etcd 백업에 대한 중요성을 깨달았다. 매 특정 시간 스냅샷을 찍어내는 CronJob 을 배포하는 과정을 다룬다.</p><a id="more"></a><h1 id="준비사항"><a href="#준비사항" class="headerlink" title="준비사항"></a>준비사항</h1><h2 id="control-plane-노드의-인증서-파일"><a href="#control-plane-노드의-인증서-파일" class="headerlink" title="control plane 노드의 인증서 파일"></a>control plane 노드의 인증서 파일</h2><ul><li><code>/etc/kubernetes/pki/ca.crt</code></li><li><code>/etc/kubernetes/pki/ca.key</code></li></ul><h2 id="etcd-버전-확인"><a href="#etcd-버전-확인" class="headerlink" title="etcd 버전 확인"></a>etcd 버전 확인</h2><ul><li>cluster 내 etcd 파드를 조회하여 버젼 체크<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl describe pod/etcd-k8s-master -n kube-system | grep Image:</span><br><span class="line">    Image:         k8s.gcr.io/etcd:3.3.10</span><br></pre></td></tr></table></figure></li></ul><h2 id="setup-sh"><a href="#setup-sh" class="headerlink" title="setup.sh"></a><code>setup.sh</code></h2><ul><li>apply 할 yaml 파일 내 docker registry 와 node name 을 변경하는 스크립트</li><li><code>Usage</code><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ ./setup.sh</span><br><span class="line">Usage   : ./script.sh &#123;registry_endpoint&#125; &#123;master_node&#125;</span><br><span class="line">Example : ./script.sh 192.168.179.185:5000 k8s-master</span><br></pre></td></tr></table></figure></li></ul><h1 id="etcd-snapshot-yaml"><a href="#etcd-snapshot-yaml" class="headerlink" title="etcd_snapshot.yaml"></a><code>etcd_snapshot.yaml</code></h1><h2 id="control-plane-인증서-파일을-volume-부분-etc-kubernetes-pki-etcd-폴더에-복사"><a href="#control-plane-인증서-파일을-volume-부분-etc-kubernetes-pki-etcd-폴더에-복사" class="headerlink" title="control plane 인증서 파일을 volume 부분 /etc/kubernetes/pki/etcd 폴더에 복사"></a>control plane 인증서 파일을 <code>volume</code> 부분 <code>/etc/kubernetes/pki/etcd</code> 폴더에 복사</h2><h2 id="etcd-백업-스냅샷의-저장-위치를-volume-부분-backup-으로-설정"><a href="#etcd-백업-스냅샷의-저장-위치를-volume-부분-backup-으로-설정" class="headerlink" title="etcd 백업 스냅샷의 저장 위치를 volume 부분 backup 으로 설정"></a>etcd 백업 스냅샷의 저장 위치를 <code>volume</code> 부분 <code>backup</code> 으로 설정</h2><ul><li>스냅샷이 해당 디렉토리에 저장</li><li>원하는 스냅샷 저장 주기를 crontab 으로 표현<ul><li>본인은 매일 오전 6시에 수행하는 <code>0 6 * * *</code> 으로 설정</li></ul></li></ul><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">batch/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">CronJob</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">etcd-backup</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">schedule:</span> <span class="string">&quot;0 6 * * *&quot;</span></span><br><span class="line">  <span class="attr">jobTemplate:</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">template:</span></span><br><span class="line">        <span class="attr">spec:</span></span><br><span class="line">          <span class="attr">containers:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">etcd-backup</span></span><br><span class="line">            <span class="attr">image:</span> &#123;<span class="string">registry_endpoint</span>&#125;<span class="string">/k8s.gcr.io/etcd:3.3.10</span></span><br><span class="line">            <span class="attr">env:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">ETCDCTL_API</span></span><br><span class="line">              <span class="attr">value:</span> <span class="string">&quot;3&quot;</span></span><br><span class="line">            <span class="attr">command:</span> [<span class="string">&quot;/bin/sh&quot;</span>]</span><br><span class="line">            <span class="attr">args:</span> [<span class="string">&quot;-c&quot;</span>, <span class="string">&quot;etcdctl --endpoints=https://127.0.0.1:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/healthcheck-client.crt --key=/etc/kubernetes/pki/etcd/healthcheck-client.key snapshot save /backup/etcd-snapshot-$(date +%Y-%m-%d_%H-%M-%S_%Z).db&quot;</span>]</span><br><span class="line">            <span class="attr">volumeMounts:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">/etc/kubernetes/pki/etcd</span></span><br><span class="line">              <span class="attr">name:</span> <span class="string">etcd-certs</span></span><br><span class="line">              <span class="attr">readOnly:</span> <span class="literal">true</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">/backup</span></span><br><span class="line">              <span class="attr">name:</span> <span class="string">backup</span></span><br><span class="line">          <span class="attr">restartPolicy:</span> <span class="string">OnFailure</span></span><br><span class="line">          <span class="attr">nodeSelector:</span></span><br><span class="line">            <span class="attr">kubernetes.io/hostname:</span> &#123;<span class="string">master_node</span>&#125;</span><br><span class="line">          <span class="attr">tolerations:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="attr">effect:</span> <span class="string">NoSchedule</span></span><br><span class="line">            <span class="attr">operator:</span> <span class="string">Exists</span></span><br><span class="line">          <span class="attr">hostNetwork:</span> <span class="literal">true</span></span><br><span class="line">          <span class="attr">volumes:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">etcd-certs</span></span><br><span class="line">            <span class="attr">hostPath:</span></span><br><span class="line">              <span class="attr">path:</span> <span class="string">/etc/kubernetes/pki/etcd</span></span><br><span class="line">              <span class="attr">type:</span> <span class="string">DirectoryOrCreate</span></span><br><span class="line">          <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">backup</span></span><br><span class="line">            <span class="attr">hostPath:</span></span><br><span class="line">              <span class="attr">path:</span> <span class="string">/etc/kubernetes/pki/etcd/snapshot</span></span><br><span class="line">              <span class="attr">type:</span> <span class="string">DirectoryOrCreate</span></span><br></pre></td></tr></table></figure><h2 id="script-실행으로-etcd-snapshot-yaml-내-node-name-과-private-registry-주소-변경"><a href="#script-실행으로-etcd-snapshot-yaml-내-node-name-과-private-registry-주소-변경" class="headerlink" title="script 실행으로 etcd_snapshot.yaml 내 node name 과 private registry 주소 변경"></a>script 실행으로 <code>etcd_snapshot.yaml</code> 내 node name 과 private registry 주소 변경</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ./setup.sh 192.168.179.185:5000 k8s-master</span><br></pre></td></tr></table></figure><h1 id="배포"><a href="#배포" class="headerlink" title="배포"></a>배포</h1><p><code>$ kubectl apply -f etcd_snapshot.yaml</code></p><h1 id="배포-확인"><a href="#배포-확인" class="headerlink" title="배포 확인"></a>배포 확인</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get cronjob -n kube-system</span><br><span class="line">NAME          SCHEDULE    SUSPEND   ACTIVE   LAST SCHEDULE   AGE</span><br><span class="line">etcd-backup   0 6 * * *   False     0        &lt;none&gt;          14m</span><br><span class="line"></span><br><span class="line">$ ll /etc/kubernetes/pki/etcd/snapshot</span><br><span class="line">drwxr-xr-x 2 root root     4096  6월 18 15:00 ./</span><br><span class="line">drwxr-xr-x 3 root root     4096  6월 17 15:00 ../</span><br><span class="line">-rw-r--r-- 1 root root 41095200  6월 17 15:00 etcd-snapshot-2020-06-17_06-00-04_UTC.db</span><br><span class="line">-rw-r--r-- 1 root root 41095200  6월 18 15:00 etcd-snapshot-2020-06-18_06-00-08_UTC.db</span><br></pre></td></tr></table></figure><ul><li><code>hostPath</code> 로 설정한 <code>/etc/kubernetes/pki/etcd/snapshot</code> 내 snapshot.db 확인</li></ul><blockquote><p><em>소스코드는 <a href="https://github.com/jx2lee/etcd-cronjob">https://github.com/jx2lee/etcd-cronjob</a> 에서 확인할 수 있다.</em></p></blockquote><h1 id="Refernce"><a href="#Refernce" class="headerlink" title="Refernce"></a>Refernce</h1><ul><li><a href="https://labs.consol.de/kubernetes/2018/05/25/kubeadm-backup.html">https://labs.consol.de/kubernetes/2018/05/25/kubeadm-backup.html</a></li></ul><hr><p>made by <em>jaejun.lee</em></p>]]></content:encoded>
      
      <comments>https://jx2lee.github.io/cloud-etcd_cronjob/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[TroubleShoot] Control Plane Node 추가</title>
      <link>https://jx2lee.github.io/troubleshoot-add_controlplane_node/</link>
      <guid>https://jx2lee.github.io/troubleshoot-add_controlplane_node/</guid>
      <pubDate>Mon, 15 Jun 2020 15:00:00 GMT</pubDate>
      <description>
      
        &lt;p&gt;클러스터 운영 시 컨트롤 플레인 노드가 삭제되었을 때 클러스터에 재 추가하는 과정을 살펴본다.&lt;/p&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p>클러스터 운영 시 컨트롤 플레인 노드가 삭제되었을 때 클러스터에 재 추가하는 과정을 살펴본다.</p><a id="more"></a><h1 id="Kubernetes-컨트롤-플레인-노드-추가-방법"><a href="#Kubernetes-컨트롤-플레인-노드-추가-방법" class="headerlink" title="Kubernetes 컨트롤 플레인 노드 추가 방법"></a>Kubernetes 컨트롤 플레인 노드 추가 방법</h1><h2 id="기존-클러스터-노드-내-etc-kubernetes-pki-폴더를-추가-노드의-etc-kubernetes에-복사"><a href="#기존-클러스터-노드-내-etc-kubernetes-pki-폴더를-추가-노드의-etc-kubernetes에-복사" class="headerlink" title="기존 클러스터 노드 내 /etc/kubernetes/pki 폴더를 추가 노드의 /etc/kubernetes에 복사"></a>기존 클러스터 노드 내 <strong>/etc/kubernetes/pki</strong> 폴더를 추가 노드의 <strong>/etc/kubernetes</strong>에 복사</h2><h2 id="기존-클러스터-노드-내-var-lib-etcd-member-폴더를-추가-노드-내-특정-디렉토리-나의-경우-init-폴더-로-복사"><a href="#기존-클러스터-노드-내-var-lib-etcd-member-폴더를-추가-노드-내-특정-디렉토리-나의-경우-init-폴더-로-복사" class="headerlink" title="기존 클러스터 노드 내 /var/lib/etcd/member 폴더를 추가 노드 내 특정 디렉토리 (나의 경우 init 폴더) 로 복사"></a>기존 클러스터 노드 내 <strong>/var/lib/etcd/member</strong> 폴더를 추가 노드 내 특정 디렉토리 <em>(나의 경우 init 폴더)</em> 로 복사</h2><h3 id="기존-클러스터-노드-내-etcd-파드에-접근하여-다음-명령어로-snap-db-생성"><a href="#기존-클러스터-노드-내-etcd-파드에-접근하여-다음-명령어로-snap-db-생성" class="headerlink" title="기존 클러스터 노드 내 etcd 파드에 접근하여 다음 명령어로 snap db 생성"></a>기존 클러스터 노드 내 etcd 파드에 접근하여 다음 명령어로 snap db 생성</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ETCDCTL_API=3 etcdctl --endpoints 127.0.0.1:2379 \</span><br><span class="line">--cacert /etc/kubernetes/pki/etcd/ca.crt --cert /etc/kubernetes/pki/etcd/server.crt \</span><br><span class="line">--key /etc/kubernetes/pki/etcd/server.key snapshot save snap</span><br></pre></td></tr></table></figure><h3 id="생성한-snap-db-를-추가하고자-하는-노드의-특정-디렉토리-나의-경우-init-폴더-로-이동"><a href="#생성한-snap-db-를-추가하고자-하는-노드의-특정-디렉토리-나의-경우-init-폴더-로-이동" class="headerlink" title="생성한 snap db 를 추가하고자 하는 노드의 특정 디렉토리 (나의 경우 init 폴더) 로 이동"></a>생성한 snap db 를 추가하고자 하는 노드의 특정 디렉토리 <em>(나의 경우 init 폴더)</em> 로 이동</h3><h2 id="초기-클러스터-구축-시-사용한-kubeadm-config-yaml-준비"><a href="#초기-클러스터-구축-시-사용한-kubeadm-config-yaml-준비" class="headerlink" title="초기 클러스터 구축 시 사용한 kubeadm-config.yaml 준비"></a>초기 클러스터 구축 시 사용한 kubeadm-config.yaml 준비</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: kubeadm.k8s.io/v1beta2</span><br><span class="line">kind: ClusterConfiguration</span><br><span class="line">kubernetesVersion: <span class="string">&quot;v1.15.3&quot;</span></span><br><span class="line">controlPlaneEndpoint: <span class="string">&quot;192.168.179.185:6443&quot;</span> <span class="comment"># VIP:6443</span></span><br><span class="line">networking:</span><br><span class="line"> serviceSubnet: <span class="string">&quot;10.96.0.0/16&quot;</span></span><br><span class="line"> podSubnet: <span class="string">&quot;10.244.0.0/16&quot;</span> <span class="comment"># must equal CIDR in calico.yaml</span></span><br><span class="line">apiServer:</span><br><span class="line">  extraArgs:</span><br><span class="line">    advertise-address: <span class="string">&quot;192.168.179.185&quot;</span> <span class="comment"># VIP</span></span><br></pre></td></tr></table></figure><h2 id="새로-추가할-노드-k8s-node5-라-가정-에-다음-명령어를-통해-etcd-데이터-복구"><a href="#새로-추가할-노드-k8s-node5-라-가정-에-다음-명령어를-통해-etcd-데이터-복구" class="headerlink" title="새로 추가할 노드(k8s-node5 라 가정)에 다음 명령어를 통해 etcd 데이터 복구"></a>새로 추가할 노드<em>(k8s-node5 라 가정)</em>에 다음 명령어를 통해 etcd 데이터 복구</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">docker run --rm \</span><br><span class="line">    -v <span class="string">&#x27;/root/init:/backup&#x27;</span> \</span><br><span class="line">    -v <span class="string">&#x27;/var/lib/etcd:/var/lib/etcd&#x27;</span> \</span><br><span class="line">    --env ETCDCTL_API=3 \</span><br><span class="line">    <span class="string">&#x27;k8s.gcr.io/etcd:3.3.10&#x27;</span> \</span><br><span class="line">    /bin/sh -c <span class="string">&quot;etcdctl snapshot restore &#x27;/backup/snap.db&#x27; ; mv /default.etcd/member/ /var/lib/etcd/&quot;</span></span><br></pre></td></tr></table></figure><h2 id="새로-추가할-노드-k8s-node5-에-kubeadm-init-수행"><a href="#새로-추가할-노드-k8s-node5-에-kubeadm-init-수행" class="headerlink" title="새로 추가할 노드 (k8s-node5) 에 kubeadm init 수행"></a>새로 추가할 노드 <em>(k8s-node5)</em> 에 kubeadm init 수행</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubeadm init --config=kubeadm-config.yaml\</span><br><span class="line">--ignore-preflight-errors=DirAvailable--var-lib-etcd</span><br></pre></td></tr></table></figure><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li><a href="https://codefarm.me/2019/05/22/kubernetes-recovery-master-failure/">https://codefarm.me/2019/05/22/kubernetes-recovery-master-failure/</a></li></ul><hr><p>made by <em>jaejun.lee</em></p>]]></content:encoded>
      
      <comments>https://jx2lee.github.io/troubleshoot-add_controlplane_node/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[Etc] DevOps 인터뷰 기술질문 정리</title>
      <link>https://jx2lee.github.io/etc-devops_interview_questions/</link>
      <guid>https://jx2lee.github.io/etc-devops_interview_questions/</guid>
      <pubDate>Wed, 10 Jun 2020 15:00:00 GMT</pubDate>
      <description>
      
        &lt;p&gt;DevOps 직무 실무자 면접에서 받은 인터뷰 내용을 정리하였다. 기술적인 내용만 정리하였고 이외에는 &lt;strong&gt;이직 사유&lt;/strong&gt;에 대해 많이 궁금해 하였다. 나에게 이직사유 보단 기술을 알고 넓히는게 중요할 것 같아.. 추가적으로 면접을 진행하면 앞으로 계속 추가할 예정이며 질문은 기술을 기준으로 나누었다.&lt;/p&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p>DevOps 직무 실무자 면접에서 받은 인터뷰 내용을 정리하였다. 기술적인 내용만 정리하였고 이외에는 <strong>이직 사유</strong>에 대해 많이 궁금해 하였다. 나에게 이직사유 보단 기술을 알고 넓히는게 중요할 것 같아.. 추가적으로 면접을 진행하면 앞으로 계속 추가할 예정이며 질문은 기술을 기준으로 나누었다.</p><a id="more"></a><p>구분은 다음과 같다.<br><strong><span class="github-emoji" alias="sparkles" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png?v8">&#x2728;</span> Contents:</strong>  </p><ol><li><a href="#kubernetes">Kubernetes</a></li><li><a href="#python">Python</a></li><li><a href="#db">Database</a></li><li><a href="#linux">Linux</a></li><li><a href="#network">Network</a></li><li><a href="#hadoop">Hadoop</a></li></ol><hr><ol><li><a name="kubernetes"></a> <em>Kubernetes</em><ul><li><code>VM vs container</code><br>:두 개 모두 가상화 기술로 큰 차이는 HyperVisor 유무이다. VM의 경우 운영체제에서 프로세스가 시작하는 반면 컨테이너는 호스트 운영체제의 내부에서 실행되어 좀 더 가볍고 MSA 구현이 가능하다. 또한, 운영체제 커널의 공유함으로써 빠르며 메모리 사용량이 적다.<br><img src="https://www.weave.works/assets/images/bltb6200bc085503718/containers-vs-virtual-machines.jpg" alt="https://www.weave.works/assets/images/bltb6200bc085503718/containers-vs-virtual-machines.jpg"></li><li><code>Docker Swarm</code><br>:컨테이너 오케스트레이션 도구 중 하나로 Docker 호스트들을 하나인 것처럼 만들어주는 도구. Master/Worker 노드로 시스템을 구성한다.<br>:<a href="https://medium.com/@chrisjune_13837/infra-docker-swarm이란-595d33160379">https://medium.com/@chrisjune_13837/infra-docker-swarm이란-595d33160379</a></li><li><code>proxy (k8s)</code><br>:Pod 로 연결되는 네트워크를 관리하며 파드 간 통신을 위해 라우팅을 돕는다. 파드 간 통신을 관리하기 때문에 파드 네트워크를 관리하는 서비스와의 통신을 원활히 해준다.</li><li><code>istio</code><br>:Data Plane <em>(프록시들로 이루어져 트래픽을 설정값에 따라 컨트롤 하는 부분)</em>의 메인 프록시로 Envoy Proxy를 사용하며 이를 컨트롤 해주는 Control Plane <em>(프록시들에 설정값을 전달하고 관리하는 컨트롤러)</em>의 오픈소스 솔루션. 이 부분에 대해서는 아래 포스트가 잘 정리되어 있다.<br>:<a href="https://gruuuuu.github.io/cloud/service-mesh-istio/#">https://gruuuuu.github.io/cloud/service-mesh-istio/#</a></li><li><code>readiness/liveness probe</code><br>:컨테이너가 살아있는지 확인하는 health check 방법인 liveness probe, 컨테이너가 서비스 가능한 상태인지 확인하는 health check 방법인 readiness probe<br>:<a href="https://bcho.tistory.com/m/1264">https://bcho.tistory.com/m/1264</a>  </li></ul></li></ol><ol start="2"><li><a name="python"></a> <strong><em>Python</em></strong><ul><li><code>global interpreter lock</code><br>:특정 시점에서의 하나의 쓰레드만 실행하도록 만드는 것<br>:<a href="https://m.blog.naver.com/alice_k106/221566619995">https://m.blog.naver.com/alice_k106/221566619995</a></li><li><code>async</code><br>:하나의 쓰레드로 동시 처리를 할 수 있는 비동기 프로그래밍을 위한 파이썬 패키지<br>:<a href="https://www.daleseo.com/python-asyncio/">https://www.daleseo.com/python-asyncio/</a></li><li><code>garbage collection</code><br>:파이썬은 보통 <code>garbage collection</code> 과 <code>reference counting</code> 을 통해 할당된 메모리를 관리한다. 기본적으로 참조 횟수가 0이 된 객체를 해제하는 <code>reference counting</code> 방식을 사용하지만, <code>reference cycles (순환참조)</code> 가 발생하면 <code>garbage collection</code> 으로 이를 해결한다.<br>:<a href="https://winterj.me/python-gc/">https://winterj.me/python-gc/</a></li><li><code>decorator</code><br>:대상 함수를 wrapping 하고 이 wrapping 된 함수의 앞뒤에 추가적으로 꾸며질 구문들을 정의해 손쉽게 재사용 가능하게 해주는 기능으로 <code>함수를 꾸며주는 함수</code> 라고 표현할 수 있다.<br>:<a href="https://bluese05.tistory.com/30">https://bluese05.tistory.com/30</a><br>:<a href="https://nachwon.github.io/decorator/">https://nachwon.github.io/decorator/</a>  </li></ul></li></ol><ol start="3"><li><a name="db"></a> <strong><em>Database</em></strong><ul><li><code>Tibero vs MySQL</code><br>:사실 이 질문에 답변은 오픈소스 vs 상용이라고 말했다.. 구글링을 하면 비교자료가 나오긴 하지만 어떻게 작성해야될 지 몰라 링크만 남겨두고 면접전에 보고 들어가자!<br>:<a href="https://db-engines.com/en/system/MySQL%3BTibero">https://db-engines.com/en/system/MySQL%3BTibero</a></li><li><code>Dead Lock</code><br>:데이터 일관성을 보장하기 위한 방법 중 하나로 트랜잭션 간 교착상태를 의미한다. 두 개의 트랜잭션 간 각각의 트랜잭션이 가지고 있는 리소스의 Lock 을 획득하려고 할 때 발생한다. (예를들어, A-&gt;D1 트랜잭션 발생 / B-&gt;D2 트랜잭션 발생 이후 A가 D2에 커밋을 하게 되면 Dead Lock 발생)<br>:<a href="https://medium.com/@chrisjune_13837/db-lock-락이란-무엇인가-d908296d0279">https://medium.com/@chrisjune_13837/db-lock-락이란-무엇인가-d908296d0279</a>  </li></ul></li></ol><ol start="4"><li><a name="linux"></a> <strong><em>Linux</em></strong><ul><li><code>fstrim</code><br>:디스크 IO(주로 SSD) 성능 저하를 피하기 위해 사용하는 리눅스 명령어<br><img src="http://forum.falinux.com/zbxe/files/attach/images/583/202/783/7cb0f69c8ebc4e151fcf222b4bffb25b.png" alt="http://forum.falinux.com/zbxe/files/attach/images/583/202/783/7cb0f69c8ebc4e151fcf222b4bffb25b.png"></li><li><code>/dev 디렉토리</code><br>:장치 파일을 위한 디렉토리로 <code>Node(노드)</code> 라고 불리는 요소를 포함하며, 각 노드는 시스템의 한 장치를 나타낸다. <code>/dev/null</code> 은 가상장치로써 프로그램의 출력을 무시하여 화면 상 텍스트를 표시하지 않을 때 유용하다. <code>/dev/0(zero)</code> 는 write 수행 시 성공적인 리턴 코드를 제공하며 특정 크기의 파일을 생성하거나 저장 장치를 포맷하기 위해 주로 사용한다.</li><li><code>LAID (리눅스 디스크)</code><br>:여러 개 HDD 를 하나의 HDD 로 사용하는 방식<br>:<a href="https://infrajp.tistory.com/9">https://infrajp.tistory.com/9</a></li><li><code>pipe / redirect</code><br>:<code>pipe)</code> 프로세스 혹은 실행된 프로그램의 결과를 다른 프로그램으로 전달하거나 남길 때 사용<br>:<code>redirect)</code> 프로그램의 결과 혹은 출력을 파일이나 다른 스트림으로 전달하거나 남길 때 사용  <ul><li>0: standard input <em>(stdin)</em></li><li>1: standard output <em>(stdout)</em></li><li>2: standard error <em>(stderr)</em><br><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/7/70/Stdstreams-notitle.svg/450px-Stdstreams-notitle.svg.png" alt="https://upload.wikimedia.org/wikipedia/commons/thumb/7/70/Stdstreams-notitle.svg/450px-Stdstreams-notitle.svg.png">  </li></ul></li><li><code>kernel parameter 변경 이유 (사용 이유)</code><br>:<code>kernel parameter</code> 란 커널(시스템을 관리하는 거대한 어플리케이션)이 메모리와 프로세스를 할당하기 위한 값으로, <code>/proc/sys</code> (OS마다 상이할 수 있음) 디렉토리에 존재한다.  </li></ul></li></ol><ol start="5"><li><a name="network"></a> <strong><em>Network</em></strong><ul><li><code>DNS(Domain Name System)</code><br>:전화번호부, 도메인 명을 IP 주소로 변환해주는 system<br>:<a href="https://samsikworld.tistory.com/489">https://samsikworld.tistory.com/489</a></li><li><code>CNAME, A record</code><br>:<code>CNAME)</code> Canonical Name 으로 하나의 도메인에 다른 이름을 부여하는 방식<br>:<code>A record)</code> 도메인 이름에 하나의 IP address 가 있음을 의미<br>:<a href="https://twpower.github.io/40-difference-between-cname-and-a-record">https://twpower.github.io/40-difference-between-cname-and-a-record</a>  </li><li><code>TCP / UDP</code><br>:TCP/IP의 전송계층에 사용하는 프로토콜로, <code>전송계층</code>이란 IP에 의해 전달하는 패킷의 오류를 검사하고 재전송 요구 등의 제어를 담당한다.<br>:<code>TCP(Transmission Control Protocol)</code> 신뢰성을 요구하는 애플리케이션에 사용(양방향 전송)<br>:<code>UDP(User Datagram Protocol)</code> 간단한 데이터를 빠른 속도로 전송하고자 하는 애플리케이션에 사용  </li></ul></li></ol><ul><li><a name="hadoop"></a> <strong><em>Hadoop / Spark</em></strong><ul><li><code>broadcasting join vs shuffle join</code><br>:<code>broadcasting join)</code> 데이터를 executor 로 복사하지만 executor 간 데이터 복사가 없어 속도가 빨라질 수 있다.<br><img src="https://henningkropponlinede.files.wordpress.com/2016/12/spark-broadcast.png" alt="broadcasting join"><br>:<code>shuffle join</code> 은 조인된 데이터로 동일한 executor 로 이동하기 위해 셔플 연산을 사용하여 데이터 이동이 많이 발생한다.<br><img src="https://henningkropponlinede.files.wordpress.com/2016/12/spark-broadcast-torrent.png" alt="shuffle join"><br>:<a href="https://knight76.tistory.com/entry/spark-스파크-조인-전략-셔플-조인-브로캐스트-조인-shuffle-join-broadcast-join">https://knight76.tistory.com/entry/spark-스파크-조인-전략-셔플-조인-브로캐스트-조인-shuffle-join-broadcast-join</a>  </li></ul></li></ul><hr><p>made by <em>jaejun.lee</em></p>]]></content:encoded>
      
      <comments>https://jx2lee.github.io/etc-devops_interview_questions/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[Cloud] 스토리지 클래스를 이용한 파드 배포 시 transport endpoint is not connected</title>
      <link>https://jx2lee.github.io/cloud-mount_failed/</link>
      <guid>https://jx2lee.github.io/cloud-mount_failed/</guid>
      <pubDate>Sun, 07 Jun 2020 15:00:00 GMT</pubDate>
      <description>
      
        &lt;p&gt;kubernetes 클러스터 운영 중 control plane 노드 장애가 발생해 여러 삽질을 하던 중.. 기존 운영중이던 Pod 가 떠 있지를 못해 확인 &lt;em&gt;(describe)&lt;/em&gt; 해보니 &lt;strong&gt;transport endpoint is not connected&lt;/strong&gt; 문구와 함께 Mount failed 하였다. 이 문제를 해결하는 과정을 다룬다.&lt;/p&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p>kubernetes 클러스터 운영 중 control plane 노드 장애가 발생해 여러 삽질을 하던 중.. 기존 운영중이던 Pod 가 떠 있지를 못해 확인 <em>(describe)</em> 해보니 <strong>transport endpoint is not connected</strong> 문구와 함께 Mount failed 하였다. 이 문제를 해결하는 과정을 다룬다.</p><a id="more"></a><h1 id="Describe-Pod"><a href="#Describe-Pod" class="headerlink" title="Describe Pod"></a>Describe Pod</h1><p>장애가 발생한 파드를 Describe 한 결과이다.<br>총 3 Waring-FailedMount 에러가 발생하였는데, <code>Rook-ceph</code> 의 cephfs 스토리지 클래스를 이용해 공유볼륨<em>(rserver-share)</em>/Home디렉토리<em>(rserver-home)</em>/토큰<em>(default-token-4w7gr)</em> 저장소를 생성하고자 했다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Events:</span><br><span class="line">  Type     Reason       Age                     From                Message</span><br><span class="line">  ----     ------       ----                    ----                -------</span><br><span class="line">  Warning  FailedMount  13m (x143 over 5h2m)    kubelet, k8s-node1  MountVolume.SetUp failed <span class="keyword">for</span> volume <span class="string">&quot;pvc-3a160803-5d2e-4a45-9c9f-b78a0a2e7339&quot;</span> : <span class="built_in">stat</span> /var/lib/kubelet/pods/eea991e6-9908-4d41-bd01-141af0566079/volumes/kubernetes.io~csi/pvc-3a160803-5d2e-4a45-9c9f-b78a0a2e7339/mount: transport endpoint is not connected</span><br><span class="line">  Warning  FailedMount  3m1s (x125 over 4h44m)  kubelet, k8s-node1  Unable to mount volumes <span class="keyword">for</span> pod <span class="string">&quot;rserver-3.6.3-deployment-776f6b8ddc-gwbk5_nps(eea991e6-9908-4d41-bd01-141af0566079)&quot;</span>: timeout expired waiting <span class="keyword">for</span> volumes to attach or mount <span class="keyword">for</span> pod <span class="string">&quot;nps&quot;</span>/<span class="string">&quot;rserver-3.6.3-deployment-776f6b8ddc-gwbk5&quot;</span>. list of unmounted volumes=[rserver-home]. list of unattached volumes=[rserver-share rserver-home default-token-6w7gr]</span><br></pre></td></tr></table></figure><p>Pod log 를 확인하고 싶었지만 애초에 뜨기도 전에 Mount Failed 되었기에 해당 노드로 접속하여 Kubelet 로그를 확인하였다.</p><h1 id="Kubelet-log"><a href="#Kubelet-log" class="headerlink" title="Kubelet log"></a>Kubelet log</h1><p>해당 노드에 <code>journal -f | grep kubelet</code> 으로 로그를 살펴보니 Describe 와 다른 점이 없어보이지만, 각 Pod 의 볼륨의 Path <em>(메세지에는 <code>stat /</code> 으로 표현)</em> 를 출력해준다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Jun 08 13:49:57 k8s-node1 kubelet[4638]: E0608 13:49:57.738465    4638 nestedpendingoperations.go:270] Operation <span class="keyword">for</span> <span class="string">&quot;\&quot;kubernetes.io/csi/rook-ceph.cephfs.csi.ceph.com^0001-0009-rook-ceph-0000000000000001-d90262eb-a499-11ea-a466-52c8f6e260ed\&quot;&quot;</span> failed. No retries permitted until 2020-06-08 13:51:59.738429723 +0000 UTC m=+21911.512876014 (durationBeforeRetry 2m2s). Error: <span class="string">&quot;MountVolume.SetUp failed for volume \&quot;pvc-3a160803-5d2e-4a45-9c9f-b78a0a2e7339\&quot; (UniqueName: \&quot;kubernetes.io/csi/rook-ceph.cephfs.csi.ceph.com^0001-0009-rook-ceph-0000000000000001-d90262eb-a499-11ea-a466-52c8f6e260ed\&quot;) pod \&quot;rserver-3.6.3-deployment-776f6b8ddc-gwbk5\&quot; (UID: \&quot;eea991e6-9908-4d41-bd01-141af0566079\&quot;) : stat /var/lib/kubelet/pods/eea991e6-9908-4d41-bd01-141af0566079/volumes/kubernetes.io~csi/pvc-3a160803-5d2e-4a45-9c9f-b78a0a2e7339/mount: transport endpoint is not connected&quot;</span></span><br><span class="line">Jun 08 13:50:01 k8s-node1 kubelet[4638]: E0608 13:50:01.755308    4638 kubelet.go:1665] Unable to mount volumes <span class="keyword">for</span> pod <span class="string">&quot;rserver-3.6.3-deployment-776f6b8ddc-gwbk5_nps(eea991e6-9908-4d41-bd01-141af0566079)&quot;</span>: timeout expired waiting <span class="keyword">for</span> volumes to attach or mount <span class="keyword">for</span> pod <span class="string">&quot;nps&quot;</span>/<span class="string">&quot;rserver-3.6.3-deployment-776f6b8ddc-gwbk5&quot;</span>. list of unmounted volumes=[rserver-home]. list of unattached volumes=[rserver-share rserver-home default-token-6w7gr]; skipping pod</span><br><span class="line">Jun 08 13:50:01 k8s-node1 kubelet[4638]: E0608 13:50:01.755344    4638 pod_workers.go:190] Error syncing pod eea991e6-9908-4d41-bd01-141af0566079 (<span class="string">&quot;rserver-3.6.3-deployment-776f6b8ddc-gwbk5_nps(eea991e6-9908-4d41-bd01-141af0566079)&quot;</span>), skipping: timeout expired waiting <span class="keyword">for</span> volumes to attach or mount <span class="keyword">for</span> pod <span class="string">&quot;nps&quot;</span>/<span class="string">&quot;rserver-3.6.3-deployment-776f6b8ddc-gwbk5&quot;</span>. list of unmounted volumes=[rserver-home]. list of unattached volumes=[rserver-share rserver-home default-token-6w7gr]</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">stat</span> /var/lib/kubelet/pods/eea991e6-9908-4d41-bd01-141af0566079/volumes/kubernetes.io~csi/pvc-3a160803-5d2e-4a45-9c9f-b78a0a2e7339/mount</span><br></pre></td></tr></table></figure><p>이 부분을 주목해서 보자. 해당 Path 으로 접근하여 <code>mount</code> 폴더를 확인하면 다음과 같다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">pwd</span></span><br><span class="line">/var/lib/kubelet/pods/eea991e6-9908-4d41-bd01-141af0566079/volumes/kubernetes.io~csi/pvc-3a160803-5d2e-4a45-9c9f-b78a0a2e7339</span><br><span class="line">$ ll</span><br><span class="line">ls: cannot access <span class="string">&#x27;mount&#x27;</span>: Transport endpoint is not connected</span><br><span class="line">total 12</span><br><span class="line">drwxr-x--- 3 root root 4096 Jun  2 06:27 ./</span><br><span class="line">drwxr-x--- 4 root root 4096 Jun  2 06:27 ../</span><br><span class="line">d????????? ? ?    ?       ?            ? mount/</span><br><span class="line">-rw-r--r-- 1 root root  328 Jun  8 13:54 vol_data.json</span><br></pre></td></tr></table></figure><p><code>mount</code> 폴더의 표현이 이상한 것을 확인할 수 있다. 분명 폴더에 대한 읽기쓰기실행 권한과 소유자/그룹이 나와야 하지만 모두 물음표로 출력한다. 이에 대한 구글링을 해본 결과, 해당 디렉토리의 mount 가 비정상 작동하여 이를 해제해야 한다고 한다.  </p><p>정확한 원인은 모르겠지만, <strong>해당 폴더에 대한 잘못된 mount 로 인해 PVC 가 PV 를 제대로 바라보지 못하여 Mount Failed</strong> 난 것으로 이해했다. 해당 디렉토리를 <code>umount</code> 를 이용해 마운트를 해제하고, Kubelet 로그를 더 확인하여 다른 PV stat 으로 접근해 마운트 해제를 수행한다.  </p><p>보아하니 한 번의 <code>umount</code> 로 끝나지 않아 나의 경우,<br><code>/var/lib/kubelet/pods/&#123;위UUID&#125;/volumes/kubernetes.io~csi/</code> 내 모든 PVC 로 접근하여 폴더가 이상하면 마운트를 해제 하였다. 무식할 수 있지만.. 어쨌든 모두 수행하여 Kubelet 로그에 이상이 없는 것을 확인한다.</p><h1 id="정상-작동-확인"><a href="#정상-작동-확인" class="headerlink" title="정상 작동 확인"></a>정상 작동 확인</h1><p><code>kubectl get all -n nps</code> 명령어로 기존에 떠있지 못한 파드가 정상 기동하였는지 확인한다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">root@k8s-node2:~<span class="comment"># kubectl get all -n nps -o wide</span></span><br><span class="line">NAME                                            READY   STATUS              RESTARTS   AGE</span><br><span class="line">pod/jupyter-3.7-deployment-f798bb7f4-n2kqq      1/1     Running             0          5h21m</span><br><span class="line">pod/jupyter-3.8-deployment-d47c78475-9gsb8      0/1     ContainerCreating   0          4d12h</span><br><span class="line">pod/pypiserver-deployment-7bbbb48f8c-rkbhk      1/1     Running             0          6d3h</span><br><span class="line">pod/rserver-3.5.3-deployment-96f5d6b4-dhgkg     1/1     Running             0          6d7h</span><br><span class="line">pod/rserver-3.6.3-deployment-776f6b8ddc-gwbk5   1/1     Running             0          6d7h</span><br></pre></td></tr></table></figure><blockquote><p><em>3.8 jupyter 의 경우 해당 파드는 노드가 달라.. 귀찮아서 아직 고치지 않은 모습이다. 이를 고치려면 같은 방법으로 해당 노드로 접근하여 해결해야 할 것 같다.</em></p></blockquote><h1 id="결론"><a href="#결론" class="headerlink" title="결론"></a>결론</h1><p>이번 장애를 겪으면서 얻은 교훈으로는,<br>Pod Describe 도 좋지만 장애가 나는 리소스의 노드로 접근하여 Kubelet 로그도 함께 보는 습관을 가져야 겠다. 물론 Describe로도 충분히 알아낼 수 있는 정보지만, 노드들 간 통신을 담당하는 Kubelet 로그를 통해 좀 더 세부적인 Info 와 Warning 을 확인하여 일석이조의 효과를 누릴 수 있다 생각한다.</p><blockquote><p>엔지니어의 삶은.. 로그라는 선배의 말이 생각난다.</p></blockquote><hr><p>made by <em>jaejun.lee</em></p>]]></content:encoded>
      
      <comments>https://jx2lee.github.io/cloud-mount_failed/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[Python] Jupyter Notebook 로그 파일 생성</title>
      <link>https://jx2lee.github.io/python-jupyter_logging/</link>
      <guid>https://jx2lee.github.io/python-jupyter_logging/</guid>
      <pubDate>Wed, 27 May 2020 15:00:00 GMT</pubDate>
      <description>
      
        &lt;p&gt;Jupyter Notebook 실행 이력을 로그파일로 생성하는 과정을 다룬다.  &lt;/p&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p>Jupyter Notebook 실행 이력을 로그파일로 생성하는 과정을 다룬다.  </p><a id="more"></a><p><strong><span class="github-emoji" alias="sparkles" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png?v8">&#x2728;</span> Contents:</strong>  </p><ol><li><a href="#env">Environment</a></li><li><a href="#history">log_history.py</a></li><li><a href="#tunning">Customize IPython/core/history.py</a></li><li><a href="#result">Result</a></li><li><a href="#ref">Reference</a></li></ol><h1 id="Environment"><a href="#Environment" class="headerlink" title=" Environment"></a><a name="env"></a> Environment</h1><p>Kubernetes 환경에서 Jupyter Notebook 도커 이미지를 이용해 deployment 로 관리하고 서비스는 노드포트로 구성하여 특정 포트로 Jupyter Notebook 에 접근할 수 있게 설정하였다. Python 버젼은 3.8에서 진행하였으며, Docker image는 3.8, 3.7 등 소수 첫 째 자리까지의 버젼만 빌드할 수 있게 작성되었다. 환경 구성은 다음과 같다.  </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">$ kg all -n jupyter-logging</span><br><span class="line">NAME                                            READY   STATUS    RESTARTS   AGE</span><br><span class="line">pod/jupyter-3.8-deployment-6676cc56d6-xth7b     1/1     Running   0          13h</span><br><span class="line"></span><br><span class="line">NAME                      TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE</span><br><span class="line">service/jupyter-38-svc    NodePort   10.96.187.185   &lt;none&gt;        8888:30180/TCP   24d</span><br><span class="line"></span><br><span class="line">NAME                                       READY   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">deployment.apps/jupyter-3.8-deployment     1/1     1            1           13h</span><br><span class="line"></span><br><span class="line">NAME                                                  DESIRED   CURRENT   READY   AGE</span><br><span class="line">replicaset.apps/jupyter-3.8-deployment-6676cc56d6     1         1         1       13h</span><br></pre></td></tr></table></figure><p>Dockerfile 를 통한 빌드 환경은 다음과 같다.  </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">$ ll</span><br><span class="line">total 104</span><br><span class="line">drwxr-xr-x 2 root root  4096 May 28 09:05 ./</span><br><span class="line">drwxr-xr-x 6 root root  4096 May 26 09:56 ../</span><br><span class="line">-rw-r--r-- 1 root root    71 May 26 01:52 create_pwd_hash.py</span><br><span class="line">-rw-r--r-- 1 root root  8160 May 26 08:10 Dockerfile</span><br><span class="line">-rw-r--r-- 1 root root   965 May 26 01:52 fix-permissions</span><br><span class="line">-rw-r--r-- 1 root root 33023 May 28 09:05 history.py</span><br><span class="line">-rw-r--r-- 1 root root  1877 May 26 09:54 jupyter_notebook_config.py</span><br><span class="line">-rw-r--r-- 1 root root   949 May 28 09:04 log_history.py</span><br><span class="line">-rwxr-xr-x 1 root root   524 May 26 01:52 start-notebook.sh*</span><br><span class="line">-rwxr-xr-x 1 root root  6302 May 26 01:52 start.sh*</span><br><span class="line">-rwxr-xr-x 1 root root  1181 May 26 01:52 start-singleuser.sh*</span><br></pre></td></tr></table></figure><blockquote><p><em>파일 목록 중 새로 추가한 것은 <code>log_history.py</code>와 <code>history.py</code>이다. 추후에 Dockerfile 커스텀화 한 결과를 추가할 예정이다.</em></p></blockquote><h1 id="log-history-py"><a href="#log-history-py" class="headerlink" title=" log_history.py"></a><a name="history"></a> log_history.py</h1><p>생소할 것이다. 맞다. 왜냐하면 파일 네이밍은 내가 생각한 것이기 때문이다.  </p><p>고객사 요건은 간단하다. Jupyter Notebook 을 이용해 분석가가 코드를 실행하였을 때, 코드 이력을 남기는 log가 필요했다. 물론 python 로그의 경우 python 을 실행한 디렉토리에서 .python_history 가 작성되지만, 이놈의 Jupyter Notebook 은 어디에다 로그를 남기는지 파악이 어려웠다.  </p><p>But, Jupyter Notebook 과 IPython 의 관계를 파악하면 쉽게 풀린다. 구글링 하면 쉽게 차이점을 살필 수 있겠지만, 간단히 설명하면 차이가 있는 것 보다 <code>포함관계</code>를 가진다. 즉, Jupyter Notebook 은 다양한 언어를 지원하는데 그 중 IPython 이 파이썬 언어를 이용할 수 있게 도와준다. 다시 말해 Jupyter Notebook은 IPython 을 포함한다.  </p><p>서론이 길었다. 결국 Jupyter POD 를 기동하면 $HOME 디렉토리에 <code>.ipython</code> 폴더가 생성된다. 구조를 살피면 다음과 같다.  </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">root@jupyter-3:~/.ipython<span class="comment"># tree .</span></span><br><span class="line">.</span><br><span class="line">├── extensions</span><br><span class="line">├── nbextensions</span><br><span class="line">└── profile_default</span><br><span class="line">    ├── db</span><br><span class="line">    ├── history.sqlite</span><br><span class="line">    ├── jupyter.log</span><br><span class="line">    ├── <span class="built_in">log</span></span><br><span class="line">    ├── pid</span><br><span class="line">    ├── security</span><br><span class="line">    └── startup</span><br><span class="line">        └── README</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>우리가 주의있게 볼 디렉토리는 <code>startup</code> 폴더다. 이름에서 보이듯, startup 폴더의 README 를 열어보면 쉽게 파악할 수 있다. Notebook 을 실행하기 전 환경 구성을 도와주는 공간이다. 이 폴더에 다음과 같이 <code>log_history.py</code> 를 작성한다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> atexit</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"></span><br><span class="line">ip = get_ipython()</span><br><span class="line">LIMIT = <span class="number">100000</span> <span class="comment"># limit the size of the history</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save_history</span>():</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;save the IPython history to a plaintext file&quot;&quot;&quot;</span></span><br><span class="line">    histfile = os.path.join(ip.profile_dir.location, <span class="string">&quot;jupyter.log&quot;</span>)</span><br><span class="line">    print(<span class="string">&quot;[INFO] Saving plaintext history to %s&quot;</span> % histfile)</span><br><span class="line">    lines = []</span><br><span class="line">    lines.extend(<span class="string">&#x27;[&#123;0&#125;]\n&#123;1&#125;&#x27;</span>.<span class="built_in">format</span>(record[<span class="number">2</span>][<span class="number">1</span>], record[<span class="number">2</span>][<span class="number">0</span>]) + <span class="string">&#x27;\n&#x27;</span> <span class="keyword">for</span> record <span class="keyword">in</span> ip.history_manager.get_range())</span><br><span class="line">    <span class="comment"># get previous lines</span></span><br><span class="line">    <span class="comment"># this is only necessary because we truncate the history,</span></span><br><span class="line">    <span class="comment"># otherwise we chould just open with mode=&#x27;a&#x27;</span></span><br><span class="line">    <span class="keyword">if</span> os.path.exists(histfile):</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(histfile, <span class="string">&#x27;a&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            <span class="comment">#lines = f.readlines()</span></span><br><span class="line">            f.writelines(lines)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(histfile, <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            f.writelines(lines)</span><br><span class="line"></span><br><span class="line"><span class="comment"># do the save at exit</span></span><br><span class="line">atexit.register(save_history)</span><br></pre></td></tr></table></figure><p>코드를 간단히 설명하면 log 파일이 있다면 파일을 수정모드로 열어 Notebook 안에서 실행한 Python 실행 이력을 추가하고, 없으면 새로 만들어 처음으로 생성한 로그를 추가한다. 여기서 주의깊게 볼 곳은 <code>history_manager</code> 오브젝트이다.  </p><p>Jupyter Notebook 의 History 를 관리하는 클래스로 다음 챕터에서 간단히 설명하고 수정한 부분을 설명하겠다.  </p><h1 id="Customize-IPython-core-history-py"><a href="#Customize-IPython-core-history-py" class="headerlink" title=" Customize IPython/core/history.py"></a><a name="tunning"></a> Customize IPython/core/history.py</h1><p>python 설치 디렉토리 내 site-package 에는 IPython 이 존재할 것이다. <code>$PY_PKG_DIR/IPython/core</code> 에 <code>history.py</code> 를 수정할 것이다.  </p><p>수정이 왜 필요한가? 기본적으로 Jupyter Notebook 은 sqlite3 DB에 history 를 남기지만 Timestamp 는 없다. 이를 위해 수정하는 것이기 때문에 <code>history.py</code> 를 열어 <code>def store_inputs</code> 함수를 찾아본다. 그 다음은 쉽다, 한 줄만 추가하면 된다.  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">store_inputs</span>(<span class="params">self, line_num, source, source_raw=<span class="literal">None</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Store source and raw input in history and create input cache</span></span><br><span class="line"><span class="string">    variables ``_i*``.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    line_num : int</span></span><br><span class="line"><span class="string">      The prompt number of this input.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    source : str</span></span><br><span class="line"><span class="string">      Python input.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    source_raw : str, optional</span></span><br><span class="line"><span class="string">      If given, this is the raw input without any IPython transformations</span></span><br><span class="line"><span class="string">      applied to it.  If not given, ``source`` is used.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> source_raw <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        source_raw = source</span><br><span class="line">    source = source.rstrip(<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">    source_raw = source_raw.rstrip(<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># do not store exit/quit commands</span></span><br><span class="line">    <span class="keyword">if</span> self._exit_re.match(source_raw.strip()):</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">    self.input_hist_parsed.append(source)</span><br><span class="line">    <span class="comment">#############</span></span><br><span class="line">    <span class="comment">###append</span></span><br><span class="line">    self.input_hist_raw.append([source_raw, datetime.datetime.now().strftime(<span class="string">&#x27;%Y-%m-%d %H:%M:%S&#x27;</span>)])</span><br><span class="line">    <span class="comment">###append-end</span></span><br><span class="line">    <span class="comment">#############</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> self.db_input_cache_lock:</span><br><span class="line">        self.db_input_cache.append((line_num, source, source_raw))</span><br><span class="line">        <span class="comment"># Trigger to flush cache and write to DB.</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(self.db_input_cache) &gt;= self.db_cache_size:</span><br><span class="line">            self.save_flag.<span class="built_in">set</span>()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># update the auto _i variables</span></span><br><span class="line">    self._iii = self._ii</span><br><span class="line">    self._ii = self._i</span><br><span class="line">    self._i = self._i00</span><br><span class="line">    self._i00 = source_raw</span><br><span class="line"></span><br><span class="line">    <span class="comment"># hackish access to user namespace to create _i1,_i2... dynamically</span></span><br><span class="line">    new_i = <span class="string">&#x27;_i%s&#x27;</span> % line_num</span><br><span class="line">    to_main = &#123;<span class="string">&#x27;_i&#x27;</span>: self._i,</span><br><span class="line">               <span class="string">&#x27;_ii&#x27;</span>: self._ii,</span><br><span class="line">               <span class="string">&#x27;_iii&#x27;</span>: self._iii,</span><br><span class="line">               new_i : self._i00 &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> self.shell <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        self.shell.push(to_main, interactive=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><p>이 함수를 간단히 설명하면, Jupyter Notebook 에서 발생한 python history input 을 sqlite3 DB 에 저장하는 함수다. 추가 내용은 다음과 같다.<br><code>self.input_hist_raw.append([source_raw, datetime.datetime.now().strftime(&#39;%Y-%m-%d %H:%M:%S&#39;)])</code>  </p><blockquote><p><em>이 함수를 추적한 과정은 history_manager.get_range() 가 generator 형태로 반환하기 때문에 <code>yield</code> 검색을 해보니 담당 함수는 <code>_get_range_session</code> 임을 확인했다. 첫 줄 <code>input_hist_raw</code> 가 history 의 raw data 를 포함하는 것을 확인하고 <code>input_hist_raw</code> 검색하여 <code>store_inputs</code> 함수를 trace 하였다.</em></p></blockquote><h1 id="Result"><a href="#Result" class="headerlink" title=" Result"></a><a name="result"></a> Result</h1><p>간단한 Notebook 을 생성하고 log 파일을 확인해본다. log 경로는 <code>$HOME/.ipython/profile-default</code> 경로이다. <em>(log_history.py 에서 수정 가능)</em></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">$ cat jupyter.log</span><br><span class="line">[2020-05-28 08:43:46]</span><br><span class="line"><span class="built_in">print</span>(1)</span><br><span class="line">[2020-05-28 08:43:47]</span><br><span class="line">def sum(a, b): <span class="comment"># 3과 5가 각각 매개변수 a와 b에 할당된다.</span></span><br><span class="line">    result = a + b <span class="comment"># 변수 result에는 매개변수 a와 b의 합이 할당되므로 이 경우에는 3과 5의 합인 8이 할당된다.</span></span><br><span class="line">    <span class="built_in">return</span> result <span class="comment"># 8을 함수 바깥으로 반환한다.</span></span><br><span class="line">[2020-05-28 08:43:47]</span><br><span class="line">sum(10,20)</span><br><span class="line">[2020-05-28 08:43:47]</span><br><span class="line"><span class="built_in">print</span>(2)</span><br><span class="line">[2020-05-28 08:43:47]</span><br><span class="line">def say_hello(func):                    <span class="comment"># 1</span></span><br><span class="line">    def wrapper2(*args, **kwargs):      <span class="comment"># 8</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Hello&#x27;</span>)                  <span class="comment"># 11</span></span><br><span class="line">        <span class="built_in">return</span> func(*args, **kwargs)    <span class="comment"># 12</span></span><br><span class="line">    <span class="built_in">return</span> wrapper2                     <span class="comment"># 9</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def say_hi(func):                       <span class="comment"># 2</span></span><br><span class="line">    def wrapper1(*args, **kwargs):      <span class="comment"># 5</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Hi&#x27;</span>)                     <span class="comment"># 13</span></span><br><span class="line">        <span class="built_in">return</span> func(*args, **kwargs)    <span class="comment"># 14</span></span><br><span class="line">    <span class="built_in">return</span> wrapper1                     <span class="comment"># 6</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">@say_hello                              <span class="comment"># 7</span></span><br><span class="line">@say_hi                                 <span class="comment"># 4</span></span><br><span class="line">def introduce(name):                    <span class="comment"># 3</span></span><br><span class="line">    <span class="built_in">print</span>(f<span class="string">&#x27;My name is &#123;name&#125;!&#x27;</span>)        <span class="comment"># 15</span></span><br><span class="line"></span><br><span class="line">introduce(<span class="string">&#x27;Jaejun&#x27;</span>)                    <span class="comment"># 10</span></span><br></pre></td></tr></table></figure><blockquote><p><em>추가적으로 고려할 사항이 있다. log 파일 관리인데 최대 크기를 지정해서 용량이 커지면 이전 History 를 삭제하는 방안을 마련하고 있다. 조만간 Dockerfile 도 완성되면 추가할 예정이다.</em></p></blockquote><h1 id="Reference"><a href="#Reference" class="headerlink" title=" Reference"></a><a name="ref"></a> Reference</h1><ul><li><a href="https://anaconda.org/conda-forge/python-json-logger">https://anaconda.org/conda-forge/python-json-logger</a></li><li><a href="https://velog.io/@log327">https://velog.io/@log327</a></li><li><a href="https://stackoverflow.com/">https://stackoverflow.com/questions/16858724/how-to-log-ipython-history-to-text-file</a></li></ul><hr><p>made by <em>jaejun.lee</em></p>]]></content:encoded>
      
      <comments>https://jx2lee.github.io/python-jupyter_logging/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[Hadoop] CDH(ClouDera Hadoop) Client 설정</title>
      <link>https://jx2lee.github.io/hadoop-client/</link>
      <guid>https://jx2lee.github.io/hadoop-client/</guid>
      <pubDate>Thu, 14 May 2020 15:00:00 GMT</pubDate>
      <description>
      
        &lt;center&gt; Python 을 이용해 CDH 연동하는 과정에서 Hadoop Client 설정 방법에 대해 정리한다.&lt;/center&gt;
&lt;br&gt;
&lt;br&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<center> Python 을 이용해 CDH 연동하는 과정에서 Hadoop Client 설정 방법에 대해 정리한다.</center><br><br><a id="more"></a><p><strong><span class="github-emoji" alias="sparkles" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png?v8">&#x2728;</span> Contents:</strong>  </p><ol><li><a href="#download">Bianry Download</a></li><li><a href="#config">CDH Hadoop Config</a></li><li><a href="#tar">Client 압축 해제</a></li><li><a href="#apply">Config 적용</a></li><li><a href="#hosts">/etc/hosts 적용</a></li><li><a href="#env">ENV</a></li></ol><h1 id="Bianry-Download"><a href="#Bianry-Download" class="headerlink" title=" Bianry Download"></a><a name="donwload"></a> Bianry Download</h1><ul><li>wget 을 이용해 CDH hadoop 3 버젼을 다운</li><li><code>$ wget https://archive.apache.org/dist/hadoop/core/hadoop-3.0.0/hadoop-3.0.0.tar.gz</code></li></ul><h1 id="CDH-Hadoop-Config"><a href="#CDH-Hadoop-Config" class="headerlink" title=" CDH Hadoop Config"></a><a name="config"></a> CDH Hadoop Config</h1><ul><li>CDH 웹에서 hadoop config 을 다운로드<ul><li>core-site.xml</li><li>hdfs-site.xml</li><li>yarn-site.xml <em>(Yarn 구성 시)</em></li></ul></li></ul><h1 id="Client-압축-해제"><a href="#Client-압축-해제" class="headerlink" title=" Client 압축 해제"></a><a name="tar"></a> Client 압축 해제</h1><ul><li>특정 directory 에 압축 해제</li><li><code>$ tar -xvf hadoop-3.0.0.tar.gz &amp;&amp; mv hadoop-3.0.0 hadoop</code></li></ul><h1 id="Config-적용"><a href="#Config-적용" class="headerlink" title=" Config 적용"></a><a name="apply"></a> Config 적용</h1><ul><li><code>$HADOOP_HOME/etc/hadoop</code> 에 위에 준비한 CDH config 을 copy &amp; paste</li><li><code>$ cp *-site.xml $HADOOP_HOME/etc/hadoop</code></li></ul><h1 id="etc-hosts-적용"><a href="#etc-hosts-적용" class="headerlink" title=" /etc/hosts 적용"></a><a name="hosts"></a> /etc/hosts 적용</h1><ul><li>Client 서버의 IP hostname 에 마찬가지로 클러스터 노드 정보를 작성<ul><li>example<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#/etc/hosts</span></span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line">192.168.179.181  chd1</span><br><span class="line">192.168.179.182  chd2</span><br><span class="line">192.168.179.183  chd3</span><br></pre></td></tr></table></figure></li><li><code>$ touch /etc/hosts</code> 로 파일 적용</li></ul></li></ul><h1 id="ENV"><a href="#ENV" class="headerlink" title=" ENV"></a><a name="env"></a> ENV</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64</span><br><span class="line"><span class="built_in">export</span> HADOOP_HOME=/home/jovyan/hadoop</span><br><span class="line"><span class="built_in">export</span> HADOOP_CMD=/home/jovyan/hadoop/bin/hadoop</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HADOOP_HOME</span>/bin:<span class="variable">$HADOOP_HOME</span>/sbin:<span class="variable">$JAVA_HOME</span>/bin:<span class="variable">$HADOOP_CMD</span></span><br></pre></td></tr></table></figure><hr><p>made by <em>jaejun.lee</em></p>]]></content:encoded>
      
      <comments>https://jx2lee.github.io/hadoop-client/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[Python] Jupyter Notebook 을 이용한 Hadoop 및 Database 연동</title>
      <link>https://jx2lee.github.io/python-connection_test/</link>
      <guid>https://jx2lee.github.io/python-connection_test/</guid>
      <pubDate>Thu, 14 May 2020 15:00:00 GMT</pubDate>
      <description>
      
        &lt;p&gt;Python 을 이용한 Hadoop 및 Database 연동 테스트를 진행하였다.  &lt;/p&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p>Python 을 이용한 Hadoop 및 Database 연동 테스트를 진행하였다.  </p><a id="more"></a><p><strong>환경구성</strong>:  </p><ul><li>Hadoop Cluster <em>(CDH 6.3.2)</em><ul><li>cdh1: 192.168.179.181</li><li>cdh2: 192.168.179.182</li><li>cdh3: 192.168.179.183 <em>(master)</em></li></ul></li><li>Jupyter Notebook<ul><li>docker custom image <em>(python version: 3.7)</em></li><li>kubernetes deployment 배포<ul><li><code>namespace</code>: nps</li></ul></li><li>컨테이너 환경 : Ubuntu 18.04.4 LTS</li></ul></li></ul><p><strong><span class="github-emoji" alias="sparkles" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png?v8">&#x2728;</span> Contents</strong>:  </p><ol><li><a href="#hdfs">HDFS</a></li><li><a href="#hive">Hive</a></li><li><a href="#hbase">HBase</a></li><li><a href="#impala">Impala</a></li><li><a href="#spark">Spark</a></li><li><a href="#sybase">Sybase</a></li><li><a href="#oracle">Oracle</a></li></ol><h1 id="HDFS"><a href="#HDFS" class="headerlink" title=" HDFS"></a><a name="hdfs"></a> HDFS</h1><h2 id="설치-패키지"><a href="#설치-패키지" class="headerlink" title="설치 패키지"></a>설치 패키지</h2><h3 id="Linux"><a href="#Linux" class="headerlink" title="Linux"></a>Linux</h3><h3 id="Python"><a href="#Python" class="headerlink" title="Python"></a>Python</h3><ul><li>hdfs<ul><li>Requirement already satisfied: docopt in /opt/conda/lib/python3.8/site-packages (from hdfs) (0.6.2)</li><li>Requirement already satisfied: six&gt;=1.9.0 in /opt/conda/lib/python3.8/site-packages (from hdfs) (1.14.0)</li><li>Requirement already satisfied: requests&gt;=2.7.0 in /opt/conda/lib/python3.8/site-packages (from hdfs) (2.23.0)</li><li>Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /opt/conda/lib/python3.8/site-packages (from   - requests&gt;=2.7.0-&gt;hdfs) (1.25.9)</li><li>Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /opt/conda/lib/python3.8/site-packages (from requests&gt;=2.7.0-&gt;hdfs) (3.0.4)</li><li>Requirement already satisfied: idna&lt;3,&gt;=2.5 in /opt/conda/lib/python3.8/site-packages (from requests&gt;=2.7.0-&gt;hdfs) (2.9)</li><li>Requirement already satisfied: certifi&gt;=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests&gt;=2.7.0-&gt;hdfs) (2020.4.5.1)</li></ul></li></ul><h2 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> hdfs <span class="keyword">import</span> InsecureClient</span><br><span class="line"></span><br><span class="line"><span class="comment"># Connection</span></span><br><span class="line"></span><br><span class="line">client_hdfs = InsecureClient(<span class="string">&#x27;http://192.168.179.183:9870&#x27;</span>, user=<span class="string">&#x27;hdfs&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">## Create</span></span><br><span class="line"></span><br><span class="line">create_df = pd.DataFrame([<span class="number">1000</span>, <span class="number">2000</span>, <span class="number">3000</span>, <span class="number">4000</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> client_hdfs.write(<span class="string">&#x27;/tmp/t2.csv&#x27;</span>, encoding = <span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> writer:</span><br><span class="line">    create_df.to_csv(writer)</span><br><span class="line"></span><br><span class="line">print(client_hdfs.<span class="built_in">list</span>(<span class="string">&#x27;/tmp&#x27;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">## Read</span></span><br><span class="line"></span><br><span class="line">client_hdfs.<span class="built_in">list</span>(<span class="string">&#x27;/tmp&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> client_hdfs.read(<span class="string">&#x27;/tmp/test.csv&#x27;</span>, encoding = <span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> reader:</span><br><span class="line">    df = pd.read_csv(reader)</span><br><span class="line">    print(df)</span><br><span class="line"></span><br><span class="line"><span class="comment">## Update</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> client_hdfs.read(<span class="string">&#x27;/tmp/t2.csv&#x27;</span>, encoding = <span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> reader:</span><br><span class="line">    df = pd.read_csv(reader)</span><br><span class="line">df.loc[<span class="number">4</span>] = [<span class="number">4</span>,<span class="number">5000</span>]</span><br><span class="line">client_hdfs.delete(<span class="string">&#x27;/tmp/t2.csv&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> client_hdfs.write(<span class="string">&#x27;/tmp/t2.csv&#x27;</span>, encoding = <span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> writer:</span><br><span class="line">    df.to_csv(writer)</span><br><span class="line">    print(df)</span><br><span class="line"></span><br><span class="line"><span class="comment">## Delete</span></span><br><span class="line"></span><br><span class="line">client_hdfs.delete(<span class="string">&#x27;/tmp/t2.csv&#x27;</span>)</span><br><span class="line"><span class="string">&#x27;t2.csv&#x27;</span> <span class="keyword">in</span> client_hdfs.<span class="built_in">list</span>(<span class="string">&#x27;/tmp&#x27;</span>)</span><br></pre></td></tr></table></figure><h2 id="Ref"><a href="#Ref" class="headerlink" title="Ref"></a>Ref</h2><ul><li><a href="https://pypi.org/project/hdfs/">https://pypi.org/project/hdfs/</a></li><li><a href="https://hdfscli.readthedocs.io/en/latest/api.html">https://hdfscli.readthedocs.io/en/latest/api.html</a></li></ul><h1 id="Hive"><a href="#Hive" class="headerlink" title=" Hive"></a><a name="hive"></a> Hive</h1><h2 id="설치-패키지-1"><a href="#설치-패키지-1" class="headerlink" title="설치 패키지"></a>설치 패키지</h2><h3 id="Linux-1"><a href="#Linux-1" class="headerlink" title="Linux"></a>Linux</h3><ul><li>libsasl2-dev</li><li>libsasl2-modules</li></ul><h3 id="Python-1"><a href="#Python-1" class="headerlink" title="Python"></a>Python</h3><ul><li>pyhive<ul><li>Requirement already satisfied: future in /opt/conda/lib/python3.8/site-packages (from pyhive) (0.18.2)</li><li>Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.8/site-packages (from pyhive) (2.8.1)</li><li>Requirement already satisfied: six&gt;=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil-&gt;pyhive) (1.14.0)</li></ul></li><li>thrift</li><li>sasl</li><li>thrift_sasl<ul><li>Requirement already satisfied: thrift&gt;=0.10.0 in /opt/conda/lib/python3.8/site-packages (from thrift_sasl) (0.13.0)</li><li>Requirement already satisfied: sasl&gt;=0.2.1 in /opt/conda/lib/python3.8/site-packages (from thrift_sasl) (0.2.1)</li><li>Requirement already satisfied: six&gt;=1.13.0 in /opt/conda/lib/python3.8/site-packages (from thrift_sasl) (1.14.0)</li></ul></li></ul><h3 id="Code-1"><a href="#Code-1" class="headerlink" title="Code"></a>Code</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pyhive.hive <span class="keyword">as</span> hive</span><br><span class="line">conn = hive.Connection(host=<span class="string">&#x27;cdh3&#x27;</span>, port=<span class="number">10000</span>, username=<span class="string">&#x27;root&#x27;</span>, password=<span class="string">&#x27;tmaxtmax&#x27;</span>, database=<span class="string">&#x27;default&#x27;</span>, auth=<span class="string">&#x27;CUSTOM&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create</span></span><br><span class="line"></span><br><span class="line">cur = conn.cursor()</span><br><span class="line">sql = <span class="string">&quot;CREATE TABLE `default`.`create_test` (  `field_1` bigint ,  `field_2` string ,  `field_3` string ,  `field_4` string ,  `field_5` string ,  `field_6` bigint ,  `field_7` string ,  `field_8` double ,  `field_9` string ,  `field_10` string ) ROW FORMAT   DELIMITED    FIELDS TERMINATED BY &#x27;|&#x27;    COLLECTION ITEMS TERMINATED BY &#x27;&#x27;    MAP KEYS TERMINATED BY &#x27;&#x27;  STORED AS TextFile&quot;</span></span><br><span class="line">cur.execute(sql)</span><br><span class="line"></span><br><span class="line">cur = conn.cursor()</span><br><span class="line">cur.execute(<span class="string">&#x27;select * from create_test&#x27;</span>)</span><br><span class="line">res = cur.fetchall()</span><br><span class="line">print(res)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Read</span></span><br><span class="line"></span><br><span class="line">cur.execute(<span class="string">&#x27;select count(*) from create_test&#x27;</span>)</span><br><span class="line">res = cur.fetchall()</span><br><span class="line"></span><br><span class="line">print(res)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Update</span></span><br><span class="line"></span><br><span class="line">cur.execute(<span class="string">&#x27;alter table create_test rename to create_test_rev&#x27;</span>)</span><br><span class="line">cur.execute(<span class="string">&#x27;describe create_test_rev&#x27;</span>)</span><br><span class="line">res = cur.fetchall()</span><br><span class="line">print(res)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Delete</span></span><br><span class="line"></span><br><span class="line">cur.execute(<span class="string">&#x27;drop table create_test_rev&#x27;</span>)</span><br><span class="line">cur.execute(<span class="string">&#x27;show tables&#x27;</span>)</span><br><span class="line">res = cur.fetchall()</span><br><span class="line">print(res)</span><br></pre></td></tr></table></figure><h2 id="Ref-1"><a href="#Ref-1" class="headerlink" title="Ref"></a>Ref</h2><ul><li><a href="https://sungwookkang.com/m/1367">https://sungwookkang.com/m/1367</a></li></ul><h1 id="HBase"><a href="#HBase" class="headerlink" title=" HBase"></a><a name="hbase"></a> HBase</h1><ul><li>Hbase 서버 내 Thrift 가 동작하고 있어야함<ul><li>클러스터 내 아무 노드에서 <code>$ hbase thrift &amp;</code> 명령어로 thrift 실행<ul><li>backgroud 구동</li></ul></li></ul></li></ul><h2 id="설치-패키지-2"><a href="#설치-패키지-2" class="headerlink" title="설치 패키지"></a>설치 패키지</h2><h3 id="Linux-2"><a href="#Linux-2" class="headerlink" title="Linux"></a>Linux</h3><h3 id="Python-2"><a href="#Python-2" class="headerlink" title="Python"></a>Python</h3><ul><li>happybase<ul><li>Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from happybase) (1.14.0)</li><li>Requirement already satisfied: thriftpy2&gt;=0.4 in /opt/conda/lib/python3.8/site-packages (from happybase) (0.4.11)</li><li>Requirement already satisfied: ply&lt;4.0,&gt;=3.4 in /opt/conda/lib/python3.8/site-packages (from thriftpy2&gt;=0.4-&gt;happybase) (3.11)</li></ul></li></ul><h2 id="Code-2"><a href="#Code-2" class="headerlink" title="Code"></a>Code</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> happybase</span><br><span class="line"></span><br><span class="line"><span class="comment"># Connection</span></span><br><span class="line"></span><br><span class="line">conn = happybase.Connection(<span class="string">&#x27;192.168.179.183&#x27;</span>, <span class="number">9090</span>, autoconnect=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">## Create </span></span><br><span class="line"></span><br><span class="line">conn.create_table(<span class="string">&#x27;test&#x27;</span>,&#123;<span class="string">&#x27;cf&#x27;</span>:&#123;&#125;&#125;)</span><br><span class="line">conn.tables()</span><br><span class="line"></span><br><span class="line"><span class="comment">## Read</span></span><br><span class="line"></span><br><span class="line">tbl = conn.table(<span class="string">&#x27;test&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> key, data <span class="keyword">in</span> tbl.scan():</span><br><span class="line">    print(key, data)</span><br><span class="line"></span><br><span class="line"><span class="comment">## Update</span></span><br><span class="line"></span><br><span class="line">table = conn.table(<span class="string">&#x27;test&#x27;</span>)</span><br><span class="line">table.put(<span class="string">&#x27;row-key&#x27;</span>,&#123;<span class="string">&#x27;cf:col1&#x27;</span>:<span class="string">&#x27;value1&#x27;</span>,<span class="string">&#x27;cf:col2&#x27;</span>:<span class="string">&#x27;value2&#x27;</span>&#125;)</span><br><span class="line">row = <span class="built_in">dict</span>(table.row(<span class="string">&#x27;row-key&#x27;</span>))</span><br><span class="line"></span><br><span class="line">print(row)</span><br><span class="line"></span><br><span class="line"><span class="comment">## Delete</span></span><br><span class="line"></span><br><span class="line">conn.delete_table(<span class="string">&#x27;test&#x27;</span>, disable=<span class="literal">True</span>)</span><br><span class="line">conn.tables()</span><br></pre></td></tr></table></figure><h2 id="Ref-2"><a href="#Ref-2" class="headerlink" title="Ref"></a>Ref</h2><ul><li><a href="https://creatorw.tistory.com/entry/Hbase%EC%99%80-python-%EC%97%B0%EA%B2%B0-%ED%95%98%EA%B8%B0%EA%B8%B0">https://creatorw.tistory.com/entry/Hbase%EC%99%80-python-%EC%97%B0%EA%B2%B0-%ED%95%98%EA%B8%B0%EA%B8%B0</a></li></ul><h1 id="Impala"><a href="#Impala" class="headerlink" title=" Impala"></a><a name="impala"></a> Impala</h1><ul><li>connection 시 host 는 마스터로 작성하니 connection 이 되지 않음<ul><li>마스터 외 노드 ip로 host 설정 필요</li></ul></li></ul><h2 id="설치-패키지-3"><a href="#설치-패키지-3" class="headerlink" title="설치 패키지"></a>설치 패키지</h2><h3 id="Linux-3"><a href="#Linux-3" class="headerlink" title="Linux"></a>Linux</h3><h3 id="Python-3"><a href="#Python-3" class="headerlink" title="Python"></a>Python</h3><ul><li>impyla==0.15a1 *(꼭 0.15a1 버젼 설치 필요)<ul><li>Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from impyla==0.15a1) (1.14.0)</li><li>Requirement already satisfied: bitarray in /opt/conda/lib/python3.8/site-packages (from impyla==0.15a1) (1.2.1)</li><li>Requirement already satisfied: thrift&gt;=0.9.3 in /opt/conda/lib/python3.8/site-packages (from impyla==0.15a1) (0.13.0)</li><li>Requirement already satisfied: thriftpy2==0.4.0; python_version &gt;= “3.0” in /opt/conda/lib/python3.8/site-packages (from impyla==0.15a1) (  - 0.4.0)</li><li>Requirement already satisfied: ply&lt;4.0,&gt;=3.4 in /opt/conda/lib/python3.8/site-packages (from thriftpy2==0.4.0; python_version &gt;=   - “3.0”-&gt;impyla==0.15a1) (3.11)</li></ul></li></ul><h2 id="Code-3"><a href="#Code-3" class="headerlink" title="Code"></a>Code</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> impala.dbapi <span class="keyword">import</span> connect</span><br><span class="line"></span><br><span class="line"><span class="comment"># Connection</span></span><br><span class="line"></span><br><span class="line">conn = connect(host=<span class="string">&#x27;192.168.179.181&#x27;</span>, port=<span class="number">21050</span>)</span><br><span class="line">cursor = conn.cursor()</span><br><span class="line">cursor.get_databases()</span><br><span class="line"></span><br><span class="line">drop = <span class="string">&quot;DROP TABLE IF EXISTS default.tab;&quot;</span></span><br><span class="line"></span><br><span class="line">create = <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">CREATE EXTERNAL TABLE default.tab</span></span><br><span class="line"><span class="string">(</span></span><br><span class="line"><span class="string">id INT,</span></span><br><span class="line"><span class="string">col_1 BOOLEAN,</span></span><br><span class="line"><span class="string">col_2 DOUBLE,</span></span><br><span class="line"><span class="string">col_3 TIMESTAMP</span></span><br><span class="line"><span class="string">)</span></span><br><span class="line"><span class="string">ROW FORMAT DELIMITED FIELDS TERMINATED BY &#x27;,&#x27;</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## Create</span></span><br><span class="line"></span><br><span class="line">cursor.execute(create)</span><br><span class="line">cursor.execute(<span class="string">&#x27;select count(*) from default.tab&#x27;</span>)</span><br><span class="line">res = cursor.fetchall()</span><br><span class="line">print(res)</span><br><span class="line"></span><br><span class="line"><span class="comment">## Read</span></span><br><span class="line"></span><br><span class="line">cursor.execute(<span class="string">&#x27;select * from tab&#x27;</span>)</span><br><span class="line">res = cursor.fetchall()</span><br><span class="line">print(res)</span><br><span class="line"></span><br><span class="line"><span class="comment">## Update</span></span><br><span class="line"></span><br><span class="line">cursor.execute(<span class="string">&#x27;alter table default.tab rename to default.tab_rev&#x27;</span>)</span><br><span class="line">cursor.execute(<span class="string">&#x27;describe default.tab_rev&#x27;</span>)</span><br><span class="line">res = cursor.fetchall()</span><br><span class="line">print(res)</span><br><span class="line"></span><br><span class="line"><span class="comment">## Delete</span></span><br><span class="line"></span><br><span class="line">cursor.execute(<span class="string">&#x27;drop table default.tab_rev&#x27;</span>)</span><br><span class="line">cursor.execute(<span class="string">&#x27;show tables&#x27;</span>)</span><br><span class="line">res = cursor.fetchall()</span><br><span class="line">print(res)</span><br></pre></td></tr></table></figure><h2 id="Ref-3"><a href="#Ref-3" class="headerlink" title="Ref"></a>Ref</h2><ul><li><a href="https://euriion.com/?p=411856">https://euriion.com/?p=411856</a></li><li><a href="https://m.blog.naver.com/PostView.nhn?blogId=hancury&logNo=220755129588&proxyReferer=https:%2F%2Fwww.google.com%2F">https://m.blog.naver.com/PostView.nhn?blogId=hancury&amp;logNo=220755129588&amp;proxyReferer=https:%2F%2Fwww.google.com%2F</a></li></ul><h1 id="Spark"><a href="#Spark" class="headerlink" title=" Spark"></a><a name="spark"></a> Spark</h1><ul><li>python 3.8 패키지 미지원 <em>(pyspark)</em><ul><li><code>python 3.7</code> 환경에서만 진행</li></ul></li><li>컨테이너 내 <code>hadoop client</code> 및 <code>spark</code> 설치 필요<ul><li>hadoop: <code>$ wget https://archive.apache.org/dist/hadoop/core/hadoop-3.0.0/hadoop-3.0.0.tar.gz</code><ul><li>cdh 클라이언트 구성파일 중 core-site.xml, hdfs-site.xml, yarn-site.xml 을 <code>$HADOOP_HOME/etc/hadoop</code> 으로 copy&amp;paste</li></ul></li><li>spark: <a href="https://archive.apache.org/dist/spark/spark-2.4.0/spark-2.4.0-bin-hadoop2.7.tgz">spark-2.4.0-bin-hadoop2.7.tgz</a></li></ul></li><li><code>.bashrc</code><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64</span><br><span class="line"><span class="built_in">export</span> HADOOP_HOME=/home/jovyan/hadoop</span><br><span class="line"><span class="built_in">export</span> HADOOP_CMD=/home/jovyan/hadoop/bin/hadoop</span><br><span class="line"><span class="built_in">export</span> HADOOP_CONF_DIR=/home/jovyan/hadoop/etc/hadoop</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HADOOP_HOME</span>/bin:<span class="variable">$HADOOP_HOME</span>/sbin:<span class="variable">$JAVA_HOME</span>/bin:<span class="variable">$HADOOP_CMD</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> SPARK_HOME=/home/jovyan/spark</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$SPARK_HOME</span>/bin</span><br></pre></td></tr></table></figure></li><li>CDH spark 구성<ul><li>master: yarn</li><li>deploy mode: client</li></ul></li></ul><h2 id="설치-패키지-4"><a href="#설치-패키지-4" class="headerlink" title="설치 패키지"></a>설치 패키지</h2><h3 id="Linux-4"><a href="#Linux-4" class="headerlink" title="Linux"></a>Linux</h3><ul><li>default-jdk</li></ul><h3 id="Python-4"><a href="#Python-4" class="headerlink" title="Python"></a>Python</h3><ul><li>pyspark<ul><li>Requirement already satisfied: py4j==0.10.7 in /opt/conda/lib/python3.8/site-packages (from pyspark) (0.10.7)</li></ul></li></ul><h2 id="Ref-4"><a href="#Ref-4" class="headerlink" title="Ref"></a>Ref</h2><ul><li><a href="https://www.sicara.ai/blog/2017-05-02-get-started-pyspark-jupyter-notebook-3-minutes">https://www.sicara.ai/blog/2017-05-02-get-started-pyspark-jupyter-notebook-3-minutes</a></li><li><a href="https://docs.cloudera.com/documentation/enterprise/6/6.3/topics/spark_ipython.html">https://docs.cloudera.com/documentation/enterprise/6/6.3/topics/spark_ipython.html</a></li><li><a href="https://github.com/mike-wendt/cloudera-jupyter-notebook-spark">https://github.com/mike-wendt/cloudera-jupyter-notebook-spark</a></li></ul><blockquote><p>위와 같은 환경에서 Spark 연동은 사실 어렵다. 왜냐하면 CDH 내 Yarn 에서 컨테이너 호스트명을 인식하지 못하는 문제가 발생하기 때문이다. 이는 CDH Yarn config 를 수정해서, 컨테이너 호스트명을 바라볼 수 있게끔 변경하면 yarn-client 모드로 연동이 가능하다. 아니면 <code>Livy</code> 라는 패키지를 CDH 클러스터에 설치하여 API 로 연동하는 방법이 존재한다. <em>(구글링 추천)</em></p></blockquote><h1 id="Sybase"><a href="#Sybase" class="headerlink" title=" Sybase"></a><a name="sybase"></a> Sybase</h1><ul><li>jar file<ul><li>jconn3.jar <em>(jdk 버젼별로 상이할 수 있으므로 jconn4.jar 준비 필요)</em></li></ul></li></ul><h2 id="설치-패키지-5"><a href="#설치-패키지-5" class="headerlink" title="설치 패키지"></a>설치 패키지</h2><h3 id="Linux-5"><a href="#Linux-5" class="headerlink" title="Linux"></a>Linux</h3><ul><li>default-jdk</li></ul><h3 id="Python-5"><a href="#Python-5" class="headerlink" title="Python"></a>Python</h3><ul><li>jpype1==0.6.3</li><li>Jaydebeapi</li></ul><h2 id="Code-4"><a href="#Code-4" class="headerlink" title="Code"></a>Code</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> jpype</span><br><span class="line"><span class="keyword">import</span> jaydebeapi</span><br><span class="line"></span><br><span class="line"><span class="comment"># Connection</span></span><br><span class="line"></span><br><span class="line">conn = jaydebeapi.connect(<span class="string">&#x27;com.sybase.jdbc3.jdbc.SybDriver&#x27;</span>, <span class="string">&#x27;jdbc:sybase:Tds:192.168.179.169:2638&#x27;</span>, &#123;<span class="string">&#x27;user&#x27;</span>: <span class="string">&#x27;DBA&#x27;</span>, <span class="string">&#x27;password&#x27;</span>: <span class="string">&#x27;sql&#x27;</span>&#125;</span><br><span class="line">                          ,[<span class="string">&#x27;/home/jovyan/jconn3.jar&#x27;</span>])</span><br><span class="line">c1 = conn.cursor()</span><br><span class="line"></span><br><span class="line"><span class="comment">## Create</span></span><br><span class="line"></span><br><span class="line">c1.execute(<span class="string">&#x27;create table aa(a int)&#x27;</span>)</span><br><span class="line">c1.execute(<span class="string">&#x27;select * from aa&#x27;</span>)</span><br><span class="line">print(c1.fetchall())</span><br><span class="line"></span><br><span class="line"><span class="comment">## Read</span></span><br><span class="line"></span><br><span class="line">c1.execute(<span class="string">&#x27;select * from aa&#x27;</span>)</span><br><span class="line">print(c1.fetchall())</span><br><span class="line"></span><br><span class="line"><span class="comment">## Update</span></span><br><span class="line"></span><br><span class="line">c1.execute(<span class="string">&#x27;insert into aa (a) values (1000)&#x27;</span>)</span><br><span class="line">c1.execute(<span class="string">&#x27;select * from aa&#x27;</span>)</span><br><span class="line">print(c1.fetchall())</span><br><span class="line"></span><br><span class="line"><span class="comment">## Delete</span></span><br><span class="line"></span><br><span class="line">c1.execute(<span class="string">&#x27;drop table aa&#x27;</span>)</span><br><span class="line">c1.execute(<span class="string">&#x27;select * from aa&#x27;</span>)</span><br><span class="line">print(c1.fetchall())</span><br></pre></td></tr></table></figure><h2 id="Ref-5"><a href="#Ref-5" class="headerlink" title="Ref"></a>Ref</h2><ul><li><a href="https://stackoverflow.com/questions/3319788/what-is-the-best-way-to-connect-to-a-sybase-database-from-python">https://stackoverflow.com/questions/3319788/what-is-the-best-way-to-connect-to-a-sybase-database-from-python</a></li></ul><h1 id="Oracle"><a href="#Oracle" class="headerlink" title=" Oracle"></a><a name="oracle"></a> Oracle</h1><ul><li>jar file<ul><li>ojdbc6.jar</li></ul></li></ul><h2 id="설치-패키지-6"><a href="#설치-패키지-6" class="headerlink" title="설치 패키지"></a>설치 패키지</h2><h3 id="Linux-6"><a href="#Linux-6" class="headerlink" title="Linux"></a>Linux</h3><ul><li>default-jdk</li></ul><h3 id="Python-6"><a href="#Python-6" class="headerlink" title="Python"></a>Python</h3><ul><li>jpype1==0.6.3</li><li>Jaydebeapi</li></ul><h2 id="Code-5"><a href="#Code-5" class="headerlink" title="Code"></a>Code</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> jpype</span><br><span class="line"><span class="keyword">import</span> jaydebeapi</span><br><span class="line"></span><br><span class="line"><span class="comment"># Connection</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#jpype.startJVM(jpype.getDefaultJVMPath(), classpath=&#x27;/home/jovyan/ojdbc6.jar&#x27;, convertStrings=True)</span></span><br><span class="line">conn = jaydebeapi.connect(<span class="string">&#x27;oracle.jdbc.driver.OracleDriver&#x27;</span>, <span class="string">&#x27;jdbc:oracle:thin:scott/tiger@192.168.179.167:1521:ORCL1&#x27;</span>, jars=<span class="string">&#x27;/home/jovyan/ojdbc6.jar&#x27;</span>)</span><br><span class="line"></span><br><span class="line">c1 = conn.cursor()</span><br><span class="line"></span><br><span class="line"><span class="comment">## Create</span></span><br><span class="line"></span><br><span class="line">c1.execute(<span class="string">&#x27;create table bb(a number)&#x27;</span>)</span><br><span class="line">c1.execute(<span class="string">&#x27;select * from bb&#x27;</span>)</span><br><span class="line">res = c1.fetchone()</span><br><span class="line">print(res)</span><br><span class="line"></span><br><span class="line"><span class="comment">## Read</span></span><br><span class="line"></span><br><span class="line">c1.execute(<span class="string">&#x27;select * from emp&#x27;</span>)</span><br><span class="line">res = c1.fetchone()</span><br><span class="line">print(res)</span><br><span class="line"></span><br><span class="line"><span class="comment">## Update</span></span><br><span class="line"></span><br><span class="line">c1.execute(<span class="string">&#x27;insert into bb values (1000)&#x27;</span>)</span><br><span class="line">c1.execute(<span class="string">&#x27;select * from bb&#x27;</span>)</span><br><span class="line">print(c1.fetchall())</span><br><span class="line"></span><br><span class="line"><span class="comment">## Delete</span></span><br><span class="line"></span><br><span class="line">c1.execute(<span class="string">&#x27;drop table scott.bb&#x27;</span>)</span><br></pre></td></tr></table></figure><h2 id="Ref-6"><a href="#Ref-6" class="headerlink" title="Ref"></a>Ref</h2><ul><li><a href="https://m.blog.naver.com/PostView.nhn?blogId=axzswq&logNo=221533592504&proxyReferer=https:%2F%2Fwww.google.com%2F">https://m.blog.naver.com/PostView.nhn?blogId=axzswq&amp;logNo=221533592504&amp;proxyReferer=https:%2F%2Fwww.google.com%2F</a></li><li><a href="http://blog.naver.com/PostView.nhn?blogId=delfood&logNo=221666021769&categoryNo=33&parentCategoryNo=0&viewDate=&currentPage=1&postListTopCurrentPage=1&from=search">http://blog.naver.com/PostView.nhn?blogId=delfood&amp;logNo=221666021769&amp;categoryNo=33&amp;parentCategoryNo=0&amp;viewDate=&amp;currentPage=1&amp;postListTopCurrentPage=1&amp;from=search</a></li></ul><hr><p>made by <em>jaejun.lee</em></p>]]></content:encoded>
      
      <comments>https://jx2lee.github.io/python-connection_test/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[Cloud] Kubeflow Katib</title>
      <link>https://jx2lee.github.io/cloud-kubeflow_katib/</link>
      <guid>https://jx2lee.github.io/cloud-kubeflow_katib/</guid>
      <pubDate>Sun, 03 May 2020 15:00:00 GMT</pubDate>
      <description>
      
        &lt;p&gt;Kubeflow 책을 공부하며 내용을 정리하고자 한다. 이번 파트는 &lt;strong&gt;Katib&lt;/strong&gt; 이다.&lt;/p&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p>Kubeflow 책을 공부하며 내용을 정리하고자 한다. 이번 파트는 <strong>Katib</strong> 이다.</p><a id="more"></a><h1 id="Katib"><a href="#Katib" class="headerlink" title="Katib"></a>Katib</h1><p>Kubeflow 설치 시 자동으로 설치하는 컴포넌트로, 하이퍼파라미터 최적화와 뉴럴 아키텍처 탐색(NAS)으로 구성한다. 하이퍼파라미터 최적화와 뉴럴 아키텍처 탐색(NAS)에 대한 개념은 이번 글에서 다루지 않는다.</p><ul><li>하이퍼파라미터 참고<ul><li><a href="https://jx2lee.github.io/2019/07/02/ml-introduction_to_grid_search/">https://jx2lee.github.io/2019/07/02/ml-introduction_to_grid_search/</a></li><li><a href="https://medium.com/@jorgesleonel/hyperparameters-in-machine-deep-learning-ca69ad10b981">Hyperparameters in Machine /Deep Learning</a></li></ul></li><li>뉴럴 아키텍쳐 탐색(NAS)<ul><li><a href="http://research.sualab.com/review/2018/09/28/nasnet-review.html">AutoML을 이용한 Architecture Search 소개 및 NASNet 논문 리뷰</a></li><li><a href="http://www.secmem.org/blog/2019/07/19/Network-Architecture-Search/">Network Architecture Search - Samsung Software Membership</a></li></ul></li></ul><h1 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h1><p>Katib 은 크게 4가지 개념으로 이루어졌다.</p><p><img src="/image/katib-architecture.png" alt="Katib 구조"></p><ul><li>Experiment : 하나의 실행단위로 Job 개념으로 생각하면 된다. K8s 커스텀 리소스로 Trial 를 실행하는 역할을 하며 Experiment 는 5개 영역으로 나뉜다.<ul><li>Trial Count : 실행 횟수 (병렬)</li><li>Trial Template : Trial 파드 템플릿</li><li>Objective : 목표 수치 (최곳값 또는 최솟값 설정)</li><li>Search Parameter : 탐색하고자 하는 파라미터 값의 range</li><li>Search Algorithm : 탐색 알고리즘</li></ul></li><li>Trial : 최적화 과정의 반복 단위. Experiment의 Trial Count 값 만큼 Trial을 생성하고 순차적으로 실행. 하나의 Trial 에서 하나의 worker job을 실행, Trial 도 K8s 커스텀 리소스</li><li>Suggestion : Search Algorithm 으로 생성한 하이퍼파라미터 값의 모음. 하나의 Experiment 에서 하나의 Suggestion 을 생성</li><li>Worker job : 파라미터와 Suggestion 값을 이용해 Trial, 각각의 값을 평가하고 계사하는 프로세스. 실제로 학습을 수행하며 K8s Job 과 TFJob, PyTorch 을 사용</li></ul><h2 id="Experiment-example"><a href="#Experiment-example" class="headerlink" title="Experiment example"></a>Experiment example</h2><p>MNIST 예제 템플릿을 함께 보며 위에서 설명한 구조와 비교하며 살펴본다.</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">&quot;kubeflow.org/v1alpha3&quot;</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Experiment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kubeflow</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">controller-tools.k8s.io:</span> <span class="string">&quot;1.0&quot;</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">handson-experiment-1</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">parallelTrialCount:</span> <span class="number">5</span><span class="comment"># 병렬로 실행할 Trial 수로, 리소스 허용한도까지 동시에 5개 Trial 을 실행한다.</span></span><br><span class="line">  <span class="attr">maxTrialCount:</span> <span class="number">30</span> <span class="comment"># 최대로 실행할 Trial 수로 총 30개. parallelTrialCount 가 5 이므로 병렬로 5개 Trial 을 실행하고 6번 반복</span></span><br><span class="line">  <span class="attr">maxFailedTrialCount:</span> <span class="number">3</span>  <span class="comment"># 실패 한도 수로 Trial 이 3번 실패하면 Experiment 를 중지한다. </span></span><br><span class="line">  <span class="attr">objective:</span>  <span class="comment"># 수집 대상의 메트릭 설정 단계 </span></span><br><span class="line">    <span class="attr">type:</span> <span class="string">maximize</span>  <span class="comment"># 최댓값 또는 최솟값 설정 (본 예제는 maximize, 최댓값)</span></span><br><span class="line">    <span class="attr">goal:</span> <span class="number">0.99</span><span class="comment"># 목표 수치 설정</span></span><br><span class="line">    <span class="attr">objectiveMetricName:</span> <span class="string">validation-accuracy</span>  <span class="comment"># 수집할 메트릭 name, validation-accuracy 로 설정</span></span><br><span class="line">    <span class="attr">additionalMetricNames:</span>  <span class="comment"># 이외 수집할 메트릭을 정의 (accuracy, loss, validation-loss 3개를 추가 수집할 예정)</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">accuracy</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">loss</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">Validation-loss</span></span><br><span class="line">  <span class="attr">algorithm:</span>  <span class="comment"># Search Algorithm 설정 (그리드, 랜덤, 하이퍼밴드, 베이지안최적화 선택 가능)</span></span><br><span class="line">    <span class="attr">algorithmName:</span> <span class="string">random</span></span><br><span class="line">  <span class="attr">trialTemplate:</span>  <span class="comment"># Trial 템플릿 정의</span></span><br><span class="line">    <span class="attr">goTemplate:</span></span><br><span class="line">        <span class="attr">rawTemplate:</span> <span class="string">|-</span></span><br><span class="line">          <span class="attr">apiVersion:</span> <span class="string">batch/v1</span></span><br><span class="line">          <span class="attr">kind:</span> <span class="string">Job</span></span><br><span class="line">          <span class="attr">metadata:</span></span><br><span class="line">            <span class="attr">name:</span> &#123;&#123;<span class="string">.Trial</span>&#125;&#125;</span><br><span class="line">            <span class="attr">namespace:</span> &#123;&#123;<span class="string">.NameSpace</span>&#125;&#125;</span><br><span class="line">          <span class="attr">spec:</span></span><br><span class="line">            <span class="attr">template:</span></span><br><span class="line">              <span class="attr">spec:</span></span><br><span class="line">                <span class="attr">containers:</span></span><br><span class="line">                <span class="bullet">-</span> <span class="attr">name:</span> &#123;&#123;<span class="string">.Trial</span>&#125;&#125;</span><br><span class="line">                  <span class="attr">image:</span> <span class="string">brightfly/katib-job:handson</span></span><br><span class="line">                  <span class="attr">command:</span></span><br><span class="line">                  <span class="bullet">-</span> <span class="string">&quot;python&quot;</span></span><br><span class="line">                  <span class="bullet">-</span> <span class="string">&quot;/app/katib_keras_mnist.py&quot;</span></span><br><span class="line">                  &#123;&#123;<span class="bullet">-</span> <span class="string">with</span> <span class="string">.HyperParameters</span>&#125;&#125;<span class="comment"># 설정 파라미터의 iteration 구문. .Name=.Value 형태로 메트릭 수집</span></span><br><span class="line">                  &#123;&#123;<span class="bullet">-</span> <span class="string">range</span> <span class="string">.</span>&#125;&#125;</span><br><span class="line">                  <span class="bullet">-</span> <span class="string">&quot;<span class="template-variable">&#123;&#123;.Name&#125;&#125;</span>=<span class="template-variable">&#123;&#123;.Value&#125;&#125;</span>&quot;</span></span><br><span class="line">                  &#123;&#123;<span class="bullet">-</span> <span class="string">end</span>&#125;&#125;</span><br><span class="line">                  &#123;&#123;<span class="bullet">-</span> <span class="string">end</span>&#125;&#125;</span><br><span class="line">                <span class="attr">restartPolicy:</span> <span class="string">Never</span></span><br><span class="line">  <span class="attr">parameters:</span> <span class="comment"># 하이퍼파라미터 입력값으로 learning rate 와 dropout 설정</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">--learning_rate</span></span><br><span class="line">      <span class="attr">parameterType:</span> <span class="string">double</span></span><br><span class="line">      <span class="attr">feasibleSpace:</span>  <span class="comment"># 각 하이퍼파라미터의 range 설정</span></span><br><span class="line">        <span class="attr">min:</span> <span class="string">&quot;0.01&quot;</span></span><br><span class="line">        <span class="attr">max:</span> <span class="string">&quot;0.03&quot;</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">--dropout_rate</span></span><br><span class="line">      <span class="attr">parameterType:</span> <span class="string">double</span></span><br><span class="line">      <span class="attr">feasibleSpace:</span></span><br><span class="line">        <span class="attr">min:</span> <span class="string">&quot;0.1&quot;</span></span><br><span class="line">        <span class="attr">max:</span> <span class="string">&quot;0.9&quot;</span></span><br></pre></td></tr></table></figure><p>본 템플릿으로 Experiment 를 실행하면 <code>python /app/katib_keras_mnist.py -learning_rate=0.012--dropout_rate=0.381</code> 와 같이 실행한다.</p><h1 id="Katib-Component"><a href="#Katib-Component" class="headerlink" title="Katib Component"></a>Katib Component</h1><p>Katib 을 구성하는 컴포넌트로는 총 4개가 존재한다. 각 컴포넌트는 kubectl 로 조회 가능하며 K8s의 Deployment 로 관리한다.</p><ul><li>katib-manager : GRPC API server</li><li>katib-db : Katib 의 백엔드 저장소, mysql</li><li>katib-ui : Katib UI</li><li>katib-controller : katib CRD의 컨트롤러</li></ul><h1 id="Katib-UI"><a href="#Katib-UI" class="headerlink" title="Katib UI"></a>Katib UI</h1><p>Web UI를 제공하는데 크게 Hyperparameter Tuning 과 NAS 2개 메뉴가 존재한다. Hyperparameter Tuning 에서는 직접 YAML 을 작성하거나 마우스와 키보드로 값을 추가할 수 있는 페이지를 제공한다.</p><p><img src="https://www.kubeflow.org/docs/images/katib-home.png" alt="Katib UI"></p><h1 id="Katib-Command-line-interface"><a href="#Katib-Command-line-interface" class="headerlink" title="Katib Command-line interface"></a>Katib Command-line interface</h1><p>UI 외에 커맨드라인 인터페이스를 제공하는데, kfctl 또는 kubectl 을 이용해 Experiment 을 실행할 수 있다. 단, Experiment 리소스 권한이 존재해야한다.</p><p><code>kubectl apply -f mnist_experiment_random.yaml</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@master my-kubeflow]<span class="comment"># kubectl get pod -n kubeflow</span></span><br><span class="line">NAME                                                           READY   STATUS             RESTARTS   AGE</span><br><span class="line">admission-webhook-bootstrap-stateful-set-0                     1/1     Running            1          26d</span><br><span class="line">admission-webhook-deployment-68c6dd4cc5-sb6hq                  1/1     Running            0          6d23h</span><br><span class="line">application-controller-stateful-set-0                          1/1     Running            1          26d</span><br><span class="line">argo-ui-78bf45b698-r5zhr                                       1/1     Running            0          26d</span><br><span class="line">centraldashboard-6957f8bcbc-6nktd                              1/1     Running            1          26d</span><br><span class="line">handson-experiment-1-random-57698b477b-dzmwm                   1/1     Running            0          98s</span><br><span class="line">...</span><br></pre></td></tr></table></figure><blockquote><p>mnist_experimnet_random.yaml : (<a href="https://github.com/mojokb/handson-kubeflow/blob/master/katib/mnist_experiment_random.yaml">https://github.com/mojokb/handson-kubeflow/blob/master/katib/mnist_experiment_random.yaml</a>)</p></blockquote><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li><a href="http://book.interpark.com/product/BookDisplay.do?_method=detail&sc.prdNo=332046194&gclid=CjwKCAjwqJ_1BRBZEiwAv73uwCjVo3_lnw6A88qkbJmo2pBfMRE2p047vy7Cg4VhMsimg_dqSpxewBoCKwsQAvD_BwE">쿠버네티스에서 머신러닝이 처음이라면! 쿠브플로우!</a></li><li><a href="https://www.kubeflow.org">Kubeflow.org</a></li></ul><hr><p>made by <em>jaejun.lee</em></p>]]></content:encoded>
      
      <comments>https://jx2lee.github.io/cloud-kubeflow_katib/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[Cloud] Kubeflow Fairing</title>
      <link>https://jx2lee.github.io/cloud-kubeflow_fairing/</link>
      <guid>https://jx2lee.github.io/cloud-kubeflow_fairing/</guid>
      <pubDate>Mon, 27 Apr 2020 15:00:00 GMT</pubDate>
      <description>
      
        &lt;p&gt;Kubeflow 책을 공부하며 내용을 정리하고자 한다. 이번 파트는 &lt;strong&gt;Fairing&lt;/strong&gt; 이다. &lt;/p&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p>Kubeflow 책을 공부하며 내용을 정리하고자 한다. 이번 파트는 <strong>Fairing</strong> 이다. </p><a id="more"></a><h1 id="Fairing"><a href="#Fairing" class="headerlink" title="Fairing"></a>Fairing</h1><p>Kubeflow 환경에서 ML 모델을 손쉽게 학습-배포할 수 있는 Python Package</p><h1 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h1><ul><li>work flow<ul><li>python 으로 작성한 파일을 도커 이미지로 빌드</li><li>빌드된 이미지를 레지스트리 push</li><li>배포 리소스에 따라 k8s Job, TFJob, KFServing 등으로 변환하여 k8s API 서버 요청</li></ul></li><li>위 work flow 는 <em>preprocessor, builder, deployer</em> 구조로 설계<ul><li><em>preprocessor</em> : Python 파일을 도커 이미지로 빌드할 수 있게 패키지화</li><li><em>builder</em> : 패키지된 파일을 도커 이미지화</li><li><em>deployer</em> : 생성한 이미지 배포</li></ul></li></ul><p>Fairing 은 Kubeflow 설치 후 생성한 노트북 이미지에는 default 로 설정되어 있어 따로 설치를 하지 않아도 테스트가 가능하다.</p><h1 id="Fairing-예제"><a href="#Fairing-예제" class="headerlink" title="Fairing 예제"></a>Fairing 예제</h1><p>Fairing 예제는 Kubeflow 노트북 서버에서 진행한다. 앞서 소개했듯이 Fairing 은 K8s 리소스를 이용하기 때문에 docker registry 와 kubeflow 접근 권한이 필요하다. 테스트 결과, private docker registry 에 fairing 이미지를 push/pull 할 때 에러가 발생하였다. 장애 해결이 이루어지지 않아 docker hub 의 공개 레지스트리를 사용할 예정이며, 책 순서에 따라 진행한다.</p><h2 id="fairing-config-ipynb"><a href="#fairing-config-ipynb" class="headerlink" title="fairing_config.ipynb"></a>fairing_config.ipynb</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># in Jupyter Notebook</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> kubeflow.fairing <span class="keyword">as</span> fairing</span><br><span class="line"></span><br><span class="line">docker_registry = <span class="string">&quot;jaejunlee&quot;</span></span><br><span class="line"></span><br><span class="line">fairing.config.set_builder(</span><br><span class="line">    <span class="string">&#x27;append&#x27;</span>,</span><br><span class="line">    base_image=<span class="string">&#x27;gcr.io/kubeflow-images-public/tensorflow-1.14.0-notebook-cpu:v0.7.0&#x27;</span>,</span><br><span class="line">    image_name=<span class="string">&#x27;fairing-test&#x27;</span>,</span><br><span class="line">    registry=docker_registry, push=<span class="literal">True</span>)</span><br><span class="line">fairing.config.set_deployer(<span class="string">&#x27;job&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>():</span></span><br><span class="line">    hostname = tf.constant(os.environ[<span class="string">&#x27;HOSTNAME&#x27;</span>])</span><br><span class="line">    sess = tf.Session()</span><br><span class="line">    print(<span class="string">&#x27;Hostname : &#x27;</span>, sess.run(hostname).decode(<span class="string">&#x27;utf-8&#x27;</span>))</span><br><span class="line"></span><br><span class="line">remote_train = fairing.config.fn(train)</span><br><span class="line">remote_train()</span><br></pre></td></tr></table></figure><p>위 예제는 HOSTNAME 환경변수를 출력한다. <code>gcr.io/kubeflow-images-public/tensorflow-1.14.0-notebook-cpu:v0.7.0</code> 기본 이미지에 위 출력함수를 입힌 이미지를 생성하여 jaejunlee public registry 에 푸쉬하고 fairing-job 이란 이름의 k8s job 이 k8s 에 실행을 요청한다. 쉽게 말해, 기본 이미지 위에 담아 fairing 이미지를 생성하고 k8s 를 이용해 정의한 함수를 k8s 에서 실행한다. <code>hub.docker.io</code> 에 접속하여 확인하면 해당 프로세스를 실행할 때 마다 이미지를 push 한다. <em>(set_builder 부분 push=True 로 설정했기 때문이다.)</em></p><p>코드를 통해 Config 클래스는 preprocessor, builder, deployer 에 대응하는 setter들을 가지고 있다. 각각의 default 값은 다음과 같다.</p><ul><li><em>preprocessor</em> : Notebook 환경은 “notebook”, else “python”</li><li><em>builder</em> : “append”</li><li><em>deploy”</em> : job</li></ul><p>Fairing 구조 3개를 살펴보도록 한다.</p><h1 id="Preprocessor-in-Fairing"><a href="#Preprocessor-in-Fairing" class="headerlink" title="Preprocessor in Fairing"></a>Preprocessor in Fairing</h1><p>preprocessor 는 도커 이미지로 패키지화 할 대상을 설정하는데, 타입은 총 4개이다.</p><ul><li><p><em>python</em> : 파이썬 file 패키지화</p><ul><li>도커 이미지 내 app/{파일명}.py 로 cmd 생성</li><li>example<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os, time</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>():</span></span><br><span class="line">    print(<span class="string">&quot;Training...&quot;</span>)</span><br><span class="line">    <span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">    x = tf.constant([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>], shape=[<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">    print(x)</span><br><span class="line">    time.sleep(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="keyword">if</span> os.getenv(<span class="string">&#x27;FAIRING_RUNTIME&#x27;</span>, <span class="literal">None</span>) <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:  <span class="comment"># if else 구조를 통해 이미지가 생성되면 FAIRING_RUNTIME 변수가 1 로 변환된다. 즉, 이미지 생성후에는 train() 만 작업</span></span><br><span class="line">        train()</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">from</span> kubeflow <span class="keyword">import</span> fairing</span><br><span class="line">        DOCKER_REGISTRY = <span class="string">&#x27;jaejunlee&#x27;</span></span><br><span class="line">        file_name = os.path.basename(__file__)</span><br><span class="line">        print(<span class="string">&quot;Executing &#123;&#125; remotely.&quot;</span>.<span class="built_in">format</span>(file_name))</span><br><span class="line">        fairing.config.set_preprocessor(<span class="string">&#x27;python&#x27;</span>, executable=file_name, input_files=[file_name])</span><br><span class="line">        fairing.config.set_builder(</span><br><span class="line">            <span class="string">&#x27;append&#x27;</span>, base_image=<span class="string">&#x27;gcr.io/kubeflow-images-public/tensorflow-1.14.0-notebook-cpu:v0.7.0&#x27;</span>,</span><br><span class="line">            registry=DOCKER_REGISTRY, push=<span class="literal">True</span>)</span><br><span class="line">        fairing.config.run()</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">$ chmod +x fairing_preprocessor_python.py</span><br><span class="line">$ ./fairing_preprocessor_python.py</span><br><span class="line">[I 200428 09:36:11 config:134] Using preprocessor: &lt;kubeflow.fairing.preprocessors.base.BasePreProcessor object at 0x7f8b2fc1e278&gt;</span><br><span class="line">[I 200428 09:36:11 config:136] Using builder: &lt;kubeflow.fairing.builders.append.append.AppendBuilder object at 0x7f8b2fc1e2b0&gt;</span><br><span class="line">[I 200428 09:36:11 config:138] Using deployer: &lt;kubeflow.fairing.deployers.job.job.Job object at 0x7f8b2fc1e9b0&gt;</span><br><span class="line">[W 200428 09:36:11 append:50] Building image using Append builder...</span><br><span class="line">[I 200428 09:36:11 base:107] Creating docker context: /tmp/fairing_context_jvkk0mqm</span><br><span class="line">[I 200428 09:36:11 docker_creds_:234] Loading Docker credentials <span class="keyword">for</span> repository <span class="string">&#x27;gcr.io/kubeflow-images-public/tensorflow-1.14.0-notebook-cpu:v0.7.0&#x27;</span></span><br><span class="line">[W 200428 09:36:13 append:54] Image successfully built <span class="keyword">in</span> 1.9876068960002158s.</span><br><span class="line">[W 200428 09:36:13 append:94] Pushing image jaejunlee/fairing-job:13B00B9B...</span><br><span class="line">[I 200428 09:36:13 docker_creds_:234] Loading Docker credentials <span class="keyword">for</span> repository <span class="string">&#x27;jaejunlee/fairing-job:13B00B9B&#x27;</span></span><br><span class="line">[W 200428 09:36:15 append:81] Uploading jaejunlee/fairing-job:13B00B9B</span><br><span class="line">[I 200428 09:36:16 docker_session_:280] Layer sha256:823f4685c03b26a545ca41dcdca1e782ad5e52cf85bac03113edaa6aebdca1b3 exists, skipping</span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line">sha256:19f71f3a178549ee1bacd534f061e4b465d85c3378ecddb4dc716f1283aff2a8 pushed.</span><br><span class="line">[I 200428 09:36:22 docker_session_:334] Finished upload of: jaejunlee/fairing-job:13B00B9B</span><br><span class="line">[W 200428 09:36:22 append:99] Pushed image jaejunlee/fairing-job:13B00B9B <span class="keyword">in</span> 9.045510983996792s.</span><br><span class="line">[W 200428 09:36:22 job:101] The job fairing-job-gm8hb launched.</span><br><span class="line">[W 200428 09:36:22 manager:296] Waiting <span class="keyword">for</span> fairing-job-gm8hb-87z5h to start...</span><br><span class="line">[W 200428 09:36:22 manager:296] Waiting <span class="keyword">for</span> fairing-job-gm8hb-87z5h to start...</span><br><span class="line">[W 200428 09:36:22 manager:296] Waiting <span class="keyword">for</span> fairing-job-gm8hb-87z5h to start...</span><br><span class="line">[W 200428 09:36:23 manager:296] Waiting <span class="keyword">for</span> fairing-job-gm8hb-87z5h to start...</span><br><span class="line">[I 200428 09:36:29 manager:302] Pod started running True</span><br><span class="line">Training...</span><br><span class="line">Tensor(<span class="string">&quot;Const:0&quot;</span>, shape=(2, 3), dtype=int32)</span><br><span class="line">[W 200428 09:36:31 job:173] Cleaning up job fairing-job-gm8hb...</span><br></pre></td></tr></table></figure></li></ul></li><li><p><em>notebook</em> : jupyter notebook 내용을 파이썬 file 로 변환하여 파이썬 file 을 패키지화  </p><ul><li>example<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># jupyter notebook</span><br><span class="line"># fairing_preprocessor_notebook.ipynb</span><br><span class="line"></span><br><span class="line">import os, time</span><br><span class="line"></span><br><span class="line">def train():</span><br><span class="line">    print(&quot;Training...&quot;)</span><br><span class="line">    import tensorflow as tf</span><br><span class="line">    x &#x3D; tf.constant([1,2,3,4,5,6], shape&#x3D;[2,3])</span><br><span class="line">    print(x)</span><br><span class="line">    time.sleep(1)</span><br><span class="line"></span><br><span class="line">if __name__ &#x3D;&#x3D; &#39;__main__&#39;:</span><br><span class="line">train()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># fairing_preprocessor_notebook.py</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> kubeflow <span class="keyword">import</span> fairing</span><br><span class="line"></span><br><span class="line">DOCKER_REGISTRY = <span class="string">&#x27;jaejunlee&#x27;</span></span><br><span class="line">file_name = os.path.basename(__file__)</span><br><span class="line">print(<span class="string">&quot;Executing &#123;&#125; remotely.&quot;</span>.<span class="built_in">format</span>(file_name))</span><br><span class="line">fairing.config.set_preprocessor(<span class="string">&#x27;notebook&#x27;</span>, notebook_file=<span class="string">&#x27;fairing_preprocessor_notebook.ipynb&#x27;</span>)</span><br><span class="line">fairing.config.set_builder(</span><br><span class="line">    <span class="string">&#x27;append&#x27;</span>, base_image=<span class="string">&#x27;gcr.io/kubeflow-images-public/tensorflow-1.14.0-notebook-cpu:v0.7.0&#x27;</span>,</span><br><span class="line">    registry=DOCKER_REGISTRY, push=<span class="literal">True</span>)</span><br><span class="line">fairing.config.run()</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">$ chmod +x fairing_preprocessor_notebook.py</span><br><span class="line">$ ./fairing_preprocessor_notebook.py</span><br><span class="line">[I 200428 09:44:27 config:134] Using preprocessor: &lt;kubeflow.fairing.preprocessors.converted_notebook.ConvertNotebookPreprocessor object at 0x7fd480eb0908&gt;</span><br><span class="line">[I 200428 09:44:27 config:136] Using builder: &lt;kubeflow.fairing.builders.append.append.AppendBuilder object at 0x7fd480eb0860&gt;</span><br><span class="line">[I 200428 09:44:27 config:138] Using deployer: &lt;kubeflow.fairing.deployers.job.job.Job object at 0x7fd4809d6240&gt;</span><br><span class="line">[W 200428 09:44:27 append:50] Building image using Append builder...</span><br><span class="line">[I 200428 09:44:27 base:107] Creating docker context: /tmp/fairing_context_b254bkb_</span><br><span class="line">[I 200428 09:44:27 converted_notebook:127] Converting fairing_preprocessor_notebook.ipynb to fairing_preprocessor_notebook.py</span><br><span class="line">[I 200428 09:44:27 docker_creds_:234] Loading Docker credentials <span class="keyword">for</span> repository <span class="string">&#x27;gcr.io/kubeflow-images-public/tensorflow-1.14.0-notebook-cpu:v0.7.0&#x27;</span></span><br><span class="line">[W 200428 09:44:29 append:54] Image successfully built <span class="keyword">in</span> 1.9862057870050194s.</span><br><span class="line">[W 200428 09:44:29 append:94] Pushing image jaejunlee/fairing-job:7FCC2CF6...</span><br><span class="line">[I 200428 09:44:29 docker_creds_:234] Loading Docker credentials <span class="keyword">for</span> repository <span class="string">&#x27;jaejunlee/fairing-job:7FCC2CF6&#x27;</span></span><br><span class="line">[W 200428 09:44:31 append:81] Uploading jaejunlee/fairing-job:7FCC2CF6</span><br><span class="line">[I 200428 09:44:31 docker_session_:280] Layer </span><br><span class="line">sha256:380fe9d3ba2fe8c69d05cc9038b72aa9ec669cd0d51b0e61f312edb13586d5a8 pushed.</span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line">[I 200428 09:44:38 docker_session_:334] Finished upload of: jaejunlee/fairing-job:7FCC2CF6</span><br><span class="line">[W 200428 09:44:38 append:99] Pushed image jaejunlee/fairing-job:7FCC2CF6 <span class="keyword">in</span> 9.018262255005538s.</span><br><span class="line">[W 200428 09:44:38 job:101] The job fairing-job-bbpjs launched.</span><br><span class="line">[W 200428 09:44:38 manager:296] Waiting <span class="keyword">for</span> fairing-job-bbpjs-5jq22 to start...</span><br><span class="line">[W 200428 09:44:38 manager:296] Waiting <span class="keyword">for</span> fairing-job-bbpjs-5jq22 to start...</span><br><span class="line">[W 200428 09:44:38 manager:296] Waiting <span class="keyword">for</span> fairing-job-bbpjs-5jq22 to start...</span><br><span class="line">[W 200428 09:44:39 manager:296] Waiting <span class="keyword">for</span> fairing-job-bbpjs-5jq22 to start...</span><br><span class="line">[I 200428 09:44:44 manager:302] Pod started running True</span><br><span class="line">Training...</span><br><span class="line">Tensor(<span class="string">&quot;Const:0&quot;</span>, shape=(2, 3), dtype=int32)</span><br><span class="line">[W 200428 09:44:47 job:173] Cleaning up job fairing-job-bbpjs...</span><br></pre></td></tr></table></figure></li></ul></li><li><p><em>full_notebook</em> : jupyter notebook 패키지화 하는데, 수행 후 결과를 다시 노트북 파일로 생성</p></li><li><p><em>function</em> : 단일 함수 패키지화</p></li></ul><h1 id="Builder-in-Fairing"><a href="#Builder-in-Fairing" class="headerlink" title="Builder in Fairing"></a>Builder in Fairing</h1><p>Builder 는 preprocessor 가 생성한 패키지를 도커 이미지화 한다.</p><ul><li><p>빌드 타입</p><ul><li>append : docker client 를 사용하지 않고 파이선 라이브러리를 이용해 이미지를 빌드하는 방식<ul><li>self-signed 로 인증된 레지스트리는 사용 불가</li><li><code>.local</code> 로 설정한 주소는 허용 가능 (kubeflow 용 레지스트리 구축시)</li><li>로그인이 필요한 레지스트리, 즉 docker hub 를 이용하려면 노트북 컨테이너 내 <code>.docker/config.json</code> 파일을 수정해야한다. 수정하는 방법은, 도커 서버에서 docker loging 을 통해 생성한 <code>config.json</code> 파일을 노트북 컨테이너 <code>~/.docker/config.json</code> 으로 복사한다.<ul><li>cluster : 구글 컨테이너 툴인 Kaniko 를 이용해 이미지를 빌드하는 방식</li><li>docker : 도커 클라이언트를 이용해 이미지를 빌드하는 방식</li></ul></li><li>해당 환경이 도커 레지스트리에 접근할 수 있는 권한이 존재해야함</li></ul></li></ul></li><li><p>format</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">DOCKER_REGISTRY=<span class="string">&#x27;&#123;이미지를 push/pull 주소&#125;&#x27;</span></span><br><span class="line">fairing.config.set_builder(</span><br><span class="line">    <span class="string">&#x27;&#123;build type&#125;&#x27;</span>,</span><br><span class="line">    base_image=<span class="string">&#x27;&#123;python 을 실행한 기본 이미지 명&#125;:&#123;태그&#125;&#x27;</span>,</span><br><span class="line">    registry=DOCKER_REGISTRY,</span><br><span class="line">    push=True)</span><br></pre></td></tr></table></figure></li></ul><h1 id="Deployer-in-Fairing"><a href="#Deployer-in-Fairing" class="headerlink" title="Deployer in Fairing"></a>Deployer in Fairing</h1><p>Deployer 는 builder 로 생성한 이미지를 배포한다.</p><ul><li>format</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">fairing.config.set_deployer(<span class="string">&#x27;job&#x27;</span>,</span><br><span class="line">                       namespace=?,</span><br><span class="line">                       pod_spec_mutators=?                         </span><br><span class="line">                       )</span><br></pre></td></tr></table></figure><ul><li>배포형태 : job, tfjob, pytorchjob, serving, kfserving, gcpjob, gcpserving등</li><li>namespace : 배포가 실행할 namespace</li><li>pod_spec_mutators : 배포 pod 의 spec 정의</li></ul><p>이렇게 각각 정의한 Preprocessor, Builder, Deployer 를 통해 <code>Config.run</code> 으로 Fairing 을 실행한다. 실행 순서는 설명한 흐름대로 preprocessor 로 패키지화 할 대상을 선택하고, Builder 로 이미지를 생성하며 Deployer 로 배포한다.</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li><a href="http://book.interpark.com/product/BookDisplay.do?_method=detail&sc.prdNo=332046194&gclid=CjwKCAjwqJ_1BRBZEiwAv73uwCjVo3_lnw6A88qkbJmo2pBfMRE2p047vy7Cg4VhMsimg_dqSpxewBoCKwsQAvD_BwE">쿠버네티스에서 머신러닝이 처음이라면! 쿠브플로우!</a></li></ul><hr><p>made by <em>jaejun.lee</em></p>]]></content:encoded>
      
      <comments>https://jx2lee.github.io/cloud-kubeflow_fairing/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[Shell] k8s Pod 내 alias 적용 스크립트</title>
      <link>https://jx2lee.github.io/shell-append_alias_to_pod/</link>
      <guid>https://jx2lee.github.io/shell-append_alias_to_pod/</guid>
      <pubDate>Thu, 23 Apr 2020 15:00:00 GMT</pubDate>
      <description>
      
        &lt;p&gt;사내 클라우드 제품을 이용해 업무를 보던 중, 계속되는 패치 작업으로 매번 생성하는 컨테이너의 alias 가 사라지는 문제가 발생하였다. 이러한 문제를 해결하고자 패치 이후 새로 생성하는 파드를 검색하고 &lt;code&gt;.bashrc&lt;/code&gt; 에 alias 를 추가하는 스크립트를 작성하였다.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Update Note&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;2020.05.07 : 스크립트 개선 (exec, alias 두 개 함수로 분리)&lt;/li&gt;
&lt;/ul&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p>사내 클라우드 제품을 이용해 업무를 보던 중, 계속되는 패치 작업으로 매번 생성하는 컨테이너의 alias 가 사라지는 문제가 발생하였다. 이러한 문제를 해결하고자 패치 이후 새로 생성하는 파드를 검색하고 <code>.bashrc</code> 에 alias 를 추가하는 스크립트를 작성하였다.</p><p><strong>Update Note</strong></p><ul><li>2020.05.07 : 스크립트 개선 (exec, alias 두 개 함수로 분리)</li></ul><a id="more"></a><h1 id="문제-발생"><a href="#문제-발생" class="headerlink" title="문제 발생"></a>문제 발생</h1><p>패치가 진행되면 해당 파드의 이미지를 교체해야 한다. 이 작업을 수행하면 기존 파드를 삭제하고 교체된 이미지로 파드를 재생성하는데, 그럼 기존 파드에서 작업을 하던 alias 들이 사라진다. (당연히 컨테이너가 재기동하면서 <code>.bashrc</code> 초기화)</p><ul><li>기존 사용하는 alias</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">alias dasboot=&#x27;startDomainAdminServer -u jeus -p jeus&#x27;</span><br><span class="line">alias dasdown=&#x27;stopServer -host localhost:9736 -u jeus -p jeus&#x27;</span><br><span class="line"></span><br><span class="line">alias hdstart=&#x27;startManagedServer -server hyperdata -u jeus -p jeus&#x27;</span><br><span class="line">alias hdstop=&#x27;stopServer -host localhost:19736 -u jeus -p jeus&#x27;</span><br><span class="line"></span><br><span class="line">alias pastart=&#x27;startManagedServer -server ProAuth -u jeus -p jeus&#x27;</span><br><span class="line">alias pastop=&#x27;stopServer -host localhost:29736 -u jeus -p jeus&#x27;</span><br><span class="line"></span><br><span class="line">alias polog=&#x27;tail -100f /hyperdata/proobject7/logs/ProObject.log&#x27;</span><br><span class="line">alias slog=&#x27;tail -100f /db/tibero6/instance/tibero/log/slog/sys.log&#x27;</span><br></pre></td></tr></table></figure><h1 id="Script-설계-방안"><a href="#Script-설계-방안" class="headerlink" title="Script 설계 방안"></a>Script 설계 방안</h1><p>우선 고려할 점은 교체한 이미지로 기동한 파드를 찾아야 한다. 이는 <code>grep</code> 과 <code>awk</code> 를 이용해 파드 정보를 조회하여 <strong>파드 이름</strong>을 검색하고 <code>kubectl exec</code> 으로 <code>.bashrc</code> 에 alias 를 추가한다. 당연히 추가만 한다고 적용이 안되므로 마지막에 <code>source ~/.bashrc</code> 명령어를 파드에 넘겨주고 접속하는 방향으로 설계를 하였다.</p><p>정리하면 다음과 같다.</p><ul><li>파드 이름 검색</li><li>alias 를 <code>.bashrc</code> 에 추가</li><li>bash 적용을 위한 <code>source</code> 명령어 전달</li><li>파드 접근</li></ul><blockquote><p><em>사내 클라우드에서는 namespace 가 default 로 정의되어 있다. 이는 바뀌지 않기 때문에 스크립트 안에 default 로 설정하였다.</em></p></blockquote><h1 id="Script"><a href="#Script" class="headerlink" title="Script"></a>Script</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/sh</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> script <span class="keyword">for</span> managing hd container</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> jaejun.lee.1991@gmail.com</span></span><br><span class="line"></span><br><span class="line">namespace=$(kubectl get namespace  | grep hpcd | awk &#x27;&#123;print $1&#125;&#x27;)</span><br><span class="line">pod=$(kubectl describe pod -n hpcd-510fdc58 | grep -B 20 hyperdata8.3_hd | grep &quot;Name:           hpcd&quot; | awk &#x27;&#123;print $2&#125;&#x27;)</span><br><span class="line"></span><br><span class="line">function hd_container_exec() &#123;</span><br><span class="line">echo &quot;[INFO] exec $pod in $namespace&quot; | grep &quot;[INFO]&quot; --color</span><br><span class="line">kubectl exec -ti -n $namespace $pod -- bash</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">function append_alias() &#123;</span><br><span class="line">echo &quot;[INFO] append alias to $pod&quot; | grep &quot;[INFO]&quot; --color</span><br><span class="line">kubectl exec -n $namespace $pod -- bash -c &#x27;echo alias dasboot=&quot;\&quot;startDomainAdminServer -u jeus -p jeus&quot;\&quot; &gt;&gt; ~/.bashrc&#x27;</span><br><span class="line">kubectl exec -n $namespace $pod -- bash -c &#x27;echo alias dasdown=&quot;\&quot;stopServer -host localhost:9736 -u jeus -p jeus&quot;\&quot; &gt;&gt; ~/.bashrc&#x27;</span><br><span class="line">kubectl exec -n $namespace $pod -- bash -c &#x27;echo alias pastart=&quot;\&quot;startManagedServer -server ProAuth -u jeus -p jeus&quot;\&quot; &gt;&gt; ~/.bashrc&#x27;</span><br><span class="line">kubectl exec -n $namespace $pod -- bash -c &#x27;echo alias pastop=&quot;\&quot;stopServer -host localhost:29736 -u jeus -p jeus&quot;\&quot; &gt;&gt; ~/.bashrc&#x27;</span><br><span class="line">kubectl exec -n $namespace $pod -- bash -c &#x27;echo alias hdstart=&quot;\&quot;startManagedServer -server hyperdata -u jeus -p jeus&quot;\&quot; &gt;&gt; ~/.bashrc&#x27;</span><br><span class="line">kubectl exec -n $namespace $pod -- bash -c &#x27;echo alias hdstop=&quot;\&quot;stopServer -host localhost:19736 -u jeus -p jeus&quot;\&quot; &gt;&gt; ~/.bashrc&#x27;</span><br><span class="line">kubectl exec -n $namespace $pod -- bash -c &#x27;echo alias polog=&quot;\&quot;tail -f /hyperdata/proobject7/logs/ProObject.log&quot;\&quot; &gt;&gt; ~/.bashrc&#x27;</span><br><span class="line">kubectl exec -n $namespace $pod -- bash -c &#x27;echo alias slog=&quot;\&quot;tail -f /db/tibero6/instance/tibero/log/slog/sys.log&quot;\&quot; &gt;&gt; ~/.bashrc&#x27;</span><br><span class="line">kubectl exec -n $namespace $pod -- bash -c &#x27;source ~/.bashrc&#x27;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">function main() &#123;</span><br><span class="line">  case &quot;$&#123;1:-&#125;&quot; in</span><br><span class="line">  exec)</span><br><span class="line">    hd_container_exec</span><br><span class="line">    ;;</span><br><span class="line">  alias)</span><br><span class="line">    append_alias</span><br><span class="line">    ;;</span><br><span class="line">  *)</span><br><span class="line">    set +x</span><br><span class="line">    echo &quot;usage:&quot; &gt;&amp;2</span><br><span class="line">    echo &quot;   $0 exec&quot; &gt;&amp;2</span><br><span class="line">    echo &quot;   $0 alias&quot; &gt;&amp;2</span><br><span class="line">    ;;</span><br><span class="line">  esac</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">main $1</span><br></pre></td></tr></table></figure><ul><li><del>6번째 줄 : alias 를 적용할 파드 이름을 검색</del></li><li><del>9번째 줄 ~ 29번째 줄 : <code>source ~/.bashrc</code> 이전 : alias 를 <code>.bashrc</code> 에 추가</del></li><li><del>30번째 줄 : <code>.bashrc</code> 적용</del></li><li><del>31번째 줄 : 파드 접근</del></li><li>hd_container_exec : 특정 컨테이너<em>(hd)</em> 로 접근하는 함수</li><li>append_alias : 특정 컨테이너<em>(hd)</em> 내 <code>.bashrc</code> 파일에 alias 를 추가하고 적용하는 함수</li><li>main : <code>exec</code> or <code>alias</code> 인자를 받는 메인 함수</li></ul><hr><p>made by <em>jaejun.lee</em></p>]]></content:encoded>
      
      <comments>https://jx2lee.github.io/shell-append_alias_to_pod/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[Shell] 두 docker registry 간 이미지 최신화</title>
      <link>https://jx2lee.github.io/shell-image_updater/</link>
      <guid>https://jx2lee.github.io/shell-image_updater/</guid>
      <pubDate>Wed, 08 Apr 2020 15:00:00 GMT</pubDate>
      <description>
      
        &lt;p&gt;최근들어 이미지 패치가 자주 이루어지면서, 이미지를 일일이 풀(pull)하고 울팀 레지스트리에 푸쉬(push)하는 작업이 지속적으로 발생하였다. 최신화가 이루어지는 레지스트리를 daemon / insecure-registry 에 등록해 사용해도 되지만 스크립트 짜는 연습도 할 겸 이미지 최신화 스크립트를 작성하였고 이를 소개하고자 한다.&lt;/p&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p>최근들어 이미지 패치가 자주 이루어지면서, 이미지를 일일이 풀(pull)하고 울팀 레지스트리에 푸쉬(push)하는 작업이 지속적으로 발생하였다. 최신화가 이루어지는 레지스트리를 daemon / insecure-registry 에 등록해 사용해도 되지만 스크립트 짜는 연습도 할 겸 이미지 최신화 스크립트를 작성하였고 이를 소개하고자 한다.</p><a id="more"></a><h1 id="상황"><a href="#상황" class="headerlink" title="상황"></a>상황</h1><p>간단히 A팀, B팀(내가 속한)에 대해 간략히 설명하면 다음과 같다.</p><ul><li>A팀 : 이미지를 최신화 하며 이미지 태그는 날짜_v?으로 설정한다. 최신 이미지는 a 레지스트리에 등록한다.</li><li>B팀 : 테스트 작업을 마친 최신 이미지를 사용해 패치하고 이를 다시 테스트 한다. b 레지스트리를 이용한다.</li></ul><p>사실 수작업으로 일일이 이미지 태그를 확인하며 pull&amp;push 해도 된다. docker pull a/이미지:태그, docker tag a/이미지:태그 b/이미지:태그, docker push b/이미지:태그.. 하나의 제품을 정상기동하려면 <strong>4</strong>개의 이미지를 사용하니 굉장히 불편하였다. 이미지 최적화가 안되어서 그런지 한 이미지의 pull&amp;push 가 2분정도 걸리는 경우도 존재한다.</p><p>이를 타파하고자 간단한 쉘 스크립트를 작성하여 모두 update 하던가, 한 이미지만 update 하게끔 만들었다.</p><h1 id="Usage"><a href="#Usage" class="headerlink" title="Usage"></a>Usage</h1><p>컨피그 파일 하나와 쉘 스크립트 하나가 존재한다. 컨피그 파일(registry.config)은 pull 하기 위한 레지스트리 주소와 push 하기 위한 레지스트리 주소를 작성한다. 이 중 하나라도 작성하지 않으면 ERROR가 발생한다. 간단히 살펴보자.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">❯ ./updateImage.sh</span><br><span class="line">usage:</span><br><span class="line">   ./updateImage.sh all</span><br><span class="line">   ./updateImage.sh img &#123;image_name&#125;</span><br></pre></td></tr></table></figure><p>쉘 스크립트는 크게 <code>all</code> 과 <code>img</code>가 있다. registry.config 를 모두 작성했다는 가정하에 스크립트를 실행해본다.</p><ul><li><p><strong>all</strong> : 총 4개의 최신 이미지를 pull&amp;push</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">❯ ./updateImage.sh all</span><br><span class="line">[PULL REGISTRY] 192.xxx.xxx.xxx</span><br><span class="line">[PUSH REGISTRY] 192.yyy.yyy.yyy</span><br><span class="line">...</span><br><span class="line">...</span><br></pre></td></tr></table></figure></li><li><p><strong>img</strong> : 4개 중 원하는 이미지를 pull&amp;push</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">❯ ./updateImage.sh img</span><br><span class="line">[PULL REGISTRY] 192.xxx.xxx.xxx</span><br><span class="line">[PUSH REGISTRY] 192.yyy.yyy.yyy</span><br><span class="line">Enter the image name : ???</span><br><span class="line">...</span><br><span class="line">...</span><br></pre></td></tr></table></figure><ul><li>image name 을 작성하면 그 이미지의 최신 태그를 찾고 이를 pull&amp;push 한다.</li><li><del>최사 제품명이 혹시나 노출되면 안될까 싶어 수행 결과는 작성하지 않았따.</del></li></ul></li></ul><h1 id="Script"><a href="#Script" class="headerlink" title="Script"></a>Script</h1><p>스크립트는 <a href="https://github.com/jx2lee/image-updater">내 깃헙</a>에 올려두었지만 하기에도 있다. <del>난 착한 편이다.</del></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"><span class="comment"># Thu, 08.04.2020</span></span><br><span class="line"><span class="comment"># jaejun.lee.1991@gmail.com</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#include</span></span><br><span class="line">base_dir=$(dirname <span class="string">&quot;<span class="variable">$0</span>&quot;</span>)</span><br><span class="line">. <span class="variable">$&#123;base_dir&#125;</span>/registry.config</span><br><span class="line"></span><br><span class="line"><span class="keyword">function</span> <span class="function"><span class="title">check_env</span></span>()&#123;</span><br><span class="line">    <span class="keyword">if</span> [ -z <span class="variable">$&#123;pull_registry&#125;</span> ] || [ -z <span class="variable">$&#123;push_registry&#125;</span> ]; <span class="keyword">then</span></span><br><span class="line">      <span class="built_in">echo</span> <span class="string">&quot;[ERROR] You must set registry variables in registry.config!&quot;</span> | grep <span class="string">&quot;[ERROR]&quot;</span> --color</span><br><span class="line">      <span class="built_in">exit</span> 1</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">      <span class="built_in">echo</span> <span class="string">&quot;[PULL REGISTRY] <span class="variable">$&#123;pull_registry&#125;</span>&quot;</span> | grep <span class="string">&quot;[PULL REGISTRY]&quot;</span> --color</span><br><span class="line">      <span class="built_in">echo</span> <span class="string">&quot;[PUSH REGISTRY] <span class="variable">$&#123;push_registry&#125;</span>&quot;</span> | grep <span class="string">&quot;[PUSH REGISTRY]&quot;</span> --color</span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">function</span> <span class="function"><span class="title">check_img</span></span>() &#123;</span><br><span class="line">    image_pattern=<span class="string">&#x27;hyperdata8.3_&#x27;</span></span><br><span class="line">    <span class="built_in">echo</span> -n <span class="string">&quot;Enter the image name : &quot;</span></span><br><span class="line">    <span class="built_in">read</span> name</span><br><span class="line">    <span class="keyword">if</span> [[ <span class="variable">$&#123;name&#125;</span> == *<span class="variable">$&#123;image_pattern&#125;</span>* ]];<span class="keyword">then</span></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">&quot;Update <span class="variable">$name</span> image..&quot;</span></span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">&quot;[ERROR] you must enter the image on hyperdata8.3_&#123;tb, hl, efa, hd&#125;!&quot;</span> | grep <span class="string">&quot;[ERROR]&quot;</span> --color</span><br><span class="line">        <span class="built_in">exit</span> 1</span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">function</span> <span class="function"><span class="title">update_all</span></span>()&#123;</span><br><span class="line">    image_list=( <span class="string">&quot;hyperdata8.3_hd&quot;</span> <span class="string">&quot;hyperdata8.3_tb&quot;</span> <span class="string">&quot;hyperdata8.3_hl&quot;</span> <span class="string">&quot;hyperdata8.3_eda&quot;</span> )</span><br><span class="line">    <span class="keyword">for</span> image <span class="keyword">in</span> <span class="variable">$&#123;image_list[@]&#125;</span>;<span class="keyword">do</span></span><br><span class="line">        version=$(curl -X GET <span class="variable">$&#123;pull_registry&#125;</span>/v2/<span class="variable">$&#123;image&#125;</span>/tags/list | jq -r <span class="string">&#x27;.tags | .[-1]&#x27;</span>)</span><br><span class="line">        docker pull <span class="variable">$&#123;pull_registry&#125;</span>/<span class="variable">$&#123;image&#125;</span>:<span class="variable">$&#123;version&#125;</span></span><br><span class="line">        docker tag <span class="variable">$&#123;pull_registry&#125;</span>/<span class="variable">$&#123;image&#125;</span>:<span class="variable">$&#123;version&#125;</span> <span class="variable">$&#123;push_registry&#125;</span>/<span class="variable">$&#123;image&#125;</span>:<span class="variable">$&#123;version&#125;</span></span><br><span class="line">        docker push <span class="variable">$&#123;push_registry&#125;</span>/<span class="variable">$&#123;image&#125;</span>:<span class="variable">$&#123;version&#125;</span></span><br><span class="line">    <span class="keyword">done</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">function</span> <span class="function"><span class="title">update_once</span></span>()&#123;</span><br><span class="line">    version=$(curl -X GET <span class="variable">$&#123;pull_registry&#125;</span>/v2/<span class="variable">$&#123;name&#125;</span>/tags/list | jq -r <span class="string">&#x27;.tags | .[-1]&#x27;</span>)</span><br><span class="line">    docker pull <span class="variable">$&#123;pull_registry&#125;</span>/<span class="variable">$&#123;name&#125;</span>:<span class="variable">$&#123;version&#125;</span></span><br><span class="line">    docker tag <span class="variable">$&#123;pull_registry&#125;</span>/<span class="variable">$&#123;name&#125;</span>:<span class="variable">$&#123;version&#125;</span> <span class="variable">$&#123;push_registry&#125;</span>/<span class="variable">$&#123;name&#125;</span>:<span class="variable">$&#123;version&#125;</span></span><br><span class="line">    docker push <span class="variable">$&#123;push_registry&#125;</span>/<span class="variable">$&#123;name&#125;</span>:<span class="variable">$&#123;version&#125;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">function</span> <span class="function"><span class="title">main</span></span>() &#123;</span><br><span class="line">  <span class="keyword">case</span> <span class="string">&quot;<span class="variable">$&#123;1:-&#125;</span>&quot;</span> <span class="keyword">in</span></span><br><span class="line">  all)</span><br><span class="line">    check_env</span><br><span class="line">    update_all</span><br><span class="line">    ;;</span><br><span class="line">  img)</span><br><span class="line">    check_env</span><br><span class="line">    check_img</span><br><span class="line">    update_once</span><br><span class="line">    ;;</span><br><span class="line">  *)</span><br><span class="line">    <span class="built_in">set</span> +x</span><br><span class="line">    <span class="built_in">echo</span> <span class="string">&quot;usage:&quot;</span> &gt;&amp;2</span><br><span class="line">    <span class="built_in">echo</span> <span class="string">&quot;   <span class="variable">$0</span> all&quot;</span> &gt;&amp;2</span><br><span class="line">    <span class="built_in">echo</span> <span class="string">&quot;   <span class="variable">$0</span> img hyperdata8.3_hd&quot;</span> &gt;&amp;2</span><br><span class="line">    ;;</span><br><span class="line">  <span class="keyword">esac</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">main <span class="variable">$1</span></span><br></pre></td></tr></table></figure><hr><p>made by <em>jaejun.lee</em></p>]]></content:encoded>
      
      <comments>https://jx2lee.github.io/shell-image_updater/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[Cloud] Metallb for Kubernetes 설치</title>
      <link>https://jx2lee.github.io/cloud-install_metallb/</link>
      <guid>https://jx2lee.github.io/cloud-install_metallb/</guid>
      <pubDate>Sun, 05 Apr 2020 15:00:00 GMT</pubDate>
      <description>
      
        &lt;p&gt;Kubernetes 에 로드밸런서 생성을 위해 설치한 Metallb 설치 과정을 다뤄본다. 서비스를 특정 IP 로 노출하기 위해 Metallb config 설정과 서비스로 노출하는 단계로 설명한다. &lt;/p&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p>Kubernetes 에 로드밸런서 생성을 위해 설치한 Metallb 설치 과정을 다뤄본다. 서비스를 특정 IP 로 노출하기 위해 Metallb config 설정과 서비스로 노출하는 단계로 설명한다. </p><a id="more"></a><h1 id="Metallb-란"><a href="#Metallb-란" class="headerlink" title="Metallb 란"></a>Metallb 란</h1><h2 id="MetalLB는-표준-라우팅-프로토콜을-사용하여-베어-메탈-깡통-Kubernetes-클러스터에-대한-로드-밸런서-구현-이라고-한다-표준-네트워크-장비와-통합되는-네트워크-LB-구현을-제공하여-베어-메탈-클러스터의-외부-서비스도-가능하게-만들어주는-것을-목표로-한다"><a href="#MetalLB는-표준-라우팅-프로토콜을-사용하여-베어-메탈-깡통-Kubernetes-클러스터에-대한-로드-밸런서-구현-이라고-한다-표준-네트워크-장비와-통합되는-네트워크-LB-구현을-제공하여-베어-메탈-클러스터의-외부-서비스도-가능하게-만들어주는-것을-목표로-한다" class="headerlink" title="MetalLB는 표준 라우팅 프로토콜을 사용하여 베어 메탈 (깡통) Kubernetes 클러스터에 대한 로드 밸런서 구현 이라고 한다. 표준 네트워크 장비와 통합되는 네트워크 LB 구현을 제공하여 베어 메탈 클러스터의 외부 서비스도 가능하게 만들어주는 것을 목표로 한다."></a>MetalLB는 표준 라우팅 프로토콜을 사용하여 베어 메탈 (깡통) Kubernetes 클러스터에 대한 <strong>로드 밸런서 구현</strong> 이라고 한다. 표준 네트워크 장비와 통합되는 네트워크 LB 구현을 제공하여 베어 메탈 클러스터의 외부 서비스도 가능하게 만들어주는 것을 목표로 한다.</h2><blockquote><p><em>프라이빗 클라우드 환경에서 벤더사를 이용하지 않는 kubernetes 운영의 경우, 공유 IP 만 존재한다면 이를 로드밸런서로 활용할 수 있게 만들어주는 장점이 있다.</em></p></blockquote><h1 id="Metallb-설치"><a href="#Metallb-설치" class="headerlink" title="Metallb 설치"></a>Metallb 설치</h1><p><a href="https://raw.githubusercontent.com/google/metallb/v0.7.3/manifests/metallb.yaml">github</a> 의 공식 배포 야믈을 특정 디렉토리 <em>(ex. metallb)</em> 에 다운받아 배포한다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p /data/jlee/metallb</span><br><span class="line">wget -O metallb.yaml https://raw.githubusercontent.com/google/metallb/v0.7.3/manifests/metallb.yaml</span><br><span class="line">kubectl apply -f metallb.yaml</span><br></pre></td></tr></table></figure><p>이후 생성하는 metallb-system 네임스페이스에 파드를 정상 배포하였는지 확인한다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">root@k8s-node2:/data/jlee<span class="comment"># kubectl get pods -n metallb-system</span></span><br><span class="line">NAME                          READY   STATUS    RESTARTS   AGE</span><br><span class="line">controller-547d466688-25w69   1/1     Running   0          2d21h</span><br><span class="line">speaker-dfhwc                 1/1     Running   0          2d21h</span><br><span class="line">speaker-kpc2n                 1/1     Running   0          2d21h</span><br><span class="line">speaker-lfjv9                 1/1     Running   0          2d21h</span><br><span class="line">speaker-p9qlt                 1/1     Running   0          2d21h</span><br></pre></td></tr></table></figure><h1 id="Metallb-Configmap-배포"><a href="#Metallb-Configmap-배포" class="headerlink" title="Metallb Configmap 배포"></a>Metallb Configmap 배포</h1><p>로드밸런서 생성에 대한 컨피그 맵 야믈을 생성하고 배포한다. 이름은 <code>metallb-Configmap.yaml</code> 로 설정하였다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">root@k8s-node2:/data/jlee/metallb<span class="comment"># cat metallb-ConfigMap.yaml</span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ConfigMap</span><br><span class="line">metadata:</span><br><span class="line">  namespace: metallb-system</span><br><span class="line">  name: config</span><br><span class="line">data:</span><br><span class="line">  config: |</span><br><span class="line">    address-pools:</span><br><span class="line">    - name: my-ip-space</span><br><span class="line">      protocol: layer2</span><br><span class="line">      addresses:</span><br><span class="line">      - 192.168.179.184-192.168.179.195</span><br></pre></td></tr></table></figure><ul><li>adresses 부분은 로드밸런서로 사용할 IP 대역대를 설정한다. 나의 경우, 184-195 까지 여유 IP 가 존재하기 때문에 range 를 추가하였다.<ul><li>만약 하나의 IP 로 설정할 경우 IP 하나만 작성한다.</li></ul></li><li>정상 배포하였는지 configmap 을 출력한다.<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">root@k8s-node2:/data/jlee/metallb<span class="comment"># kubectl get configmap -n metallb-system -o yaml</span></span><br><span class="line">apiVersion: v1</span><br><span class="line">items:</span><br><span class="line">- apiVersion: v1</span><br><span class="line">  data:</span><br><span class="line">    config: |</span><br><span class="line">      address-pools:</span><br><span class="line">      - name: my-ip-space</span><br><span class="line">        protocol: layer2</span><br><span class="line">        addresses:</span><br><span class="line">        - 192.168.179.184-192.168.179.195</span><br><span class="line">  kind: ConfigMap</span><br><span class="line">  metadata:</span><br><span class="line">    annotations:</span><br><span class="line">      kubectl.kubernetes.io/last-applied-configuration: |</span><br><span class="line">        &#123;<span class="string">&quot;apiVersion&quot;</span>:<span class="string">&quot;v1&quot;</span>,<span class="string">&quot;data&quot;</span>:&#123;<span class="string">&quot;config&quot;</span>:<span class="string">&quot;address-pools:\n- name: my-ip-space\n  protocol: layer2\n  addresses:\n  - 192.168.179.184-192.168.179.195\n&quot;</span>&#125;,<span class="string">&quot;kind&quot;</span>:<span class="string">&quot;ConfigMap&quot;</span>,<span class="string">&quot;metadata&quot;</span>:&#123;<span class="string">&quot;annotations&quot;</span>:&#123;&#125;,<span class="string">&quot;name&quot;</span>:<span class="string">&quot;config&quot;</span>,<span class="string">&quot;namespace&quot;</span>:<span class="string">&quot;metallb-system&quot;</span>&#125;&#125;</span><br><span class="line">    creationTimestamp: <span class="string">&quot;2020-04-03T09:56:44Z&quot;</span></span><br><span class="line">    name: config</span><br><span class="line">    namespace: metallb-system</span><br><span class="line">    resourceVersion: <span class="string">&quot;13110630&quot;</span></span><br><span class="line">    selfLink: /api/v1/namespaces/metallb-system/configmaps/config</span><br><span class="line">    uid: 49765aca-9364-48c2-9f9a-4fa2304651e8</span><br><span class="line">kind: List</span><br><span class="line">metadata:</span><br><span class="line">  resourceVersion: <span class="string">&quot;&quot;</span></span><br><span class="line">  selfLink: <span class="string">&quot;&quot;</span></span><br></pre></td></tr></table></figure></li></ul><h1 id="서비스-배포-후-확인"><a href="#서비스-배포-후-확인" class="headerlink" title="서비스 배포 후 확인"></a>서비스 배포 후 확인</h1><p>4개 파드 통신을 위한 서비스 야믈 파일에 type 을 로드밸런서로 설정하고 서비스를 배포한다.</p><p><em>loadbalancer.yaml</em></p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="string">..</span></span><br><span class="line"><span class="string">..</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="string">...</span></span><br><span class="line">  <span class="string">...</span></span><br><span class="line">  <span class="string">...</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">LoadBalancer</span></span><br><span class="line">  <span class="attr">loadBalancerIP:</span> <span class="number">192.168</span><span class="number">.179</span><span class="number">.184</span> <span class="comment"># 이 부분 수정 필요</span></span><br></pre></td></tr></table></figure><ul><li><code>loadBalancerIP</code> 값을 위 <strong>metallb-Configmap.yaml 내 IP 범위에 포함한 하나</strong>로 사용한다.</li><li>이후 배포한 서비스를 확인하면 <code>EXTERNA-IP</code> 에 IP 가 표시될 것이다.<ul><li>반드시 <code>ip 값은 configmap ip range 안</code>에서 사용해야한다.</li><li><code>kubectl get svc</code><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">NAME       TYPE           CLUSTER-IP    EXTERNAL-IP       PORT(S)                                                                                                                                                                      AGE</span><br><span class="line">hyper-lb   LoadBalancer   10.96.25.50   192.168.179.184   20:31010/TCP,21:31020/TCP,22:31030/TCP,23:31040/TCP,80:31060/TCP,8080:31080/TCP,9736:31085/TCP,8629:31090/TCP,8630:31100/TCP,28080:31160/TCP,1883:31190/TCP,2883:31200/TCP   6h42m</span><br></pre></td></tr></table></figure></li></ul></li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li><a href="https://metallb.universe.tf">https://metallb.universe.tf</a></li><li><a href="https://ssup2.github.io/record/Kubernetes_MetalLB_설치_Ubuntu_18.04/">https://ssup2.github.io/record/Kubernetes_MetalLB_설치_Ubuntu_18.04/</a></li></ul><hr><p>made by <em>jaejun.lee</em></p>]]></content:encoded>
      
      <comments>https://jx2lee.github.io/cloud-install_metallb/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[Cloud] kubectl 명령어 수행 시 실행속도 저하 문제</title>
      <link>https://jx2lee.github.io/cloud-kubectl_hang/</link>
      <guid>https://jx2lee.github.io/cloud-kubectl_hang/</guid>
      <pubDate>Mon, 30 Mar 2020 15:00:00 GMT</pubDate>
      <description>
      
        &lt;p&gt;도커 루트 디렉토리를 변경하던 도중, k8s 클러스터에서 kubectl 명령어가 느려지는 문제가 발생하였다. 원인을 파악하고 이를 해결하는 과정을 다룬다.&lt;/p&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p>도커 루트 디렉토리를 변경하던 도중, k8s 클러스터에서 kubectl 명령어가 느려지는 문제가 발생하였다. 원인을 파악하고 이를 해결하는 과정을 다룬다.</p><a id="more"></a><h1 id="문제-발생"><a href="#문제-발생" class="headerlink" title="문제 발생"></a>문제 발생</h1><p>kubectl 를 사용하면 결과는 나오지만 엄청 오래 걸리는 문제가 발생하였다. <code>time kubectl get nodes</code> 를 수행하면 다음과 같이 결과가 나왔다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">NAME         STATUS   ROLES    AGE   VERSION</span><br><span class="line">k8s-master   Ready    master   14d   v1.15.3</span><br><span class="line">k8s-node1    Ready    master   14d   v1.15.3</span><br><span class="line">k8s-node2    Ready    master   14d   v1.15.3</span><br><span class="line">k8s-node3    Ready    &lt;none&gt;   14d   v1.15.3</span><br><span class="line">k8s-node4    Ready    &lt;none&gt;   14d   v1.15.3</span><br><span class="line"></span><br><span class="line">real    2m1.032s</span><br><span class="line">user    2m0.089s</span><br><span class="line">sys    2m0.041s</span><br></pre></td></tr></table></figure><h1 id="원인-파악"><a href="#원인-파악" class="headerlink" title="원인 파악"></a>원인 파악</h1><ul><li><p><code>kube-system</code> 의 파드 상태</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">root@k8s-master:/data<span class="comment"># kubectl get pods -n kube-system</span></span><br><span class="line">NAME                                       READY   STATUS    RESTARTS   AGE</span><br><span class="line">calico-kube-controllers-56cd854695-hvnkl   1/1     Running   0          7d</span><br><span class="line">calico-node-4f2bt                          1/1     Running   0          7d</span><br><span class="line">calico-node-bhk4z                          1/1     Running   0          7d</span><br><span class="line">calico-node-kmvm9                          1/1     Running   0          7d</span><br><span class="line">calico-node-q928k                          1/1     Running   0          7d</span><br><span class="line">calico-node-snf8z                          0/1     Evicted   0          90m</span><br><span class="line">coredns-5c98db65d4-7665n                   1/1     Running   35         10d</span><br><span class="line">coredns-5c98db65d4-7hpxb                   1/1     Running   34         10d</span><br><span class="line">etcd-k8s-master                            1/1     Running   3          14d</span><br><span class="line">etcd-k8s-node1                             1/1     Running   0          14d</span><br><span class="line">etcd-k8s-node2                             1/1     Running   0          14d</span><br><span class="line">kube-apiserver-k8s-master                  1/1     Running   3          5d23h</span><br><span class="line">kube-apiserver-k8s-node1                   1/1     Running   0          5d23h</span><br><span class="line">kube-apiserver-k8s-node2                   1/1     Running   0          5d23h</span><br><span class="line">kube-controller-manager-k8s-master         1/1     Running   3          14d</span><br><span class="line">kube-controller-manager-k8s-node1          1/1     Running   7          14d</span><br><span class="line">kube-controller-manager-k8s-node2          1/1     Running   8          14d</span><br><span class="line">kube-proxy-4dhqc                           1/1     Running   0          14d</span><br><span class="line">kube-proxy-9v87c                           1/1     Running   0          14d</span><br><span class="line">kube-proxy-pfrwf                           1/1     Running   0          14d</span><br><span class="line">kube-proxy-tsdb6                           0/1     Evicted   0          91m</span><br><span class="line">kube-proxy-vsk6r                           1/1     Running   0          14d</span><br><span class="line">kube-scheduler-k8s-master                  1/1     Running   2          14d</span><br><span class="line">kube-scheduler-k8s-node1                   1/1     Running   6          14d</span><br><span class="line">kube-scheduler-k8s-node2                   1/1     Running   8          14d</span><br><span class="line">tiller-deploy-758bcdc94f-c92cw             1/1     Running   0          14d</span><br></pre></td></tr></table></figure><ul><li>Evcited 된 캘리코 노드 파드가 보인다. 분명 k8s-master 노드에서 발생한 것으로 보인다. (미리 캡쳐를 떠놓지 못해 -o wide 옵션을 준 결과는 없다) </li><li><code>calico-node-snf8z</code> 파드를 describe 해보자.</li></ul></li><li><p><code>calico-node-snf8z</code> 캘리코 파드</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">root@k8s-master:/data<span class="comment"># kd pod -n kube-system calico-node-snf8z</span></span><br><span class="line">Name:                 calico-node-snf8z</span><br><span class="line">Namespace:            kube-system</span><br><span class="line">Priority:             2000001000</span><br><span class="line">Priority Class Name:  system-node-critical</span><br><span class="line">Node:                 k8s-master/</span><br><span class="line">Start Time:           Tue, 31 Mar 2020 14:14:44 +0900</span><br><span class="line">Labels:               controller-revision-hash=5744776c47</span><br><span class="line">                    k8s-app=calico-node</span><br><span class="line">                    pod-template-generation=1</span><br><span class="line">Annotations:          scheduler.alpha.kubernetes.io/critical-pod:</span><br><span class="line">Status:               Failed</span><br><span class="line">Reason:               Evicted</span><br><span class="line">Message:              The node was low on resource: ephemeral-storage.</span><br><span class="line">...</span><br><span class="line">...</span><br></pre></td></tr></table></figure><ul><li><p><code>The node was low on resource: ephemeral-storage.</code> 메세지가 눈에 띈다.</p></li><li><p>구글링을 통해 알아본 결과, 해당 파드가 배포된 노드 용량이 부족하다 느끼면 파드를 띄우지 못하고 Evicted 상태로 변한다고 한다.</p><ul><li>상태를 보아하니 docker root directory 가 /var/lib 가 default 로 설정되어 있어 이미지 등 데이터가 많이 쌓이는 문제가 발생하였다. <em>(루트 전체 디렉토리의 약 80% 이상을 차지하고 있었다. 이는 full 나지 않아도 kubernetest 가 설정한 적정 용량(?)에서만 파드를 띄우게끔 설계된 것으로 보인다-뇌피셜)</em></li><li>이를 해결하고자 docker root directory 를 변경하였다. <a href="">참고</a></li></ul></li><li><p>이후 <strong>Evicted 된 캘리코 노드 파드를 재기동</strong>하였다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker restart 7feda4bd2162</span><br></pre></td></tr></table></figure></li></ul></li></ul><h1 id="결과"><a href="#결과" class="headerlink" title="결과"></a>결과</h1><p>해당 컨테이너를 재시작하고 <code>kube-system</code> 파드 상태를 확인한 결과, 모두 정상 작동하고 있으며 kubectl 명령어 실행도 이전처럼 빠르게 복구되었다. 이를 통해 kubectl 명령어어가 느려진 이유는 *”<strong>노드 용량 및 노드 간 통신</strong>일 수도 있다”* 라는 것을 깨달았다. 이외에도 많은 부분에서 명령어 실행 시간이 길어질 수 있는데 그때마다 글에 추가할 예정이다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">root@k8s-master:/data<span class="comment"># kgpo -n kube-system</span></span><br><span class="line">NAME                                       READY   STATUS    RESTARTS   AGE     IP                NODE         NOMINATED NODE   READINESS GATES</span><br><span class="line">calico-kube-controllers-56cd854695-hvnkl   1/1     Running   0          7d      10.244.169.130    k8s-node2    &lt;none&gt;           &lt;none&gt;</span><br><span class="line">calico-node-2wvj8                          1/1     Running   0          54s     192.168.179.172   k8s-master   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">calico-node-4f2bt                          1/1     Running   0          7d      192.168.179.176   k8s-node4    &lt;none&gt;           &lt;none&gt;</span><br><span class="line">calico-node-bhk4z                          1/1     Running   0          7d      192.168.179.174   k8s-node2    &lt;none&gt;           &lt;none&gt;</span><br><span class="line">calico-node-kmvm9                          1/1     Running   0          7d      192.168.179.175   k8s-node3    &lt;none&gt;           &lt;none&gt;</span><br><span class="line">calico-node-q928k                          1/1     Running   0          7d      192.168.179.173   k8s-node1    &lt;none&gt;           &lt;none&gt;</span><br><span class="line">coredns-5c98db65d4-7665n                   1/1     Running   35         10d     10.244.36.71      k8s-node1    &lt;none&gt;           &lt;none&gt;</span><br><span class="line">coredns-5c98db65d4-7hpxb                   1/1     Running   34         10d     10.244.169.148    k8s-node2    &lt;none&gt;           &lt;none&gt;</span><br><span class="line">etcd-k8s-master                            1/1     Running   3          14d     192.168.179.172   k8s-master   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">etcd-k8s-node1                             1/1     Running   0          14d     192.168.179.173   k8s-node1    &lt;none&gt;           &lt;none&gt;</span><br><span class="line">etcd-k8s-node2                             1/1     Running   0          14d     192.168.179.174   k8s-node2    &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-apiserver-k8s-master                  1/1     Running   3          5d23h   192.168.179.172   k8s-master   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-apiserver-k8s-node1                   1/1     Running   0          5d23h   192.168.179.173   k8s-node1    &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-apiserver-k8s-node2                   1/1     Running   0          5d23h   192.168.179.174   k8s-node2    &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-controller-manager-k8s-master         1/1     Running   3          14d     192.168.179.172   k8s-master   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-controller-manager-k8s-node1          1/1     Running   7          14d     192.168.179.173   k8s-node1    &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-controller-manager-k8s-node2          1/1     Running   8          14d     192.168.179.174   k8s-node2    &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-proxy-4dhqc                           1/1     Running   0          14d     192.168.179.175   k8s-node3    &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-proxy-8qtnp                           1/1     Running   0          54s     192.168.179.172   k8s-master   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-proxy-9v87c                           1/1     Running   0          14d     192.168.179.174   k8s-node2    &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-proxy-pfrwf                           1/1     Running   0          14d     192.168.179.173   k8s-node1    &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-proxy-vsk6r                           1/1     Running   0          14d     192.168.179.176   k8s-node4    &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-scheduler-k8s-master                  1/1     Running   2          14d     192.168.179.172   k8s-master   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-scheduler-k8s-node1                   1/1     Running   6          14d     192.168.179.173   k8s-node1    &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-scheduler-k8s-node2                   1/1     Running   8          14d     192.168.179.174   k8s-node2    &lt;none&gt;           &lt;none&gt;</span><br><span class="line">tiller-deploy-758bcdc94f-c92cw             1/1     Running   0          14d     10.244.122.75     k8s-node4    &lt;none&gt;           &lt;none&gt;</span><br><span class="line">root@k8s-node2:/data<span class="comment"># time kubectl get nodes</span></span><br><span class="line">NAME         STATUS   ROLES    AGE   VERSION</span><br><span class="line">k8s-master   Ready    master   14d   v1.15.3</span><br><span class="line">k8s-node1    Ready    master   14d   v1.15.3</span><br><span class="line">k8s-node2    Ready    master   14d   v1.15.3</span><br><span class="line">k8s-node3    Ready    &lt;none&gt;   14d   v1.15.3</span><br><span class="line">k8s-node4    Ready    &lt;none&gt;   14d   v1.15.3</span><br><span class="line"></span><br><span class="line">real    0m1.032s</span><br><span class="line">user    0m0.089s</span><br><span class="line">sys    0m0.041s</span><br></pre></td></tr></table></figure><hr><p>2020.03.31 made by <em>jaejun.lee</em></p>]]></content:encoded>
      
      <comments>https://jx2lee.github.io/cloud-kubectl_hang/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[Cloud] Kubeflow 폐쇄망 설치 중 Namespace 생성 화면이 나오지 않는 문제</title>
      <link>https://jx2lee.github.io/cloud-kubeflow_error/</link>
      <guid>https://jx2lee.github.io/cloud-kubeflow_error/</guid>
      <pubDate>Sun, 29 Mar 2020 15:00:00 GMT</pubDate>
      <description>
      
        &lt;p&gt;Kubeflow 를 폐쇄 환경에서 설치하던 중 Namespace 생성화면이 나오지 않고 심지어 선택할 수 없는 문제가 발생하였다. 문제의 원인을 파악하고 해결하는 과정을 다룬다.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Update Note&lt;/strong&gt;  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;2020.05.19 : Image list 변경&lt;/li&gt;
&lt;li&gt;2020.06.24 : Image List 변경 (kfserving-system 에 필요한 이미지 추가)&lt;/li&gt;
&lt;/ul&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p>Kubeflow 를 폐쇄 환경에서 설치하던 중 Namespace 생성화면이 나오지 않고 심지어 선택할 수 없는 문제가 발생하였다. 문제의 원인을 파악하고 해결하는 과정을 다룬다.</p><p><strong>Update Note</strong>  </p><ul><li>2020.05.19 : Image list 변경</li><li>2020.06.24 : Image List 변경 (kfserving-system 에 필요한 이미지 추가)</li></ul><a id="more"></a><h1 id="문제"><a href="#문제" class="headerlink" title="문제"></a>문제</h1><ul><li>UI 접속 후 namespace 생성이 되지 않음<ul><li><code>.cache</code> 폴더 삭제 후 재 배포해도 계속되는 문제 발생</li></ul></li></ul><h2 id="kubectl-get-pod-n-kubeflow"><a href="#kubectl-get-pod-n-kubeflow" class="headerlink" title="kubectl get pod -n kubeflow"></a>kubectl get pod -n kubeflow</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">NAME                                                           READY   STATUS             RESTARTS   AGE</span><br><span class="line">admission-webhook-bootstrap-stateful-set-0                     1/1     Running            0          15m</span><br><span class="line">admission-webhook-deployment-68c6dd4cc5-sgtn6                  1/1     Running            0          14m</span><br><span class="line">application-controller-stateful-set-0                          1/1     Running            0          15m</span><br><span class="line">argo-ui-78bf45b698-2vhm9                                       1/1     Running            0          15m</span><br><span class="line">centraldashboard-fd9549bd5-nv68z                               1/1     Running            0          15m</span><br><span class="line">jupyter-web-app-deployment-7797778d74-5lq9d                    1/1     Running            0          15m</span><br><span class="line">katib-controller-55545cb4c8-8xdtn                              1/1     Running            1          15m</span><br><span class="line">katib-db-d99f776cd-zww8n                                       0/1     Running            1          15m</span><br><span class="line">katib-manager-67d9689545-9j6cf                                 0/1     CrashLoopBackOff   6          15m</span><br><span class="line">katib-ui-889499864-fd4hc                                       1/1     Running            0          15m</span><br><span class="line">metacontroller-0                                               1/1     Running            0          15m</span><br><span class="line">metadata-db-68df96445c-br8vt                                   1/1     Running            0          15m</span><br><span class="line">metadata-deployment-865fddd777-dpkbv                           1/1     Running            0          15m</span><br><span class="line">metadata-envoy-deployment-68f64489cc-ts88j                     1/1     Running            0          15m</span><br><span class="line">metadata-grpc-deployment-7f5d6c8ccb-j99d2                      1/1     Running            1          15m</span><br><span class="line">metadata-ui-84c76df48f-4sxdq                                   1/1     Running            0          15m</span><br><span class="line">minio-75d8cbbb5c-2dsgt                                         1/1     Running            0          15m</span><br><span class="line">ml-pipeline-7f6548ff8-74lh6                                    0/1     ImagePullBackOff   0          15m</span><br><span class="line">ml-pipeline-ml-pipeline-visualizationserver-559c875d6b-9mth7   0/1     ImagePullBackOff   0          15m</span><br><span class="line">ml-pipeline-persistenceagent-796c6c4c75-l8vx2                  0/1     ImagePullBackOff   0          15m</span><br><span class="line">ml-pipeline-scheduledworkflow-f86df57bd-87j5p                  0/1     ImagePullBackOff   0          15m</span><br><span class="line">ml-pipeline-ui-fb8b6778f-zgx59                                 1/1     Running            0          15m</span><br><span class="line">ml-pipeline-viewer-controller-deployment-78bdcc54fc-pf8lb      1/1     Running            0          15m</span><br><span class="line">mysql-6c5ddbd98b-pkqqr                                         1/1     Running            0          15m</span><br><span class="line">notebook-controller-deployment-7694b76c89-fb6g5                1/1     Running            0          15m</span><br><span class="line">profiles-deployment-5c48f8d6d8-kxfx5                           1/2     CrashLoopBackOff   7          15m</span><br><span class="line">pytorch-operator-dfb77d487-92dqx                               1/1     Running            0          15m</span><br><span class="line">seldon-operator-controller-manager-0                           1/1     Running            1          15m</span><br><span class="line">spartakus-volunteer-74f96589f9-g2xd5                           1/1     Running            0          15m</span><br><span class="line">tensorboard-6867797f97-kqxsc                                   1/1     Running            0          15m</span><br><span class="line">tf-job-operator-58d7d7d976-jhpk4                               1/1     Running            0          15m</span><br><span class="line">workflow-controller-66dd745699-9bqcx                           1/1     Running            0          15m</span><br></pre></td></tr></table></figure><h2 id="log"><a href="#log" class="headerlink" title="log"></a>log</h2><h3 id="profiles-deployment-manager"><a href="#profiles-deployment-manager" class="headerlink" title="profiles-deployment manager"></a>profiles-deployment manager</h3><p><code>k logs -n kubeflow profiles-deployment-5c48f8d6d8-kxfx5 -c manager</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">flag provided but not defined: -workload-identity</span><br><span class="line">Usage of /manager:</span><br><span class="line">  -kubeconfig string</span><br><span class="line">        Paths to a kubeconfig. Only required <span class="keyword">if</span> out-of-cluster.</span><br><span class="line">  -master string</span><br><span class="line">        The address of the Kubernetes API server. Overrides any value <span class="keyword">in</span> kubeconfig. Only required <span class="keyword">if</span> out-of-cluster.</span><br><span class="line">  -metrics-addr string</span><br><span class="line">        The address the metric endpoint binds to. (default <span class="string">&quot;:8080&quot;</span>)</span><br><span class="line">  -userid-header string</span><br><span class="line">        Key of request header containing user id (default <span class="string">&quot;x-goog-authenticated-user-email&quot;</span>)</span><br><span class="line">  -userid-prefix string</span><br><span class="line">        Request header user id common prefix (default <span class="string">&quot;accounts.google.com:&quot;</span>)</span><br></pre></td></tr></table></figure><h3 id="profiles-deployment-kfam"><a href="#profiles-deployment-kfam" class="headerlink" title="profiles-deployment kfam"></a>profiles-deployment kfam</h3><p><code>k logs -n kubeflow profiles-deployment-5c48f8d6d8-kxfx5 -c kfam</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">time=<span class="string">&quot;2020-03-26T06:02:16Z&quot;</span> level=info msg=<span class="string">&quot;Server started&quot;</span></span><br></pre></td></tr></table></figure><h3 id="centraldashboard-pod-logs"><a href="#centraldashboard-pod-logs" class="headerlink" title="centraldashboard pod logs"></a>centraldashboard pod logs</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">Initializing Kubernetes configuration</span><br><span class="line">Unable to fetch Nodes &#123; kind: <span class="string">&#x27;Status&#x27;</span>,</span><br><span class="line">  apiVersion: <span class="string">&#x27;v1&#x27;</span>,</span><br><span class="line">  metadata: &#123;&#125;,</span><br><span class="line">  status: <span class="string">&#x27;Failure&#x27;</span>,</span><br><span class="line">  message:</span><br><span class="line">   <span class="string">&#x27;nodes is forbidden: User &quot;system:serviceaccount:kubeflow:centraldashboard&quot; cannot list resource &quot;nodes&quot; in API group &quot;&quot; at the cluster scope&#x27;</span>,</span><br><span class="line">  reason: <span class="string">&#x27;Forbidden&#x27;</span>,</span><br><span class="line">  details: &#123; kind: <span class="string">&#x27;nodes&#x27;</span> &#125;,</span><br><span class="line">  code: 403 &#125;</span><br><span class="line">Unable to fetch Application information: &#123; kind: <span class="string">&#x27;Status&#x27;</span>,</span><br><span class="line">  apiVersion: <span class="string">&#x27;v1&#x27;</span>,</span><br><span class="line">  metadata: &#123;&#125;,</span><br><span class="line">  status: <span class="string">&#x27;Failure&#x27;</span>,</span><br><span class="line">  message:</span><br><span class="line">   <span class="string">&#x27;applications.app.k8s.io is forbidden: User &quot;system:serviceaccount:kubeflow:centraldashboard&quot; cannot list resource &quot;applications&quot; in API group &quot;app.k8s.io&quot; in the namespace &quot;kubeflow&quot;&#x27;</span>,</span><br><span class="line">Using Profiles service at http://profiles-kfam.kubeflow:8081/kfam</span><br><span class="line">  reason: <span class="string">&#x27;Forbidden&#x27;</span>,</span><br><span class="line">  details: &#123; group: <span class="string">&#x27;app.k8s.io&#x27;</span>, kind: <span class="string">&#x27;applications&#x27;</span> &#125;,</span><br><span class="line">  code: 403 &#125;</span><br><span class="line">Unable to fetch Nodes &#123; kind: <span class="string">&#x27;Status&#x27;</span>,</span><br><span class="line">  apiVersion: <span class="string">&#x27;v1&#x27;</span>,</span><br><span class="line">  metadata: &#123;&#125;,</span><br><span class="line">  status: <span class="string">&#x27;Failure&#x27;</span>,</span><br><span class="line">  message:</span><br><span class="line">   <span class="string">&#x27;nodes is forbidden: User &quot;system:serviceaccount:kubeflow:centraldashboard&quot; cannot list resource &quot;nodes&quot; in API group &quot;&quot; at the cluster scope&#x27;</span>,</span><br><span class="line">  reason: <span class="string">&#x27;Forbidden&#x27;</span>,</span><br><span class="line">  details: &#123; kind: <span class="string">&#x27;nodes&#x27;</span> &#125;,</span><br><span class="line">  code: 403 &#125;</span><br><span class="line"><span class="string">&quot;other&quot;</span> is not a supported platform <span class="keyword">for</span> Metrics</span><br><span class="line">Server listening on port http://localhost:8082 (<span class="keyword">in</span> production mode)</span><br><span class="line">Unable to contact Profile Controller [object Object]</span><br><span class="line">Unable to contact Profile Controller [object Object]</span><br><span class="line">Unable to contact Profile Controller [object Object]</span><br><span class="line">Unable to contact Profile Controller [object Object]</span><br><span class="line">Unable to contact Profile Controller [object Object]</span><br><span class="line">Unable to contact Profile Controller [object Object]</span><br><span class="line">Unable to contact Profile Controller [object Object]</span><br><span class="line">Unable to contact Profile Controller [object Object]</span><br></pre></td></tr></table></figure><ul><li><strong>Unable to contact Profile Controller [object Object]</strong><ul><li>centraldashboard 에서 profile controller 요청을 받지 못하는 상황</li><li>각 파드에 접속하여 (centraldashboard, profile controller-manager) 핑을 때려봤는데 모두 통신이 원활하였음</li></ul></li></ul><h1 id="해결-방안"><a href="#해결-방안" class="headerlink" title="해결 방안"></a>해결 방안</h1><ul><li>엄청나게 많은 방법을 시도했다.<ul><li>kfctl verion, 각종 이미지 변경 등..</li><li>시행착오 끝에 centraldashobard 와 profile-deployment 파드에 생성한 컨테이너 이미지 버젼 문제였다.<ul><li><code>gcr.io/kubeflow-images-public/centraldashboard:latest</code></li><li><code>gcr.io/kubeflow-images-public/profile-controller:v20191024-v0.7.0-rc.5-12-g956569ba-e3b0c4</code></li><li><code>gcr.io/kubeflow-images-public/kfam@sha256:3b0d4be7e59a3fa5ed1d80dccc832312caa94f3b2d36682524d3afc4e45164f0</code><ul><li>digest 로 이루어진 이미지는 pull 이후에 tag 를 생성하여 레지스트리에 push 하였다. (ex. <code>gcr.io/kubeflow-images-public/kfam:3b0d4be7e59a3fa5ed1d80dccc832312caa94f3b2d36682524d3afc4e45164f0</code>)</li></ul></li></ul></li></ul></li><li>최종적으로 kubeflow 0.7.1 버젼에 사용한 이미지 리스트는 다음과 같다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line">argoproj/argoui:v2.3.0</span><br><span class="line">argoproj/workflow-controller:v2.3.0</span><br><span class="line">docker.io/istio/citadel:1.1.6</span><br><span class="line">docker.io/istio/galley:1.1.6</span><br><span class="line">docker.io/istio/kubectl:1.1.6</span><br><span class="line">docker.io/istio/mixer:1.1.6</span><br><span class="line">docker.io/istio/pilot:1.1.6</span><br><span class="line">docker.io/istio/proxy_init:1.1.6</span><br><span class="line">docker.io/istio/proxyv2:1.1.6</span><br><span class="line">docker.io/istio/sidecar_injector:1.1.6</span><br><span class="line">docker.io/jaegertracing/all-in-one:1.9</span><br><span class="line">docker.io/kiali/kiali:v0.16</span><br><span class="line">docker.io/prom/prometheus:v2.3.1</span><br><span class="line">docker.io/seldonio/seldon-core-operator:0.4.1</span><br><span class="line">gcr.io/google_containers/spartakus-amd64:v1.1.0</span><br><span class="line">gcr.io/kfserving/alibi-explainer:0.2.2</span><br><span class="line">gcr.io/kfserving/kfserving-controller:0.2.2</span><br><span class="line">gcr.io/kfserving/logger:0.2.2</span><br><span class="line">gcr.io/kfserving/pytorchserver:0.2.2</span><br><span class="line">gcr.io/kfserving/sklearnserver:0.2.2</span><br><span class="line">gcr.io/kfserving/storage-initializer:0.2.2 </span><br><span class="line">gcr.io/kfserving/xgbserver:0.2.2</span><br><span class="line">gcr.io/kubebuilder/kube-rbac-proxy:v0.4.0</span><br><span class="line">gcr.io/kubeflow-images-public/admission-webhook:v20190520-v0-139-gcee39dbc-dirty-0d8f4c</span><br><span class="line">gcr.io/kubeflow-images-public/ingress-setup:latest</span><br><span class="line">gcr.io/kubeflow-images-public/jupyter-web-app:9419d4d</span><br><span class="line">gcr.io/kubeflow-images-public/katib/v1alpha3/katib-controller:v0.7.0</span><br><span class="line">gcr.io/kubeflow-images-public/katib/v1alpha3/katib-manager:v0.7.0</span><br><span class="line">gcr.io/kubeflow-images-public/katib/v1alpha3/katib-ui:v0.7.0</span><br><span class="line">gcr.io/kubeflow-images-public/kubernetes-sigs/application:1.0-beta</span><br><span class="line">gcr.io/kubeflow-images-public/metadata-frontend:v0.1.8</span><br><span class="line">gcr.io/kubeflow-images-public/metadata:v0.1.11</span><br><span class="line">gcr.io/kubeflow-images-public/pytorch-operator:v0.7.0</span><br><span class="line">gcr.io/kubeflow-images-public/tensorflow-1.14.0-notebook-cpu:v-base-ef41372-1177829795472347138</span><br><span class="line">gcr.io/kubeflow-images-public/tensorflow-1.14.0-notebook-cpu:v0.7.0</span><br><span class="line">gcr.io/kubeflow-images-public/tensorflow-1.14.0-notebook-gpu:v0.7.0</span><br><span class="line">gcr.io/kubeflow-images-public/tensorflow-2.0.0a0-notebook-cpu:v0.7.0</span><br><span class="line">gcr.io/kubeflow-images-public/tensorflow-2.0.0a0-notebook-gpu:v0.7.0</span><br><span class="line">gcr.io/kubeflow-images-public/tf_operator:kubeflow-tf-operator-postsubmit-v1-5adee6f-6109-a25c</span><br><span class="line">gcr.io/ml-pipeline/api-server:0.1.31</span><br><span class="line">gcr.io/ml-pipeline/envoy:metadata-grpc</span><br><span class="line">gcr.io/ml-pipeline/frontend:0.1.31</span><br><span class="line">gcr.io/ml-pipeline/persistenceagent:0.1.31</span><br><span class="line">gcr.io/ml-pipeline/scheduledworkflow:0.1.31</span><br><span class="line">gcr.io/ml-pipeline/viewer-crd-controller:0.1.31</span><br><span class="line">gcr.io/ml-pipeline/visualization-server:0.1.27</span><br><span class="line">gcr.io/tfx-oss-public/ml_metadata_store_server:0.15.1</span><br><span class="line">grafana/grafana:6.0.2</span><br><span class="line">mcr.microsoft.com/onnxruntime/server:v0.5.1</span><br><span class="line">metacontroller/metacontroller:v0.3.0</span><br><span class="line">minio/minio:RELEASE.2018-02-09T22-40-05Z</span><br><span class="line">mysql:5.6</span><br><span class="line">mysql:8</span><br><span class="line">mysql:8.0.3</span><br><span class="line">nvcr.io/nvidia/tensorrtserver:19.05-py3</span><br><span class="line">tensorflow/serving:1.11.0</span><br><span class="line">tensorflow/serving:1.11.0-gpu</span><br><span class="line">tensorflow/serving:1.12.0</span><br><span class="line">tensorflow/serving:1.12.0-gpu</span><br><span class="line">tensorflow/serving:1.13.0</span><br><span class="line">tensorflow/serving:1.13.0-gpu</span><br><span class="line">tensorflow/serving:1.14.0</span><br><span class="line">tensorflow/serving:1.14.0-gpu</span><br><span class="line">tensorflow/tensorflow:1.8.0</span><br><span class="line">gcr.io/knative-releases/knative.dev/serving/cmd/activator@sha256:88d864eb3c47881cf7ac058479d1c735cc3cf4f07a11aad0621cd36dcd9ae3c6</span><br><span class="line">gcr.io/knative-releases/knative.dev/serving/cmd/autoscaler-hpa@sha256:a7801c3cf4edecfa51b7bd2068f97941f6714f7922cb4806245377c2b336b723</span><br><span class="line">gcr.io/knative-releases/knative.dev/serving/cmd/autoscaler@sha256:aeaacec4feedee309293ac21da13e71a05a2ad84b1d5fcc01ffecfa6cfbb2870</span><br><span class="line">gcr.io/knative-releases/knative.dev/serving/cmd/controller@sha256:3b096e55fa907cff53d37dadc5d20c29cea9bb18ed9e921a588fee17beb937df</span><br><span class="line">gcr.io/knative-releases/knative.dev/serving/cmd/networking/istio@sha256:057c999bccfe32e9889616b571dc8d389c742ff66f0b5516bad651f05459b7bc</span><br><span class="line">gcr.io/knative-releases/knative.dev/serving/cmd/queue@sha256:e0654305370cf3bbbd0f56f97789c92cf5215f752b70902eba5d5fc0e88c5aca</span><br><span class="line">gcr.io/knative-releases/knative.dev/serving/cmd/webhook@sha256:c2076674618933df53e90cf9ddd17f5ddbad513b8c95e955e45e37be7ca9e0e8</span><br><span class="line">gcr.io/kubeflow-images-public/centraldashboard@sha256:4299297b8390599854aa8f77e9eb717db684b32ca9a94a0ab0e73f3f73e5d8b5</span><br><span class="line">gcr.io/kubeflow-images-public/kfam@sha256:3b0d4be7e59a3fa5ed1d80dccc832312caa94f3b2d36682524d3afc4e45164f0</span><br><span class="line">gcr.io/kubeflow-images-public/notebook-controller@sha256:6490f737000bd1d2520ac4b8cbde2b09749cdb291b1967ddda95d05131db49db</span><br><span class="line">gcr.io/kubeflow-images-public/profile-controller@sha256:e601b2226e534a4f8e0722cfc44ae4a919a90265c4c6c9e7a7a55fcb57032f25</span><br></pre></td></tr></table></figure><blockquote><p><em>tar 파일로 용량을 계산한 결과 약 30G(no gzip option)</em></p></blockquote><h1 id="추가로-발생한-문제"><a href="#추가로-발생한-문제" class="headerlink" title="추가로 발생한 문제"></a>추가로 발생한 문제</h1><ul><li>Kubeflow 배포 전 istio-system 와 knative-serving 네임스페이스가 존재하여 배포할 때 에러가 발생</li><li>해결 방안<ul><li>배포 전 istio-system / knative-serving 네임스페이스를 삭제 후 재 배포</li></ul></li></ul><h1 id="Ref"><a href="#Ref" class="headerlink" title="Ref"></a>Ref</h1><ul><li><a href="https://github.com/kubeflow/kubeflow/issues/3859">https://github.com/kubeflow/kubeflow/issues/3859</a></li><li><a href="https://github.com/kubeflow/kubeflow/issues/4788">https://github.com/kubeflow/kubeflow/issues/4788</a></li><li><a href="https://github.com/kubeflow/kubeflow/issues/4718">https://github.com/kubeflow/kubeflow/issues/4718</a></li><li><a href="https://github.com/kubeflow/kubeflow/issues/3900">https://github.com/kubeflow/kubeflow/issues/3900</a></li><li><a href="https://stackoverflow.com/questions/54203646/kubernetes-how-to-increase-ephemeral-storage">https://stackoverflow.com/questions/54203646/kubernetes-how-to-increase-ephemeral-storage</a></li><li><a href="https://www.kangwoo.kr/2020/02/18/pc에-kubeflow-설치하기-3부-kubeflow-설치하기/">https://www.kangwoo.kr/2020/02/18/pc에-kubeflow-설치하기-3부-kubeflow-설치하기/</a></li></ul><hr><p>2020.03.30 made by <em>jaejun.lee</em></p>]]></content:encoded>
      
      <comments>https://jx2lee.github.io/cloud-kubeflow_error/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[Cloud] Docker private registry 설정</title>
      <link>https://jx2lee.github.io/cloud-docker_registry/</link>
      <guid>https://jx2lee.github.io/cloud-docker_registry/</guid>
      <pubDate>Tue, 17 Mar 2020 15:00:00 GMT</pubDate>
      <description>
      
        &lt;p&gt;docker private registry 를 생성하고, tar 파일로 변환한 이미지를 load 및 registry 에 push 하는 과정을 다룬다. 사이트에 나가게 되면 폐쇄망인 경우가 대부분인데, 이럴 경우를 대비해서 &lt;strong&gt;이미지 리스트를 읽어들여 docker image 를 tar로 변환하고 registry 에 push 하는 쉘 스크립트&lt;/strong&gt;를 작성하였다.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Update Note&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;2020.03.30 : docker run 시 mount 방법(-v 옵션) 및 docker image pull / push 스크립트 추가&lt;/li&gt;
&lt;li&gt;2020.04.13 : docker run 시 restart argument 추가&lt;/li&gt;
&lt;/ul&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p>docker private registry 를 생성하고, tar 파일로 변환한 이미지를 load 및 registry 에 push 하는 과정을 다룬다. 사이트에 나가게 되면 폐쇄망인 경우가 대부분인데, 이럴 경우를 대비해서 <strong>이미지 리스트를 읽어들여 docker image 를 tar로 변환하고 registry 에 push 하는 쉘 스크립트</strong>를 작성하였다.</p><p><strong>Update Note</strong></p><ul><li>2020.03.30 : docker run 시 mount 방법(-v 옵션) 및 docker image pull / push 스크립트 추가</li><li>2020.04.13 : docker run 시 restart argument 추가</li></ul><a id="more"></a><h1 id="Private-registry-생성"><a href="#Private-registry-생성" class="headerlink" title="Private registry 생성"></a>Private registry 생성</h1><h2 id="registry-이미지를-Pull-하고-container-를-기동한다"><a href="#registry-이미지를-Pull-하고-container-를-기동한다" class="headerlink" title="registry 이미지를 Pull 하고 container 를 기동한다."></a>registry 이미지를 Pull 하고 container 를 기동한다.</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker pull registry</span><br><span class="line">docker run -dit --name bips-registry --restart=always -p 5000:5000 -v /data/registry:/var/lib/registry registry:latest</span><br></pre></td></tr></table></figure><blockquote><ul><li><em>-v 플래그를 이용해 레지스트리 저장소를 /data/registry 마운트 시켜 컨테이너를 동작한다. 이는 이미지가 많아질 경우 용량이 큰 디바이스에 직접 설정하여 이미지를 관리할 수 있다.</em></li><li><em><code>--restart=alyways</code> 인자를 추가하여 도커 데몬 재시작 시 기동할 수 있도록 설정한다.</em></li></ul></blockquote><h1 id="Image-push"><a href="#Image-push" class="headerlink" title="Image push"></a>Image push</h1><h2 id="tar-파일로-묶은-이미지를-docker-load-i-tar-name-으로-이미지를-생성하고-이를-registry-에-push-한다"><a href="#tar-파일로-묶은-이미지를-docker-load-i-tar-name-으로-이미지를-생성하고-이를-registry-에-push-한다" class="headerlink" title="tar 파일로 묶은 이미지를 docker load -i {tar_name} 으로 이미지를 생성하고, 이를 registry 에 push 한다."></a>tar 파일로 묶은 이미지를 <code>docker load -i &#123;tar_name&#125;</code> 으로 이미지를 생성하고, 이를 registry 에 push 한다.</h2><ul><li>원래 이미지 : 192.168.17.131:5000/ubuntu_t6:2020303_v2</li><li>태그 변경 후 이미지 : 92.168.179.185:5000/ubuntu_t6:2020303_v2</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker tag 192.168.17.131:5000/ubuntu_t6:2020303_v2 192.168.179.185:5000/ubuntu_t6:2020303_v2</span><br><span class="line">docker push 192.168.179.185:5000/ubuntu_t6:2020303_v2</span><br></pre></td></tr></table></figure><h1 id="Image-확인"><a href="#Image-확인" class="headerlink" title="Image 확인"></a>Image 확인</h1><h2 id="registry-에-해당-이미지가-존재하는지-curl-명령어로-확인한다"><a href="#registry-에-해당-이미지가-존재하는지-curl-명령어로-확인한다" class="headerlink" title="registry 에 해당 이미지가 존재하는지 curl 명령어로 확인한다."></a>registry 에 해당 이미지가 존재하는지 <code>curl</code> 명령어로 확인한다.</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">root@k8s-master:~<span class="comment"># curl -X GET 192.168.179.185:5000/v2/_catalog</span></span><br><span class="line">&#123;<span class="string">&quot;repositories&quot;</span>:[<span class="string">&quot;dfa-module&quot;</span>,<span class="string">&quot;hd8.3_rel&quot;</span>,<span class="string">&quot;hl_r172919&quot;</span>,<span class="string">&quot;ubuntu_t6&quot;</span>]&#125;</span><br><span class="line">root@k8s-master:~<span class="comment"># curl -X GET 192.168.179.185:5000/v2/ubuntu_t6/tags/list</span></span><br><span class="line">&#123;<span class="string">&quot;name&quot;</span>:<span class="string">&quot;ubuntu_t6&quot;</span>,<span class="string">&quot;tags&quot;</span>:[<span class="string">&quot;2020303_v2&quot;</span>]&#125;</span><br></pre></td></tr></table></figure><blockquote><p>잘~ 등록되었다!</p></blockquote><h1 id="Docker-Image-Pull-amp-Push-스크립트"><a href="#Docker-Image-Pull-amp-Push-스크립트" class="headerlink" title="Docker Image Pull &amp; Push 스크립트"></a>Docker Image Pull &amp; Push 스크립트</h1><p>폐쇄망 환경에서 docker hub 의 이미지를 가져올 수 없는 상황이 발생하였다. 이를 위해 폐쇄망이 아닌 환경에서 이미지를 pull 하고, 이를 폐쇄망 환경에서 특정 registry (private docker registry) 에 push 하는 스크립트를 작성하였다.</p><ul><li><p>기능</p><ul><li><p>pull : docker hub 의 이미지를 가져와 <code>tars</code> 디렉토리에 저장한다.</p></li><li><p>push : <code>tars</code> 디렉토리에 tar 이미지를 특정 registry 에 push 한다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">❯ ./imageLoader</span><br><span class="line">usage:</span><br><span class="line">  ./imageLoader pull</span><br><span class="line">  ./imageLoader push &#123;registry_endpoint&#125;</span><br></pre></td></tr></table></figure></li></ul></li><li><p>이미지:태그 로 이루어진 <code>.imageList</code> 가 선행으로 준비해야 한다. 또한, <code>.imageListHash</code> 는 태그명이 아닌 해쉬값으로 되어있는 이미지를 받을 때 사용한다. 만약 둘 중 사용하지 않는 이미지가 있다면 선택에 따라 삭제하고 해당 스크립트에 주석을 설정한다.</p><ul><li>첫 번째 pull/push 하는 부분이 <code>이미지:태그</code>, 두 번째가 <code>이미지@해쉬</code></li><li>본인은 Kubeflow 를 폐쇄망 환경에서 설치할 이슈가 생겨 필요한 이미지 리스트를 .imageList / .imageListHash 에 작성하였다.</li><li>참고로 Kubeflow 폐쇄망 설치 시 이미지 버젼 때문에 애를 많이 먹었다..</li></ul></li><li><p><a href="https://github.com/jx2lee/kubeflow-image-loader">github</a> 에 올려두었다. 허접한 스크립트 이지만, 현 직장에서는 편하게(?) 사용할 수 있을 것 같아 공유한다. <del>많은 별 부탁드립니다.</del></p></li></ul><hr><p>2020.03.18 made by <em>jaejun.lee</em></p>]]></content:encoded>
      
      <comments>https://jx2lee.github.io/cloud-docker_registry/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[Cloud] Orphaned pod found 에러 해결</title>
      <link>https://jx2lee.github.io/cloud_orphaned_pod_error/</link>
      <guid>https://jx2lee.github.io/cloud_orphaned_pod_error/</guid>
      <pubDate>Wed, 11 Mar 2020 15:00:00 GMT</pubDate>
      <description>
      
        &lt;p&gt;본 포스트에서는 kubelet log 에 Orphaned pod found , but volume subpaths are still present on disk 로그가 발생하는 문제를 해결한다.&lt;/p&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p>본 포스트에서는 kubelet log 에 Orphaned pod found , but volume subpaths are still present on disk 로그가 발생하는 문제를 해결한다.</p><a id="more"></a><h1 id="Kubelet-log"><a href="#Kubelet-log" class="headerlink" title="Kubelet log"></a>Kubelet log</h1><p><code>journalctl -f | grep kubelet</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Mar 12 01:31:04 k8s-node2 kubelet[19210]: E0312 01:31:04.964272   19210 kubelet_volumes.go:154] Orphaned pod <span class="string">&quot;1a2f73f1-77a2-4053-975f-57a89fdba1db&quot;</span> found, but volume subpaths are still present on disk : There were a total of 1 errors similar to this. Turn up verbosity to see them.</span><br></pre></td></tr></table></figure><h1 id="문제-원인"><a href="#문제-원인" class="headerlink" title="문제 원인"></a>문제 원인</h1><p>해당 노드를 재부팅했을 때 기존에 남아있던 파드 정보로 인하여 에러가 발생한 것 같다. 사라진 파드의 정보에 volume subpath 도 삭제되지 않은 것이다.</p><h1 id="문제-해결"><a href="#문제-해결" class="headerlink" title="문제 해결"></a>문제 해결</h1><p>의외로 간단하다. 해당 pod 정보를 삭제하면 되는데, 본인 환경으로는 <code>/var/lib/kubelet/pods/</code> 에 파드 UUID 폴더를 삭제하면 된다. 이후 kubelet 을 재기동하면 위 로그가 삭제됨을 확인할 수 있다.</p><h1 id="이외-문제들"><a href="#이외-문제들" class="headerlink" title="이외 문제들"></a>이외 문제들</h1><p>오늘 있었던 다양한 에러들을 한 번 정리했다 (네트워크 문제로 인해 file stroage 를 배포하는 문제는 더 파악해본 후 포스팅 할 예정이다).</p><h2 id="kubelet-로그에-Unable-to-read-config-path-“-etc-kubernetes-manifests”-path-does-not-exist-ignoring-메세지-발생"><a href="#kubelet-로그에-Unable-to-read-config-path-“-etc-kubernetes-manifests”-path-does-not-exist-ignoring-메세지-발생" class="headerlink" title="kubelet 로그에 Unable to read config path “/etc/kubernetes/manifests”: path does not exist, ignoring 메세지 발생"></a>kubelet 로그에 Unable to read config path “/etc/kubernetes/manifests”: path does not exist, ignoring 메세지 발생</h2><ul><li><code>/etc/kubernets/path</code> 에 <code>manifests</code> 폴더를 생성하여 해결</li></ul><h2 id="노드에-node-kubernetes-io-unreachable-NoSchedule-Taint-가-걸려있는-경우"><a href="#노드에-node-kubernetes-io-unreachable-NoSchedule-Taint-가-걸려있는-경우" class="headerlink" title="노드에 node.kubernetes.io/unreachable:NoSchedule Taint 가 걸려있는 경우"></a>노드에 node.kubernetes.io/unreachable:NoSchedule Taint 가 걸려있는 경우</h2><ul><li>해당 노드가 Not Ready 인 상태</li><li>이런 경우에는 대게 노드가 재부팅 후 스왑 메모리가 켜져 있을 확률이 높다. <code>free</code> 명령어를 통해 확인한 다음 swap off 이후 kubelet 을 재기동 한다.</li></ul><hr><p>2020.03.12 made by <em>jaejun.lee</em></p>]]></content:encoded>
      
      <comments>https://jx2lee.github.io/cloud_orphaned_pod_error/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[Cloud] R docker 컨테이너에서 Hadoop HDFS 연동</title>
      <link>https://jx2lee.github.io/cloud-r_hadoop_connection/</link>
      <guid>https://jx2lee.github.io/cloud-r_hadoop_connection/</guid>
      <pubDate>Wed, 19 Feb 2020 15:00:00 GMT</pubDate>
      <description>
      
        &lt;p&gt;본 포스트에서는 R Container 에서 Hadoop HDFS 데이터를 연동하는 과정을 다룬다.&lt;/p&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p>본 포스트에서는 R Container 에서 Hadoop HDFS 데이터를 연동하는 과정을 다룬다.</p><a id="more"></a><h1 id="R-Container-기동"><a href="#R-Container-기동" class="headerlink" title="R Container 기동"></a>R Container 기동</h1><h2 id="R-container-image-를-이용해-컨테이너를-기동한다"><a href="#R-container-image-를-이용해-컨테이너를-기동한다" class="headerlink" title="R container image 를 이용해 컨테이너를 기동한다."></a>R container image 를 이용해 컨테이너를 기동한다.</h2><ul><li><p><strong>docker run -d -p 8787:8787 -e PASSWORD=tmaxtmax rocker_with_java:latest</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">root@k8s-master:~<span class="comment"># docker rename competent_dubinsky r-hadoop-test</span></span><br><span class="line">root@k8s-master:/app<span class="comment"># docker ps</span></span><br><span class="line">CONTAINER ID        IMAGE                     COMMAND                  CREATED             STATUS              PORTS                    NAMES</span><br><span class="line">aa91a42054dc        rocker_with_java:latest   <span class="string">&quot;/init&quot;</span>                  23 minutes ago      Up 23 minutes       0.0.0.0:8787-&gt;8787/tcp   r-hadoop-test</span><br></pre></td></tr></table></figure></li><li><p>http://{NODE_IP}:8787 로 접속하여 rstudio/tmaxtmax 입력하여 Rstduio 화면으로 이동한다.</p><p><img src="/image/rstudio-login.png" alt=""></p></li></ul><blockquote><p><em>rocker_with_java image 는 rocker-rstudio 이미지 보다 상위 이미지인 rocker/tidyverse 를 사용한 새로운 이미지다. <a href="https://github.com/gadenbuie/docker-tidyverse-rjava/blob/master/Dockerfile">참고</a></em></p></blockquote><h1 id="R-Container-환경설정"><a href="#R-Container-환경설정" class="headerlink" title="R Container 환경설정"></a>R Container 환경설정</h1><h2 id="Hadoop-연동을-위한-Hadoop-client-와-java-를-설치한다"><a href="#Hadoop-연동을-위한-Hadoop-client-와-java-를-설치한다" class="headerlink" title="Hadoop 연동을 위한 Hadoop client 와 java 를 설치한다."></a>Hadoop 연동을 위한 Hadoop client 와 java 를 설치한다.</h2><ul><li><p>로컬에 있는 hadoop client 와 java tar 파일들을 R container 로 옮긴다.</p><ul><li><strong>docker cp hadoop-client.tar r-hadoop-test:/root</strong></li><li><strong>docker cp jdk.tar r-hadoop-test:/root</strong></li></ul></li><li><p>R container로 접속하여 hadoop client 를 설치한다</p><ul><li><strong>docker exec -it r-hadoop-test /bin/bash</strong> 명령어 이후 hadoop client 설치</li><li>설치 확인</li></ul></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">rstudio@4935816e695d:~$ java -version</span><br><span class="line">java version <span class="string">&quot;1.8.0_241&quot;</span></span><br><span class="line">Java(TM) SE Runtime Environment (build 1.8.0_241-b07)</span><br><span class="line">Java HotSpot(TM) 64-Bit Server VM (build 25.241-b07, mixed mode)</span><br><span class="line">rstudio@aa91a42054dc:~$ hdfs version</span><br><span class="line">Hadoop 2.10.0</span><br><span class="line">Subversion ssh://git.corp.linkedin.com:29418/hadoop/hadoop.git -r e2f1f118e465e787d8567dfa6e2f3b72a0eb9194</span><br><span class="line">Compiled by jhung on 2019-10-22T19:10Z</span><br><span class="line">Compiled with protoc 2.5.0</span><br><span class="line">From <span class="built_in">source</span> with checksum 7b2d8877c5ce8c9a2cca5c7e81aa4026</span><br><span class="line">This <span class="built_in">command</span> was run using /app/hadoop/2.10.0/share/hadoop/common/hadoop-common-2.10.0.jar</span><br></pre></td></tr></table></figure><h1 id="Hadoop-연동을-위한-R-package-설치"><a href="#Hadoop-연동을-위한-R-package-설치" class="headerlink" title="Hadoop 연동을 위한 R package 설치"></a>Hadoop 연동을 위한 R package 설치</h1><p>Hadoop 연동을 위한 R 패키지를 설치한다.</p><ul><li><p>R Studio 에서 아래 커맨드를 이용해 패키지를 설치한다. HADOOP_CMD와 JAVA_HOME 은 각 환경에 맞는 PATH 로 변경하여 수행한다.</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">library(devtools)</span><br><span class="line">library(rJava)</span><br><span class="line">install_github(<span class="built_in">c</span>(<span class="string">&quot;RevolutionAnalytics/rmr2/pkg&quot;</span>, <span class="string">&quot;RevolutionAnalytics/plyrmr2/pkg&quot;</span>))</span><br><span class="line"></span><br><span class="line">Sys.setenv(HADOOP_CMD=<span class="string">&quot;/app/hadoop/2.10.0/bin/hadoop&quot;</span>)</span><br><span class="line">Sys.setenv(JAVA_HOME=<span class="string">&quot;/usr/lib/jvm/java-8-openjdk-amd64&quot;</span>)</span><br><span class="line"></span><br><span class="line">install_github(<span class="string">&quot;RevolutionAnalytics/rhdfs/pkg&quot;</span>)</span><br></pre></td></tr></table></figure></li><li><p>Packeges 관리 화면에 설치가 되었는지 확인한다.</p></li></ul><h1 id="R-Hadoop-연동"><a href="#R-Hadoop-연동" class="headerlink" title="R - Hadoop 연동"></a>R - Hadoop 연동</h1><p>R 환경변수를 설정하고 HDFS 에 접근하여 목록과 데이터 일부를 확인한다.</p><ul><li><p><code>rhdfs / rmr2</code> 패키지를 활성화 시키고 hdfs 객체를 초기화한다.</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">library(rmr2)</span><br><span class="line">library(rhdfs)</span><br><span class="line"></span><br><span class="line">hdfs.init()</span><br></pre></td></tr></table></figure></li><li><p><strong>hdfs.ls / hdfs.cat</strong> 함수를 이용해 디렉토리 목록과 데이터 일부를 확인한다.</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">hdfs.ls(<span class="string">&quot;/user/spark&quot;</span>)</span><br><span class="line">hdfs.cat(<span class="string">&quot;/user/spark/test.csv&quot;</span>)</span><br><span class="line"></span><br><span class="line">&gt; hdfs.ls(<span class="string">&quot;/user/spark&quot;</span>)</span><br><span class="line">  permission   owner      group   size          modtime                      file</span><br><span class="line">1 drwxr-xr-x   spark supergroup      <span class="number">0</span> <span class="number">2020</span>-<span class="number">02</span>-<span class="number">19</span> <span class="number">05</span>:<span class="number">02</span> /user/spark/.sparkStaging</span><br><span class="line">2 -rw-r--r-- rstudio supergroup <span class="number">970491</span> <span class="number">2020</span>-<span class="number">02</span>-<span class="number">19</span> <span class="number">09</span>:<span class="number">52</span>      /user/spark/test.csv</span><br></pre></td></tr></table></figure></li></ul><h1 id="참고-R-Hadoop-연동-REST-API-방식"><a href="#참고-R-Hadoop-연동-REST-API-방식" class="headerlink" title="(참고) R - Hadoop 연동 (REST API 방식)"></a>(참고) R - Hadoop 연동 (REST API 방식)</h1><p>이전의 방법은 R container 에 hadoop client 를 이용해 연동하였다. 또다른 방법으로써 hadoop 에서 제공하는 REST API 를 이용해 R과 연동하는 방법이다.</p><ul><li><p>R package <strong>httr</strong> 을 설치한다.</p><p><code>install.packages(&quot;httr&quot;)</code></p></li><li><p>hadoop URI 변수를 생성한다. 형태는 <code>http://namenodedns:port/webhdfs/v1/user/username/myfile.csv?user.name=MYUSERNAME&amp;op=OPEN</code> 으로 hadoop 설정에 맞게끔 변경한다.</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hdfsUri=<span class="string">&quot;http://192.168.179.178:50070/webhdfs/v1&quot;</span></span><br><span class="line">fileUri=<span class="string">&quot;/user/spark/test.csv&quot;</span></span><br><span class="line">readParameter=<span class="string">&quot;?user.name=&quot;</span></span><br><span class="line">usernameParameter=<span class="string">&quot;spark&amp;&quot;</span></span><br><span class="line">optionnalParameters=<span class="string">&quot;op=OPEN&quot;</span></span><br><span class="line">uri &lt;- paste0(hdfsUri, fileUri, readParameter, usernameParameter, optionnalParameters)</span><br></pre></td></tr></table></figure></li><li><p>URI 형태로 데이터를 불러오고 상단 부분 <em>(head)</em> 을 확인한다.</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data = read.csv(uri)</span><br><span class="line">head(data))</span><br></pre></td></tr></table></figure></li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li><a href="https://github.com/RevolutionAnalytics/RHadoop/wiki/Installing-RHadoop-on-RHEL">https://github.com/RevolutionAnalytics/RHadoop/wiki/Installing-RHadoop-on-RHEL</a></li><li><a href="https://niceguy1575.tistory.com/40">https://niceguy1575.tistory.com/40</a></li><li><a href="https://hub.docker.com/r/rocker/tidyverse">https://hub.docker.com/r/rocker/tidyverse</a></li><li><a href="https://github.com/gadenbuie/docker-tidyverse-rjava/blob/master/Dockerfile">https://github.com/gadenbuie/docker-tidyverse-rjava/blob/master/Dockerfile</a></li><li><a href="https://wikidocs.net/52630">https://wikidocs.net/52630</a></li></ul><hr><p>2020.02.20 made by <em>jaejun.lee</em></p>]]></content:encoded>
      
      <comments>https://jx2lee.github.io/cloud-r_hadoop_connection/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[Cloud] Elasticsearch 배포 on K8s</title>
      <link>https://jx2lee.github.io/cloud-deploy_es/</link>
      <guid>https://jx2lee.github.io/cloud-deploy_es/</guid>
      <pubDate>Thu, 06 Feb 2020 15:00:00 GMT</pubDate>
      <description>
      
        &lt;p&gt;toy project를 위해 es를 구축하려던 찰나, 사내에 K8s cluster를 구축하였다. 클라우드 공부 겸 Elasticsearch와 kibana를 K8s에 배포하는 과정을 다룬다&lt;/p&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p>toy project를 위해 es를 구축하려던 찰나, 사내에 K8s cluster를 구축하였다. 클라우드 공부 겸 Elasticsearch와 kibana를 K8s에 배포하는 과정을 다룬다</p><a id="more"></a><h1 id="Elasticsearch-배포하기"><a href="#Elasticsearch-배포하기" class="headerlink" title="Elasticsearch 배포하기"></a>Elasticsearch 배포하기</h1><h2 id="3-node로-구성된-elasticsearch-클러스터를-배포한다-한-개의-노드로만-구성될-경우-장애가-발생하면-고가용성을-확보할-수-없으므로-이와-같이-3개-노드로-구성된-클러스트를-배포함으로써-split-brain을-피하고자-한다"><a href="#3-node로-구성된-elasticsearch-클러스터를-배포한다-한-개의-노드로만-구성될-경우-장애가-발생하면-고가용성을-확보할-수-없으므로-이와-같이-3개-노드로-구성된-클러스트를-배포함으로써-split-brain을-피하고자-한다" class="headerlink" title="3-node로 구성된 elasticsearch 클러스터를 배포한다. 한 개의 노드로만 구성될 경우 장애가 발생하면 고가용성을 확보할 수 없으므로 이와 같이 3개 노드로 구성된 클러스트를 배포함으로써 split-brain을 피하고자 한다."></a>3-node로 구성된 elasticsearch 클러스터를 배포한다. 한 개의 노드로만 구성될 경우 장애가 발생하면 고가용성을 확보할 수 없으므로 이와 같이 3개 노드로 구성된 클러스트를 배포함으로써 <strong>split-brain</strong>을 피하고자 한다.</h2><h2 id="Namespace-생성"><a href="#Namespace-생성" class="headerlink" title="Namespace 생성"></a>Namespace 생성</h2><p><em>elastic_ns.yaml</em>을 작성하고 namespace를 생성한다</p><p><code>kubectl create -f elastic_ns.yaml</code></p><p><em>elastic_ns.yaml</em></p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">kind:</span> <span class="string">Namespace</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">        <span class="attr">name:</span> <span class="string">elasticsearch</span></span><br></pre></td></tr></table></figure><h2 id="Service-생성"><a href="#Service-생성" class="headerlink" title="Service 생성"></a>Service 생성</h2><p><em>elastic_svc.yaml</em>을 작성하고 service를 생성한다</p><p><code>kubectl create -f  elastic_svc.yaml</code></p><p><em>elastic_svc.yaml</em></p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">elastic-svc</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">elastic</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">elasticsearch</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">NodePort</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">elasticsearch</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">port:</span> <span class="number">9200</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">rest</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">port:</span> <span class="number">9300</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">inter-node</span></span><br></pre></td></tr></table></figure><ul><li><p><code>kubectl get service -n &#123;namespace-name&#125;</code>으로 서비스 생성을 확인한다</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">root@k8s-master:~/jlee/elastic<span class="comment"># kg svc -n elastic</span></span><br><span class="line">NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE</span><br><span class="line">elastic-svc NodePort 10.96.42.114 &lt;none&gt; 9200:30117/TCP,9300:30395/TCP 56m</span><br></pre></td></tr></table></figure></li></ul><div class="note ">            <p><em>기존 사이트에서 소개한 service와는 조금 다르게 작성하였다. 클러스터에 배보된 es node에 접속하기 위해서는 service의 타입을 NodePort로 설정하였다. NodePort로 설정하지 않는다면 이후에 curl 하는 명령이 connected refused 될 것이다, K8s Service 정리가 필요!</em></p>          </div><h2 id="StatefulSet-생성"><a href="#StatefulSet-생성" class="headerlink" title="StatefulSet 생성"></a>StatefulSet 생성</h2><p><em>elastic_statefulset.yaml</em>을 작성하고 StatefulSet을 생성한다</p><div class="note ">            <p><em>StatefulSet이란, 상태를 가지고 있는 Pod들을 관리하는 컨트롤러로 순서를 지정하여 Pod를 실행하고 volume을 지정하여 Pod가 내려가도 정보를 잃지 않게 한다</em></p>          </div><p><code>kubectl create -f elastic_statefulset.yaml</code></p><p><em>elastic_statefulset.yaml</em></p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">StatefulSet</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">es-cluster</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">elastic</span> <span class="comment"># namepsace name for elasticsearch</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">serviceName:</span> <span class="string">elasticsearch</span> <span class="comment"># service name for elasticsearch</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">3</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">elasticsearch</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">elasticsearch</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">elasticsearch</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">docker.elastic.co/elasticsearch/elasticsearch:7.5.2</span></span><br><span class="line">        <span class="attr">resources:</span></span><br><span class="line">            <span class="attr">limits:</span></span><br><span class="line">              <span class="attr">cpu:</span> <span class="string">1000m</span></span><br><span class="line">            <span class="attr">requests:</span></span><br><span class="line">              <span class="attr">cpu:</span> <span class="string">100m</span></span><br><span class="line">        <span class="attr">ports:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">9200</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">rest</span></span><br><span class="line">          <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">9300</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">inter-node</span></span><br><span class="line">          <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">        <span class="attr">volumeMounts:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">data</span></span><br><span class="line">          <span class="attr">mountPath:</span> <span class="string">/usr/share/elasticsearch/data</span></span><br><span class="line">        <span class="attr">env:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">cluster.name</span></span><br><span class="line">            <span class="attr">value:</span> <span class="string">k8s-elastic</span></span><br><span class="line">          <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">node.name</span></span><br><span class="line">            <span class="attr">valueFrom:</span></span><br><span class="line">              <span class="attr">fieldRef:</span></span><br><span class="line">                <span class="attr">fieldPath:</span> <span class="string">metadata.name</span></span><br><span class="line">          <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">discovery.seed_hosts</span></span><br><span class="line">            <span class="attr">value:</span> <span class="string">&quot;es-cluster-0.elasticsearch,es-cluster-1.elasticsearch,es-cluster-2.elasticsearch&quot;</span> <span class="comment"># hostname for each container</span></span><br><span class="line">          <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">cluster.initial_master_nodes</span></span><br><span class="line">            <span class="attr">value:</span> <span class="string">&quot;es-cluster-0,es-cluster-1,es-cluster-2&quot;</span> <span class="comment"># node name in es-cluster</span></span><br><span class="line">          <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">ES_JAVA_OPTS</span></span><br><span class="line">            <span class="attr">value:</span> <span class="string">&quot;-Xms512m -Xmx512m&quot;</span></span><br><span class="line">      <span class="attr">initContainers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">fix-permissions</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">busybox</span></span><br><span class="line">        <span class="attr">command:</span> [<span class="string">&quot;sh&quot;</span>, <span class="string">&quot;-c&quot;</span>, <span class="string">&quot;chown -R 1000:1000 /usr/share/elasticsearch/data&quot;</span>]</span><br><span class="line">        <span class="attr">securityContext:</span></span><br><span class="line">          <span class="attr">privileged:</span> <span class="literal">true</span></span><br><span class="line">        <span class="attr">volumeMounts:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">data</span></span><br><span class="line">          <span class="attr">mountPath:</span> <span class="string">/usr/share/elasticsearch/data</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">increase-vm-max-map</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">busybox</span></span><br><span class="line">        <span class="attr">command:</span> [<span class="string">&quot;sysctl&quot;</span>, <span class="string">&quot;-w&quot;</span>, <span class="string">&quot;vm.max_map_count=262144&quot;</span>]</span><br><span class="line">        <span class="attr">securityContext:</span></span><br><span class="line">          <span class="attr">privileged:</span> <span class="literal">true</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">increase-fd-ulimit</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">busybox</span></span><br><span class="line">        <span class="attr">command:</span> [<span class="string">&quot;sh&quot;</span>, <span class="string">&quot;-c&quot;</span>, <span class="string">&quot;ulimit -n 65536&quot;</span>]</span><br><span class="line">        <span class="attr">securityContext:</span></span><br><span class="line">          <span class="attr">privileged:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">volumeClaimTemplates:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">data</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">elasticsearch</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">accessModes:</span> [ <span class="string">&quot;ReadWriteOnce&quot;</span> ]</span><br><span class="line">      <span class="attr">storageClassName:</span> <span class="string">rook-ceph-block</span> <span class="comment"># your storageclass</span></span><br><span class="line">      <span class="attr">resources:</span></span><br><span class="line">        <span class="attr">requests:</span></span><br><span class="line">          <span class="attr">storage:</span> <span class="string">10Gi</span></span><br></pre></td></tr></table></figure><ul><li><p><code>kubectl get pods -n elastic</code>으로 생성한 파드를 확인한다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">root@k8s-master:~/jlee/elastic<span class="comment"># kgpo -n elastic</span></span><br><span class="line">NAME READY STATUS RESTARTS AGE</span><br><span class="line">es-cluster-0 1/1 Running 0 159m</span><br><span class="line">es-cluster-1 1/1 Running 0 158m</span><br><span class="line">es-cluster-2 1/1 Running 0 158m</span><br></pre></td></tr></table></figure></li></ul><h2 id="Check-Status"><a href="#Check-Status" class="headerlink" title="Check Status"></a>Check Status</h2><p>정상적으로 ES 노드가 배포되었는지 <code>kubectl get svc -n elastic</code>을 통해 확인된 포트로 <em>(9200에 포트포워딩 된 포트 확인)</em> curl 명령을 수행한다.</p><p><code>curl http://&#123;ip&#125;:&#123;port&#125;</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">root@k8s-master:~/jlee/elastic<span class="comment"># curl http://192.168.179.172:30117</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">&quot;name&quot;</span> : <span class="string">&quot;es-cluster-2&quot;</span>,</span><br><span class="line">  <span class="string">&quot;cluster_name&quot;</span> : <span class="string">&quot;k8s-elastic&quot;</span>,</span><br><span class="line">  <span class="string">&quot;cluster_uuid&quot;</span> : <span class="string">&quot;GqGwbyKYSfGZoEcqFumVzw&quot;</span>,</span><br><span class="line">  <span class="string">&quot;version&quot;</span> : &#123;</span><br><span class="line">    <span class="string">&quot;number&quot;</span> : <span class="string">&quot;7.2.0&quot;</span>,</span><br><span class="line">    <span class="string">&quot;build_flavor&quot;</span> : <span class="string">&quot;default&quot;</span>,</span><br><span class="line">    <span class="string">&quot;build_type&quot;</span> : <span class="string">&quot;docker&quot;</span>,</span><br><span class="line">    <span class="string">&quot;build_hash&quot;</span> : <span class="string">&quot;508c38a&quot;</span>,</span><br><span class="line">    <span class="string">&quot;build_date&quot;</span> : <span class="string">&quot;2019-06-20T15:54:18.811730Z&quot;</span>,</span><br><span class="line">    <span class="string">&quot;build_snapshot&quot;</span> : <span class="literal">false</span>,</span><br><span class="line">    <span class="string">&quot;lucene_version&quot;</span> : <span class="string">&quot;8.0.0&quot;</span>,</span><br><span class="line">    <span class="string">&quot;minimum_wire_compatibility_version&quot;</span> : <span class="string">&quot;6.8.0&quot;</span>,</span><br><span class="line">    <span class="string">&quot;minimum_index_compatibility_version&quot;</span> : <span class="string">&quot;6.0.0-beta1&quot;</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">&quot;tagline&quot;</span> : <span class="string">&quot;You Know, for Search&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="Kibana-배포하기"><a href="#Kibana-배포하기" class="headerlink" title="Kibana 배포하기"></a>Kibana 배포하기</h1><h2 id="Kibana-배포에-경우-Service와-Deployment가-명시된-yaml-파일을-생성하고-배포하면-된다-위와-같은-서비스-방식인-NodePort로-서비스를-배포하고-포트번호-5601만-명시해주면-노드포트-형식의-서비스가-포트-포워딩을-수행하여-웹에서-접속할-수-있다"><a href="#Kibana-배포에-경우-Service와-Deployment가-명시된-yaml-파일을-생성하고-배포하면-된다-위와-같은-서비스-방식인-NodePort로-서비스를-배포하고-포트번호-5601만-명시해주면-노드포트-형식의-서비스가-포트-포워딩을-수행하여-웹에서-접속할-수-있다" class="headerlink" title="Kibana 배포에 경우 Service와 Deployment가 명시된 yaml 파일을 생성하고 배포하면 된다. 위와 같은 서비스 방식인 NodePort로 서비스를 배포하고 포트번호 5601만 명시해주면 노드포트 형식의 서비스가 포트 포워딩을 수행하여 웹에서 접속할 수 있다."></a>Kibana 배포에 경우 Service와 Deployment가 명시된 yaml 파일을 생성하고 배포하면 된다. 위와 같은 서비스 방식인 NodePort로 서비스를 배포하고 포트번호 5601만 명시해주면 노드포트 형식의 서비스가 포트 포워딩을 수행하여 웹에서 접속할 수 있다.</h2><p><code>kubectl create -f kibana.yaml</code></p><p><em>kibana.yaml</em></p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">kibana</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">elastic</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">kibana</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">NodePort</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">port:</span> <span class="number">5601</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">kibana</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">kibana</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">elastic</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">kibana</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">kibana</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">kibana</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">kibana</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">docker.elastic.co/kibana/kibana:7.2.0</span></span><br><span class="line">        <span class="attr">resources:</span></span><br><span class="line">          <span class="attr">limits:</span></span><br><span class="line">            <span class="attr">cpu:</span> <span class="string">1000m</span></span><br><span class="line">          <span class="attr">requests:</span></span><br><span class="line">            <span class="attr">cpu:</span> <span class="string">100m</span></span><br><span class="line">        <span class="attr">env:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">ELASTICSEARCH_URL</span></span><br><span class="line">            <span class="attr">value:</span> <span class="string">http://elasticsearch:9200</span></span><br><span class="line">        <span class="attr">ports:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">5601</span></span><br></pre></td></tr></table></figure><h2 id="Check-Status-1"><a href="#Check-Status-1" class="headerlink" title="Check Status"></a>Check Status</h2><p>정상적으로 Kibana 가 배포되었는지 <code>kubectl get svc -n elastic</code>을 통해 확인된 포트로 <em>(9200에 포트포워딩 된 포트 확인)</em> curl 명령을 수행한다.</p><p><code>curl http://&#123;ip&#125;:&#123;port&#125;</code></p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li><a href="https://www.digitalocean.com/community/tutorials/how-to-set-up-an-elasticsearch-fluentd-and-kibana-efk-logging-stack-on-kubernetes">How To Set Up an Elasticsearch, Fluentd and Kibana (EFK) Logging Stack on Kubernetes</a></li><li><a href="https://blog.voidmainvoid.net/153">Elasticsearch와 Kibana, filebeat 를 활용한 쿠버네티스 로깅 아키텍쳐</a></li></ul><hr><p>2020.02.06 made by <em>jaejun.lee</em></p>]]></content:encoded>
      
      <comments>https://jx2lee.github.io/cloud-deploy_es/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[Cloud] rook-ceph을 이용한 Ceph cluster 구성</title>
      <link>https://jx2lee.github.io/cloud-install_rook_ceph/</link>
      <guid>https://jx2lee.github.io/cloud-install_rook_ceph/</guid>
      <pubDate>Wed, 05 Feb 2020 15:00:00 GMT</pubDate>
      <description>
      
        &lt;p&gt;K8s 클러스터 내 rook-ceph을 이용한 Ceph cluster를 구성한다.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Update Note&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;2020.03.30 : yaml 파일이 포함된 github 주소 추가&lt;/li&gt;
&lt;/ul&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p>K8s 클러스터 내 rook-ceph을 이용한 Ceph cluster를 구성한다.</p><p><strong>Update Note</strong></p><ul><li>2020.03.30 : yaml 파일이 포함된 github 주소 추가</li></ul><a id="more"></a><h1 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h1><p><img src="https://rook.io/docs/rook/v0.9/media/rook-architecture.png" alt=""></p><p>rook-ceph은 Ceph 클러스터 및 다른 component들을 CRD(Custom Resource Definition)으로 관리하며 CRD의 변경사항을 Rook Operator를 이용해 일괄 적용할 수 있다</p><h1 id="설치-순서"><a href="#설치-순서" class="headerlink" title="설치 순서"></a>설치 순서</h1><h2 id="아래-순서와-같이-설치를-진행"><a href="#아래-순서와-같이-설치를-진행" class="headerlink" title="아래 순서와 같이 설치를 진행"></a>아래 순서와 같이 설치를 진행</h2><ul><li><p>특정 위치에 git repository를 clone 한다. 이후 common.yaml, operator.yaml 을 이용해 rook-ceph에서 제공하는 CRD와 operator를 생성한다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/rook/rook.git</span><br><span class="line"><span class="built_in">cd</span> /rook/cluster/example/kubenetes/cephfs</span><br><span class="line">kubectl apply -f common.yamlkubectl apply -f operator.yaml</span><br></pre></td></tr></table></figure></li><li><p>ceph_config_override.yaml 과 cluster.yaml 을 이용해 configmap을 생성하고 cluster를 구성한다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f ceph_config_override.yaml</span><br><span class="line">kubectl apply -f cluster.yaml</span><br></pre></td></tr></table></figure><p><em>ceph_config_override.yaml</em></p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ConfigMap</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">rook-config-override</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">rook-ceph</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="attr">config:</span> <span class="string">|</span></span><br><span class="line">    [<span class="string">global</span>]</span><br><span class="line">    <span class="string">mon</span> <span class="string">osd</span> <span class="string">down</span> <span class="string">out</span> <span class="string">interval</span> <span class="string">=</span> &#123;<span class="string">osd_down_out_interval</span>&#125;</span><br><span class="line">    <span class="string">mon</span> <span class="string">clock</span> <span class="string">drift</span> <span class="string">allowed</span> <span class="string">=</span> <span class="number">0.2</span></span><br></pre></td></tr></table></figure><p><em>cluster.yaml</em></p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">ceph.rook.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">CephCluster</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">rook-ceph</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">rook-ceph</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">cephVersion:</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">ceph/ceph:v14.2.4-20190917</span></span><br><span class="line">    <span class="attr">allowUnsupported:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">dataDirHostPath:</span> <span class="string">/var/lib/rook</span></span><br><span class="line">  <span class="attr">skipUpgradeChecks:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">mon:</span></span><br><span class="line">    <span class="attr">count:</span> <span class="number">1</span>    <span class="comment"># Recommendation: Use odd numbers (ex. 3, 5)</span></span><br><span class="line">  <span class="attr">dashboard:</span></span><br><span class="line">    <span class="attr">enabled:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">ssl:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">monitoring:</span></span><br><span class="line">    <span class="attr">enabled:</span> <span class="literal">false</span>  <span class="comment"># Require Prometheus to be pre-installed</span></span><br><span class="line">    <span class="attr">rulesNamespace:</span> <span class="string">rook-ceph</span></span><br><span class="line">  <span class="attr">network:</span></span><br><span class="line">    <span class="attr">hostNetwork:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">rbdMirroring:</span></span><br><span class="line">    <span class="attr">workers:</span> <span class="number">0</span></span><br><span class="line">  <span class="attr">mgr:</span></span><br><span class="line">    <span class="attr">modules:</span></span><br><span class="line">    <span class="comment"># The pg_autoscaler is only available on nautilus or newer. remove this if testing mimic.</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">pg_autoscaler</span></span><br><span class="line">      <span class="attr">enabled:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">storage:</span></span><br><span class="line">    <span class="attr">useAllNodes:</span> <span class="literal">true</span>      <span class="comment"># Apply ceph-osd to all nodes.</span></span><br><span class="line">    <span class="attr">useAllDevices:</span> <span class="literal">false</span></span><br><span class="line">    <span class="attr">deviceFilter:</span></span><br><span class="line">    <span class="attr">config:</span></span><br><span class="line">      <span class="attr">journalSizeMB:</span> <span class="string">&quot;1024&quot;</span>  <span class="comment"># This value can be removed for environments with normal sized disks (20 GB or larger)</span></span><br><span class="line">      <span class="attr">osdsPerDevice:</span> <span class="string">&quot;1&quot;</span>   <span class="comment"># This value can be overridden at the node or device level</span></span><br><span class="line">    <span class="attr">directories:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">path:</span> <span class="string">/var/lib/rook</span></span><br></pre></td></tr></table></figure></li><li><p>toolbox.yaml 을 이용해 ceph 클러스터 이용을 위한 client를 설치한다</p><p><code>kubectl apply -f toolbox.yaml</code></p></li><li><p>block_pool.yaml  과 file_system.yaml을 이용해 block / file storage 배포 준비</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f block_pool.yaml</span><br><span class="line">kubectl apply -f file_system.yaml</span><br></pre></td></tr></table></figure><p><em>block_pool.yaml</em></p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">ceph.rook.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">CephBlockPool</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">replicapool</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">rook-ceph</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">failureDomain:</span> <span class="string">host</span></span><br><span class="line">  <span class="attr">replicated:</span></span><br><span class="line">    <span class="attr">size:</span> <span class="number">2</span></span><br></pre></td></tr></table></figure><p><em>file_system.yaml</em></p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">ceph.rook.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">CephFilesystem</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">myfs</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">rook-ceph</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">metadataPool:</span></span><br><span class="line">  <span class="comment"># failureDomain - values are possible for &#x27;osd&#x27; and &#x27;host&#x27;</span></span><br><span class="line">    <span class="attr">failureDomain:</span> <span class="string">host</span>  <span class="comment"># ceph-osd must exist equal or more than replicated size</span></span><br><span class="line">    <span class="attr">replicated:</span></span><br><span class="line">      <span class="attr">size:</span> <span class="number">2</span></span><br><span class="line">  <span class="attr">dataPools:</span></span><br><span class="line">  <span class="comment"># failureDomain - values are possible for &#x27;osd&#x27; and &#x27;host&#x27;</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">failureDomain:</span> <span class="string">host</span>  <span class="comment"># ceph-osd must exist equal or more than replicated size</span></span><br><span class="line">      <span class="attr">replicated:</span></span><br><span class="line">        <span class="attr">size:</span> <span class="number">2</span></span><br><span class="line">  <span class="attr">metadataServer:</span></span><br><span class="line">    <span class="attr">activeCount:</span> <span class="number">1</span></span><br><span class="line">    <span class="attr">activeStandby:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure></li><li><p>마지막으로 block_sc.yaml 과 file_sc.yaml을 이용해 각 block / file storageclass를 생성한다</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f block_sc.yaml</span><br><span class="line">kubectl apply -f file_sc.yaml</span><br></pre></td></tr></table></figure><p><em>block_sc.yaml</em></p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">storage.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">StorageClass</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">   <span class="attr">name:</span> <span class="string">rook-ceph-block</span></span><br><span class="line"><span class="attr">provisioner:</span> <span class="string">rook-ceph.rbd.csi.ceph.com</span></span><br><span class="line"><span class="attr">parameters:</span></span><br><span class="line">    <span class="comment"># clusterID is the namespace where the rook cluster is running</span></span><br><span class="line">    <span class="comment"># If you change this namespace, also change the namespace below where the secret namespaces are defined</span></span><br><span class="line">    <span class="attr">clusterID:</span> <span class="string">rook-ceph</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Ceph pool into which the RBD image shall be created</span></span><br><span class="line">    <span class="attr">pool:</span> <span class="string">replicapool</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># RBD image format. Defaults to &quot;2&quot;.</span></span><br><span class="line">    <span class="attr">imageFormat:</span> <span class="string">&quot;2&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># RBD image features. Available for imageFormat: &quot;2&quot;. CSI RBD currently supports only `layering` feature.</span></span><br><span class="line">    <span class="attr">imageFeatures:</span> <span class="string">layering</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># The secrets contain Ceph admin credentials. These are generated automatically by the operator</span></span><br><span class="line">    <span class="comment"># in the same namespace as the cluster.</span></span><br><span class="line">    <span class="attr">csi.storage.k8s.io/provisioner-secret-name:</span> <span class="string">rook-csi-rbd-provisioner</span></span><br><span class="line">    <span class="attr">csi.storage.k8s.io/provisioner-secret-namespace:</span> <span class="string">rook-ceph</span></span><br><span class="line">    <span class="attr">csi.storage.k8s.io/node-stage-secret-name:</span> <span class="string">rook-csi-rbd-node</span></span><br><span class="line">    <span class="attr">csi.storage.k8s.io/node-stage-secret-namespace:</span> <span class="string">rook-ceph</span></span><br><span class="line">    <span class="comment"># Specify the filesystem type of the volume. If not specified, csi-provisioner</span></span><br><span class="line">    <span class="comment"># will set default as `ext4`.</span></span><br><span class="line">    <span class="attr">csi.storage.k8s.io/fstype:</span> <span class="string">ext4</span></span><br><span class="line"><span class="comment"># uncomment the following to use rbd-nbd as mounter on supported nodes</span></span><br></pre></td></tr></table></figure><p><em>file_sc.yaml</em></p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">storage.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">StorageClass</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">csi-cephfs-sc</span></span><br><span class="line"><span class="attr">provisioner:</span> <span class="string">rook-ceph.cephfs.csi.ceph.com</span></span><br><span class="line"><span class="attr">parameters:</span></span><br><span class="line">  <span class="comment"># clusterID is the namespace where operator is deployed.</span></span><br><span class="line">  <span class="attr">clusterID:</span> <span class="string">rook-ceph</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># CephFS filesystem name into which the volume shall be created</span></span><br><span class="line">  <span class="attr">fsName:</span> <span class="string">myfs</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Ceph pool into which the volume shall be created</span></span><br><span class="line">  <span class="comment"># Required for provisionVolume: &quot;true&quot;</span></span><br><span class="line">  <span class="attr">pool:</span> <span class="string">myfs-data0</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Root path of an existing CephFS volume</span></span><br><span class="line">  <span class="comment"># Required for provisionVolume: &quot;false&quot;</span></span><br><span class="line">  <span class="comment"># rootPath: /absolute/path</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># The secrets contain Ceph admin credentials. These are generated automatically by the operator</span></span><br><span class="line">  <span class="comment"># in the same namespace as the cluster.</span></span><br><span class="line">  <span class="attr">csi.storage.k8s.io/provisioner-secret-name:</span> <span class="string">rook-csi-cephfs-provisioner</span></span><br><span class="line">  <span class="attr">csi.storage.k8s.io/provisioner-secret-namespace:</span> <span class="string">rook-ceph</span></span><br><span class="line">  <span class="attr">csi.storage.k8s.io/node-stage-secret-name:</span> <span class="string">rook-csi-cephfs-node</span></span><br><span class="line">  <span class="attr">csi.storage.k8s.io/node-stage-secret-namespace:</span> <span class="string">rook-ceph</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># (optional) The driver can use either ceph-fuse (fuse) or ceph kernel client (kernel)</span></span><br><span class="line">  <span class="comment"># If omitted, default volume mounter will be used - this is determined by probing for ceph-fuse</span></span><br><span class="line">  <span class="comment"># or by setting the default mounter explicitly via --volumemounter command-line argument.</span></span><br><span class="line">  <span class="comment"># mounter: kernel</span></span><br><span class="line"><span class="attr">reclaimPolicy:</span> <span class="string">Delete</span></span><br><span class="line"><span class="attr">mountOptions:</span></span><br><span class="line">  <span class="comment"># uncomment the following line for debugging</span></span><br><span class="line">  <span class="comment">#- debug</span></span><br></pre></td></tr></table></figure></li></ul><p>위 단계를 거치고 난 뒤 rook-ceph namespace의 pod와 storageclass 를 확인한다</p><p><code>kubectl get pods -n rook-ceph</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">root@k8s-master:~/jlee/rook-ceph-master<span class="comment"># kubectl get pods -n rook-ceph</span></span><br><span class="line">NAME                                           READY   STATUS      RESTARTS   AGE</span><br><span class="line">csi-cephfsplugin-7kz7v                         3/3     Running     0          4d19h</span><br><span class="line">csi-cephfsplugin-9rt7t                         3/3     Running     0          4d19h</span><br><span class="line">csi-cephfsplugin-dnggh                         3/3     Running     0          4d19h</span><br><span class="line">csi-cephfsplugin-provisioner-974b566d9-7k2rb   4/4     Running     0          4d19h</span><br><span class="line">csi-cephfsplugin-provisioner-974b566d9-kxg2f   4/4     Running     0          4d19h</span><br><span class="line">csi-cephfsplugin-xzt9b                         3/3     Running     0          4d19h</span><br><span class="line">csi-rbdplugin-2npvg                            3/3     Running     0          4d19h</span><br><span class="line">csi-rbdplugin-drzkp                            3/3     Running     0          4d19h</span><br><span class="line">csi-rbdplugin-hhsm5                            3/3     Running     0          4d19h</span><br><span class="line">csi-rbdplugin-provisioner-579c546f5-qprb8      5/5     Running     0          4d19h</span><br><span class="line">csi-rbdplugin-provisioner-579c546f5-svhlw      5/5     Running     0          4d19h</span><br><span class="line">csi-rbdplugin-qhsw6                            3/3     Running     0          4d19h</span><br><span class="line">rook-ceph-mds-myfs-a-58ddc89fc8-s4f44          1/1     Running     0          4d19h</span><br><span class="line">rook-ceph-mds-myfs-b-85dc7c7cf4-x68lk          1/1     Running     0          4d19h</span><br><span class="line">rook-ceph-mgr-a-69df8d6794-glbjb               1/1     Running     0          4d19h</span><br><span class="line">rook-ceph-mon-a-7b9cb64846-zfbwf               1/1     Running     0          4d19h</span><br><span class="line">rook-ceph-mon-b-7fc7c8fbb4-75j9j               1/1     Running     0          4d19h</span><br><span class="line">rook-ceph-mon-c-6c59c89fbc-rn8nv               1/1     Running     0          4d19h</span><br><span class="line">rook-ceph-operator-7985c4b57d-8qtht            1/1     Running     0          4d19h</span><br><span class="line">rook-ceph-osd-0-55888686c-pf6wn                1/1     Running     0          4d19h</span><br><span class="line">rook-ceph-osd-1-f56d885d4-tnrmv                1/1     Running     0          4d19h</span><br><span class="line">rook-ceph-osd-2-68f99d999f-zlrl4               1/1     Running     0          4d19h</span><br><span class="line">rook-ceph-osd-3-7545f4df9b-ng4tf               1/1     Running     0          4d19h</span><br><span class="line">rook-ceph-osd-prepare-k8s-node1-msfs9          0/1     Completed   0          4d19h</span><br><span class="line">rook-ceph-osd-prepare-k8s-node2-z858m          0/1     Completed   0          4d19h</span><br><span class="line">rook-ceph-osd-prepare-k8s-node3-lwh4c          0/1     Completed   0          4d19h</span><br><span class="line">rook-ceph-osd-prepare-k8s-node4-w8rfw          0/1     Completed   0          4d19h</span><br><span class="line">rook-ceph-tools-8648fbb998-5q7v2               1/1     Running     0          4d19h</span><br><span class="line">rook-discover-85fzl                            1/1     Running     0          4d19h</span><br><span class="line">rook-discover-djj97                            1/1     Running     0          4d19h</span><br><span class="line">rook-discover-p7cwx                            1/1     Running     0          4d19h</span><br><span class="line">rook-discover-zvn5f                            1/1     Running     0          4d19h</span><br></pre></td></tr></table></figure><p><code>kubectl get storageclass</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">root@k8s-master:~<span class="comment"># kubectl get storageclass</span></span><br><span class="line">NAME                        PROVISIONER                     AGE</span><br><span class="line">csi-cephfs-sc               rook-ceph.cephfs.csi.ceph.com   16h</span><br><span class="line">rook-ceph-block             rook-ceph.rbd.csi.ceph.com      16h</span><br></pre></td></tr></table></figure><blockquote><p><em>위 언급한 yaml 파일은 github 에 올려 두었다. <a href="https://github.com/jaejuning/rook-ceph-deploy">https://github.com/jaejuning/rook-ceph-deploy</a></em></p></blockquote><h1 id="Ceph-cluster-상태-확인"><a href="#Ceph-cluster-상태-확인" class="headerlink" title="Ceph cluster 상태 확인"></a>Ceph cluster 상태 확인</h1><h2 id="설치가-완료되었다면-구축한-ceph-cluster가-정상-구동되었는지-확인하여야-한다-toolbox-pod-를-통해-pod-네임을-확인한다"><a href="#설치가-완료되었다면-구축한-ceph-cluster가-정상-구동되었는지-확인하여야-한다-toolbox-pod-를-통해-pod-네임을-확인한다" class="headerlink" title="설치가 완료되었다면 구축한 ceph cluster가 정상 구동되었는지 확인하여야 한다. toolbox pod 를 통해 pod 네임을 확인한다"></a>설치가 완료되었다면 구축한 ceph cluster가 정상 구동되었는지 확인하여야 한다. toolbox pod 를 통해 pod 네임을 확인한다</h2><p><code>kubectl -n rook-ceph get pod -l &quot;app=rook-ceph-tools&quot;</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">root@k8s-master:~<span class="comment"># kubectl -n rook-ceph get pod -l &quot;app=rook-ceph-tools&quot;</span></span><br><span class="line">NAME                               READY   STATUS    RESTARTS   AGE</span><br><span class="line">rook-ceph-tools-8648fbb998-dzbbd   1/1     Running   0          15h</span><br></pre></td></tr></table></figure><ul><li><p>확인된 pod 네임을 통해 exec 명령어로 해당 컨테이너로 접속</p><p><code>kubectl exec -it -n rook-ceph [위 결과로 나온 pod NAME] -- /bin/bash</code></p></li></ul><h2 id="Ceph-cluster-상태-확인-1"><a href="#Ceph-cluster-상태-확인-1" class="headerlink" title="Ceph cluster 상태 확인"></a>Ceph cluster 상태 확인</h2><p><code>ceph -s</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-node3 /] ceph -s</span><br><span class="line">  cluster:</span><br><span class="line">    id:     9d3a534e-797f-4659-af8d-4bfb5f60f76c</span><br><span class="line">    health: HEALTH_OK</span><br><span class="line"></span><br><span class="line">  services:</span><br><span class="line">    mon: 1 daemons, quorum a (age 16h)</span><br><span class="line">    mgr: a(active, since 15h)</span><br><span class="line">    mds: myfs:1 &#123;0=myfs-a=up:active&#125; 1 up:standby-replay</span><br><span class="line">    osd: 2 osds: 2 up (since 15h), 2 <span class="keyword">in</span> (since 15h)</span><br><span class="line"></span><br><span class="line">  data:</span><br><span class="line">    pools:   3 pools, 24 pgs</span><br><span class="line">    objects: 537 objects, 1.4 GiB</span><br><span class="line">    usage:   53 GiB used, 45 GiB / 98 GiB avail</span><br><span class="line">    pgs:     24 active+clean</span><br></pre></td></tr></table></figure><h2 id="Ceph-cluster-disk-확인"><a href="#Ceph-cluster-disk-확인" class="headerlink" title="Ceph cluster disk 확인"></a>Ceph cluster disk 확인</h2><p><code>ceph df</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-node3 /] ceph df</span><br><span class="line">RAW STORAGE:</span><br><span class="line">    CLASS     SIZE       AVAIL      USED       RAW USED     %RAW USED </span><br><span class="line">    ssd       98 GiB     45 GiB     53 GiB       53 GiB         53.87 </span><br><span class="line">    TOTAL     98 GiB     45 GiB     53 GiB       53 GiB         53.87 </span><br><span class="line"></span><br><span class="line">POOLS:</span><br><span class="line">    POOL              ID     STORED      OBJECTS     USED        %USED     MAX AVAIL </span><br><span class="line">    replicapool        1     1.4 GiB         509     1.4 GiB      3.50        19 GiB </span><br><span class="line">    myfs-metadata      2     2.2 KiB          28     2.2 KiB         0        19 GiB </span><br><span class="line">    myfs-data0         3         0 B           0         0 B         0        19 GiB </span><br></pre></td></tr></table></figure><h2 id="RBD-image-사용량"><a href="#RBD-image-사용량" class="headerlink" title="RBD image 사용량"></a>RBD image 사용량</h2><p><code>rbd du -p replicapoll</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">NAME                                         PROVISIONED USED    </span><br><span class="line">csi-vol-22712573-4815-11ea-9f90-aea9eb69a9f1      10 GiB 344 MiB </span><br><span class="line">csi-vol-6d1536b2-47fc-11ea-9f90-aea9eb69a9f1      10 GiB 300 MiB </span><br><span class="line">csi-vol-d38c964b-4814-11ea-9f90-aea9eb69a9f1      10 GiB 404 MiB </span><br><span class="line">csi-vol-d4745340-4814-11ea-9f90-aea9eb69a9f1      10 GiB 396 MiB </span><br><span class="line">csi-vol-d4937470-4814-11ea-9f90-aea9eb69a9f1      20 GiB 244 MiB </span><br><span class="line">csi-vol-d4a291d9-4814-11ea-9f90-aea9eb69a9f1      20 GiB 400 MiB </span><br><span class="line">&lt;TOTAL&gt;                                           80 GiB 2.0 GiB </span><br></pre></td></tr></table></figure><p>reclaimPolicy를 Retain으로 설정할 경우, pv를 지워도 RBD image가 ceph cluster에 남게 되는데, 이 경우에는 <code>rbd ls</code>, <code>rbd rm</code> 등을 통해 rbd 리스트를 확인하고 삭제해야 한다.</p><h1 id="Troubleshooting"><a href="#Troubleshooting" class="headerlink" title="Troubleshooting"></a>Troubleshooting</h1><p>내 경우 클러스터의 한 노드에서 csi-rbdplugin pod가 생성되지 않고 CrashLoopBack 이 걸리는 현상이 발생하였다. 노드 문제를 해결하지 못하여 우회하는 방안으로 <strong>해당 노드에 파드가 설정되지 않게 taint 조건을 추가</strong>하여 문제를 해결하였다.</p><p><code>kubectl taint nodes &#123;csi-rbdplugin pod를 생성하지 못하는 노드&#125; key=value:NoSchedule-</code></p><p>이후에 rook-ceph cluster를 재 구축하면 csi-rbdplugin이 정상 작동함을 확인할 수 있다.</p><hr><p>2020.02.06 made by <em>jaejun.lee</em></p>]]></content:encoded>
      
      <comments>https://jx2lee.github.io/cloud-install_rook_ceph/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[Cloud] Terminating State에 빠진 Namespace 삭제</title>
      <link>https://jx2lee.github.io/cloud-delete_ns_at_terminating_state/</link>
      <guid>https://jx2lee.github.io/cloud-delete_ns_at_terminating_state/</guid>
      <pubDate>Mon, 03 Feb 2020 15:00:00 GMT</pubDate>
      <description>
      
        &lt;p&gt;K8s namespace를 삭제하다보면 state가 &lt;code&gt;Terminating&lt;/code&gt;이면서 get namespace 결과에 계속 남아있는 문제가 발생하는데, 이를 해결하는 방법을 다룬다.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Updata Note&lt;/strong&gt;  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;2020.03.13 : Advanced 추가 (shell script)&lt;/li&gt;
&lt;/ul&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p>K8s namespace를 삭제하다보면 state가 <code>Terminating</code>이면서 get namespace 결과에 계속 남아있는 문제가 발생하는데, 이를 해결하는 방법을 다룬다.</p><p><strong>Updata Note</strong>  </p><ul><li>2020.03.13 : Advanced 추가 (shell script)</li></ul><a id="more"></a><h1 id="문제-발생"><a href="#문제-발생" class="headerlink" title="문제 발생"></a>문제 발생</h1><p><code>rook-ceph</code> 네임스페이스를 생성하다 삭제하면서 아래와 같은 문제가 발생하였다</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get ns</span><br><span class="line">NAME              STATUS        AGE</span><br><span class="line">default           Active        2d21h</span><br><span class="line">istio-system      Active        6h40m</span><br><span class="line">knative-serving   Active        6h40m</span><br><span class="line">kube-node-lease   Active        2d21h</span><br><span class="line">kube-public       Active        2d21h</span><br><span class="line">kube-system       Active        2d21h</span><br><span class="line">rook-ceph         Terminating   16m</span><br></pre></td></tr></table></figure><p>Terminating 중인 네임스페이스를 한 번 더 지우는 명령어를 수행하면 에러가 발생한다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Error from server (Conflict): Operation cannot be fulfilled on namespaces <span class="string">&quot;rook-ceph&quot;</span>: The system is ensuring all content is removed from this namespace.  Upon completion, this namespace will automatically be purged by the system.</span><br></pre></td></tr></table></figure><h1 id="문제-해결"><a href="#문제-해결" class="headerlink" title="문제 해결"></a>문제 해결</h1><p>해당 Namespace의 yaml 파일을 살펴보면, <code>.spec/finalizers</code>  부분에 Kubernetes라 명시되어 있다. 이를 빈 공백으로 바꾸고 적용하는 순서로 진행한다.</p><ul><li><code>jq</code> 패키지를 설치하고 <em>(apt get install jq)</em> 아래 명령어를 수행한다.<br><code>kubectl get namespace $NAMESPACE -o json |jq &#39;.spec = &#123;&quot;finalizers&quot;:[]&#125;&#39; &gt; temp.json</code></li><li>명령어를 수행한 디렉토리에 <code>temp.json</code>이 생성되는데, 이를 yaml 파일로 적용할 것이다. 이때 필요한 Ip/port를 아래 명령어로 확인한다.<br><code>kubectl proxy &amp;</code></li><li>curl 명령어로 수정사항을 반영한다.<br><code>curl -k -H &quot;Content-Type: application/json&quot; -X PUT --data-binary @temp.json http://127.0.0.1:8001/api/v1/namespaces/&#123;Namespace-name&#125;/finalize</code></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get namespaces</span><br><span class="line">NAME              STATUS   AGE</span><br><span class="line">default           Active   2d21h</span><br><span class="line">istio-system      Active   6h43m</span><br><span class="line">knative-serving   Active   6h43m</span><br><span class="line">kube-node-lease   Active   2d21h</span><br><span class="line">kube-public       Active   2d21h</span><br><span class="line">kube-system       Active   2d21h</span><br></pre></td></tr></table></figure><h1 id="Advanced-Shell-script"><a href="#Advanced-Shell-script" class="headerlink" title="(Advanced) Shell script"></a>(Advanced) Shell script</h1><p>이런 에러가 발생할 때마다 일일이 찾기 귀찮아서 쉘 스크립트 공부도 할 겸 <code>deleteNS.sh</code> 스크립트를 구현하였다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#! /bin/bash</span></span><br><span class="line"><span class="comment">#  delete namespaces</span></span><br><span class="line"></span><br><span class="line">NS=<span class="variable">$1</span></span><br><span class="line"></span><br><span class="line">kubectl get namespace <span class="variable">$NS</span> -o json |jq <span class="string">&#x27;.spec = &#123;&quot;finalizers&quot;:[]&#125;&#x27;</span> &gt; temp.json</span><br><span class="line">kubectl proxy &amp;</span><br><span class="line">curl -k -H <span class="string">&quot;Content-Type: application/json&quot;</span> -X PUT --data-binary @temp.json http://127.0.0.1:8001/api/v1/namespaces/<span class="variable">$NS</span>/finalize</span><br><span class="line"><span class="built_in">kill</span> %1 &amp;&amp; rm tmp.json <span class="comment"># proxy process 를 다운하고 tmp.json 파일을 삭제</span></span><br></pre></td></tr></table></figure><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li><a href="https://nasermirzaei89.net/2019/01/27/delete-namespace-stuck-at-terminating-state/">Delete Namespace Stuck At Terminating State, https://nasermirzaei89.net/2019/01/27/delete-namespace-stuck-at-terminating-state/</a></li></ul><hr><p>2020.02.04 made by <em>jaejun.lee</em></p>]]></content:encoded>
      
      <comments>https://jx2lee.github.io/cloud-delete_ns_at_terminating_state/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[Cloud] K8s cluster node 추가 및 삭제</title>
      <link>https://jx2lee.github.io/cloud-manage_node/</link>
      <guid>https://jx2lee.github.io/cloud-manage_node/</guid>
      <pubDate>Tue, 28 Jan 2020 15:00:00 GMT</pubDate>
      <description>
      
        &lt;p&gt;K8s 클러스터에 노드를 추가 및 삭제하는 과정을 다룬다.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Update Node&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;2020.03.10 : kubectl drain 설명 추가&lt;/li&gt;
&lt;/ul&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p>K8s 클러스터에 노드를 추가 및 삭제하는 과정을 다룬다.</p><p><strong>Update Node</strong></p><ul><li>2020.03.10 : kubectl drain 설명 추가</li></ul><a id="more"></a><h1 id="상태-확인"><a href="#상태-확인" class="headerlink" title="상태 확인"></a>상태 확인</h1><p><code>kubectl get nodes</code>로 노드 상태를 확인한다</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">root@k8s-master:~<span class="comment"># kubectl get nodes</span></span><br><span class="line">NAME          STATUS     ROLES    AGE     VERSION</span><br><span class="line">ai.bips       NotReady   &lt;none&gt;   11m     v1.15.3</span><br><span class="line">bigdata-svr   Ready      &lt;none&gt;   8m24s   v1.15.3</span><br><span class="line">k8s-master    Ready      master   6d17h   v1.15.3</span><br><span class="line">k8s-node1     Ready      master   6d17h   v1.15.3</span><br><span class="line">k8s-node2     Ready      master   6d17h   v1.15.3</span><br><span class="line">k8s-node3     Ready      &lt;none&gt;   4m14s   v1.15.3</span><br></pre></td></tr></table></figure><p><strong>k8s-node3</strong> 노드를 삭제하고 추가해보도록 하자</p><h1 id="Delete-k8s-master-amp-Reset-k8s-node3"><a href="#Delete-k8s-master-amp-Reset-k8s-node3" class="headerlink" title="Delete (k8s-master) &amp; Reset (k8s-node3)"></a>Delete (k8s-master) &amp; Reset (k8s-node3)</h1><p>마스터 노드에서 <strong>k8s-node3</strong>를 delete 한다</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl delete node k8s-node3</span><br><span class="line">node <span class="string">&quot;k8s-node3&quot;</span> deleted</span><br></pre></td></tr></table></figure><blockquote><p><em>kubectl delete 말고 kubectl drain {node_name} 을 수행하면 이미 띄워져있는 해당 노드의 파드들을 클러스터 내 다른 노드로 이동시키는 명령이다. delete 보다 drain을 수행하여 클러스터에서 제외시키는 것이 관리에 더 용이할 것으로 보인다.</em></p></blockquote><p>이후, 삭제한 노드에서 <code>kubeadm</code>을 통해 reset 한다. reset을 하게되면 이후 마스터 노드에서 k8s-node3가 삭제되었음을 확인한다</p><p><code>kubeadm reset</code></p><blockquote><p><em>reset 하지 않으면 이전 정보가 남아있어 추후에 join 수행 시 error 발생</em></p></blockquote><h1 id="In-master-node"><a href="#In-master-node" class="headerlink" title="In master node,"></a>In master node,</h1><p>kubeadm init 을 통해 생성된 토큰을 확인하기 위해서 3개 마스터 중 한 대 노드에서 token 을 확인한다</p><p><code>kubeadm token list</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">TOKEN                     TTL         EXPIRES                USAGES                   DESCRIPTION                                           EXTRA GROUPS</span><br><span class="line">4e3bad.gzp017frj86g4ngi   23h         2020-01-30T01:53:56Z   authentication,signing   &lt;none&gt;                                                system:bootstrappers:kubeadm:default-node-token</span><br><span class="line">eo7vb4.3ck3l5ja3ry78bef   &lt;invalid&gt;   2020-01-23T08:03:36Z   authentication,signing   &lt;none&gt;                                                system:bootstrappers:kubeadm:default-node-token</span><br><span class="line">m8470a.r8fo1tbwmdhb39eo   22h         2020-01-30T00:58:34Z   authentication,signing   &lt;none&gt;                                                system:bootstrappers:kubeadm:default-node-token</span><br><span class="line">sds92v.mk937ek75jygtrlo   &lt;invalid&gt;   2020-01-22T10:03:36Z   &lt;none&gt;                   Proxy <span class="keyword">for</span> managing TTL <span class="keyword">for</span> the kubeadm-certs secret   &lt;none&gt;</span><br></pre></td></tr></table></figure><p><em>EXPIERS</em> : invalid 하지 않는 토큰이 없는 경우, <code>kubeadm token create(or generate)</code>로 토큰을 설정한다. 만약 expired 되지 않았다면, join 명렁어의 토큰으로 사용한다</p><p>이후 hash 값이 필요하므로 다음 명령어를 통해 hash 값을 확인한다</p><p><code>openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2&gt;/dev/null | openssl dgst -sha256 -hex | sed &#39;s/^.* //&#39;</code></p><h1 id="In-k8s-node3"><a href="#In-k8s-node3" class="headerlink" title="In k8s-node3,"></a>In k8s-node3,</h1><p><strong>k8s-node3</strong>에서 위 master 노드를 통해 확인한 값으로 join 을 수행한다</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ kubeadm join 192.168.179.171:6443 --token 4e3bad.gzp017frj86g4ngi \</span><br><span class="line">    --discovery-token-ca-cert-hash sha256:b141f77ea7c5749767bd7a1dfc54f256ef374969b08f660f1c131453ebed7091</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line">This node has joined the cluster:</span><br><span class="line">* Certificate signing request was sent to apiserver and a response was received.</span><br><span class="line">* The Kubelet was informed of the new secure connection details.</span><br></pre></td></tr></table></figure><blockquote><p><em>IP는 마스터 IP(주의 : 삼중화를 진행하였으므로, 3개 마스터 통신을 담당하는 VIP로 작성) port는 6443</em></p></blockquote><p>추가를 완료하였다. 이후에 master 노드에서 <code>kubectl get nodes</code> 를 하면 하기 출력을 확인할 수 있다</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">NAME          STATUS   ROLES    AGE     VERSION</span><br><span class="line">bigdata-svr   Ready    &lt;none&gt;   23m     v1.15.3</span><br><span class="line">k8s-master    Ready    master   6d18h   v1.15.3</span><br><span class="line">k8s-node1     Ready    master   6d18h   v1.15.3</span><br><span class="line">k8s-node2     Ready    master   6d18h   v1.15.3</span><br><span class="line">k8s-node3     Ready    &lt;none&gt;   3m16s   v1.15.3 <span class="comment"># Check!</span></span><br></pre></td></tr></table></figure><h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><p>K8s cluster에 노드를 추가하고 삭제하는 과정을 다뤘다. 노드를 삭제하고 삭제한 노드에서 리셋을 진행한 다음, 마스터에서의 <code>token 및 hash value</code>를 추가 노드에 join에 이용하였다</p><ul><li><p>노드 삭제</p><p><code>kubectl delete node &#123;node-name&#125;</code></p></li><li><p>노드 리셋</p><p><code>kubeadm reset</code></p></li><li><p>cluster token 확인</p><p><code>kubadm token list</code>, expired 된 토큰 발견 시 <code>kubeadm token create</code>로 새 토큰 생성</p></li><li><p>hash값 확인</p><p><code>openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2&gt;/dev/null | openssl dgst -sha256 -hex | sed &#39;s/^.* //&#39;</code></p></li></ul><hr><p>2020.01.29 made by <em>jaejun.lee</em></p>]]></content:encoded>
      
      <comments>https://jx2lee.github.io/cloud-manage_node/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[Shell] .DS_Store 삭제 script</title>
      <link>https://jx2lee.github.io/shell-delete_ds_store/</link>
      <guid>https://jx2lee.github.io/shell-delete_ds_store/</guid>
      <pubDate>Sun, 26 Jan 2020 15:00:00 GMT</pubDate>
      <description>
      
        &lt;p&gt;Mac Finder로 파일을 탐색하다 보면 &lt;code&gt;.DS_Store&lt;/code&gt; 이라는 파일이 생성된다. 성가시다! git을 사용할 때항상 .gitignore로 명시를 해야되며, 모든 폴더에 적용하고 push 한 경험이 있을거다. 쉘 공부하면서 간단한 스크립트를 이번 포스트에서 작성해본다.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Update Note&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;2020.04.01 : 스크립트 수정 (변수 입력 및 usage)&lt;/li&gt;
&lt;/ul&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p>Mac Finder로 파일을 탐색하다 보면 <code>.DS_Store</code> 이라는 파일이 생성된다. 성가시다! git을 사용할 때항상 .gitignore로 명시를 해야되며, 모든 폴더에 적용하고 push 한 경험이 있을거다. 쉘 공부하면서 간단한 스크립트를 이번 포스트에서 작성해본다.</p><p><strong>Update Note</strong></p><ul><li>2020.04.01 : 스크립트 수정 (변수 입력 및 usage)</li></ul><a id="more"></a><h1 id="DS-Store"><a href="#DS-Store" class="headerlink" title=".DS_Store?"></a>.DS_Store?</h1><p>DS_STORE 파일이란 Desktop Services Store의 약자로 애플에서 정의한 파일 포맷이다. 애플의 맥 OS X 시스템이 폴더에 접근할 때 생기는 해당 폴더에 대한 메타데이터를 저장하는 파일이다. 윈도우의 thumb.db 파일과 비슷하다. 분석해보면 해당 디렉토리 크기, 아이콘의 위치, 폴더의 배경에 대한 정보들을 얻을 수 있다. 맥 OS 환경에서만 생성 및 사용되지만,파일을 공유하는 과정에서 이 파일도 같이 공유되는 경우가 있다. <a href="https://chp747.tistory.com/54">출처</a></p><h1 id="Shell-script"><a href="#Shell-script" class="headerlink" title="Shell script"></a>Shell script</h1><p>간단한 한 줄 짜리 명령으로 루트 디렉토리부터 삭제할 수 있다. 하지만 나는 이게 귀찮았다. 그리고 Finder로는 <code>/</code> 디렉토리까지 갈 일이 없고, <code>/Users/jj</code> 아래 하위 디렉토리에 생성되는 DS_Store 파일이 거슬렸다. hackerrank shell 문제를 푸는 디렉토리에 <code>rm-ds-store.sh</code> 파일을 생성하고 스크립트를 작성하였다</p><p>sudo와 find로 쉽게 작성할 수 있다. 쉘 실행 시 <code>stdin</code>으로 패스워드 입력값을 받아 sudo 명렁을 실행하고, 삭제되는 내역을 print 한다. 내용은 하기와 같다</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"><span class="comment"># delete all .DS_Store file from path</span></span><br><span class="line"><span class="comment"># Wed, 01.04.2020</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span>  [ <span class="string">&quot;<span class="variable">$#</span>&quot;</span> -ne 1 ]; <span class="keyword">then</span></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">&quot;usage   : <span class="variable">$0</span> &#123;path&#125;&quot;</span></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">&quot;example : <span class="variable">$0</span> /Users/jj&quot;</span></span><br><span class="line">        <span class="built_in">exit</span> 0</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line">path=<span class="variable">$1</span></span><br><span class="line">sudo --stdin find <span class="variable">$&#123;path&#125;</span> -name <span class="string">&quot;.DS_Store&quot;</span> -<span class="built_in">print</span> -delete</span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;.DS_Store clear in <span class="variable">$&#123;path&#125;</span> !&quot;</span></span><br></pre></td></tr></table></figure><h1 id="Result"><a href="#Result" class="headerlink" title="Result"></a>Result</h1><p><code>./rm-ds-store.sh</code> 명령어를 실행하면 Usage 를 확인할 수 있다. 정리하고자 하는 디렉토리를 삽입하면 된다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">~/shell/custom</span><br><span class="line">❯ ./rm-ds-store.sh</span><br><span class="line">usage   : ./rm-ds-store.sh &#123;path&#125;</span><br><span class="line">example : ./rm-ds-store.sh /Users/jj</span><br><span class="line"></span><br><span class="line">~/shell/custom</span><br><span class="line">❯ ./rm-ds-store.sh /Users/jj</span><br><span class="line">Password:</span><br><span class="line">.DS_Store clear <span class="keyword">in</span> /Users/jj..!</span><br></pre></td></tr></table></figure><p><del>삭제된 위치가 프린트하며 동시에 삭제하여 완료됨을 확인할 수 있다. 원래는 <code>/</code> 하위 디렉토리 모두 검사하였지만 굳이 검사할 필요가 없었다 <em>(이유는 .DS_Store 의 정의를 생각하면 된다)</em>. 쉘 스크립트 공부하면서 실제로 써볼 수 있는 toy project 였고 더 활용할 수 있는 방안을 고민해야겠다.</del> 인자값이 없으면 Usage 를 출력하고 원하는 디렉토리를 추가하여 하위 디렉토리를 모두 검색하는 스크립트로 수정하였다. 조금씩 더 활용할 방안을 생각하면서 수정할 계획이다.</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li><a href="https://chp747.tistory.com/54">.DS_STORE 파일이란, https://chp747.tistory.com/54</a></li></ul><hr><p>2020.01.27 made by <em>jaejun.lee</em></p>]]></content:encoded>
      
      <comments>https://jx2lee.github.io/shell-delete_ds_store/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[Cloud] Kubeflow 설치</title>
      <link>https://jx2lee.github.io/cloud-install_kubeflow/</link>
      <guid>https://jx2lee.github.io/cloud-install_kubeflow/</guid>
      <pubDate>Wed, 22 Jan 2020 15:00:00 GMT</pubDate>
      <description>
      
        &lt;p&gt;K8s cluster 위 Kubeflow를 설치하는 과정을 다룬다. K8s cluster 구축은 &lt;a href=&quot;https://jaejuning.github.io/2020/01/22/2020-01-22-cloud-k8s_cluster/&quot;&gt;포스트&lt;/a&gt; 참고&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Update Note&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;2020.02.25 : Trouble Shooting 추가&lt;/li&gt;
&lt;li&gt;2020.03.13 : Kubeflow 삭제 추가, kfctl 버젼 명시&lt;/li&gt;
&lt;li&gt;2020.03.18 : Dashboard UI 수정&lt;/li&gt;
&lt;/ul&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p>K8s cluster 위 Kubeflow를 설치하는 과정을 다룬다. K8s cluster 구축은 <a href="https://jaejuning.github.io/2020/01/22/2020-01-22-cloud-k8s_cluster/">포스트</a> 참고</p><p><strong>Update Note</strong></p><ul><li>2020.02.25 : Trouble Shooting 추가</li><li>2020.03.13 : Kubeflow 삭제 추가, kfctl 버젼 명시</li><li>2020.03.18 : Dashboard UI 수정</li></ul><a id="more"></a><h1 id="K8s-환경"><a href="#K8s-환경" class="headerlink" title="K8s 환경"></a>K8s 환경</h1><h2 id="Storageclass-default-설정-확인"><a href="#Storageclass-default-설정-확인" class="headerlink" title="Storageclass default 설정 확인"></a>Storageclass default 설정 확인</h2><p>storage class name default 설정을 위해 아래 커맨드라인을 수행</p><p><code>kubectl patch storageclass [storage class 명] -p &#39;&#123;&quot;metadata&quot;: &#123;&quot;annotations&quot;:&#123;&quot;storageclass.kubernetes.io/is-default-class&quot;:&quot;true&quot;&#125;&#125;&#125;&#39;</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get sc <span class="comment">#sc : storageclass</span></span><br><span class="line">NAME                        PROVISIONER                     AGE</span><br><span class="line">csi-cephfs-sc               rook-ceph.cephfs.csi.ceph.com   16h</span><br><span class="line">rook-ceph-block (default)   rook-ceph.rbd.csi.ceph.com      16h</span><br></pre></td></tr></table></figure><h1 id="Kubeflow-설치-파일-다운"><a href="#Kubeflow-설치-파일-다운" class="headerlink" title="Kubeflow 설치 파일 다운"></a>Kubeflow 설치 파일 다운</h1><p><a href="https://github.com/kubeflow/kubeflow/releases">https://github.com/kubeflow/kubeflow/releases</a>에서 최신 kfctl 바이너리를 다운받아 압축을 해제한다.</p><blockquote><p><em>본인은 v1.0.1-0-gf3edb9b 을 사용하였다.</em></p></blockquote><h1 id="Kubeflow-환경-설정"><a href="#Kubeflow-환경-설정" class="headerlink" title="Kubeflow 환경 설정"></a>Kubeflow 환경 설정</h1><h2 id="bashrc"><a href="#bashrc" class="headerlink" title=".bashrc"></a>.bashrc</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> KF_NAME=my-kubeflow</span><br><span class="line"><span class="built_in">export</span> BASE_DIR=/root/kubeflow</span><br><span class="line"><span class="built_in">export</span> KF_DIR=<span class="variable">$&#123;BASE_DIR&#125;</span>/<span class="variable">$&#123;KF_NAME&#125;</span></span><br><span class="line"><span class="built_in">export</span> CONFIG_FILE=<span class="variable">$&#123;KF_DIR&#125;</span>/kfctl_k8s_istio.0.7.0.yaml</span><br><span class="line"><span class="built_in">export</span> CONFIG_URI=<span class="string">&quot;https://raw.githubusercontent.com/kubeflow/manifests/v0.7-branch/kfdef/kfctl_k8s_istio.0.7.0.yaml&quot;</span></span><br></pre></td></tr></table></figure><h2 id="kfctl-k8s-istio-0-7-0-yaml"><a href="#kfctl-k8s-istio-0-7-0-yaml" class="headerlink" title="kfctl_k8s_istio.0.7.0.yaml"></a>kfctl_k8s_istio.0.7.0.yaml</h2><p><code>wget -O kfctl_k8s_istio.0.7.0.yaml $CONFIG_URI</code></p><h1 id="Kubeflow-deploy"><a href="#Kubeflow-deploy" class="headerlink" title="Kubeflow deploy"></a>Kubeflow deploy</h1><h2 id="Binary-이동"><a href="#Binary-이동" class="headerlink" title="Binary 이동"></a>Binary 이동</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> <span class="variable">$&#123;BASE_DIR&#125;</span></span><br><span class="line">chmod 755 kfctl</span><br><span class="line">mv kfctl /usr/bin</span><br></pre></td></tr></table></figure><h2 id="설치"><a href="#설치" class="headerlink" title="설치"></a>설치</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> <span class="variable">$KF_DIR</span></span><br><span class="line">kfctl apply -V -f <span class="variable">$&#123;CONFIG_FILE&#125;</span></span><br></pre></td></tr></table></figure><h1 id="Access-Kubeflow-UI"><a href="#Access-Kubeflow-UI" class="headerlink" title="Access Kubeflow UI"></a>Access Kubeflow UI</h1><p><code>http://&#123;NODE_IP&#125;:31380</code>로 접속한다</p><p><img src="/image/kubeflow-login.png" alt=""></p><blockquote><p><em><del>Master가 많은 경우에 <strong>centraldashboard</strong> pod 를 확인하여, 해당 host ip를 사용하면 접속이 가능하다 (Master, Worker IP로 접속이 모두 가능)</del></em></p><ul><li>Service 를 NodePort 로 설정하였기때문에 클러스터 내 모든 노드의 IP로 접근이 가능하다.</li></ul></blockquote><h1 id="Truuble-Shooting"><a href="#Truuble-Shooting" class="headerlink" title="Truuble Shooting"></a>Truuble Shooting</h1><h2 id="Artifacts-또는-Executions-탭에서-error-mysql-query-failed-errno-2006"><a href="#Artifacts-또는-Executions-탭에서-error-mysql-query-failed-errno-2006" class="headerlink" title="Artifacts 또는 Executions 탭에서 error mysql_query failed errno 2006"></a>Artifacts 또는 Executions 탭에서 error mysql_query failed errno 2006</h2><p>해당 탭으로 이동하면 mysql_query failed 에러가 발생하는 경우가 있다. 이때에는 metadata-grpc-deployment pod 를 재시작 하면 된다.</p><p><code>kubectl get pod &#123;pod_name&#125; -n kubeflow -o yaml | kubectl replace --force -f-</code> </p><h1 id="Kubeflow-삭제"><a href="#Kubeflow-삭제" class="headerlink" title="Kubeflow 삭제"></a>Kubeflow 삭제</h1><p>Kubeflow 를 삭제하는 방법은 아래 <strong>kfctl delete</strong> 명령어를 사용한다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> <span class="variable">$&#123;KF_DIR&#125;</span></span><br><span class="line">kfctl delete -f <span class="variable">$&#123;CONFIG_FILE&#125;</span></span><br></pre></td></tr></table></figure><ul><li><p>명령어 수행 후 <strong>kubectl get all -n kubeflow</strong> 로 모든 내용이 삭제되었는-f 지 확인한다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">  root@k8s-master:~/kubeflow/my-kubeflow<span class="comment"># kubectl get all -n kubeflow</span></span><br><span class="line">No resources found.</span><br></pre></td></tr></table></figure></li></ul><hr><p>2020.01.23 made by <em>jaejun.lee</em></p>]]></content:encoded>
      
      <comments>https://jx2lee.github.io/cloud-install_kubeflow/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[Cloud] K8s cluster 구축</title>
      <link>https://jx2lee.github.io/cloud-install_k8s/</link>
      <guid>https://jx2lee.github.io/cloud-install_k8s/</guid>
      <pubDate>Tue, 21 Jan 2020 15:00:00 GMT</pubDate>
      <description>
      
        &lt;p&gt;K8s cluster는 Master 3대&lt;em&gt;(k8s-master/k8s-node1/k8s-node2)&lt;/em&gt; / Worker 2대&lt;em&gt;(k8s-node3/k8s-node4)&lt;/em&gt;로 구성한다.&lt;/p&gt;
&lt;p&gt;k8s는 1.15.3 version 이며 서버는 ubuntu 18.04 이다.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Update Note&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;2020.02.25 : kubeadm init 시 &lt;code&gt;--upload-certs&lt;/code&gt; 옵션 부가 설명&lt;/li&gt;
&lt;li&gt;2020.03.12 : Calico CNI 설정 추가&lt;/li&gt;
&lt;li&gt;2020.03.17 : Trouble shooting 추가 - Calico node not working &lt;/li&gt;
&lt;li&gt;2020.06.04 : Taint NoSchedule 변경 (&lt;code&gt;/ to =&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;2020.09.26 : VIP 포트 추가 설명&lt;/li&gt;
&lt;/ul&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p>K8s cluster는 Master 3대<em>(k8s-master/k8s-node1/k8s-node2)</em> / Worker 2대<em>(k8s-node3/k8s-node4)</em>로 구성한다.</p><p>k8s는 1.15.3 version 이며 서버는 ubuntu 18.04 이다.</p><p><strong>Update Note</strong></p><ul><li>2020.02.25 : kubeadm init 시 <code>--upload-certs</code> 옵션 부가 설명</li><li>2020.03.12 : Calico CNI 설정 추가</li><li>2020.03.17 : Trouble shooting 추가 - Calico node not working </li><li>2020.06.04 : Taint NoSchedule 변경 (<code>/ to =</code>)</li><li>2020.09.26 : VIP 포트 추가 설명</li></ul><a id="more"></a><h1 id="공통"><a href="#공통" class="headerlink" title="공통"></a>공통</h1><p>모든 노드에 공통으로 수행한다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">apt-get install -y apt-transport-https curl</span><br><span class="line">apt-get update</span><br><span class="line">curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - <span class="comment"># Docker 공식 GPG key</span></span><br><span class="line">sudo apt-key fingerprint 0EBFCD88</span><br><span class="line">sudo add-apt-repository \</span><br><span class="line">    <span class="string">&quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu \</span></span><br><span class="line"><span class="string">    <span class="subst">$(lsb_release -cs)</span> \</span></span><br><span class="line"><span class="string">    stable&quot;</span></span><br><span class="line">apt-get install -y docker-ce</span><br></pre></td></tr></table></figure><p>1.15.3 버젼에 맞는 kubelet / kubeadm / kubectl 를 설치한다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -</span><br><span class="line"><span class="built_in">echo</span> deb http://apt.kubernetes.io/ kubernetes-xenial main &gt; /etc/apt/sources.list.d/kubernetes.list</span><br><span class="line">apt-get update</span><br><span class="line">apt-get install -y kubeadm=1.15.3-00 kubelet=1.15.3-00 kubectl=1.15.3-00</span><br></pre></td></tr></table></figure><p>Kubernets는 swap를 off시켜야 작동하므로 swap 메모리를 끈다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">free</span><br><span class="line">swapoff -a</span><br><span class="line">vi /etc/fstab</span><br><span class="line"><span class="comment"># 재부팅시 자동으로 swapoff 하려면 위 파일에서 swap 부분 주석 처리</span></span><br></pre></td></tr></table></figure><p>방화벽을 내려준다 <em>(ubnut : ufw)</em></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">systemctl stop ufw </span><br><span class="line">systemctl <span class="built_in">disable</span> ufw</span><br></pre></td></tr></table></figure><p><strong>/etc/docker/daemon.json</strong> 의 파일을 수정하고 도커 데몬을 재실행한다</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">cat &gt; /etc/docker/daemon.json &lt;&lt;<span class="string">EOF</span></span><br><span class="line"><span class="string">&#123;</span></span><br><span class="line"><span class="string">&quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;], &quot;log-driver&quot;: &quot;json-file&quot;,</span></span><br><span class="line"><span class="string">&quot;log-opts&quot;: &#123;</span></span><br><span class="line"><span class="string">&quot;max-size&quot;: &quot;100m&quot; &#125;,</span></span><br><span class="line"><span class="string">&quot;storage-driver&quot;: &quot;overlay2&quot;, &quot;insecure-registries&quot;: [&quot;192.168.179.185:5000&quot;]</span></span><br><span class="line"><span class="string">&#125;</span></span><br><span class="line"><span class="string">EOF</span></span><br><span class="line"></span><br><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl restart docker</span><br></pre></td></tr></table></figure><blockquote><ul><li><em>insecure-registries 는 VIP(virtual ip)를 나타내는데, 삼중화 했을 때 마스터 간 통신을 위한 ip. 뒤에 포트 5000은 후에 마스터 부분에 설치할 keepalived 에서 사용한다</em></li><li>VIP Port: kubernetes API Server 통신에 사용하는 default port</li></ul></blockquote><p>위 커맨드 라인을 정리한 script는 다음과 같다. 해당 스크립트를 작성하여 5개 노드에서 모두 실행한다</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"><span class="comment"># install basic package &amp; docker-ce &amp; k8s utils</span></span><br><span class="line"></span><br><span class="line">apt-get install -y apt-transport-https curl</span><br><span class="line">apt-get update</span><br><span class="line">curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - <span class="comment"># Docker 공식 GPG key</span></span><br><span class="line">sudo apt-key fingerprint 0EBFCD88</span><br><span class="line">sudo add-apt-repository \</span><br><span class="line">  <span class="string">&quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu \</span></span><br><span class="line"><span class="string">  <span class="subst">$(lsb_release -cs)</span> \</span></span><br><span class="line"><span class="string">  stable&quot;</span></span><br><span class="line">apt-get install -y docker-ce</span><br><span class="line"></span><br><span class="line">curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -</span><br><span class="line"><span class="built_in">echo</span> deb http://apt.kubernetes.io/ kubernetes-xenial main &gt; /etc/apt/sources.list.d/kubernetes.list</span><br><span class="line">apt-get update</span><br><span class="line">apt-get install -y kubeadm=1.15.3-00 kubelet=1.15.3-00 kubectl=1.15.3-00</span><br><span class="line"></span><br><span class="line">free</span><br><span class="line">swapoff -a</span><br><span class="line"></span><br><span class="line">ufw <span class="built_in">disable</span></span><br><span class="line">ufw status</span><br><span class="line"></span><br><span class="line">cat &gt; /etc/docker/daemon.json &lt;&lt;<span class="string">EOF</span></span><br><span class="line"><span class="string">&#123;</span></span><br><span class="line"><span class="string">&quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;], &quot;log-driver&quot;: &quot;json-file&quot;,</span></span><br><span class="line"><span class="string">&quot;log-opts&quot;: &#123;</span></span><br><span class="line"><span class="string">&quot;max-size&quot;: &quot;100m&quot; &#125;,</span></span><br><span class="line"><span class="string">&quot;storage-driver&quot;: &quot;overlay2&quot;, &quot;insecure-registries&quot;: [&quot;192.168.179.185:5000&quot;]</span></span><br><span class="line"><span class="string">&#125;</span></span><br><span class="line"><span class="string">EOF</span></span><br><span class="line"></span><br><span class="line">systemctl daemon-reload &amp;&amp; <span class="built_in">echo</span> <span class="string">&quot;restart docker-daemon&quot;</span></span><br><span class="line">systemctl restart docker &amp;&amp; <span class="built_in">echo</span> <span class="string">&quot;restart docker&quot;</span></span><br></pre></td></tr></table></figure><h1 id="Master-Node"><a href="#Master-Node" class="headerlink" title="Master Node"></a>Master Node</h1><h2 id="Keepalived-설치"><a href="#Keepalived-설치" class="headerlink" title="Keepalived 설치"></a>Keepalived 설치</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">apt-get install -y keepalived</span><br><span class="line">vi /etc/keepalived/keepalived.conf</span><br></pre></td></tr></table></figure><p><code>keepalived.conf</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">vrrp_instance VI_1 &#123;</span><br><span class="line">    state BACKUP</span><br><span class="line">    interface enp0s8</span><br><span class="line">    virtual_router_id 50</span><br><span class="line">    priority 100 <span class="comment"># 이후 마스터부터 1씩 감소하여 수정</span></span><br><span class="line">    advert_int 1</span><br><span class="line">    nopreempt</span><br><span class="line">    authentication &#123;</span><br><span class="line">        auth_type PASS</span><br><span class="line">        auth_pass $ place secure password here.</span><br><span class="line">    &#125;</span><br><span class="line">    virtual_ipaddress &#123;</span><br><span class="line">        192.168.179.185</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>interface : ifconfig -a로 확인</li><li>priority : master 마다 다른 값 설정<ul><li>priority 값이 높으면 최우선적으로 master 역할 수행</li><li>100, 99, 98 로 설정</li></ul></li><li>virtual_ipaddress : 앞전에 docker daemon의 vip 주소 기입<ul><li>VIP 이어도 아무 ip나 사용하면 혹여나 충돌이 일어날까봐 할당받은 ip 사용</li></ul></li></ul><p>keepalived 서비스 재시작 후 상태(<code>$ ip addr</code>)를 확인한다</p><p><code>$ systemctl restart keepalived &amp;&amp; systemctl status keepalived</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">3: enp6s0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UP group default qlen 1000</span><br><span class="line">    link/ether 64:e5:99:fa:52:c1 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 192.168.179.172/24 brd 192.168.179.255 scope global enp6s0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet 192.168.179.185/32 scope global enp6s0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fe80::66e5:99ff:fefa:52c1/64 scope link </span><br><span class="line">       valid_lft forever preferred_lft forever</span><br></pre></td></tr></table></figure><blockquote><p><em>VIP로 설정한 192.168.179.171 이 보이는 것을 확인 // 하나의 master에만 보임 (나머지는 stand by)</em></p></blockquote><h2 id="K8s-설치"><a href="#K8s-설치" class="headerlink" title="K8s 설치"></a>K8s 설치</h2><p><strong>kubeadm-config.yaml</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: kubeadm.k8s.io/v1beta2</span><br><span class="line">kind: ClusterConfiguration</span><br><span class="line">kubernetesVersion: <span class="string">&quot;v1.15.3&quot;</span></span><br><span class="line">controlPlaneEndpoint: <span class="string">&quot;192.168.179.185:6443&quot;</span></span><br><span class="line">networking:</span><br><span class="line">    serviceSubnet: <span class="string">&quot;10.96.0.0/16&quot;</span></span><br><span class="line">    podSubnet: <span class="string">&quot;10.244.0.0/16&quot;</span></span><br><span class="line">apiServer:</span><br><span class="line">    extraArgs:</span><br><span class="line">        advertise-address: <span class="string">&quot;192.168.179.185”</span></span><br></pre></td></tr></table></figure><ul><li>위 Yaml 파일은 첫 번째 master에서만 작성 후 init 을 수행한다.</li><li>v1.15.3으로 작성한다.</li></ul><blockquote><p><em>controlPlaneEndpoint와 advertise-address 는 VIP이고 포트로 6443으로 지정</em></p></blockquote><p>Yaml 파일을 이용해 클러스터 초기화를 진행한다.</p><p><code>$ kubeadm init --config=kubeadm-config.yaml --upload-certs</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">Your Kubernetes control-plane has initialized successfully!</span><br><span class="line"></span><br><span class="line">To start using your cluster, you need to run the following as a regular user:</span><br><span class="line"></span><br><span class="line">  mkdir -p <span class="variable">$HOME</span>/.kube</span><br><span class="line">  sudo cp -i /etc/kubernetes/admin.conf <span class="variable">$HOME</span>/.kube/config</span><br><span class="line">  sudo chown $(id -u):$(id -g) <span class="variable">$HOME</span>/.kube/config</span><br><span class="line"></span><br><span class="line">You should now deploy a pod network to the cluster.</span><br><span class="line">Run <span class="string">&quot;kubectl apply -f [podnetwork].yaml&quot;</span> with one of the options listed at:</span><br><span class="line">  https://kubernetes.io/docs/concepts/cluster-administration/addons/</span><br><span class="line"></span><br><span class="line">You can now join any number of the control-plane node running the following <span class="built_in">command</span> on each as root:</span><br><span class="line"></span><br><span class="line">  kubeadm join 192.168.179.185:6443 --token iicz2g.0a8b07vasikwuthz \</span><br><span class="line">    --discovery-token-ca-cert-hash sha256:249ee21a200d807f21dad0102eb638e50904102c7e7ae8f6388c1654f60ae1a0 \</span><br><span class="line">    --control-plane --certificate-key 553c75881e6825bf9e5f3887b2100de4a3d979777a313b70281c5bbe5ddeef80</span><br><span class="line"></span><br><span class="line">Please note that the certificate-key gives access to cluster sensitive data, keep it secret!</span><br><span class="line">As a safeguard, uploaded-certs will be deleted <span class="keyword">in</span> two hours; If necessary, you can use </span><br><span class="line"><span class="string">&quot;kubeadm init phase upload-certs --upload-certs&quot;</span> to reload certs afterward.</span><br><span class="line"></span><br><span class="line">Then you can join any number of worker nodes by running the following on each as root:</span><br><span class="line"></span><br><span class="line">kubeadm join 192.168.179.185:6443 --token iicz2g.0a8b07vasikwuthz \</span><br><span class="line">    --discovery-token-ca-cert-hash sha256:249ee21a200d807f21dad0102eb638e50904102c7e7ae8f6388c1654f60ae1a0 </span><br></pre></td></tr></table></figure><blockquote><p><strong>*–upload-certs</strong> 옵션은 Master 이중화 시 key 인증을 init 명령과 수행해주는 역할을 한다. 이 옵션을 주지 않으면 이중화 대상 Master 에 key 인증을 수행해야 한다.*</p></blockquote><p>서버 주소 및 인증서를 복사하는 단계로 아래 명령어를 수행한다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p <span class="variable">$HOME</span>/.kube</span><br><span class="line">cp -i /etc/kubernetes/admin.conf <span class="variable">$HOME</span>/.kube/config</span><br><span class="line">chown $(id -u):$(id -g) <span class="variable">$HOME</span>/.kube/config</span><br></pre></td></tr></table></figure><p><code>$ kubectl get pods --all-namespaces</code> 명령어로 아래와 같은 파드가 생성되기 기다린다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">root@k8s-master:~<span class="comment"># kgpo --all-namespaces</span></span><br><span class="line">NAMESPACE     NAME                                 READY   STATUS    RESTARTS   AGE</span><br><span class="line">kube-system   coredns-5c98db65d4-9fpzf             0/1     Pending   0          3m2s</span><br><span class="line">kube-system   coredns-5c98db65d4-vn8d5             0/1     Pending   0          3m2s</span><br><span class="line">kube-system   etcd-k8s-master                      1/1     Running   0          119s</span><br><span class="line">kube-system   kube-apiserver-k8s-master            1/1     Running   0          2m14s</span><br><span class="line">kube-system   kube-controller-manager-k8s-master   1/1     Running   0          2m21s</span><br><span class="line">kube-system   kube-proxy-c4cnn                     1/1     Running   0          3m1s</span><br><span class="line">kube-system   kube-scheduler-k8s-master            1/1     Running   0          2m7s</span><br></pre></td></tr></table></figure><blockquote><p><em>coredns 는 CNI를 설치해야 Running 상태가 된다.</em></p></blockquote><p><a href="https://docs.projectcalico.org/v3.9/manifests/calico.yaml">calico yaml</a>을 다운받아 <strong>CALICO_IPV4POOL_CIDR</strong> 값을 10.244.0.0/16 으로 변경한 후 CNI 를 설치한다.</p><p><code>$ kubectl apply -f kube-calico.yaml</code></p><blockquote><p><em>CALICO_IPV4POOL_CIDR 값을 10.244.0.0/16 으로 변경되어있는지 확인한 후 배포한다. 클러스터 구성한 노드 IP 대역과 같으면 충돌이 일어난다고 한다.</em></p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">root@k8s-master:~<span class="comment"># kgpo --all-namespaces</span></span><br><span class="line">NAMESPACE     NAME                                       READY   STATUS    RESTARTS   AGE</span><br><span class="line">kube-system   calico-kube-controllers-56cd854695-65j42   1/1     Running   0          89s</span><br><span class="line">kube-system   calico-node-ptq8x                          1/1     Running   0          89s</span><br><span class="line">kube-system   coredns-5c98db65d4-9fpzf                   1/1     Running   0          7m29s</span><br><span class="line">kube-system   coredns-5c98db65d4-vn8d5                   1/1     Running   0          7m29s</span><br><span class="line">kube-system   etcd-k8s-master                            1/1     Running   0          6m26s</span><br><span class="line">kube-system   kube-apiserver-k8s-master                  1/1     Running   0          6m41s</span><br><span class="line">kube-system   kube-controller-manager-k8s-master         1/1     Running   0          6m48s</span><br><span class="line">kube-system   kube-proxy-c4cnn                           1/1     Running   0          7m28s</span><br><span class="line">kube-system   kube-scheduler-k8s-master                  1/1     Running   0          6m34s</span><br></pre></td></tr></table></figure><p>kubeadm init 시 생성된 커맨드를 다른 마스터 노드에서 실행하여 Join 한다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kubeadm join 192.168.179.185:6443 --token iicz2g.0a8b07vasikwuthz \</span><br><span class="line"> --discovery-token-ca-cert-hash sha256:249ee21a200d807f21dad0102eb638e50904102c7e7ae8f6388c1654f60ae1a0 \</span><br><span class="line"> --control-plane --certificate-key 553c75881e6825bf9e5f3887b2100de4a3d979777a313b70281c5bbe5ddeef80</span><br></pre></td></tr></table></figure><p>서버 주소 및 인증서를 복사하는 명령어를 수행한다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p <span class="variable">$HOME</span>/.kube</span><br><span class="line">cp -i /etc/kubernetes/admin.conf <span class="variable">$HOME</span>/.kube/config</span><br><span class="line">chown $(id -u):$(id -g) <span class="variable">$HOME</span>/.kube/config</span><br></pre></td></tr></table></figure><h1 id="Worker-Node"><a href="#Worker-Node" class="headerlink" title="Worker Node"></a>Worker Node</h1><p>kubeadm init 시 나온 토큰과 해쉬값으로 각 워커 노드에서 Join 한다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubeadm join 192.168.179.185:6443 --token iicz2g.0a8b07vasikwuthz \</span><br><span class="line"> --discovery-token-ca-cert-hash sha256:249ee21a200d807f21dad0102eb638e50904102c7e7ae8f6388c1654f60ae1a0</span><br></pre></td></tr></table></figure><h1 id="설치-확인"><a href="#설치-확인" class="headerlink" title="설치 확인"></a>설치 확인</h1><h2 id="Kubectl-get-nodes"><a href="#Kubectl-get-nodes" class="headerlink" title="Kubectl get nodes"></a>Kubectl get nodes</h2><p><code>kubectl get nodes -o wide</code><br>master node에서 확인하면 클러스터에 포함된 노드들을 확인할 수 있다</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">NAME         STATUS   ROLES    AGE   VERSION   INTERNAL-IP       EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME</span><br><span class="line">k8s-master   Ready    master   44m   v1.15.3   192.168.179.172   &lt;none&gt;        Ubuntu 18.04.4 LTS   5.3.0-28-generic    docker://19.3.8</span><br><span class="line">k8s-node1    Ready    master   41m   v1.15.3   192.168.179.173   &lt;none&gt;        Ubuntu 18.04.3 LTS   4.15.0-88-generic   docker://19.3.6</span><br><span class="line">k8s-node2    Ready    master   40m   v1.15.3   192.168.179.174   &lt;none&gt;        Ubuntu 18.04.3 LTS   4.15.0-88-generic   docker://19.3.6</span><br><span class="line">k8s-node3    Ready    &lt;none&gt;   40m   v1.15.3   192.168.179.175   &lt;none&gt;        Ubuntu 18.04.3 LTS   4.15.0-88-generic   docker://19.3.8</span><br><span class="line">k8s-node4    Ready    &lt;none&gt;   40m   v1.15.3   192.168.179.176   &lt;none&gt;        Ubuntu 18.04.3 LTS   4.15.0-88-generic   docker://19.3.8</span><br></pre></td></tr></table></figure><h2 id="선택사항-Master-node에도-pod-배포가-가능한-상태로-변환"><a href="#선택사항-Master-node에도-pod-배포가-가능한-상태로-변환" class="headerlink" title="(선택사항) Master node에도 pod 배포가 가능한 상태로 변환"></a>(선택사항) Master node에도 pod 배포가 가능한 상태로 변환</h2><p>master에 Pod를 배포할 수 있는 상태로 변환하기 위해 taint 조건을 해제한다.</p><p><code>kubectl taint nodes &#123;node-name&#125; node-role.kubernetes.io=master:NoSchedule-</code></p><blockquote><p><em>만약 노드를 리스케쥴 되지 않게 복구하려면 kubectl taint nodes {node-name} node-role.kubernetes.io/master:NoSchedule</em></p></blockquote><h1 id="Trouble-Shooting"><a href="#Trouble-Shooting" class="headerlink" title="Trouble-Shooting"></a>Trouble-Shooting</h1><h2 id="calico-node-가-Running-상태이지만-Unhealthy-문제"><a href="#calico-node-가-Running-상태이지만-Unhealthy-문제" class="headerlink" title="calico node 가 Running 상태이지만 Unhealthy 문제"></a>calico node 가 Running 상태이지만 Unhealthy 문제</h2><p><a href="https://github.com/projectcalico/calico/issues/2904">https://github.com/projectcalico/calico/issues/2904</a> 문제와 같다. calico-node 가 클러스터 내 노드 수만큼 기동하지만 0/1 Running 상태였다. 문제가 있는 파드를 describe 해본 결과 다음과 같다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">Events:</span><br><span class="line">  Type     Reason          Age                   From                 Message</span><br><span class="line">  ----     ------          ----                  ----                 -------</span><br><span class="line">  Normal   Pulled          45m                   kubelet, k8s-master  Container image <span class="string">&quot;calico/cni:v3.9.5&quot;</span> already present on machine</span><br><span class="line">  Normal   Created         45m                   kubelet, k8s-master  Created container upgrade-ipam</span><br><span class="line">  Normal   Started         45m                   kubelet, k8s-master  Started container upgrade-ipam</span><br><span class="line">  Normal   Scheduled       45m                   default-scheduler    Successfully assigned kube-system/calico-node-v5rgk to k8s-master</span><br><span class="line">  Normal   Started         45m                   kubelet, k8s-master  Started container install-cni</span><br><span class="line">  Normal   Pulled          45m                   kubelet, k8s-master  Container image <span class="string">&quot;calico/cni:v3.9.5&quot;</span> already present on machine</span><br><span class="line">  Normal   Created         45m                   kubelet, k8s-master  Created container install-cni</span><br><span class="line">  Normal   Started         45m                   kubelet, k8s-master  Started container flexvol-driver</span><br><span class="line">  Normal   Pulled          45m                   kubelet, k8s-master  Container image <span class="string">&quot;calico/pod2daemon-flexvol:v3.9.5&quot;</span> already present on machine</span><br><span class="line">  Normal   Created         45m                   kubelet, k8s-master  Created container flexvol-driver</span><br><span class="line">  Normal   Pulled          45m                   kubelet, k8s-master  Container image <span class="string">&quot;calico/node:v3.9.5&quot;</span> already present on machine</span><br><span class="line">  Normal   Created         45m                   kubelet, k8s-master  Created container calico-node</span><br><span class="line">  Normal   Started         45m                   kubelet, k8s-master  Started container calico-node</span><br><span class="line">  Warning  Unhealthy       30m (x34 over 45m)    kubelet, k8s-master  Readiness probe failed: calico/node is not ready: felix is not ready: readiness probe reporting 503</span><br><span class="line">  Warning  Unhealthy       25m (x76 over 45m)    kubelet, k8s-master  Readiness probe failed: calico/node is not ready: felix is not ready: Get http://localhost:9099/readiness: dial tcp 127.0.0.1:9099: connect: connection refused</span><br><span class="line">  Warning  Unhealthy       15m (x116 over 45m)   kubelet, k8s-master  Liveness probe failed: calico/node is not ready: Felix is not live: Get http://localhost:9099/liveness: dial tcp 127.0.0.1:9099: connect: connection refused</span><br><span class="line">  Warning  FailedMount     12m                   kubelet, k8s-master  MountVolume.SetUp failed <span class="keyword">for</span> volume <span class="string">&quot;calico-node-token-9j8gt&quot;</span> : couldn<span class="string">&#x27;t propagate object cache: timed out waiting for the condition</span></span><br><span class="line"><span class="string">  Normal   SandboxChanged  12m                   kubelet, k8s-master  Pod sandbox changed, it will be killed and re-created.</span></span><br><span class="line"><span class="string">  Normal   Pulled          12m                   kubelet, k8s-master  Container image &quot;calico/cni:v3.9.5&quot; already present on machine</span></span><br><span class="line"><span class="string">  Normal   Started         12m                   kubelet, k8s-master  Started container upgrade-ipam</span></span><br><span class="line"><span class="string">  Normal   Created         12m                   kubelet, k8s-master  Created container upgrade-ipam</span></span><br><span class="line"><span class="string">  Normal   Started         12m (x2 over 12m)     kubelet, k8s-master  Started container install-cni</span></span><br><span class="line"><span class="string">  Normal   Pulled          12m (x2 over 12m)     kubelet, k8s-master  Container image &quot;calico/cni:v3.9.5&quot; already present on machine</span></span><br><span class="line"><span class="string">  Normal   Created         12m (x2 over 12m)     kubelet, k8s-master  Created container install-cni</span></span><br><span class="line"><span class="string">  Normal   Started         12m                   kubelet, k8s-master  Started container flexvol-driver</span></span><br><span class="line"><span class="string">  Normal   Pulled          12m                   kubelet, k8s-master  Container image &quot;calico/pod2daemon-flexvol:v3.9.5&quot; already present on machine</span></span><br><span class="line"><span class="string">  Normal   Created         12m                   kubelet, k8s-master  Created container flexvol-driver</span></span><br><span class="line"><span class="string">  Normal   Pulled          12m                   kubelet, k8s-master  Container image &quot;calico/node:v3.9.5&quot; already present on machine</span></span><br><span class="line"><span class="string">  Normal   Created         12m                   kubelet, k8s-master  Created container calico-node</span></span><br><span class="line"><span class="string">  Normal   Started         12m                   kubelet, k8s-master  Started container calico-node</span></span><br><span class="line"><span class="string">  Warning  Unhealthy       11m (x2 over 12m)     kubelet, k8s-master  Readiness probe failed: calico/node is not ready: felix is not ready: readiness probe reporting 503</span></span><br><span class="line"><span class="string">  Warning  Unhealthy       7m34s (x19 over 12m)  kubelet, k8s-master  Liveness probe failed: calico/node is not ready: Felix is not live: Get http://localhost:9099/liveness: dial tcp 127.0.0.1:9099: connect: connection refused</span></span><br><span class="line"><span class="string">  Warning  Unhealthy       2m30s (x38 over 12m)  kubelet, k8s-master  Readiness probe failed: calico/node is not ready: felix is not ready: Get http://localhost:9099/readiness: dial tcp 127.0.0.1:9099: connect: connection refused</span></span><br></pre></td></tr></table></figure><blockquote><p><em>너무 길어 Event 부분만 작성하였다.</em></p></blockquote><p>좀 더 deep 한 로그를 찾고자 해당 파드 log 를 찾아본 결과 다음과 같다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">2019-10-03 04:34:38.296 [WARNING][16449] int_dataplane.go 781: failed to wipe the XDP state error=failed to load BPF program (/tmp/felix-bpf-824808941): <span class="built_in">stat</span> /sys/fs/bpf/calico/xdp/prefilter_v1_calico_tmp_A: no such file or directory</span><br><span class="line">libbpf: Error <span class="keyword">in</span> bpf_object__probe_name():Operation not permitted(1). Couldn<span class="string">&#x27;t load basic &#x27;</span>r0 = 0<span class="string">&#x27; BPF program.</span></span><br><span class="line"><span class="string">libbpf: failed to load object &#x27;</span>/tmp/felix-bpf-824808941<span class="string">&#x27;</span></span><br><span class="line"><span class="string">Error: failed to load object file</span></span><br></pre></td></tr></table></figure><p>위 깃헙 이슈 링크를 확인해보면 OS 커널단에서 BPF 관련 오브젝트를 읽어드리지 못한 상태였다. 이슈 내용들을 살펴보니 OS secure boot 한 경우 생길 수 있다하며 <code>$ mokutil --disable-validation</code> 명령어로 안전 모드로 부팅하지 못하게 설정하고 재부팅 하였다. 재부팅 시 명령어로 설정한 패스워드로 secure-mode boot 을 disabled 로 설정하고 재부팅 하니 calico node 가 문제없이 기동됨을 확인하였다.</p><hr><p>2020.01.22 made by <em>jaejun.lee</em></p>]]></content:encoded>
      
      <comments>https://jx2lee.github.io/cloud-install_k8s/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[Cloud] Kubeflow overview</title>
      <link>https://jx2lee.github.io/cloud-introduction_to_kubeflow/</link>
      <guid>https://jx2lee.github.io/cloud-introduction_to_kubeflow/</guid>
      <pubDate>Sun, 12 Jan 2020 15:00:00 GMT</pubDate>
      <description>
      
        &lt;p&gt;Kubeflow 는 &lt;em&gt;Kubernetes 위에서 동작하는 ML toolkit&lt;/em&gt; 이자, ML 파이프 라인을 구축하고 실험하는 data scientist를 위한 플랫폼이다. 머신러닝 시스템 개발, 테스트 및 프로덕션 수준의 서비스를 위해 다양한 환경에 배포하려는 &lt;em&gt;머신러닝 엔지니어&lt;/em&gt; 및 &lt;em&gt;운영 팀&lt;/em&gt;을 위한 것이다. 포스트는 Conceptual overview, ML workflow, Kuberflow Components, interface, example 순서로 작성하였다.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;kubeflow 0.7.0 version 기준으로 작성하였다&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p>Kubeflow 는 <em>Kubernetes 위에서 동작하는 ML toolkit</em> 이자, ML 파이프 라인을 구축하고 실험하는 data scientist를 위한 플랫폼이다. 머신러닝 시스템 개발, 테스트 및 프로덕션 수준의 서비스를 위해 다양한 환경에 배포하려는 <em>머신러닝 엔지니어</em> 및 <em>운영 팀</em>을 위한 것이다. 포스트는 Conceptual overview, ML workflow, Kuberflow Components, interface, example 순서로 작성하였다.</p><blockquote><p><em>kubeflow 0.7.0 version 기준으로 작성하였다</em></p></blockquote><a id="more"></a><h1 id="Conceptual-overview"><a href="#Conceptual-overview" class="headerlink" title="Conceptual overview"></a>Conceptual overview</h1><p>Kubernetes 위 ML 시스템의 구성 요소를 배치한 그림은 다음과 같다.</p><p><img src="https://www.kubeflow.org/docs/images/kubeflow-overview-platform-diagram.svg" alt=""></p><p>Kubeflow는 ML 시스템을 배포, 확장 및 관리하기 위한 플랫폼으로 Kubernetes 기반으로 구성한다. Kubeflow interface 를 통해 ML tools을 지정할 수 있고 클라우드 뿐 아니라 on-premise 에 동일한 worflow를 배포할 수 있어 특정 플랫폼에 종속되지 않는다. 각 구성 요소들이 어떤 역할을 하는지는 뒤 kubeflow Components에서 살펴본다.</p><h1 id="ML-workflow"><a href="#ML-workflow" class="headerlink" title="ML workflow"></a>ML workflow</h1><p>많은 사람들의 선입견 중 하나가 머신러닝 모델 개발이 ML 시스템의 대부분을 차지할 것이라 생각한다. 하지만, 실제 모델을 개발하는 시간 보다 데이터 탐색부터 데이터 분석, 그리고 개발된 모델을 반복적으로 학습하며 튜닝하는 시간이 훨씬 길다. </p><p>즉, ML 시스템 개발은 반복적인 프로세스로, workflow의 각 단계를 평가하고 최고의 퍼포먼스를 낼 수 있게 모델 및 파라미터 변경 사항을 적용해야 한다. 아래는 실험 및 생산<em>(실제 배포)</em> 관점에서의 workflow를 나타낸다.</p><p><img src="https://www.kubeflow.org/docs/images/kubeflow-overview-workflow-diagram-1.svg" alt=""></p><p>각 단계별 내용은 다음과 같다.</p><ul><li>Experimental phase : 실험 단계<ul><li>Identify problem and collect and analyse data : ML 시스템으로 해결하고자 하는 문제를 식별하고 학습 훈련을 위한 데이터를 수집하고 분석한다</li><li>Choose an ML algorithm and code your model : 사용하고자 하는 ML Framework 및 알고리즘을 선별하고 모델 초기 버젼을 코딩한다</li><li>Experiment with data and model training : 앞서 준비된 데이터와 모델 코드를 통해 학습을 진행한다</li><li>Tune the model hyperparameters : 모델 결과에 영향을 주는 hyperparameter를 조정하며 학습을 반복적으로 진행한다 <em>(이후에는 반복적인 parameter 튜닝과 학습을 진행한다)</em></li></ul></li><li>Production phase : 생산 단계<em>(배포 단계)</em><ul><li>Transform data : 학습과 예측에 필요한 데이터를 변환한다. 이때, 모델 정합성을 위해 위 실험 단계에서 진행한 데이터와 같은 형태로 변환해야 함을 주의한다</li><li>Train model : 모델을 학습한다</li><li>Serve the model for online/batch prediction : 온라인 또는 배치 모드를 위한 모델을 제공한다</li><li>Monitor the model’s performance : 모델 성능을 모니터링한다. 이를 통해 모델을 수정하고 재 학습을 진행하여 모델 성능을 높여간다</li></ul></li></ul><h1 id="Kubeflow-components-in-the-ML-workflow"><a href="#Kubeflow-components-in-the-ML-workflow" class="headerlink" title="Kubeflow components in the ML workflow"></a>Kubeflow components in the ML workflow</h1><p>위에 설명한 ML workflow에 Kubeflow 컴포넌트를 대입한 그림이다.</p><p><img src="https://www.kubeflow.org/docs/images/kubeflow-overview-workflow-diagram-2.svg" alt=""></p><p>각 컴포넌트들을 experiment/production 별 나누어 설명한다.</p><p><code>Experiment</code></p><ul><li>Pytorch / scikit-learn / Tensorflow / XGBoost : ML 알고리즘을 제공하는 패키지. 가장 유명한 Tensorflow 부터 손쉽게 ML 모델을 생성할 수 있는 scikit-learn 까지, 현재 진행형으로 다양한 ML 알고리즘 패키지를 지원하고 있다.</li><li>Jupyter notebook / Fairing / pipelines<ul><li>Jupyter notebook : web 기반 파이선 인터프리터로, 인터렉티브한 환경을 제공하며 데이터 분석에 많이 사용하는 tool</li><li><a href="https://www.kubeflow.org/docs/fairing/fairing-overview/">Fairing</a> : Kubeflow에서 ML 모델을 쉽게 학습하고 배포할 수 있는 Python 패키지</li><li><a href="https://www.kubeflow.org/docs/pipelines/overview/pipelines-overview/">pipelines</a> : Kubeflow의 pipeline은 ML workflow의 모든 구성 요소를 설명하는 개념이다. 헷갈릴 수 있겠지만, ML workflow의 모든 과정을 담은 것으로 인지하고 workflow를 실행하는데 필요한 입력값 &amp; 구성 요소의 모든 입출력에 대한 정의를 포함하고 있다 <em>(공식 Doc에는 pipeline을 Kubeflow 안의 플랫폼이라 표현한다)</em>. pipeline은 다음 기능들을 포함하고 있다.<ul><li>ML workflow을 추적하고 관리하는 UI</li><li>다중 ML workflow scheduling</li><li>ML workflow를 정의하고 실행하기 위한 SDK <em>(python)</em></li><li>SDK를 이용해 ML system과 연결하는 Notebook</li></ul></li></ul></li></ul><p><img src="https://www.kubeflow.org/docs/images/pipelines-architecture.png" alt=""></p><p>[참고 - <a href="https://www.kubeflow.org/docs/images/pipelines-architecture.png">pipeline architecture</a>]</p><ul><li><a href="">Katib</a> : ML 모델의 Hyper parameter 및 아키텍쳐를 자동으로 튜닝하는 kubeflow의 컴포넌트 <em>(Hyperparameter란, 모델 학습 과정을 제어하는 변수로 learning rate / neural network의 layer 수 / layer 내 node 수 등이 있다)</em></li></ul><p><code>Prodiction</code></p><ul><li>Chainer / MPI MXNet / PyTorch / TFJob<ul><li><a href="https://v0-6.kubeflow.org/docs/components/training/chainer/">Chainer</a> : CUDA<em>(GPU)</em>, 다양한 딥러닝 아키텍쳐를 지원하는 유연한 딥러닝 프레임워크 </li><li>MPI : ?</li><li>MXNet / Pytorch / TFJob : 오픈소스 딥러닝 소프트웨어 프레임워크로 Deep Neural Network를 학습 및 배포</li></ul></li><li>KFServing / NVDIA TensorRT / PyTorch / TFServing / Seldon : 학습된 모델을 실제 배포할 때 사용하는 컴포넌트로, 크게 KFServing 과 Seldon 을 이용해 프레임워크를 배포. 각 컴포넌트들에 대한 내용은 방대하여 <a href="https://www.kubeflow.org/docs/components/serving/">공식 Doc</a> 참조</li><li>Metadata / TensorBoard<ul><li><a href="https://www.kubeflow.org/docs/components/misc/metadata/">Metadata</a> : 모델, 모델 실행, 데이터 셋 등 기타 Artifact에 대한 정보를 의미하는 컴포넌트 <em>(Artifact란, ML workflow 구성 요소의 input / output을 형성하는 file 또는 오브젝트)</em></li><li><a href="https://www.kubeflow.org/docs/pipelines/sdk/output-viewer/#tensorboard">TensorBoard</a> : Tensorflow가 포함하는 graph visualization tool</li></ul></li></ul><p>#Kubeflow interface</p><p>Kubeflow는 사용자 인터페이스와 커맨드라인 인터페이스를 모두 제공한다.</p><ul><li><p>User interface</p><p><img src="https://www.kubeflow.org/docs/images/central-ui.png" alt=""></p><p>배포된 컴포넌트에 액세스하는데 사용할 수 있는 대시 보드를 제공한다. <a href="https://www.kubeflow.org/docs/other-guides/accessing-uis/"><em>(참고)</em></a></p></li><li><p>Command line interface</p><p><code>Kfctl</code> 은 Kubeflow를 설치 및 구성하는데 사용할 수 있는 Kubeflow CLI. <a href="https://www.kubeflow.org/docs/other-guides/kustomize/"><em>(참고)</em></a></p></li></ul><p>공식 도큐먼트를 참고해 전체적인 개념을 살펴보았다. 다음 포스트에는 Kubernetes 클러스터에 Kubeflow를 설치하는 과정을 다룬다.</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li><p><a href="https://www.kubeflow.org/docs/about/kubeflow/">Kubeflow org, https://www.kubeflow.org/docs/about/kubeflow/</a></p></li><li><p><a href="https://bcho.tistory.com/1301">쿠버네티스 기반의 End2End 머신러닝 플랫폼 Kubeflow #1 - 소개, https://bcho.tistory.com/1301</a></p></li></ul><hr><p>2020.01.13 made by <em>jaejun.lee</em></p>]]></content:encoded>
      
      <comments>https://jx2lee.github.io/cloud-introduction_to_kubeflow/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[ElasticSearch] Install ElasticSearch using Docker</title>
      <link>https://jx2lee.github.io/es-install_elasticsearch/</link>
      <guid>https://jx2lee.github.io/es-install_elasticsearch/</guid>
      <pubDate>Tue, 07 Jan 2020 15:00:00 GMT</pubDate>
      <description>
      
        &lt;p&gt;데이터 마트를 구축하고 이를 시각화하는 파이프라인 구축 pilot을 수행하기 위해 ElasticSearch를 설치하고자 한다. 단일 서버에 싱글 노드로 구축하고 binary 설치를 하려고 했지만 요즘 추세에 맞게(?) Container 기반으로 구축하고자 Docker를 이용한다.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Updata Note&lt;/strong&gt;  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;2020.05.10 : Elasticsearch 용어 정리&lt;/li&gt;
&lt;li&gt;2021.02.01 : Elasticsearch 용어 정리 추가 작업&lt;/li&gt;
&lt;/ul&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p>데이터 마트를 구축하고 이를 시각화하는 파이프라인 구축 pilot을 수행하기 위해 ElasticSearch를 설치하고자 한다. 단일 서버에 싱글 노드로 구축하고 binary 설치를 하려고 했지만 요즘 추세에 맞게(?) Container 기반으로 구축하고자 Docker를 이용한다.</p><p><strong>Updata Note</strong>  </p><ul><li>2020.05.10 : Elasticsearch 용어 정리</li><li>2021.02.01 : Elasticsearch 용어 정리 추가 작업</li></ul><a id="more"></a><h1 id="Elasticsearch-용어"><a href="#Elasticsearch-용어" class="headerlink" title="Elasticsearch 용어"></a>Elasticsearch 용어</h1><h2 id="Index"><a href="#Index" class="headerlink" title="Index"></a>Index</h2><p>elasticsearch 내 데이터 저장소로 RDBMS 의 데이터베이스와 유사  </p><ul><li>ES는 JSON 문서로 데이터 저장</li><li>하나 또는 여러 개 Document</li><li>포괄적인 의미의 색인 또는 색인 파일 <em>(범용적인 의미로 사용)</em><ul><li>indice: elasticsearch 내 물리적으로 사용하는 색인 또는 색인 파일</li></ul></li><li>Index API를 이용해 색인이 시작, 이를 통해 사용자는 특정 Index 에 JSON 문서를 추가 및 업데이트</li></ul><h2 id="Shard"><a href="#Shard" class="headerlink" title="Shard"></a>Shard</h2><p>Lucene 을 기준으로 검색의 기본 데이터베이스가 되는 인덱스  </p><ul><li>분산 처리를 위한 개념</li><li>큰 크기의 인덱스 -&gt; 여러 개 작은 인덱스로 나누어 저장하는 것을 의미</li><li>단일 노드에서는 저장소 서능에 대한 한계를 해결, 클러스터에서는 분산 처리를 수행하며 빠른 결과 생성</li></ul><p><strong>Primary Shard</strong>:<br>색인 시 가장 먼저 생성되는 인덱스로 복제의 기본 소스  </p><p><strong>Replica Shard</strong>:<br>레플리카 설정값에 따라 Primary Shard 을 복제하여 생성한 샤드를 일컫음</p><h2 id="Replica"><a href="#Replica" class="headerlink" title="Replica"></a>Replica</h2><p>단어 뜻대로 복제본으로, 장애 발생 시 지속성 보장과 검색 효율성을 위해 사용  </p><ul><li>분산된 다른 노드에 Shard 와 같은 데이터를 복제 <em>(복제된 Shard 를 Replica 라 생각하자)</em></li><li>생성 절차<ul><li>Primary Shard 색인</li><li>각 노드에서 async 하게 Shard 복제<ul><li>검색 성능 저하 최소화</li></ul></li></ul></li></ul><h2 id="Document"><a href="#Document" class="headerlink" title="Document"></a>Document</h2><p>elasticsearch 에서 가장 기본이 되는 데이터 단위  </p><ul><li>하나의 item 또는 article</li><li>RDBMS 의 테이블 내 하나의 row 에 해당<ul><li><code>Type</code> 은 RDBMS 테이블</li><li><code>Field</code> 는 RDBMS 테이블의 하나의 column</li></ul></li></ul><h2 id="Node"><a href="#Node" class="headerlink" title="Node"></a>Node</h2><p>elasticsearch 를 구성하는 하나의 서버 또는 데몬  </p><ul><li>독립적으로 동작 가능한 서버</li></ul><h2 id="Cluster"><a href="#Cluster" class="headerlink" title="Cluster"></a>Cluster</h2><p>standalone 하게 동작하는 여러 노드를 하나의 그룹으로 묶어 데이터의 분산과 공유를 할 수 있도록 서비스를 구성한 것</p><h1 id="pull-Docker-Image-for-ElasticSearch"><a href="#pull-Docker-Image-for-ElasticSearch" class="headerlink" title="pull Docker-Image for ElasticSearch"></a>pull Docker-Image for ElasticSearch</h1><p>ElasticSearch docker image를 서버로 가져오는 작업을 수행한다. 현재 포스팅 기준 최신 버젼은 7.5.1 이다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">$ docker pull docker.elastic.co/elasticsearch/elasticsearch:7.5.1</span><br><span class="line"></span><br><span class="line">Trying to pull repository docker.elastic.co/elasticsearch/elasticsearch ... </span><br><span class="line">7.5.1: Pulling from docker.elastic.co/elasticsearch/elasticsearch</span><br><span class="line">c808caf183b6: Pull complete </span><br><span class="line">05ff3f896999: Pull complete </span><br><span class="line">82fb7fb0a94e: Pull complete </span><br><span class="line">c4d0024708f4: Pull complete </span><br><span class="line">136650a16cfe: Pull complete </span><br><span class="line">968db096c092: Pull complete </span><br><span class="line">42547e91692f: Pull complete </span><br><span class="line">Digest: sha256:b0960105e830085acbb1f9c8001f58626506ce118f33816ea5d38c772bfc7e6c</span><br><span class="line">Status: Downloaded newer image <span class="keyword">for</span> docker.elastic.co/elasticsearch/elasticsearch:7.5.1</span><br></pre></td></tr></table></figure><h1 id="Run-single-node-cluster"><a href="#Run-single-node-cluster" class="headerlink" title="Run single-node cluster"></a>Run single-node cluster</h1><p><code>docker run -i -t -p 9200:9200 -p 9300:9300 -e &quot;discovery.type=single-node&quot; docker.elastic.co/elasticsearch/elasticsearch:7.5.1</code></p><blockquote><p><em>docker container 실행 시 외부 접근을 허용하기 위해서 <strong>-p</strong> 인자를 추가하여 port forwarding 하게끔 설정한다. 또한, 인터렉티브한 컨테이너를 생성하기 위해 -i -p 인자를 추가하였다.</em></p></blockquote><h1 id="Check"><a href="#Check" class="headerlink" title="Check"></a>Check</h1><p>curl 을 이용해 elasticsearch 가 실행되고 있는지 확인한다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">$ curl -XGET [설치한 서버 ip]:9200</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">&quot;name&quot;</span> : <span class="string">&quot;294fb8043230&quot;</span>,</span><br><span class="line">  <span class="string">&quot;cluster_name&quot;</span> : <span class="string">&quot;docker-cluster&quot;</span>,</span><br><span class="line">  <span class="string">&quot;cluster_uuid&quot;</span> : <span class="string">&quot;UOT6i7eIRjuan8ot89zNHw&quot;</span>,</span><br><span class="line">  <span class="string">&quot;version&quot;</span> : &#123;</span><br><span class="line">    <span class="string">&quot;number&quot;</span> : <span class="string">&quot;7.5.1&quot;</span>,</span><br><span class="line">    <span class="string">&quot;build_flavor&quot;</span> : <span class="string">&quot;default&quot;</span>,</span><br><span class="line">    <span class="string">&quot;build_type&quot;</span> : <span class="string">&quot;docker&quot;</span>,</span><br><span class="line">    <span class="string">&quot;build_hash&quot;</span> : <span class="string">&quot;3ae9ac9a93c95bd0cdc054951cf95d88e1e18d96&quot;</span>,</span><br><span class="line">    <span class="string">&quot;build_date&quot;</span> : <span class="string">&quot;2019-12-16T22:57:37.835892Z&quot;</span>,</span><br><span class="line">    <span class="string">&quot;build_snapshot&quot;</span> : <span class="literal">false</span>,</span><br><span class="line">    <span class="string">&quot;lucene_version&quot;</span> : <span class="string">&quot;8.3.0&quot;</span>,</span><br><span class="line">    <span class="string">&quot;minimum_wire_compatibility_version&quot;</span> : <span class="string">&quot;6.8.0&quot;</span>,</span><br><span class="line">    <span class="string">&quot;minimum_index_compatibility_version&quot;</span> : <span class="string">&quot;6.0.0-beta1&quot;</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">&quot;tagline&quot;</span> : <span class="string">&quot;You Know, for Search&quot;</span></span><br></pre></td></tr></table></figure><p>설치를 완료하였다.</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li><a href="https://velog.io/@pa324/elasticsearch-설치-도커-2bk2h3gh7d">elasticsearch 설치 (도커), https://velog.io/@pa324/elasticsearch-설치-도커-2bk2h3gh7d</a></li><li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/docker.html">elasticsearch document, https://www.elastic.co/guide/en/elasticsearch/reference/current/docker.html</a></li></ul><hr><p>2020.01.08 made by <em>jaejun.lee</em></p>]]></content:encoded>
      
      <comments>https://jx2lee.github.io/es-install_elasticsearch/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[BI] Superset 설치</title>
      <link>https://jx2lee.github.io/bi-install_superset/</link>
      <guid>https://jx2lee.github.io/bi-install_superset/</guid>
      <pubDate>Mon, 06 Jan 2020 15:00:00 GMT</pubDate>
      <description>
      
        &lt;p&gt;Superset은 Web 기반 BI 툴로써 Python으로 개발되었고 수집-저장-처리를 거친 데이터를 마트에서 추출하여 그래프로 시각화하는데 사용한다.&lt;/p&gt;
&lt;p&gt;제공 기능으로는 EDA, Dashboard 생성 및 공유, 보안, 권한과 다양한 소스 연결을 지원한다. 이번 포스트에는 Superset을 설치해본다.&lt;/p&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p>Superset은 Web 기반 BI 툴로써 Python으로 개발되었고 수집-저장-처리를 거친 데이터를 마트에서 추출하여 그래프로 시각화하는데 사용한다.</p><p>제공 기능으로는 EDA, Dashboard 생성 및 공유, 보안, 권한과 다양한 소스 연결을 지원한다. 이번 포스트에는 Superset을 설치해본다.</p><a id="more"></a><h1 id="Install"><a href="#Install" class="headerlink" title="Install"></a>Install</h1><h2 id="VirtualenvWrapper"><a href="#VirtualenvWrapper" class="headerlink" title="VirtualenvWrapper"></a>VirtualenvWrapper</h2><p>python 가상환경을 관리하는 <code>VirtualenvWrapper</code>을 설치하고 <code>superset</code> 환경을 셋팅한다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ mkvirtualenv superset</span><br><span class="line">$ workon superset</span><br></pre></td></tr></table></figure><h2 id="Install-Superset-and-Initialization"><a href="#Install-Superset-and-Initialization" class="headerlink" title="Install Superset and Initialization"></a>Install Superset and Initialization</h2><p>virtualenv 환경에서 pip 명령어를 이용해 설치한다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ pip install apache-superset</span><br></pre></td></tr></table></figure><p>superset의 database를 초기화 하고 admin user를 생성한다</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ superset db upgrade</span><br><span class="line">$ flask fab create-admin</span><br><span class="line">Username [admin]: flask fab create-admin</span><br><span class="line">User first name [admin]: admin</span><br><span class="line">User last name [user]:</span><br><span class="line">Email [admin@fab.org]:</span><br><span class="line">Password:</span><br><span class="line">Repeat <span class="keyword">for</span> confirmation:</span><br><span class="line">2020-01-07 10:51:24,272:INFO:root:Configured event logger of <span class="built_in">type</span> &lt;class <span class="string">&#x27;superset.utils.log.DBEventLogger&#x27;</span>&gt;</span><br><span class="line">Recognized Database Authentications.</span><br><span class="line">Admin User flask fab create-admin created.</span><br></pre></td></tr></table></figure><p>이후 테스트 할 데이터를 import한다. 이후 권한/허가등을 초기화하고 마지막으로 UI 화면을 띄운다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ superset load_examples</span><br><span class="line">$ superset init</span><br><span class="line">$ superset run -p 8088 --with-threads --reload --debugger</span><br></pre></td></tr></table></figure><h1 id="Troubleshooting"><a href="#Troubleshooting" class="headerlink" title="Troubleshooting"></a>Troubleshooting</h1><p>설치 중 발생한 에러를 살펴본다.</p><h2 id="install-error-python-geohash"><a href="#install-error-python-geohash" class="headerlink" title="install error : python-geohash"></a>install error : python-geohash</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">ERROR: Command errored out with <span class="built_in">exit</span> status 1:</span><br><span class="line">     <span class="built_in">command</span>: /home/supset/.virtualenvs/superset/bin/python3 -u -c <span class="string">&#x27;import sys, setuptools, tokenize; sys.argv[0] = &#x27;</span><span class="string">&quot;&#x27;&quot;</span><span class="string">&#x27;/tmp/pip-install-onzf3pi1/python-geohash/setup.py&#x27;</span><span class="string">&quot;&#x27;&quot;</span><span class="string">&#x27;; __file__=&#x27;</span><span class="string">&quot;&#x27;&quot;</span><span class="string">&#x27;/tmp/pip-install-onzf3pi1/python-geohash/setup.py&#x27;</span><span class="string">&quot;&#x27;&quot;</span><span class="string">&#x27;;f=getattr(tokenize, &#x27;</span><span class="string">&quot;&#x27;&quot;</span><span class="string">&#x27;open&#x27;</span><span class="string">&quot;&#x27;&quot;</span><span class="string">&#x27;, open)(__file__);code=f.read().replace(&#x27;</span><span class="string">&quot;&#x27;&quot;</span><span class="string">&#x27;\r\n&#x27;</span><span class="string">&quot;&#x27;&quot;</span><span class="string">&#x27;, &#x27;</span><span class="string">&quot;&#x27;&quot;</span><span class="string">&#x27;\n&#x27;</span><span class="string">&quot;&#x27;&quot;</span><span class="string">&#x27;);f.close();exec(compile(code, __file__, &#x27;</span><span class="string">&quot;&#x27;&quot;</span><span class="string">&#x27;exec&#x27;</span><span class="string">&quot;&#x27;&quot;</span><span class="string">&#x27;))&#x27;</span> install --record /tmp/pip-record-oy48mkll/install-record.txt --single-version-externally-managed --compile --install-headers /home/supset/.virtualenvs/superset/include/site/python3.6/python-geohash</span><br><span class="line">         cwd: /tmp/pip-install-onzf3pi1/python-geohash/</span><br><span class="line">    Complete output (19 lines):</span><br><span class="line">    running install</span><br><span class="line">    running build</span><br><span class="line">    running build_py</span><br><span class="line">    creating build</span><br><span class="line">    creating build/lib.linux-x86_64-3.6</span><br><span class="line">    copying geohash.py -&gt; build/lib.linux-x86_64-3.6</span><br><span class="line">    copying quadtree.py -&gt; build/lib.linux-x86_64-3.6</span><br><span class="line">    copying jpgrid.py -&gt; build/lib.linux-x86_64-3.6</span><br><span class="line">    copying jpiarea.py -&gt; build/lib.linux-x86_64-3.6</span><br><span class="line">    running build_ext</span><br><span class="line">    building <span class="string">&#x27;_geohash&#x27;</span> extension</span><br><span class="line">    creating build/temp.linux-x86_64-3.6</span><br><span class="line">    creating build/temp.linux-x86_64-3.6/src</span><br><span class="line">    gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -fPIC -DPYTHON_MODULE=1 -I/usr/include/python3.6m -c src/geohash.cpp -o build/temp.linux-x86_64-3.6/src/geohash.o</span><br><span class="line">    src/geohash.cpp:538:20: fatal error: Python.h: No such file or directory</span><br><span class="line">     <span class="comment">#include &lt;Python.h&gt;</span></span><br><span class="line">                        ^</span><br><span class="line">    compilation terminated.</span><br><span class="line">    error: <span class="built_in">command</span> <span class="string">&#x27;gcc&#x27;</span> failed with <span class="built_in">exit</span> status 1</span><br><span class="line">    ----------------------------------------</span><br><span class="line">ERROR: Command errored out with <span class="built_in">exit</span> status 1: /home/supset/.virtualenvs/superset/bin/python3 -u -c <span class="string">&#x27;import sys, setuptools, tokenize; sys.argv[0] = &#x27;</span><span class="string">&quot;&#x27;&quot;</span><span class="string">&#x27;/tmp/pip-install-onzf3pi1/python-geohash/setup.py&#x27;</span><span class="string">&quot;&#x27;&quot;</span><span class="string">&#x27;; __file__=&#x27;</span><span class="string">&quot;&#x27;&quot;</span><span class="string">&#x27;/tmp/pip-install-onzf3pi1/python-geohash/setup.py&#x27;</span><span class="string">&quot;&#x27;&quot;</span><span class="string">&#x27;;f=getattr(tokenize, &#x27;</span><span class="string">&quot;&#x27;&quot;</span><span class="string">&#x27;open&#x27;</span><span class="string">&quot;&#x27;&quot;</span><span class="string">&#x27;, open)(__file__);code=f.read().replace(&#x27;</span><span class="string">&quot;&#x27;&quot;</span><span class="string">&#x27;\r\n&#x27;</span><span class="string">&quot;&#x27;&quot;</span><span class="string">&#x27;, &#x27;</span><span class="string">&quot;&#x27;&quot;</span><span class="string">&#x27;\n&#x27;</span><span class="string">&quot;&#x27;&quot;</span><span class="string">&#x27;);f.close();exec(compile(code, __file__, &#x27;</span><span class="string">&quot;&#x27;&quot;</span><span class="string">&#x27;exec&#x27;</span><span class="string">&quot;&#x27;&quot;</span><span class="string">&#x27;))&#x27;</span> install --record /tmp/pip-record-oy48mkll/install-record.txt --single-version-externally-managed --compile --install-headers /home/supset/.virtualenvs/superset/include/site/python3.6/python-geohash Check the logs <span class="keyword">for</span> full <span class="built_in">command</span> output.</span><br></pre></td></tr></table></figure><p>python-geohash package 설치 시 에러가 발생하였다. 이는 설치 시 필요한 시스템 패키지가 미설치되어 발생한 에러로, <code>sudo yum install gcc gcc-c++ python3-devel cyrus-sasl-devel</code> 을 통해 해결할 수 있다. 프로그램 설치 시 환경 설정이 꼭 필요하니 <a href="https://superset.incubator.apache.org/index.html#">공식 documents</a>를 참고하자.</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li><a href="https://superset.incubator.apache.org/installation.html">Superset Document, https://superset.incubator.apache.org/installation.html</a></li></ul><hr><p>2020.01.07 made by <em>jaejun.lee</em></p>]]></content:encoded>
      
      <comments>https://jx2lee.github.io/bi-install_superset/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[Python] 큰 수 만들기</title>
      <link>https://jx2lee.github.io/programmers-large_number/</link>
      <guid>https://jx2lee.github.io/programmers-large_number/</guid>
      <pubDate>Mon, 16 Dec 2019 15:00:00 GMT</pubDate>
      <description>
      
        &lt;p&gt;주어진 숫자에서 특정 갯수의 숫자만으로 가장 큰 수를 만드는 문제를 풀어본다&lt;/p&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p>주어진 숫자에서 특정 갯수의 숫자만으로 가장 큰 수를 만드는 문제를 풀어본다</p><a id="more"></a><h1 id="문제-설명"><a href="#문제-설명" class="headerlink" title="문제 설명"></a>문제 설명</h1><p>어떤 숫자에서 k개의 수를 제거했을 때 얻을 수 있는 가장 큰 숫자를 구하려 합니다. 예를 들어, 숫자 1924에서 수 두 개를 제거하면 [19, 12, 14, 92, 94, 24] 를 만들 수 있습니다. 이 중 가장 큰 숫자는 94 입니다.</p><p>문자열 형식으로 숫자 number와 제거할 수의 개수 k가 solution 함수의 매개변수로 주어집니다. number에서 k 개의 수를 제거했을 때 만들 수 있는 수 중 가장 큰 숫자를 문자열 형태로 return 하도록 solution 함수를 완성하세요.</p><h2 id="제한-조건"><a href="#제한-조건" class="headerlink" title="제한 조건"></a>제한 조건</h2><p>number는 1자리 이상, 1,000,000자리 이하인 숫자입니다. k는 1 이상 number의 자릿수 미만인 자연수입니다.</p><h2 id="입출력-예"><a href="#입출력-예" class="headerlink" title="입출력 예"></a>입출력 예</h2><table><thead><tr><th>number</th><th>k</th><th>return</th></tr></thead><tbody><tr><td>1924</td><td>2</td><td>94</td></tr><tr><td>1231234</td><td>3</td><td>3234</td></tr><tr><td>4177252841</td><td>4</td><td>775841</td></tr></tbody></table><h1 id="문제-풀이"><a href="#문제-풀이" class="headerlink" title="문제 풀이"></a>문제 풀이</h1><p>탐욕법으로 풀 수 있는 문제로, 우선 collected 라는 결과물 저장 List를 선언한다. number의 숫자를 for문으로 돌면서 <u>collected 길이 &gt; 0</u> / <u>collected의 마지막 숫자 비교</u> / <u>k &gt; 0</u> 조건을 만족하면, 원소 하나를 빼주고 k를 차감한다.</p><p>이후 k가 0이면 빈 리스트를 반환하는 걸 방지하고자 index i이상 만큼의 number를 담는다. for문의 마지막으로 조건에 안걸리는 num은 collected에 담는다. 이후 k가 음수일 경우를 대비해 -k 까지 slice를 수행하고 결과물이 담긴 collected List를 join하여 숫자로 변환한다.</p><h1 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">solution</span>(<span class="params">number, k</span>):</span></span><br><span class="line">    collected = []</span><br><span class="line">    <span class="keyword">for</span> i, num <span class="keyword">in</span> <span class="built_in">enumerate</span>(number):</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> <span class="built_in">len</span>(collected) &gt; <span class="number">0</span> <span class="keyword">and</span> collected[-<span class="number">1</span>] &lt; num <span class="keyword">and</span> k &gt; <span class="number">0</span>:</span><br><span class="line">            collected.pop()</span><br><span class="line">            k -= <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> k == <span class="number">0</span>:</span><br><span class="line">            collected += <span class="built_in">list</span>(number[i:])</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">        collected.append(num)</span><br><span class="line"></span><br><span class="line">    collected = collected[:-k] <span class="keyword">if</span> k &gt; <span class="number">0</span> <span class="keyword">else</span> collected</span><br><span class="line">    answer = <span class="string">&#x27;&#x27;</span>.join(collected)</span><br><span class="line">    <span class="keyword">return</span> answer</span><br></pre></td></tr></table></figure><hr><p>2019.12.17 made by <em>jaejun.lee</em></p>]]></content:encoded>
      
      <comments>https://jx2lee.github.io/programmers-large_number/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[SQL] 이동평균 - Moving Average</title>
      <link>https://jx2lee.github.io/sql-moving_average/</link>
      <guid>https://jx2lee.github.io/sql-moving_average/</guid>
      <pubDate>Sun, 15 Dec 2019 15:00:00 GMT</pubDate>
      <description>
      
        &lt;p&gt;모 기업 코딩테스트에 나온 이동 평균, Moving Average 를 SQL로 풀어본다&lt;/p&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p>모 기업 코딩테스트에 나온 이동 평균, Moving Average 를 SQL로 풀어본다</p><a id="more"></a><p>코딩테스트를 Hackerrank 플랫폼을 이용했던터라 문제 복원을 할 수 없었다. 이에 <a href="https://www.sqlteam.com/articles/calculating-running-totals">(https://www.sqlteam.com/articles/calculating-running-totals)</a> 에서 제공한 create database script를 활용해 생성한 데이터로 이동 평균을 구해보고자 한다.</p><p>사용한 쿼리와 결과는 아래와 같다.</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">delimiter //</span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">PROCEDURE</span> insert_row()</span><br><span class="line"><span class="keyword">BEGIN</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">DECLARE</span> DayCount <span class="built_in">smallint</span> <span class="keyword">default</span> <span class="number">5</span>;</span><br><span class="line">    <span class="keyword">DECLARE</span> Sales <span class="built_in">bigint</span> <span class="keyword">default</span> <span class="number">10</span>;</span><br><span class="line"></span><br><span class="line">    WHILE DayCount &lt;= 5000 DO</span><br><span class="line">        <span class="keyword">INSERT</span> <span class="keyword">INTO</span> Sales <span class="keyword">values</span> (DayCount, Sales);</span><br><span class="line">        <span class="keyword">SET</span> DayCount = DayCount + <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">SET</span> Sales = Sales + <span class="number">15</span>;</span><br><span class="line">    <span class="keyword">END</span> <span class="keyword">WHILE</span>;</span><br><span class="line"><span class="keyword">END</span>//</span><br><span class="line"></span><br><span class="line">delimiter;</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">mysql root@localhost:practice&gt; select count(*) from Sales;                                                                                                                                                      </span><br><span class="line">+<span class="comment">----------+</span></span><br><span class="line">| count(*) |</span><br><span class="line">+<span class="comment">----------+</span></span><br><span class="line">| 5000     |</span><br><span class="line">+<span class="comment">----------+</span></span><br><span class="line">1 row in <span class="keyword">set</span></span><br><span class="line"><span class="built_in">Time</span>: <span class="number">0.010</span>s</span><br></pre></td></tr></table></figure><h1 id="이동평균-구하기"><a href="#이동평균-구하기" class="headerlink" title="이동평균 구하기"></a>이동평균 구하기</h1><p><code>스칼라 서브쿼리</code>를 이용해 문제를 해결하였다. average 함수를 이용해 평균 Sales 값을 구하는데, 스칼라 서브쿼리 내 서브쿼리<em>(Count 절)</em>를 작성하여 count가 1과 3사이에 있을때 평균을 구하는 칼럼(MvAvg)을 조회하였다.</p><blockquote><p><em>문제를 풀다보니 SQL 실행순서나 계획 등에 대한 지식이 부족한 것 같다. 책을 한 권 구비하여 공부하는게 좋을 듯 하다.</em></p></blockquote><h1 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h1><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">    DayCount,</span><br><span class="line">    Sales,</span><br><span class="line">    (<span class="keyword">select</span></span><br><span class="line">        <span class="keyword">avg</span>(Sales) <span class="keyword">as</span> moving_average</span><br><span class="line">     <span class="keyword">from</span> Sales b</span><br><span class="line">     <span class="keyword">where</span> (<span class="keyword">select</span></span><br><span class="line">                <span class="keyword">count</span>(*)</span><br><span class="line">            <span class="keyword">from</span> Sales c</span><br><span class="line">            <span class="keyword">where</span> DayCount <span class="keyword">between</span> b.DayCount <span class="keyword">and</span> a.DayCount</span><br><span class="line">            ) <span class="keyword">between</span> <span class="number">1</span> <span class="keyword">and</span> <span class="number">3</span> </span><br><span class="line">    ) <span class="keyword">as</span> MvAvg </span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">    Sales a</span><br><span class="line">;</span><br></pre></td></tr></table></figure><hr><p>2019.12.16 made by <em>jaejun.lee</em></p>]]></content:encoded>
      
      <comments>https://jx2lee.github.io/sql-moving_average/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[Hadoop] Install Spark</title>
      <link>https://jx2lee.github.io/hadoop-install_spark/</link>
      <guid>https://jx2lee.github.io/hadoop-install_spark/</guid>
      <pubDate>Thu, 12 Dec 2019 15:00:00 GMT</pubDate>
      <description>
      
        &lt;p&gt;Spark를 설치하는 과정을 다룬다. binary를 다운받아 풀고 config를 수정하면 쉽게 설치하고 실행할 수 있다. 이 포스트에는 standalone 모드로 spark를 실행한다&lt;/p&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p>Spark를 설치하는 과정을 다룬다. binary를 다운받아 풀고 config를 수정하면 쉽게 설치하고 실행할 수 있다. 이 포스트에는 standalone 모드로 spark를 실행한다</p><a id="more"></a><h1 id="Preliminaries"><a href="#Preliminaries" class="headerlink" title="Preliminaries"></a>Preliminaries</h1><h2 id="Hadoop-Version"><a href="#Hadoop-Version" class="headerlink" title="Hadoop Version"></a>Hadoop Version</h2><p><code>hadoop version</code> 명령어를 통해 Hadoop의 버젼을 체크한다. 이 말인 즉슨, Hadoop Client가 Spark를 사용할 계정에 준비되어 있어야 한다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Hadoop 2.9.2</span><br><span class="line">Subversion https://git-wip-us.apache.org/repos/asf/hadoop.git -r 826afbeae31ca687bc2f8471dc841b66ed2c6704</span><br><span class="line">Compiled by ajisaka on 2018-11-13T12:42Z</span><br><span class="line">Compiled with protoc 2.5.0</span><br><span class="line">From <span class="built_in">source</span> with checksum 3a9939967262218aa556c684d107985</span><br><span class="line">This <span class="built_in">command</span> was run using /app/hadoop/2.9.2/share/hadoop/common/hadoop-common-2.9.2.jar</span><br></pre></td></tr></table></figure><blockquote><p><em>Hadoop Client는 설치가 필요없고 이미 구축된 하둡 클러스터에서 hadoop 바이너리만 가져와 사용하는 것을 말한다. 즉, 다른 서버에서 하둡을 사용할 수 있게끔만 설정해놓자.</em></p></blockquote><h2 id="Download-binary"><a href="#Download-binary" class="headerlink" title="Download binary"></a>Download binary</h2><p><a href="https://spark.apache.org/downloads.html">Spark Documents</a>로 접속하여 binary를 다운로드 한다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@node2 app]<span class="comment"># wget http://mirror.apache-kr.org/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz</span></span><br><span class="line">--2019-12-10 17:41:49--  http://mirror.apache-kr.org/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz</span><br><span class="line">Resolving mirror.apache-kr.org (mirror.apache-kr.org)... 1.201.139.179</span><br><span class="line">Connecting to mirror.apache-kr.org (mirror.apache-kr.org)|1.201.139.179|:80... connected.</span><br><span class="line">HTTP request sent, awaiting response... 200 OK</span><br><span class="line">Length: 230091034 (219M) [application/x-gzip]</span><br><span class="line">Saving to: ‘spark-2.4.4-bin-hadoop2.7.tgz’</span><br></pre></td></tr></table></figure><p>Spark 유저를 생성하고 Spark Home 디렉토리에 다운받은 binary를 풀어준다.</p><h1 id="Set-config"><a href="#Set-config" class="headerlink" title="Set config"></a>Set config</h1><h2 id="bash-profile"><a href="#bash-profile" class="headerlink" title="~/.bash_profile"></a>~/.bash_profile</h2><p>JAVA, HADOOP 환경변수를 설정하고 SPARK 환경변수를 새로 작성하고 update 한다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#JAVA ENV</span></span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/app/java/jdk1.8.0_181</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$JAVA_HOME</span>/bin</span><br><span class="line"></span><br><span class="line"><span class="comment">#SPARK ENV</span></span><br><span class="line">SPARK_HOME=/app/spark</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$SPARK_HOME</span>/bin</span><br><span class="line"></span><br><span class="line"><span class="comment">#HADOOP ENV</span></span><br><span class="line"><span class="built_in">export</span> HADOOP_HOME=/app/hadoop</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HADOOP_HOME</span>/bin:<span class="variable">$HADOOP_HOME</span>/sbin</span><br></pre></td></tr></table></figure><h1 id="Run"><a href="#Run" class="headerlink" title="Run"></a>Run</h1><p><code>$SPARK_HOME/bin</code> 폴더에 <code>pypark</code>를 실행한다.</p><blockquote><p>*</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[spark@node2 bin]$ ./pyspark </span><br><span class="line">Python 3.6.8 (default, Aug  7 2019, 17:28:10) </span><br><span class="line">[GCC 4.8.5 20150623 (Red Hat 4.8.5-39)] on linux</span><br><span class="line">Type <span class="string">&quot;help&quot;</span>, <span class="string">&quot;copyright&quot;</span>, <span class="string">&quot;credits&quot;</span> or <span class="string">&quot;license&quot;</span> <span class="keyword">for</span> more information.</span><br><span class="line">Welcome to</span><br><span class="line">      ____              __</span><br><span class="line">     / __/__  ___ _____/ /__</span><br><span class="line">    _\ \/ _ \/ _ `/ __/  <span class="string">&#x27;_/</span></span><br><span class="line"><span class="string">   /__ / .__/\_,_/_/ /_/\_\   version 2.4.4</span></span><br><span class="line"><span class="string">      /_/</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Using Python version 3.6.8 (default, Aug  7 2019 17:28:10)</span></span><br><span class="line"><span class="string">SparkSession available as &#x27;</span>spark<span class="string">&#x27;.</span></span><br><span class="line"><span class="string">&gt;&gt;&gt; </span></span><br></pre></td></tr></table></figure><p>간단한 RDD를 생성하여 README.md 에 포함한 라인을 세보도록 한다.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; lines &#x3D; sc.textFile(&#39;README.md&#39;)</span><br><span class="line">&gt;&gt;&gt; lines.count()</span><br><span class="line">105</span><br></pre></td></tr></table></figure><blockquote><p><em>sc.textFile로 경로를 무시하게되면 자동으로 hdfs 경로를 읽어들인다. 만약에 로컬 파일을 읽고 싶다면, <code>file://[file path]/[file name]</code>으로 작성하면 로컬 파일을 읽어들인다.</em></p></blockquote><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li><a href="https://spark.apache.org/docs/latest/index.html">Spark Documents</a></li></ul><hr><p>made by <em>jaejun.lee</em></p>]]></content:encoded>
      
      <comments>https://jx2lee.github.io/hadoop-install_spark/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[Python] 다리를 지나는 트럭</title>
      <link>https://jx2lee.github.io/programmers-truck/</link>
      <guid>https://jx2lee.github.io/programmers-truck/</guid>
      <pubDate>Mon, 02 Dec 2019 15:00:00 GMT</pubDate>
      <description>
      
        &lt;p&gt;최대 용량을 가진 다리를 트럭이 모두 지나는 시간을 구하는 문제를 풀어본다.&lt;/p&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p>최대 용량을 가진 다리를 트럭이 모두 지나는 시간을 구하는 문제를 풀어본다.</p><a id="more"></a><h1 id="문제-설명"><a href="#문제-설명" class="headerlink" title="문제 설명"></a>문제 설명</h1><p>트럭 여러 대가 강을 가로지르는 일 차선 다리를 정해진 순으로 건너려 합니다. 모든 트럭이 다리를 건너려면 최소 몇 초가 걸리는지 알아내야 합니다. 트럭은 1초에 1만큼 움직이며, 다리 길이는 bridge_length이고 다리는 무게 weight까지 견딥니다.<br>※ 트럭이 다리에 완전히 오르지 않은 경우, 이 트럭의 무게는 고려하지 않습니다.</p><p>예를 들어, 길이가 2이고 10kg 무게를 견디는 다리가 있습니다. 무게가 [7, 4, 5, 6]kg인 트럭이 순서대로 최단 시간 안에 다리를 건너려면 다음과 같이 건너야 합니다.</p><table><thead><tr><th>경과 시간</th><th>다리를 지난 트럭</th><th>다리를 건너는 트럭</th><th>대기 트럭</th></tr></thead><tbody><tr><td>0</td><td>[]</td><td>[]</td><td>[7,4,5,6]</td></tr><tr><td>1~2</td><td>[]</td><td>[7]</td><td>[4,5,6]</td></tr><tr><td>3</td><td>[7]</td><td>[4]</td><td>[5,6]</td></tr><tr><td>4</td><td>[7]</td><td>[4,5]</td><td>[6]</td></tr><tr><td>5</td><td>[7,4]</td><td>[5]</td><td>[6]</td></tr><tr><td>6~7</td><td>[7,4,5]</td><td>[6]</td><td>[]</td></tr><tr><td>8</td><td>[7,4,5,6]</td><td>[]</td><td>[]</td></tr></tbody></table><p>따라서, 모든 트럭이 다리를 지나려면 최소 8초가 걸립니다.</p><p>solution 함수의 매개변수로 다리 길이 bridge_length, 다리가 견딜 수 있는 무게 weight, 트럭별 무게 truck_weights가 주어집니다. 이때 모든 트럭이 다리를 건너려면 최소 몇 초가 걸리는지 return 하도록 solution 함수를 완성하세요.</p><h2 id="제한-조건"><a href="#제한-조건" class="headerlink" title="제한 조건"></a>제한 조건</h2><ul><li>bridge_length는 1 이상 10,000 이하입니다.</li><li>weight는 1 이상 10,000 이하입니다.</li><li>truck_weights의 길이는 1 이상 10,000 이하입니다.</li><li>모든 트럭의 무게는 1 이상 weight 이하입니다.</li></ul><h2 id="입출력-예"><a href="#입출력-예" class="headerlink" title="입출력 예"></a>입출력 예</h2><table><thead><tr><th>bridge_length</th><th>weight</th><th>truck_weights</th><th>return</th></tr></thead><tbody><tr><td>2</td><td>10</td><td>[7,4,5,6]</td><td>8</td></tr><tr><td>100</td><td>100</td><td>[10]</td><td>101</td></tr><tr><td>100</td><td>100</td><td>[10,10,10,10,10,10,10,10,10,10]</td><td>110</td></tr></tbody></table><h1 id="문제-풀이"><a href="#문제-풀이" class="headerlink" title="문제 풀이"></a>문제 풀이</h1><p>배열을 list로 풀면 하나의 케이스가 시간초과가 발생한다. pop와 append 시 index를 재배열하므로 이를 방지하기 위해, <code>collections</code> 패키지의 <code>deque</code> 배열을 사용하여 해결하였다.</p><p><code>queue</code> 변수는 다리를 지나가고 있는 트럭들을 나타낸다. truck_weights 의 모든 트럭들을 하나씩 뽑아 while 문을 실행한다.</p><p>queue 배열 길이가 bridge_length와 같다면 queue에서 pop을 실행하고, 만약 선택된 트럭의 무게를 더해도 버틸 수 있다면 <em>(sum(queue) + truck &lt;= weight)</em> queue에 트럭을 추가하고 while문을 빠져나온다. 만약 그렇지 않다면 queue에 0을 왼쪽에 추가하고 시간을 +1 한다.</p><p>위를 반복하고 마지막에 들어간 트럭의 소요시간을 구하기 위해 bridge_length를 더하고 문제를 마무리한다.</p><h1 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> deque</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">solution</span>(<span class="params">bridge_length, weight, truck_weights</span>):</span></span><br><span class="line"></span><br><span class="line">    answer = <span class="number">0</span></span><br><span class="line">    queue = deque([])</span><br><span class="line">    truck_weights = deque(truck_weights)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> truck <span class="keyword">in</span> truck_weights:</span><br><span class="line">        <span class="keyword">while</span> truck:</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(queue) == bridge_length:</span><br><span class="line">                queue.pop()</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">sum</span>(queue) + truck &lt;= weight:</span><br><span class="line">                queue.appendleft(truck)</span><br><span class="line">                truck = <span class="number">0</span></span><br><span class="line">                answer += <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                queue.appendleft(<span class="number">0</span>)</span><br><span class="line">                answer += <span class="number">1</span></span><br><span class="line">                </span><br><span class="line">    answer += bridge_length    </span><br><span class="line">    <span class="keyword">return</span> answer</span><br></pre></td></tr></table></figure><blockquote><p><em>Python 의 List 타입의 경우 pop(0)을 수행할 때 인덱스를 재배열하는 것을 깨달았다. 즉, queue를 구현하기 위해서는 List보다 Collections 패키지의 deque를 사용하는 것이 시간초과를 피할 수 있다</em> </p></blockquote><hr><p>2019.12.03 made by <em>jaejun.lee</em></p>]]></content:encoded>
      
      <comments>https://jx2lee.github.io/programmers-truck/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[Hadoop] Tibero 계정의 모든 table을 HDFS로 저장</title>
      <link>https://jx2lee.github.io/hadoop-tables_to_hdfs_using_sqoop/</link>
      <guid>https://jx2lee.github.io/hadoop-tables_to_hdfs_using_sqoop/</guid>
      <pubDate>Tue, 26 Nov 2019 15:00:00 GMT</pubDate>
      <description>
      
        &lt;p&gt;&lt;code&gt;Sqoop&lt;/code&gt;을 이용해 RDB 특정 계정의 모든 Table을 Import 하는 과정을 다룬다&lt;/p&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p><code>Sqoop</code>을 이용해 RDB 특정 계정의 모든 Table을 Import 하는 과정을 다룬다</p><a id="more"></a><p>Tibero의 ERP 계정의 모든 Table을 HDFS로 저장하고 동시에 Hive Table로 생성한다. <code>sqoop import-all-tables</code>을 이용하며 특정 스키마가 없는 테이블은 제외하였다. 구문은 다음과 같다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">sqoop import-all-tables \</span><br><span class="line">--connect jdbc:tibero:thin:@[ip]:[port]:[DB SID] \</span><br><span class="line">--driver com.tmax.tibero.jdbc.TbDriver \</span><br><span class="line">--username [user] --password [passowrd] \</span><br><span class="line">--warehouse-dir [hdfs dir] \</span><br><span class="line">--hive-import --hive-overwrite --hive-database [metastore db name] \</span><br><span class="line">--exclude-tables SM_GROUP,SM_GROUP_PERMISSION,SM_PERMISSION,SM_PROJECT_GROUP_AUTH,SM_ROLE,SM_USER,SM_USER_GROUP \</span><br><span class="line">--m 1</span><br></pre></td></tr></table></figure><ul><li><em>–connect</em> : 접속할 DB 정보</li><li><em>–driver</em> : 접속할 DB Driver</li><li><em>–username &amp; –password</em> : 계정 ID/Password</li><li><em>–warehouse-dir</em> : HDFS 위치</li><li><em>–hive-import –hive-overwrite –hive-database</em> : HDFS로 저장함과 동시에 Hive table로 import. hive-database는 MetaStore의 database</li><li><em>–exclude-table</em> : Import 시 제외할 Table</li><li><em>–m</em> : number of mappers</li></ul><blockquote><p><em>제외할 테이블 명을 명시할 떄 콤마 이후에 무조건 붙여줘야 argument를 인식한다</em></p></blockquote><p>생각보다 시간이 오래걸렸다. 결과를 확인해보자.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ beeline</span><br><span class="line">$ !connect jdbc:hive2://[ip]:[port] user password</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">use</span> tims;</span><br><span class="line"><span class="keyword">show</span> <span class="keyword">tables</span>;</span><br><span class="line"></span><br><span class="line">+<span class="comment">-----------------------+</span></span><br><span class="line">|       tab_name        |</span><br><span class="line">+<span class="comment">-----------------------+</span></span><br><span class="line">| aactv00t              |</span><br><span class="line">| aactv01t              |</span><br><span class="line">| aactv10t              |</span><br><span class="line">| aactv20t              |</span><br><span class="line">| aactv24t              |</span><br><span class="line">| aactv25t              |</span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line">| satch00t_03           |</span><br><span class="line">| sbms_common_code      |</span><br><span class="line">| sbms_document_data    |</span><br><span class="line">| sbms_error_log        |</span><br><span class="line">+<span class="comment">-----------------------+</span></span><br><span class="line">595 rows selected (0.183 seconds)</span><br></pre></td></tr></table></figure><hr><p>2019.11.27 made by <em>jaejun.lee</em></p>]]></content:encoded>
      
      <comments>https://jx2lee.github.io/hadoop-tables_to_hdfs_using_sqoop/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[Python] 문자열 압축</title>
      <link>https://jx2lee.github.io/programmers-compress_char/</link>
      <guid>https://jx2lee.github.io/programmers-compress_char/</guid>
      <pubDate>Mon, 25 Nov 2019 15:00:00 GMT</pubDate>
      <description>
      
        &lt;p&gt;주어진 문자열을 가장 짧게 압축하는 문제를 풀어본다 (2020 카카오 1차 코딩테스트)&lt;/p&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p>주어진 문자열을 가장 짧게 압축하는 문제를 풀어본다 (2020 카카오 1차 코딩테스트)</p><a id="more"></a><h1 id="문제"><a href="#문제" class="headerlink" title="문제"></a>문제</h1><p>데이터 처리 전문가가 되고 싶은 “어피치”는 문자열을 압축하는 방법에 대해 공부를 하고 있습니다. 최근에 대량의 데이터 처리를 위한 간단한 비손실 압축 방법에 대해 공부를 하고 있는데, 문자열에서 같은 값이 연속해서 나타나는 것을 그 문자의 개수와 반복되는 값으로 표현하여 더 짧은 문자열로 줄여서 표현하는 알고리즘을 공부하고 있습니다.<br>간단한 예로 “aabbaccc”의 경우 “2a2ba3c”(문자가 반복되지 않아 한번만 나타난 경우 1은 생략함)와 같이 표현할 수 있는데, 이러한 방식은 반복되는 문자가 적은 경우 압축률이 낮다는 단점이 있습니다. 예를 들면, “abcabcdede”와 같은 문자열은 전혀 압축되지 않습니다. “어피치”는 이러한 단점을 해결하기 위해 문자열을 1개 이상의 단위로 잘라서 압축하여 더 짧은 문자열로 표현할 수 있는지 방법을 찾아보려고 합니다.<br><br>예를 들어, “ababcdcdababcdcd”의 경우 문자를 1개 단위로 자르면 전혀 압축되지 않지만, 2개 단위로 잘라서 압축한다면 “2ab2cd2ab2cd”로 표현할 수 있습니다. 다른 방법으로 8개 단위로 잘라서 압축한다면 “2ababcdcd”로 표현할 수 있으며, 이때가 가장 짧게 압축하여 표현할 수 있는 방법입니다.<br>다른 예로, “abcabcdede”와 같은 경우, 문자를 2개 단위로 잘라서 압축하면 “abcabc2de”가 되지만, 3개 단위로 자른다면 “2abcdede”가 되어 3개 단위가 가장 짧은 압축 방법이 됩니다. 이때 3개 단위로 자르고 마지막에 남는 문자열은 그대로 붙여주면 됩니다.<br>압축할 문자열 s가 매개변수로 주어질 때, 위에 설명한 방법으로 1개 이상 단위로 문자열을 잘라 압축하여 표현한 문자열 중 가장 짧은 것의 길이를 return 하도록 solution 함수를 완성해주세요.<br></p><h2 id="제한사항"><a href="#제한사항" class="headerlink" title="제한사항"></a>제한사항</h2><ul><li>s의 길이는 1 이상 1,000 이하입니다.</li><li>s는 알파벳 소문자로만 이루어져 있습니다.</li></ul><h2 id="입출력-예"><a href="#입출력-예" class="headerlink" title="입출력 예"></a>입출력 예</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">     s                       result</span><br><span class="line">&quot;aabbaccc&quot;                     7</span><br><span class="line">&quot;ababcdcdababcdcd&quot;             9</span><br><span class="line">&quot;abcabcdede&quot;                   8</span><br><span class="line">&quot;abcabcabcabcdededededede&quot;    14</span><br><span class="line">&quot;xababcdcdababcdcd&quot;           17</span><br></pre></td></tr></table></figure><h2 id="입출력-예에-대한-설명"><a href="#입출력-예에-대한-설명" class="headerlink" title="입출력 예에 대한 설명"></a>입출력 예에 대한 설명</h2><ul><li>입출력 예 #1<ul><li>문자열을 1개 단위로 잘라 압축했을 때 가장 짧습니다.</li></ul></li><li>입출력 예 #2<ul><li>문자열을 8개 단위로 잘라 압축했을 때 가장 짧습니다.</li></ul></li><li>입출력 예 #3<ul><li>문자열을 3개 단위로 잘라 압축했을 때 가장 짧습니다.</li></ul></li><li>입출력 예 #4<ul><li>문자열을 2개 단위로 자르면 “abcabcabcabc6de” 가 됩니다.</li><li>문자열을 3개 단위로 자르면 “4abcdededededede” 가 됩니다.</li><li>문자열을 4개 단위로 자르면 “abcabcabcabc3dede” 가 됩니다.</li><li>문자열을 6개 단위로 자를 경우 “2abcabc2dedede”가 되며, 이때의 길이가 14로 가장 짧습니다.</li></ul></li><li>입출력 예 #5<ul><li>문자열은 제일 앞부터 정해진 길이만큼 잘라야 합니다. 따라서 주어진 문자열을 x / ababcdcd / ababcdcd 로 자르는 것은 불가능 합니다. 이 경우 어떻게 문자열을 잘라도 압축되지 않으므로 가장 짧은 길이는 이 됩니다.</li></ul></li></ul><h1 id="문제-풀이"><a href="#문제-풀이" class="headerlink" title="문제 풀이"></a>문제 풀이</h1><p>딱히 사용한 알고리즘은 없는 것 같다 <del>(굳이 선택하면 브루트포스?)</del>. 반복하는 문자열 길이에 따라 모든 문자열을 압축하고, 이에 대한 길이를 res 리스트에 담아 최솟값을 출력해낸다.</p><h1 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">solution</span>(<span class="params">s</span>):</span></span><br><span class="line">    result = <span class="string">&#x27;&#x27;</span></span><br><span class="line">    cnt = <span class="number">1</span></span><br><span class="line">    res = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> n <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(s)+<span class="number">1</span>):</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(s), n):</span><br><span class="line">            <span class="keyword">if</span> s[i:i+n] == s[i+n:i+<span class="number">2</span>*n]:</span><br><span class="line">                cnt += <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">if</span> cnt &gt; <span class="number">1</span>:</span><br><span class="line">                    result += <span class="built_in">str</span>(cnt) + s[i:i+n]</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    result += s[i:i+n]</span><br><span class="line">                cnt = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> i == <span class="built_in">len</span>(s) - n:</span><br><span class="line">                <span class="keyword">if</span> cnt &gt; <span class="number">1</span>:</span><br><span class="line">                    result += <span class="built_in">str</span>(cnt) + s[i+n:i+<span class="number">2</span>*n]</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    result += s[i+n:i+<span class="number">2</span>*n]</span><br><span class="line"></span><br><span class="line">        res.append(<span class="built_in">len</span>(result))</span><br><span class="line">        result = <span class="string">&#x27;&#x27;</span></span><br><span class="line">        cnt = <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">min</span>(res)</span><br></pre></td></tr></table></figure><hr><p>2019.11.26 made by <em>jaejun.lee</em></p>]]></content:encoded>
      
      <comments>https://jx2lee.github.io/programmers-compress_char/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[Python] 자물쇠와 열쇠</title>
      <link>https://jx2lee.github.io/programmers-unlock/</link>
      <guid>https://jx2lee.github.io/programmers-unlock/</guid>
      <pubDate>Mon, 25 Nov 2019 15:00:00 GMT</pubDate>
      <description>
      
        &lt;p&gt;key를 이용해 자물쇠를 여는 문제를 풀어본다 (2020 카카오 1차 코딩테스트)&lt;/p&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p>key를 이용해 자물쇠를 여는 문제를 풀어본다 (2020 카카오 1차 코딩테스트)</p><a id="more"></a><h1 id="문제-설명"><a href="#문제-설명" class="headerlink" title="문제 설명"></a>문제 설명</h1><p>고고학자인 튜브는 고대 유적지에서 보물과 유적이 가득할 것으로 추정되는 비밀의 문을 발견하였습니다. 그런데 문을 열려고 살펴보니 특이한 형태의 자물쇠로 잠겨 있었고 문 앞에는 특이한 형태의 열쇠와 함께 자물쇠를 푸는 방법에 대해 다음과 같이 설명해 주는 종이가 발견되었습니다.</p><p>잠겨있는 자물쇠는 격자 한 칸의 크기가 1 x 1인 N x N 크기의 정사각 격자 형태이고 특이한 모양의 열쇠는 M x M 크기인 정사각 격자 형태로 되어 있습니다.</p><p>자물쇠에는 홈이 파여 있고 열쇠 또한 홈과 돌기 부분이 있습니다. 열쇠는 회전과 이동이 가능하며 열쇠의 돌기 부분을 자물쇠의 홈 부분에 딱 맞게 채우면 자물쇠가 열리게 되는 구조입니다. 자물쇠 영역을 벗어난 부분에 있는 열쇠의 홈과 돌기는 자물쇠를 여는 데 영향을 주지 않지만, 자물쇠 영역 내에서는 열쇠의 돌기 부분과 자물쇠의 홈 부분이 정확히 일치해야 하며 열쇠의 돌기와 자물쇠의 돌기가 만나서는 안됩니다. 또한 자물쇠의 모든 홈을 채워 비어있는 곳이 없어야 자물쇠를 열 수 있습니다.</p><p>열쇠를 나타내는 2차원 배열 key와 자물쇠를 나타내는 2차원 배열 lock이 매개변수로 주어질 때, 열쇠로 자물쇠를 열수 있으면 true를, 열 수 없으면 false를 return 하도록 solution 함수를 완성해주세요.</p><h2 id="제한사항"><a href="#제한사항" class="headerlink" title="제한사항"></a>제한사항</h2><ul><li>key는 M x M(3 ≤ M ≤ 20, M은 자연수)크기 2차원 배열입니다.</li><li>lock은 N x N(3 ≤ N ≤ 20, N은 자연수)크기 2차원 배열입니다.</li><li>M은 항상 N 이하입니다.</li><li>key와 lock의 원소는 0 또는 1로 이루어져 있습니다.</li><li>0은 홈 부분, 1은 돌기 부분을 나타냅니다.</li></ul><h2 id="입출력-예"><a href="#입출력-예" class="headerlink" title="입출력 예"></a>입출력 예</h2><p>| key | lock | result |<br>| —– | —— | —————————— | —— |<br>| [[0, 0, 0], [1, 0, 0], [0, 1, 1]]   | [[1, 1, 1], [1, 1, 0], [1, 0, 1]]    | true |</p><h2 id="입출력-예-설명"><a href="#입출력-예-설명" class="headerlink" title="입출력 예 설명"></a>입출력 예 설명</h2><p><img src="https://grepp-programmers.s3.amazonaws.com/files/production/469703690b/79f2f473-5d13-47b9-96e0-a10e17b7d49a.jpg" alt="https://grepp-programmers.s3.amazonaws.com/files/production/469703690b/79f2f473-5d13-47b9-96e0-a10e17b7d49a.jpg"></p><p>key를 시계 방향으로 90도 회전하고, 오른쪽으로 한 칸, 아래로 한 칸 이동하면 lock의 홈 부분을 정확히 모두 채울 수 있습니다.</p><h1 id="문제-풀이"><a href="#문제-풀이" class="headerlink" title="문제 풀이"></a>문제 풀이</h1><p>한 줄로 문제를 풀어보면, <strong>lock을 확대하고 key를 하나씩 대입해보면서 모든 부분이 1인 경우</strong>를 찾으면 된다. 크게 세 가지 함수로 구현하였다.</p><p><code>rotate_key</code>는 말그래도 입력한 key 를 시계 방향 90도로 회전하는 함수이다. 입력은 key와 key의 길이</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rotate_key</span>(<span class="params">key, M</span>):</span></span><br><span class="line">    res = [[<span class="number">0</span> * n <span class="keyword">for</span> n <span class="keyword">in</span> <span class="built_in">range</span>(M)] <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(M)]</span><br><span class="line">    <span class="keyword">for</span> y <span class="keyword">in</span> <span class="built_in">range</span>(M):</span><br><span class="line">        <span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(M):</span><br><span class="line">            res[x][M - y - <span class="number">1</span>] = key[y][x]</span><br><span class="line">    <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure><p><code>expand_lock</code>은 lock을 key와 대조하기 위해 확장하는 함수이다. <em>n+2 X (m-1)</em> 만큼 확장한다. N은 lock의 길이, M은 열쇠의 길이</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">expand_lock</span>(<span class="params">lock, N, M, K</span>):</span></span><br><span class="line">    res = [[<span class="number">0</span> * i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(K)] <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(K)]</span><br><span class="line">    <span class="keyword">for</span> y <span class="keyword">in</span> <span class="built_in">range</span>(N):</span><br><span class="line">        <span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(N):</span><br><span class="line">            res[y + M - <span class="number">1</span>][x + M - <span class="number">1</span>] = lock[y][x]</span><br><span class="line">    <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure><p><code>is_open</code>은 확장된 lock과 key를 이용해 각 구멍 value를 더해 1이 아니면 경우는 False, 1인 경우에는 True를 반환하는 함수이다. 이 함수는 확장된 lock을 겹치는 부분부터 끝까지 key를 for문으로 돌려가며 확인한다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">is_open</span>(<span class="params">_y, _x, key, lock, N, M</span>):</span></span><br><span class="line">    _lock = copy.deepcopy(lock)</span><br><span class="line">    <span class="keyword">for</span> y <span class="keyword">in</span> <span class="built_in">range</span>(M):</span><br><span class="line">        <span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(M):</span><br><span class="line">            _lock[_y + y][_x + x] += key[y][x]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> y <span class="keyword">in</span> <span class="built_in">range</span>(N):</span><br><span class="line">        <span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(N):</span><br><span class="line">            <span class="keyword">if</span> _lock[y + M - <span class="number">1</span>][x + M - <span class="number">1</span>] != <span class="number">1</span>:</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">True</span></span><br></pre></td></tr></table></figure><p>마지막 <code>solution</code>함수는 하나하나 키를 돌려가며 모든 값들이 1인지를 판단하고, 아니면 key를 돌려가며 확인하는 함수이다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">solution</span>(<span class="params">key, lock</span>):</span></span><br><span class="line"></span><br><span class="line">    n, m = <span class="built_in">len</span>(lock), <span class="built_in">len</span>(key)</span><br><span class="line">    k = n + <span class="number">2</span> * (m - <span class="number">1</span>)</span><br><span class="line">    lock = expand_lock(lock, n, m, k)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> y <span class="keyword">in</span> <span class="built_in">range</span>(k - m +<span class="number">1</span>):</span><br><span class="line">        <span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(k - m +<span class="number">1</span>):</span><br><span class="line">            <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):</span><br><span class="line">                <span class="keyword">if</span> is_open(y, x, key, lock, n, m):</span><br><span class="line">                    <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">                key = rotate_key(key, m)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></table></figure><h1 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> copy</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rotate_key</span>(<span class="params">key, M</span>):</span></span><br><span class="line">    res = [[<span class="number">0</span> * n <span class="keyword">for</span> n <span class="keyword">in</span> <span class="built_in">range</span>(M)] <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(M)]</span><br><span class="line">    <span class="keyword">for</span> y <span class="keyword">in</span> <span class="built_in">range</span>(M):</span><br><span class="line">        <span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(M):</span><br><span class="line">            res[x][M - y - <span class="number">1</span>] = key[y][x]</span><br><span class="line">    <span class="keyword">return</span> res</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">expand_lock</span>(<span class="params">lock, N, M, K</span>):</span></span><br><span class="line">    res = [[<span class="number">0</span> * i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(K)] <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(K)]</span><br><span class="line">    <span class="keyword">for</span> y <span class="keyword">in</span> <span class="built_in">range</span>(N):</span><br><span class="line">        <span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(N):</span><br><span class="line">            res[y + M - <span class="number">1</span>][x + M - <span class="number">1</span>] = lock[y][x]</span><br><span class="line">    <span class="keyword">return</span> res</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">is_open</span>(<span class="params">_y, _x, key, lock, N, M</span>):</span></span><br><span class="line">    _lock = copy.deepcopy(lock)</span><br><span class="line">    <span class="keyword">for</span> y <span class="keyword">in</span> <span class="built_in">range</span>(M):</span><br><span class="line">        <span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(M):</span><br><span class="line">            _lock[_y + y][_x + x] += key[y][x]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> y <span class="keyword">in</span> <span class="built_in">range</span>(N):</span><br><span class="line">        <span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(N):</span><br><span class="line">            <span class="keyword">if</span> _lock[y + M - <span class="number">1</span>][x + M - <span class="number">1</span>] != <span class="number">1</span>:</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">solution</span>(<span class="params">key, lock</span>):</span></span><br><span class="line">    answer = <span class="literal">True</span></span><br><span class="line">    n, m = <span class="built_in">len</span>(lock), <span class="built_in">len</span>(key)</span><br><span class="line">    k = n + <span class="number">2</span> * (m - <span class="number">1</span>)</span><br><span class="line">    lock = expand_lock(lock, n, m, k)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> y <span class="keyword">in</span> <span class="built_in">range</span>(k - m +<span class="number">1</span>):</span><br><span class="line">        <span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(k - m +<span class="number">1</span>):</span><br><span class="line">            <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):</span><br><span class="line">                <span class="keyword">if</span> is_open(y, x, key, lock, n, m):</span><br><span class="line">                    <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">                key = rotate_key(key, m)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></table></figure><hr><p>2019.11.26 made by <em>jaejun.lee</em></p>]]></content:encoded>
      
      <comments>https://jx2lee.github.io/programmers-unlock/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[Database] MySQL 유저 관리</title>
      <link>https://jx2lee.github.io/database-manage_user_in_mysql/</link>
      <guid>https://jx2lee.github.io/database-manage_user_in_mysql/</guid>
      <pubDate>Sun, 24 Nov 2019 15:00:00 GMT</pubDate>
      <description>
      
        &lt;p&gt;MySQL의 유저를 관리해본다&lt;/p&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p>MySQL의 유저를 관리해본다</p><a id="more"></a><h1 id="User-table"><a href="#User-table" class="headerlink" title="User table"></a>User table</h1><p>MySQL에서 관리하는 유저를 조회해보자. 우선, 기본적으로 mysql database에 user 테이블에서 관리한다. mysql database를 선택하고 user table의 host, user를 조회해보자.</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; show databases;</span><br><span class="line">+<span class="comment">--------------------+</span></span><br><span class="line">| Database           |</span><br><span class="line">+<span class="comment">--------------------+</span></span><br><span class="line">| hive               |</span><br><span class="line">| information_schema |</span><br><span class="line">| mysql              |</span><br><span class="line">| performance_schema |</span><br><span class="line">| sys                |</span><br><span class="line">+<span class="comment">--------------------+</span></span><br><span class="line">5 rows in <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; <span class="keyword">use</span> mysql;</span><br><span class="line">Database changed</span><br><span class="line">mysql&gt; select host, user from user;</span><br><span class="line">+<span class="comment">-----------+------------------+</span></span><br><span class="line">| host      | user             |</span><br><span class="line">+<span class="comment">-----------+------------------+</span></span><br><span class="line">| %         | hive             |</span><br><span class="line">| localhost | mysql.infoschema |</span><br><span class="line">| localhost | mysql.session    |</span><br><span class="line">| localhost | mysql.sys        |</span><br><span class="line">| localhost | root             |</span><br><span class="line">+<span class="comment">-----------+------------------+</span></span><br><span class="line">5 rows in <span class="keyword">set</span> (<span class="number">0.01</span> sec)</span><br></pre></td></tr></table></figure><h1 id="Create-User-amp-Database"><a href="#Create-User-amp-Database" class="headerlink" title="Create User &amp; Database"></a>Create User &amp; Database</h1><h2 id="User"><a href="#User" class="headerlink" title="User"></a>User</h2><p>Sqoop를 이용해 hdfs 데이터를 MySQL table을 저장하기 위한 계정을 생성한다. <del>(Sqoop export를 위해서는 해당 RDB에 테이블이 존재해야한다. 굳이 이관하는 것 까진 필요없을 것 같아, 이번 포스팅에는 유저를 생성하고 관리하는 방법만 다룬다)</del></p><p><code>create user [user name]@[ip] identified by [password];</code></p><ul><li>user name : 생성할 계정명</li><li>ip : 접속가능 ip로 로컬 계정에서만 접속을 허용할 것이면 localhost, 본인과 같이 모든 외부 IP에서 접근이 가능하게 하려면 <strong>%</strong></li><li>password : 생성할 계정의 비밀번호</li></ul><blockquote><p><em>ip의 경우, grant 명령어로 수정이 가능함</em></p></blockquote><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; create user &#x27;tims&#x27;@&#x27;%&#x27; identified by &#x27;****&#x27;;</span><br><span class="line">Query OK, 0 rows affected (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; select host, user from user;</span><br><span class="line">+<span class="comment">-----------+------------------+</span></span><br><span class="line">| host      | user             |</span><br><span class="line">+<span class="comment">-----------+------------------+</span></span><br><span class="line">| %         | hive             |</span><br><span class="line">| %         | tims             |</span><br><span class="line">| localhost | mysql.infoschema |</span><br><span class="line">| localhost | mysql.session    |</span><br><span class="line">| localhost | mysql.sys        |</span><br><span class="line">| localhost | root             |</span><br><span class="line">+<span class="comment">-----------+------------------+</span></span><br><span class="line">6 rows in <span class="keyword">set</span> (<span class="number">0.01</span> sec)</span><br></pre></td></tr></table></figure><h2 id="Database"><a href="#Database" class="headerlink" title="Database"></a>Database</h2><p>생성한 tims계정에서 사용할 database를 생성한다.</p><p><code>create database [database name]</code></p><ul><li>database name : 생성할 database 이름</li></ul><p><code>grank all privileges on [database name].[schema] to [user name]@[ip]</code></p><ul><li>database naem : 생성한 database 이름</li><li>schema : 생성한 database 내 스키마</li><li>user name : 권한을 줄 계정명</li><li>ip : 접속 ip</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; create database tims;</span><br><span class="line">Query OK, 1 row affected (0.01 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; grant all privileges on tims.* to &#x27;tims&#x27;@&#x27;%&#x27;;</span><br><span class="line">Query OK, 0 rows affected (0.00 sec)</span><br></pre></td></tr></table></figure><p>테스트해보자. <code>show tables</code> 를 치게되면 아무 테이블이 표시되지 않을 것이다. <em>(당연)</em></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">[mysql@node2 ~]$ mysql -u tims -p</span><br><span class="line">Enter password: </span><br><span class="line">Welcome to the MySQL monitor.  Commands <span class="keyword">end</span> <span class="keyword">with</span> ; or \g.</span><br><span class="line">Your MySQL connection id is 6820</span><br><span class="line">Server version: 8.0.18 Source distribution</span><br><span class="line"></span><br><span class="line">Copyright (c) 2000, 2019, Oracle and/or its affiliates. All rights reserved.</span><br><span class="line"></span><br><span class="line">Oracle is a registered trademark of Oracle Corporation and/or its</span><br><span class="line">affiliates. Other names may be trademarks of their respective</span><br><span class="line">owners.</span><br><span class="line"></span><br><span class="line">Type &#x27;<span class="keyword">help</span>;&#x27; or &#x27;\h&#x27; for help. Type &#x27;\c&#x27; to clear the current input statement.</span><br><span class="line"></span><br><span class="line">mysql&gt; show databases;</span><br><span class="line">+<span class="comment">--------------------+</span></span><br><span class="line">| Database           |</span><br><span class="line">+<span class="comment">--------------------+</span></span><br><span class="line">| information_schema |</span><br><span class="line">| tims               |</span><br><span class="line">+<span class="comment">--------------------+</span></span><br><span class="line">2 rows in <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; <span class="keyword">use</span> tims;</span><br><span class="line">Database changed</span><br><span class="line">mysql&gt; show tables;</span><br><span class="line">Empty <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure><blockquote><p><em>ERROR 1045 (28000): Access denied for user ‘hive’@’localhost’ (using password: NO) 에러가 발생하는 경우가 있다. 이때는 해당 계정으로 Login 할 때 -p opiton을 붙여준다</em></p></blockquote><hr><p>2019.11.25 made by <em>jaejun.lee</em></p>]]></content:encoded>
      
      <comments>https://jx2lee.github.io/database-manage_user_in_mysql/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[Hadoop] Install Zeppelin and Connect to RDBMS &amp; Hive</title>
      <link>https://jx2lee.github.io/hadoop-install_zeppelin/</link>
      <guid>https://jx2lee.github.io/hadoop-install_zeppelin/</guid>
      <pubDate>Sun, 17 Nov 2019 15:00:00 GMT</pubDate>
      <description>
      
        &lt;p&gt;&lt;code&gt;Apache Zeppelin&lt;/code&gt;을 설치하고 &lt;code&gt;Tibero&lt;/code&gt;와 &lt;code&gt;Hive&lt;/code&gt;에 연동하는 과정을 살펴본다&lt;/p&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p><code>Apache Zeppelin</code>을 설치하고 <code>Tibero</code>와 <code>Hive</code>에 연동하는 과정을 살펴본다</p><a id="more"></a><h1 id="Install-Apache-Zeppelin"><a href="#Install-Apache-Zeppelin" class="headerlink" title="Install Apache Zeppelin"></a>Install Apache Zeppelin</h1><p><code>Zeppelin</code> user를 생성하고 설치파일을 다운로드한다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ adduser zepp --gid 1000 <span class="comment"># bigdata</span></span><br><span class="line">$ wget http://apache.mirror.cdnetworks.com/zeppelin/zeppelin-0.8.2/zeppelin-0.8.2-bin-all.tgz</span><br><span class="line">$ <span class="built_in">cd</span> /app &amp;&amp; tar -xvzf zeppelin-0.8.2-bin-all.tgz</span><br></pre></td></tr></table></figure><p><code>$ZEPPELIN_HOME/conf/zeppelin-site.xml</code> 파일을 수정한다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;zeppelin.server.addr&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;192.xxx.xxx.xx&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;Server binding address, Server IP&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;zeppelin.server.port&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;8001&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;Server port.&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;zeppelin.server.ssl.port&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;8443&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;Server ssl port. (used when ssl property is <span class="built_in">set</span> to <span class="literal">true</span>)&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;zeppelin.server.context.path&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;/&lt;/value&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;zeppelin.server.context.path&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;/&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;Context Path of the Web Application&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;zeppelin.war.tempdir&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;webapps&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;Location of jetty temporary directory&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;zeppelin.notebook.dir&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;notebook&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;path or URI <span class="keyword">for</span> notebook persist&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;zeppelin.notebook.homescreen&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;id of notebook to be displayed <span class="keyword">in</span> homescreen. ex) 2A94M5J1Z Empty value displays default home screen&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;zeppelin.notebook.homescreen.hide&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;<span class="literal">false</span>&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;hide homescreen notebook from list when this value <span class="built_in">set</span> to <span class="literal">true</span>&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br></pre></td></tr></table></figure><blockquote><p><em>이미 8080포트가 사용중이므로 port를 8001로 수정하였다.</em></p></blockquote><p>zeppelin 폴더 이용 권한을 <code>zepp</code>에게 주고 daemon을 실행한다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ chown -R zepp:bigdata zeppelin/</span><br><span class="line">$ su - zepp</span><br><span class="line">$ bin/zeppelin-daemon.sh start</span><br><span class="line">Zeppelin start                                             [  OK  ]</span><br></pre></td></tr></table></figure><p><a href="http://localhost:8080">http://localhost:8080</a> 으로 접속한다.</p><p><img src="/image/zeppelin-home.png" alt=""></p><h1 id="Connection"><a href="#Connection" class="headerlink" title="Connection"></a>Connection</h1><h2 id="Tibero-Connection"><a href="#Tibero-Connection" class="headerlink" title="Tibero Connection"></a>Tibero Connection</h2><p>Tibero와 연동하는 방법은 간단하다. <code>$ZEPPELIN_HOME/interpreter/jdbc</code> 안에 <code>tibero6-jdbc.jar</code> 파일을 복사한다</p><p><code>$ cp tibero6-jdbc.jar $ZEPPELIN_HOME/interpreter/jdbc/tibero6-jdbc.jar</code></p><p>이후 <code>$ZEPPELIN/conf/interpreter.json</code> 내 <code>jdbc</code> 부분에 import할 DB 정보를 작성한다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;jdbc&quot;</span>: &#123;</span><br><span class="line">  <span class="string">&quot;id&quot;</span>: <span class="string">&quot;jdbc&quot;</span>,</span><br><span class="line">  <span class="string">&quot;name&quot;</span>: <span class="string">&quot;jdbc&quot;</span>,</span><br><span class="line">  <span class="string">&quot;group&quot;</span>: <span class="string">&quot;jdbc&quot;</span>,</span><br><span class="line">  <span class="string">&quot;properties&quot;</span>: &#123;</span><br><span class="line">    <span class="string">&quot;default.url&quot;</span>: &#123;</span><br><span class="line">      <span class="string">&quot;name&quot;</span>: <span class="string">&quot;default.url&quot;</span>,</span><br><span class="line">      <span class="string">&quot;value&quot;</span>: <span class="string">&quot;jdbc:tibero:thin:@192.168.xxx.xxx:xxxx:tibero&quot;</span>,</span><br><span class="line">      <span class="string">&quot;type&quot;</span>: <span class="string">&quot;string&quot;</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">&quot;default.driver&quot;</span>: &#123;</span><br><span class="line">      <span class="string">&quot;name&quot;</span>: <span class="string">&quot;default.driver&quot;</span>,</span><br><span class="line">      <span class="string">&quot;value&quot;</span>: <span class="string">&quot;com.tmax.tibero.jdbc.TbDriver&quot;</span>,</span><br><span class="line">      <span class="string">&quot;type&quot;</span>: <span class="string">&quot;string&quot;</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">&quot;default.password&quot;</span>: &#123;</span><br><span class="line">      <span class="string">&quot;name&quot;</span>: <span class="string">&quot;default.password&quot;</span>,</span><br><span class="line">      <span class="string">&quot;value&quot;</span>: <span class="string">&quot;xxxxx&quot;</span>,</span><br><span class="line">      <span class="string">&quot;type&quot;</span>: <span class="string">&quot;password&quot;</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">&quot;default.user&quot;</span>: &#123;</span><br><span class="line">      <span class="string">&quot;name&quot;</span>: <span class="string">&quot;default.user&quot;</span>,</span><br><span class="line">      <span class="string">&quot;value&quot;</span>: <span class="string">&quot;xxx&quot;</span>,</span><br><span class="line">      <span class="string">&quot;type&quot;</span>: <span class="string">&quot;string&quot;</span></span><br></pre></td></tr></table></figure><blockquote><p><em><code>properties</code> 안에 default.url, default.driver, default.password/user 를 해당 DB 정보를 작성한다. 나머지는 세부적인 사항이므로 <a href="https://zeppelin.apache.org/docs/0.8.0/interpreter/jdbc.html">https://zeppelin.apache.org/docs/0.8.0/interpreter/jdbc.html</a>를 확인해 필요하면 수정하도록 한다.</em></p></blockquote><p><code>Zeppelin</code>을 재실행한다. Notebook을 생성하고 <code>%jdbc \n select * from tab</code>을 실행하여 정상적으로 연결되었는지 확인한다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="variable">$ZEPPELIN_HOME</span>/bin/zeppelin-daemon.sh restart</span><br><span class="line"></span><br><span class="line">%jdbc</span><br><span class="line">select * from tab</span><br></pre></td></tr></table></figure><p><img src="/image/zeppelin-jdbc.png" alt=""></p><h2 id="Hive-Connection"><a href="#Hive-Connection" class="headerlink" title="Hive Connection"></a>Hive Connection</h2><p><code>Hive</code>와의 연동도 마찬가지로 <code>$ZEPPELIN/conf/interpreter.json</code> 내 <code>interpreterSettings</code> 부분에 Hive 정보를 작성한다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="string">&quot;interpreterSettings&quot;</span>: &#123;</span><br><span class="line">    <span class="string">&quot;hive&quot;</span>: &#123;</span><br><span class="line">      <span class="string">&quot;id&quot;</span>: <span class="string">&quot;hive&quot;</span>,</span><br><span class="line">      <span class="string">&quot;name&quot;</span>: <span class="string">&quot;hive&quot;</span>,</span><br><span class="line">      <span class="string">&quot;group&quot;</span>: <span class="string">&quot;jdbc&quot;</span>,</span><br><span class="line">      <span class="string">&quot;properties&quot;</span>: &#123;</span><br><span class="line">        <span class="string">&quot;default.url&quot;</span>: &#123;</span><br><span class="line">          <span class="string">&quot;name&quot;</span>: <span class="string">&quot;default.url&quot;</span>,</span><br><span class="line">          <span class="string">&quot;value&quot;</span>: <span class="string">&quot;jdbc:hive2://localhost:10000/project&quot;</span>,</span><br><span class="line">          <span class="string">&quot;type&quot;</span>: <span class="string">&quot;string&quot;</span></span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="string">&quot;default.driver&quot;</span>: &#123;</span><br><span class="line">          <span class="string">&quot;name&quot;</span>: <span class="string">&quot;default.driver&quot;</span>,</span><br><span class="line">          <span class="string">&quot;value&quot;</span>: <span class="string">&quot;org.apache.hive.jdbc.HiveDriver&quot;</span>,</span><br><span class="line">          <span class="string">&quot;type&quot;</span>: <span class="string">&quot;string&quot;</span></span><br><span class="line">        &#125;,</span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line">        <span class="string">&quot;default.password&quot;</span>: &#123;</span><br><span class="line">          <span class="string">&quot;name&quot;</span>: <span class="string">&quot;default.password&quot;</span>,</span><br><span class="line">          <span class="string">&quot;value&quot;</span>: <span class="string">&quot;hive&quot;</span>,</span><br><span class="line">          <span class="string">&quot;type&quot;</span>: <span class="string">&quot;password&quot;</span></span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="string">&quot;default.user&quot;</span>: &#123;</span><br><span class="line">          <span class="string">&quot;name&quot;</span>: <span class="string">&quot;default.user&quot;</span>,</span><br><span class="line">          <span class="string">&quot;value&quot;</span>: <span class="string">&quot;hive&quot;</span>,</span><br><span class="line">          <span class="string">&quot;type&quot;</span>: <span class="string">&quot;string&quot;</span></span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line">      <span class="string">&quot;dependencies&quot;</span>: [</span><br><span class="line">        &#123;</span><br><span class="line">          <span class="string">&quot;groupArtifactVersion&quot;</span>: <span class="string">&quot;org.apache.hive:hive-jdbc:2.3.6&quot;</span>,</span><br><span class="line">          <span class="string">&quot;local&quot;</span>: <span class="literal">false</span></span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">          <span class="string">&quot;groupArtifactVersion&quot;</span>: <span class="string">&quot;org.apache.hadoop:hadoop-common:2.6.0&quot;</span>,</span><br><span class="line">          <span class="string">&quot;local&quot;</span>: <span class="literal">false</span>,</span><br><span class="line">          <span class="string">&quot;exclusions&quot;</span>: []</span><br><span class="line">        &#125;</span><br><span class="line">      ],</span><br><span class="line">      <span class="string">&quot;option&quot;</span>: &#123;</span><br><span class="line">        <span class="string">&quot;remote&quot;</span>: <span class="literal">true</span>,</span><br><span class="line">        <span class="string">&quot;port&quot;</span>: -1,</span><br></pre></td></tr></table></figure><blockquote><p><em>templete을 이용해 굳이 모든 정보를 입력하지 않아도 된다. Zeppelin 웹에서 interpreter를 생성한 후 <strong>default.driver, default.password, default.url, default.user, Dependencies 2개</strong>를 작성한 후 생성하면 자동으로 interpreter.json에 추가된다. 주의할 점은, HiveServer2 로 접근이 가능한 상태임을 체크해주어야 한다.</em></p></blockquote><p><code>Zeppelin</code>을 재실행한다. Notebook을 생성하고 <code>%jdbc \n show tables</code>을 실행하여 정상적으로 연결되었는지 확인한다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="variable">$ZEPPELIN_HOME</span>/bin/zeppelin-daemon.sh restart</span><br><span class="line"></span><br><span class="line">%hive</span><br><span class="line">show tables</span><br></pre></td></tr></table></figure><p><img src="/image/zeppelin-hive.png" alt=""></p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="https://zeppelin.apache.org/docs/0.8.0/interpreter/hive.html#dependencies">Zeppelin Documents, https://zeppelin.apache.org/docs/0.8.0/interpreter/hive.html#dependencies </a></li></ol><hr><p>2019.11.18 made by <em>jaejun.lee</em></p>]]></content:encoded>
      
      <comments>https://jx2lee.github.io/hadoop-install_zeppelin/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[Hadoop] Install Presto</title>
      <link>https://jx2lee.github.io/hadoop-install_presto/</link>
      <guid>https://jx2lee.github.io/hadoop-install_presto/</guid>
      <pubDate>Mon, 11 Nov 2019 15:00:00 GMT</pubDate>
      <description>
      
        &lt;p&gt;&lt;code&gt;Presto&lt;/code&gt;를 설치하는 과정을 살펴본다&lt;/p&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p><code>Presto</code>를 설치하는 과정을 살펴본다</p><a id="more"></a><h1 id="Preliminaries"><a href="#Preliminaries" class="headerlink" title="Preliminaries"></a>Preliminaries</h1><h2 id="Add-user-amp-Download"><a href="#Add-user-amp-Download" class="headerlink" title="Add user &amp; Download"></a>Add user &amp; Download</h2><p><code>Presto</code> user를 생성하고 hdclient 그룹에 포함한다. 이후 설치파일을 다운로드한다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ adduser presto --gid 8630 <span class="comment"># hdclient</span></span><br><span class="line">$ wget https://repo1.maven.org/maven2/com/facebook/presto/presto-server/0.228/presto-server-0.228.tar.gz</span><br></pre></td></tr></table></figure><h2 id="bash-profile"><a href="#bash-profile" class="headerlink" title=".bash_profile"></a>.bash_profile</h2><p><code>.bash_profile</code>에 <code>Hadoop</code> 및 <code>Presto</code> Env를 추가한다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Hadoop Env</span></span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/app/jdk</span><br><span class="line"><span class="built_in">export</span> HADOOP_HOME=/app/hadoop</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$JAVA_HOME</span>/bin:\</span><br><span class="line"><span class="variable">$HADOOP_HOME</span>/bin:\</span><br><span class="line"><span class="variable">$HADOOP_HOME</span>/sbin</span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> HADOOP_PREFIX=/app/hadoop</span><br><span class="line"><span class="built_in">export</span> HADOOP_COMMON_HOME=<span class="variable">$HADOOP_PREFIX</span></span><br><span class="line"><span class="built_in">export</span> HADOOP_CONF_DIR=<span class="variable">$HADOOP_PREFIX</span>/etc/hadoop</span><br><span class="line"><span class="built_in">export</span> HADOOP_HDFS_HOME=<span class="variable">$HADOOP_PREFIX</span></span><br><span class="line"><span class="built_in">export</span> HADOOP_MAPRED_HOME=<span class="variable">$HADOOP_PREFIX</span></span><br><span class="line"><span class="built_in">export</span> HADOOP_YARN_HOME=<span class="variable">$HADOOP_PREFIX</span></span><br><span class="line"><span class="built_in">export</span> YARN_CONF_DIR=<span class="variable">$HADOOP_PREFIX</span>/etc/hadoop</span><br><span class="line"></span><br><span class="line"><span class="comment"># Presto Env</span></span><br><span class="line"><span class="built_in">export</span> PRESTO_HOME=/app/presto</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$JAVA_HOME</span>/bin:<span class="variable">$HIVE_HOME</span>/bin</span><br></pre></td></tr></table></figure><h1 id="Configuring-Presto"><a href="#Configuring-Presto" class="headerlink" title="Configuring Presto"></a>Configuring Presto</h1><p>세 개의 설정파일을 <code>$PRESTO_HOME/etc</code> 폴더에 생성한다.</p><h2 id="etc-node-properties"><a href="#etc-node-properties" class="headerlink" title="/etc/node.properties"></a>/etc/node.properties</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">node.environment=production</span><br><span class="line">node.id=ffffffff-ffff-ffff-ffff-ffffffffffff</span><br><span class="line">node.data-dir=/app/presto/data</span><br></pre></td></tr></table></figure><h2 id="etc-jvm-confing"><a href="#etc-jvm-confing" class="headerlink" title="/etc/jvm.confing"></a>/etc/jvm.confing</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">-server</span><br><span class="line">-Xmx16G</span><br><span class="line">-XX:+UseG1GC</span><br><span class="line">-XX:G1HeapRegionSize=32M</span><br><span class="line">-XX:+UseGCOverheadLimit</span><br><span class="line">-XX:+ExplicitGCInvokesConcurrent</span><br><span class="line">-XX:+HeapDumpOnOutOfMemoryError</span><br><span class="line">-XX:+ExitOnOutOfMemoryError</span><br></pre></td></tr></table></figure><h2 id="etc-config-properties"><a href="#etc-config-properties" class="headerlink" title="/etc/config.properties"></a>/etc/config.properties</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">coordinator=<span class="literal">true</span></span><br><span class="line">node-scheduler.include-coordinator=<span class="literal">true</span></span><br><span class="line">http-server.http.port=8000</span><br><span class="line">query.max-memory=5GB</span><br><span class="line">query.max-memory-per-node=1GB</span><br><span class="line">query.max-total-memory-per-node=2GB</span><br><span class="line">discovery-server.enabled=<span class="literal">true</span></span><br><span class="line">discovery.uri=http://192.168.xxx.xxx:8000</span><br></pre></td></tr></table></figure><blockquote><ul><li><code>http-server.htt.port</code> : <em>Presto는 내부 및 외부 모든 통신에 HTTP를 사용하며, 내 경우 8000번 포트를 open, 이를 통해 통신</em></li><li><code>discovery.uri</code> : <em>Presto instance는 시작 시 Discovery service에 등록되는 URI로 Presto 구동 서버의 IP와 port (위와 같은 경우는 8000 port) 로 작성</em></li></ul></blockquote><h2 id="etc-catalog-hive-properties"><a href="#etc-catalog-hive-properties" class="headerlink" title="/etc/catalog/hive.properties"></a>/etc/catalog/hive.properties</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">connector.name=hive-hadoop2</span><br><span class="line">hive.metastore.uri=thrift://localhost:9083</span><br></pre></td></tr></table></figure><blockquote><p><em>Hive MetaStore 의 default port는 *</em>9083***</p></blockquote><p>위에 생성한 파일들을 tree로 표현하면 다음과 같다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ tree .</span><br><span class="line">.</span><br><span class="line">├── catalog</span><br><span class="line">│   └── hive.properties</span><br><span class="line">├── config.properties</span><br><span class="line">├── jvm.config</span><br><span class="line">└── node.properties</span><br><span class="line"></span><br><span class="line">1 directory, 4 files</span><br></pre></td></tr></table></figure><h1 id="Start-Presto-Server"><a href="#Start-Presto-Server" class="headerlink" title="Start Presto Server"></a>Start Presto Server</h1><p><code>$PRESTO_HOME/bin</code> 폴더에 <code>launcher</code> 파일을 실행한다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ ./launcher start</span><br><span class="line">Started as 25072</span><br><span class="line"><span class="comment"># $PRESTO_HOME/data/var/log/launcher.log</span></span><br><span class="line">2019-11-12T14:56:02.306+0900INFOmainio.airlift.log.LoggingLogging to stderr</span><br><span class="line">2019-11-12T14:56:02.308+0900INFOmainBootstrapLoading configuration</span><br><span class="line">2019-11-12T14:56:02.404+0900INFOmainBootstrapInitializing logging</span><br><span class="line">2019-11-12T14:56:02.447+0900INFOmainio.airlift.log.LoggingLogging to /app/presto/data/var/<span class="built_in">log</span>/server.log</span><br><span class="line">2019-11-12T14:56:02.497+0900INFOmainio.airlift.log.LoggingDisabling stderr output</span><br></pre></td></tr></table></figure><p>Presto CLI 을 <code>wget</code>을 이용해 다운로드 한다. 이후 실행권한을 주고 CLI 를 실행한다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ wget https://repo1.maven.org/maven2/com/facebook/presto/presto-cli/0.228/presto-cli-0.228-executable.jar &amp;&amp; mv presto-cli-0.228-executable.jar presto</span><br><span class="line">$ chmod +x presto</span><br><span class="line">$ ./presto --server 192.168.154.156:8000 --catalog hive --schema project</span><br></pre></td></tr></table></figure><blockquote><p><em>Presto CLI 명령어 arguments</em></p><ul><li><code>server</code> : <em>discovery.uri</em></li><li><code>catalog</code> : <em>Hive MetaStore</em></li><li><code>schema</code> : <em>Hive metaStore</em> 중 <em>project db **</em></li></ul></blockquote><p>테이블을 조회해보자.</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">presto:project&gt; show tables;</span><br><span class="line">  Table   </span><br><span class="line"><span class="comment">----------</span></span><br><span class="line"> binvt00t </span><br><span class="line"> bprjt00t </span><br><span class="line"> ccomp00t </span><br><span class="line"> iprsn00t </span><br><span class="line">(4 rows)</span><br><span class="line"></span><br><span class="line">Query 20191112_060524_00002_bgi94, FINISHED, 1 node</span><br><span class="line">Splits: 19 total, 19 done (100.00%)</span><br><span class="line">0:02 [4 rows, 100B] [1 rows/s, 43B/s]</span><br></pre></td></tr></table></figure><p><strong>구축 완료!</strong></p><h1 id="Query-속도-비교"><a href="#Query-속도-비교" class="headerlink" title="Query 속도 비교"></a>Query 속도 비교</h1><p>테이블의 row 수를 반환하는 쿼리문을 <code>Presto</code>와 <code>Hive</code>에서 수행해본다.</p><ul><li><code>Presto</code></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">presto:project&gt; select count(*) from bprjt00t;</span><br><span class="line"> _col0 </span><br><span class="line"><span class="comment">-------</span></span><br><span class="line"> 65355 </span><br><span class="line">(1 row)</span><br><span class="line"></span><br><span class="line">Query 20191112_061041_00004_bgi94, FINISHED, 1 node</span><br><span class="line">Splits: 23 total, 23 done (100.00%)</span><br><span class="line">0:04 [65.4K rows, 9.29MB] [15.9K rows/s, 2.26MB/s]</span><br></pre></td></tr></table></figure><ul><li><code>Hive</code></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; select count(*)</span><br><span class="line">    &gt; from bprjt00t;</span><br><span class="line">WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.</span><br><span class="line">Query ID = hive_20191112151209_8598d146-fed8-4b55-8a2b-e1ed186a82d0</span><br><span class="line">Total jobs = 1</span><br><span class="line">Launching Job 1 out of 1</span><br><span class="line">Number of reduce tasks determined at compile time: 1</span><br><span class="line">In order to <span class="keyword">change</span> the average <span class="keyword">load</span> <span class="keyword">for</span> a reducer (<span class="keyword">in</span> <span class="keyword">bytes</span>):</span><br><span class="line">  <span class="keyword">set</span> hive.exec.reducers.bytes.per.reducer=&lt;<span class="built_in">number</span>&gt;</span><br><span class="line"><span class="keyword">In</span> <span class="keyword">order</span> <span class="keyword">to</span> <span class="keyword">limit</span> the maximum <span class="built_in">number</span> <span class="keyword">of</span> reducers:</span><br><span class="line">  <span class="keyword">set</span> hive.exec.reducers.max=&lt;<span class="built_in">number</span>&gt;</span><br><span class="line"><span class="keyword">In</span> <span class="keyword">order</span> <span class="keyword">to</span> <span class="keyword">set</span> a <span class="keyword">constant</span> <span class="built_in">number</span> <span class="keyword">of</span> reducers:</span><br><span class="line">  <span class="keyword">set</span> mapreduce.job.reduces=&lt;<span class="built_in">number</span>&gt;</span><br><span class="line"><span class="keyword">Starting</span> Job = job_1567153359966_0111, <span class="keyword">Tracking</span> <span class="keyword">URL</span> = <span class="keyword">http</span>://node5.dat:<span class="number">8088</span>/proxy/application_1567153359966_0111/</span><br><span class="line"><span class="keyword">Kill</span> Command = /app/hadoop/<span class="keyword">bin</span>/hadoop job  -<span class="keyword">kill</span> job_1567153359966_0111</span><br><span class="line">Hadoop job information <span class="keyword">for</span> Stage<span class="number">-1</span>: <span class="built_in">number</span> <span class="keyword">of</span> mappers: <span class="number">2</span>; number of reducers: 1</span><br><span class="line">2019-11-12 15:12:19,615 Stage-1 map = 0%,  reduce = 0%</span><br><span class="line">2019-11-12 15:12:23,860 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 4.1 sec</span><br><span class="line">2019-11-12 15:12:27,986 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 5.98 sec</span><br><span class="line">MapReduce Total cumulative CPU time: 5 seconds 980 msec</span><br><span class="line">Ended Job = job_1567153359966_0111</span><br><span class="line">MapReduce Jobs Launched: </span><br><span class="line">Stage-Stage-1: Map: 2  Reduce: 1   Cumulative CPU: 5.98 sec   HDFS Read: 9754283 HDFS Write: 105 SUCCESS</span><br><span class="line">Total MapReduce CPU Time Spent: 5 seconds 980 msec</span><br><span class="line">OK</span><br><span class="line">65355</span><br><span class="line">Time taken: 20.718 seconds, Fetched: 1 row(s)</span><br></pre></td></tr></table></figure><blockquote><p><em>대략 Presto가 Hive 대비 쿼리속도가 *</em>20배 가량 빠르다***</p></blockquote><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="https://prestodb.io/docs/current/index.html">Presto Documents</a></li><li><a href="https://medium.com/@nil.me/a-single-node-installation-of-presto-and-simple-benchmarks-3271bc738ed1">A Single-node Installation of Presto and Simple Benchmarks</a></li></ol><hr><p>made by <em>jaejun.lee</em></p>]]></content:encoded>
      
      <comments>https://jx2lee.github.io/hadoop-install_presto/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[Hadoop] Sqoop을 이용한 Table 조회</title>
      <link>https://jx2lee.github.io/hadoop-sqoop_example/</link>
      <guid>https://jx2lee.github.io/hadoop-sqoop_example/</guid>
      <pubDate>Wed, 06 Nov 2019 15:00:00 GMT</pubDate>
      <description>
      
        &lt;p&gt;Tibero Table을 &lt;code&gt;Sqoop&lt;/code&gt; 을 이용해 HDFS에 저장함과 동시에, &lt;code&gt;Hive&lt;/code&gt; 로 조회하는 예제를 살펴본다.&lt;/p&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p>Tibero Table을 <code>Sqoop</code> 을 이용해 HDFS에 저장함과 동시에, <code>Hive</code> 로 조회하는 예제를 살펴본다.</p><a id="more"></a><h1 id="Preliminaries"><a href="#Preliminaries" class="headerlink" title="Preliminaries"></a>Preliminaries</h1><h2 id="RDBMS-Table"><a href="#RDBMS-Table" class="headerlink" title="RDBMS Table"></a>RDBMS Table</h2><p>BPRJT00T 테이블을 확인한다.</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">$ DESC BPRJT00T;</span><br><span class="line">COLUMN_NAME                              TYPE               CONSTRAINT          </span><br><span class="line"><span class="comment">---------------------------------------- ------------------ --------------------</span></span><br><span class="line">PRJT_CD                                  VARCHAR(10)        PRIMARY KEY</span><br><span class="line">PRJT_NM                                  VARCHAR(1000)      </span><br><span class="line">COMP_CD                                  VARCHAR(10)        </span><br><span class="line">CUST_CD                                  VARCHAR(10)        </span><br><span class="line">PRJT_ENV                                 VARCHAR(3000)      </span><br><span class="line">BUSI_AMT                                 NUMBER(13)         </span><br><span class="line">ATTACH_NO_ORG                            VARCHAR(20)        </span><br><span class="line">IMPORTANT_CLS                            VARCHAR(1)         NOT NULL</span><br><span class="line">MA_PRJT_CLS                              VARCHAR(1)         NOT NULL</span><br><span class="line">REPORT_CLS                               VARCHAR(4)         NOT NULL</span><br><span class="line">PRIORITY_CD                              VARCHAR(4)         NOT NULL</span><br><span class="line">PRJT_STATUS                              VARCHAR(4)         NOT NULL</span><br><span class="line">SALE_EMP                                 VARCHAR(7)         </span><br><span class="line">REMARK                                   VARCHAR(4000)      </span><br><span class="line">REG_EMP                                  VARCHAR(7)         </span><br><span class="line">REG_DATE                                 VARCHAR(8)         </span><br><span class="line">MOD_EMP                                  VARCHAR(7)         </span><br><span class="line">MOD_DATE                                 VARCHAR(8)         </span><br><span class="line">LOSS_PROD                                VARCHAR(200)       </span><br><span class="line">CURRENCY_KIND                            VARCHAR(4)         NOT NULL</span><br><span class="line">RECNTR_YN                                VARCHAR(1)         NOT NULL</span><br><span class="line">RECNTR_STATUS                            VARCHAR(4)         </span><br><span class="line">DIST_PATH                                VARCHAR(100)       </span><br><span class="line">DISTRIB_YN                               VARCHAR(1)         NOT NULL</span><br><span class="line">DISTRIB_PRJTCD                           VARCHAR(10)        </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">INDEX_NAME                       TYPE                     COLUMN_NAME           </span><br><span class="line"><span class="comment">-------------------------------- ------------------------ ----------------------</span></span><br><span class="line">BPRJT00T_IDX01                   NORMAL                   SALE_EMP</span><br><span class="line">BPRJT00T_IDX02                   NORMAL                   COMP_CD</span><br><span class="line">BPRJT00T_PK                      NORMAL                   PRJT_CD</span><br><span class="line">$ <span class="keyword">SELECT</span> <span class="keyword">COUNT</span>(*) <span class="keyword">FROM</span> BPRJT00T;</span><br><span class="line">  COUNT(*)</span><br><span class="line"><span class="comment">----------</span></span><br><span class="line">     56125</span><br></pre></td></tr></table></figure><h2 id="Sqoop-eval"><a href="#Sqoop-eval" class="headerlink" title="Sqoop eval"></a>Sqoop eval</h2><p><code>Sqoop</code>을 이용해 테이블 접근이 가능한지 확인한다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ sqoop <span class="built_in">eval</span> \</span><br><span class="line">-connect jdbc:tibero:thin:@[ip]:[port]:[DB SID]] \</span><br><span class="line">-driver com.tmax.tibero.jdbc.TbDriver \</span><br><span class="line">-username XXX -password XXX \</span><br><span class="line">-e <span class="string">&quot;select * from BPRJT00T where rownum &lt; 10&quot;</span></span><br><span class="line">...</span><br></pre></td></tr></table></figure><blockquote><p><em>보안상 조회 결과는 생략하였다</em></p></blockquote><h1 id="RDMBS-to-HDFS"><a href="#RDMBS-to-HDFS" class="headerlink" title="RDMBS to HDFS"></a>RDMBS to HDFS</h1><p>확인이 끝났다면, 아래 명령어를 통해 <code>Sqoop</code>으로 HDFS에 저장하고 동시에 <code>Hive</code>로 Import 한다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">sqoop import <span class="string">&quot;-Dorg.apache.sqoop.splitter.allow_text_splitter=true&quot;</span> \</span><br><span class="line">--connect jdbc:tibero:thin:@[ip]:[port]:[DB SID] \</span><br><span class="line">--driver com.tmax.tibero.jdbc.TbDriver \</span><br><span class="line">--target-dir /project/BPRJT00T \</span><br><span class="line">--username XXX --password XXX \</span><br><span class="line">--table BPRJT00T \</span><br><span class="line">--fields-terminated-by <span class="string">&quot;,&quot;</span> \</span><br><span class="line">--hive-import \</span><br><span class="line">--create-hive-table \</span><br><span class="line">--hive-table project.BPRJT00T</span><br></pre></td></tr></table></figure><blockquote><p><em>Hive로 Import 하기 위해서는 미리 Database가 구성되어 있어야 한다. (나의 경우 DB Name은 project)</em></p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">19/11/07 16:28:43 INFO hive.HiveImport: OK</span><br><span class="line">19/11/07 16:28:43 INFO hive.HiveImport: Time taken: 4.162 seconds</span><br><span class="line">19/11/07 16:28:44 INFO hive.HiveImport: Loading data to table project.bprjt00t</span><br><span class="line">19/11/07 16:28:44 INFO hive.HiveImport: OK</span><br><span class="line">19/11/07 16:28:44 INFO hive.HiveImport: Time taken: 0.611 seconds</span><br><span class="line">19/11/07 16:28:45 INFO hive.HiveImport: Hive import complete.</span><br><span class="line">19/11/07 16:28:45 INFO hive.HiveImport: Export directory is contains the _SUCCESS file only, removing the directory.</span><br></pre></td></tr></table></figure><h1 id="Result"><a href="#Result" class="headerlink" title="Result"></a>Result</h1><p>우선, <code>Hive</code>로 Import 한 테이블을 hdfs 명렁어로 확인한다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ hdfs dfs -ls /user/hive/warehouse</span><br><span class="line">Found 1 items</span><br><span class="line">drwxrwxrwx   - hive supergroup          0 2019-11-07 16:27 /user/hive/warehouse/project.db</span><br><span class="line">$ hdfs dfs -ls /user/hive/warehouse/project.db/</span><br><span class="line">Found 1 items</span><br><span class="line">drwxrwxrwx   - hive supergroup          0 2019-11-07 16:28 /user/hive/warehouse/project.db/bprjt00t</span><br></pre></td></tr></table></figure><blockquote><p><em>잘 들어갔다.</em></p></blockquote><p>그럼 <code>Hive</code> 콘솔로 접속하여 BPRJT00T 테이블을 조회해보자.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">$ hive</span><br><span class="line">SLF4J: Class path contains multiple SLF4J bindings.</span><br><span class="line">SLF4J: Found binding <span class="keyword">in</span> [jar:file:/app/hive/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: Found binding <span class="keyword">in</span> [jar:file:/app/hadoop/2.9.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: See http://www.slf4j.org/codes.html<span class="comment">#multiple_bindings for an explanation.</span></span><br><span class="line">SLF4J: Actual binding is of <span class="built_in">type</span> [org.apache.logging.slf4j.Log4jLoggerFactory]</span><br><span class="line"></span><br><span class="line">Logging initialized using configuration <span class="keyword">in</span> jar:file:/app/hive/lib/hive-common-2.3.6.jar!/hive-log4j2.properties Async: <span class="literal">true</span></span><br><span class="line">Hive-on-MR is deprecated <span class="keyword">in</span> Hive 2 and may not be available <span class="keyword">in</span> the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.</span><br><span class="line">hive&gt; $ use project;</span><br><span class="line">OK</span><br><span class="line">Time taken: 2.963 seconds</span><br><span class="line">hive&gt; $ select * from bprjt00t <span class="built_in">limit</span> 10;</span><br><span class="line">OK</span><br></pre></td></tr></table></figure><blockquote><p><em>이또한, 조회결과는 생략</em></p></blockquote><hr><p>2019.11.07 made by <em>jaejun.lee</em></p>]]></content:encoded>
      
      <comments>https://jx2lee.github.io/hadoop-sqoop_example/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[Hadoop] Hive Import 시 Could not initialize class org.apache.derby.jdbc.EmbeddedDriver 문제해결</title>
      <link>https://jx2lee.github.io/hadoop-sqoop_import_error/</link>
      <guid>https://jx2lee.github.io/hadoop-sqoop_import_error/</guid>
      <pubDate>Wed, 06 Nov 2019 15:00:00 GMT</pubDate>
      <description>
      
        &lt;p&gt;&lt;code&gt;Hive&lt;/code&gt; 에 RDMS 테이블을 import 하는 과정에서 발생한 문제를 해결한다&lt;/p&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p><code>Hive</code> 에 RDMS 테이블을 import 하는 과정에서 발생한 문제를 해결한다</p><a id="more"></a><h1 id="Status"><a href="#Status" class="headerlink" title="Status"></a>Status</h1><p>아래와 같은 명령어를 통해 Tibero 테이블 <em>BPRJT00T</em>를 <code>Sqoop</code>으로 땡겨오고 <code>Hive</code>로 Import 하고자 했다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">sqoop import <span class="string">&quot;-Dorg.apache.sqoop.splitter.allow_text_splitter=true&quot;</span> \</span><br><span class="line">--connect jdbc:tibero:thin:@192.168.154.xxx:xxxx:tibero \</span><br><span class="line">--driver com.tmax.tibero.jdbc.TbDriver \</span><br><span class="line">--target-dir /project/BPRJT00T \</span><br><span class="line">--username ERP --password xxxx \</span><br><span class="line">--table BPRJT00T \</span><br><span class="line">--fields-terminated-by <span class="string">&quot;,&quot;</span> \</span><br><span class="line">--hive-import \</span><br><span class="line">--create-hive-table \</span><br><span class="line">--hive-table project.BPRJT00T</span><br></pre></td></tr></table></figure><h1 id="Error-Message"><a href="#Error-Message" class="headerlink" title="Error Message"></a>Error Message</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Could not initialize class org.apache.derby.jdbc.EmbeddedDriver...</span><br><span class="line">...</span><br><span class="line">... 12 more</span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="Solution"><a href="#Solution" class="headerlink" title="Solution"></a>Solution</h1><p>분명 <code>Hive</code>의 MetaStore를 <strong>MySQL</strong>로 설정하였는데<em>(초기화까지 완료한 상태)</em> 자꾸 Derby Driver를 못찾았다는 에러가 발생하였다. 이는, <strong>hive-site.xml이 Hive가 인식을 못해 Default Database로 Derby</strong>를 사용했기 때문이다. 이는 <strong>.bash_profile 또는 .profile 내 HADOOP_CLASSPATH를 추가</strong>하여 해결할 수 있다.</p><blockquote><p><em>만약 나처럼 MetaStore를 MySQL이 아닌 Derby로 설정했는데 에러가 발생한다면, $HIVE_HOME/lib 안에 connector 파일이 있는지 확인하고 없다면 copy &amp; paste 하자</em></p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ vi ~/.profile</span><br><span class="line"><span class="built_in">export</span> HADOOP_CLASSPATH=<span class="variable">$HIVE_HOME</span>/conf:<span class="variable">$HIVE_HOME</span>/lib</span><br><span class="line">$ . ~/.profile</span><br></pre></td></tr></table></figure><p>이후 Status에서 작성한 커맨드를 실행하면 <code>Hive</code>에 미리 생성해놓은 Database에 테이블이 생성한 것을 확인할 수 있다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ hdfs dfs -ls /user/hive/warehouse</span><br><span class="line">Found 1 items</span><br><span class="line">drwxrwxrwx   - hive supergroup          0 2019-11-07 15:41 /user/hive/warehouse/project.db</span><br></pre></td></tr></table></figure><blockquote><p><em>MySQL hive 유저의 proejct DB에 bprjt00t 테이블이 들어가 있음을 확인</em></p></blockquote><hr><p>2019.11.07 made by <em>jaejun.lee</em></p>]]></content:encoded>
      
      <comments>https://jx2lee.github.io/hadoop-sqoop_import_error/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[Database] Install MySQL 8.0</title>
      <link>https://jx2lee.github.io/database-install_mysql/</link>
      <guid>https://jx2lee.github.io/database-install_mysql/</guid>
      <pubDate>Tue, 05 Nov 2019 15:00:00 GMT</pubDate>
      <description>
      
        &lt;p&gt;Hive의 Meta Store로 MySQL를 사용하기 위해 설치하고, 이를 정리한다&lt;/p&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p>Hive의 Meta Store로 MySQL를 사용하기 위해 설치하고, 이를 정리한다</p><a id="more"></a><h1 id="Setting-Environment"><a href="#Setting-Environment" class="headerlink" title="Setting Environment"></a>Setting Environment</h1><p>설치에 필요한 라이브러리 version을 맞춰줄 필요가 있다.</p><h2 id="Version-Up-CMake"><a href="#Version-Up-CMake" class="headerlink" title="Version Up CMake"></a>Version Up CMake</h2><p><code>CMake</code> 이 하위 version이라면 올려보도록 한다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ tar -xvzf cmake-3.16.0-rc3.tar.gz</span><br><span class="line">$ <span class="built_in">cd</span> cmake-3.16.0-rc3.tar.gz</span><br><span class="line">$ ./bootstrap</span><br><span class="line">$ make</span><br><span class="line">$ sudo make install</span><br></pre></td></tr></table></figure><h2 id="Version-up-gcc"><a href="#Version-up-gcc" class="headerlink" title="Version up gcc"></a>Version up gcc</h2><p>마찬가지로 <code>gcc</code> version이 하위 버젼이면 올려보도록 한다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ sudo yum install centos-release-scl</span><br><span class="line">$ sudo yum install devtoolset-7-gcc*</span><br><span class="line">$ scl <span class="built_in">enable</span> devtoolset-7 bash</span><br><span class="line">$ <span class="built_in">which</span> gcc</span><br><span class="line">$ gcc --version</span><br><span class="line">gcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-36)</span><br><span class="line">Copyright (C) 2015 Free Software Foundation, Inc.</span><br><span class="line">This is free software; see the <span class="built_in">source</span> <span class="keyword">for</span> copying conditions.  There is NO</span><br><span class="line">warranty; not even <span class="keyword">for</span> MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.</span><br></pre></td></tr></table></figure><h1 id="Make"><a href="#Make" class="headerlink" title="Make"></a>Make</h1><p><code>wget</code>을 이용해 binary 파일을 다운받고 <code>CMake</code>을 이용해 설치한다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /app/</span><br><span class="line">$ wget https://dev.mysql.com/get/Downloads/MySQL-8.0/mysql-8.0.18.tar.gz</span><br><span class="line">$ tar xvfz mysql-8.0.18.tar.gz</span><br><span class="line">$ <span class="built_in">cd</span> mysql-8.0.18</span><br><span class="line">$ cmake \</span><br><span class="line">-DCMAKE_INSTALL_PREFIX=/app/mysql \</span><br><span class="line">-DMYSQL_DATADIR=/home/mysql/data \</span><br><span class="line">-DSYSCONFDIR=/app/mysql \</span><br><span class="line">-DMYSQL_USER=mysql \</span><br><span class="line">-DWITH_MYISAM_STORAGE_ENGINE=1 \</span><br><span class="line">-DWITH_INNOBASE_STORAGE_ENGINE=1 \</span><br><span class="line">-DWITH_PARTITION_STORAGE_ENGINE=1 \</span><br><span class="line">-DWITH_FEDERATED_STORAGE_ENGINE=1 \</span><br><span class="line">-DWITH_BLACKHOLE_STORAGE_ENGINE=1 \</span><br><span class="line">-DWITH_MEMORY_STORAGE_ENGINE=1 \</span><br><span class="line">-DWITH_READLINE=1 \</span><br><span class="line">-DMYSQL_UNIX_ADDR=/app/mysql/mysql.sock \</span><br><span class="line">-DMYSOL_TCP_PORT=3306 \</span><br><span class="line">-DENABLED_LOCAL_INFILE=1 \</span><br><span class="line">-DENABLE_DOWNLOADS=1 \</span><br><span class="line">-DWITH_EXTRA_CHARSETS=all \</span><br><span class="line">-DDEFAULT_CHARSET=utf8 \</span><br><span class="line">-DDEFAULT_COLLATION=utf8_general_ci \</span><br><span class="line">-DWITH_DEBUG=0 \</span><br><span class="line">-DMYSQL_MAINTAINER_MODE=0 \</span><br><span class="line">-DDOWNLOAD_BOOST=1 \</span><br><span class="line">-DDOWNLOAD_BOOST=1 -DWITH_BOOST=/app/mysql-8.0.18</span><br><span class="line">$ make install</span><br></pre></td></tr></table></figure><h1 id="Add-Servcie"><a href="#Add-Servcie" class="headerlink" title="Add Servcie"></a>Add Servcie</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ cp mysql.server /etc/rc.d/init.d/mysql</span><br><span class="line">$ ln -s /etc/rc.d/init.d/mysql /etc/rc.d/rc3.d/S97mysql </span><br><span class="line">$ vi /usr/lib/systemd/system/mysql.service</span><br></pre></td></tr></table></figure><h1 id="etc-my-cnf"><a href="#etc-my-cnf" class="headerlink" title="/etc/my.cnf"></a>/etc/my.cnf</h1><p>/etc 에 <code>my.cnf</code> config 파일을 생성한다. <code>my.cnf</code>는 <strong>MySQL</strong>의 config를 설정하는 파일이며, 본 설치에서는 DB Engine으로 <strong>InnoDB</strong>를 사용한다.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br></pre></td><td class="code"><pre><span class="line">[client]</span><br><span class="line">default-character-set &#x3D; utf8</span><br><span class="line">port &#x3D; 3306</span><br><span class="line">socket &#x3D; &#x2F;tmp&#x2F;mysql.sock</span><br><span class="line">default-character-set &#x3D; utf8</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">[mysqld]</span><br><span class="line">socket&#x3D;&#x2F;app&#x2F;mysql&#x2F;mysql.sock</span><br><span class="line">datadir&#x3D;&#x2F;home&#x2F;mysql&#x2F;data</span><br><span class="line">basedir &#x3D; &#x2F;app&#x2F;mysql</span><br><span class="line">#user &#x3D; mysql</span><br><span class="line">#bind-address &#x3D; 0.0.0.0</span><br><span class="line"></span><br><span class="line">skip-external-locking</span><br><span class="line">key_buffer_size &#x3D; 384M</span><br><span class="line">max_allowed_packet &#x3D; 16M</span><br><span class="line">table_open_cache &#x3D; 2048</span><br><span class="line">sort_buffer_size &#x3D; 2M</span><br><span class="line">read_buffer_size &#x3D; 2M</span><br><span class="line">read_rnd_buffer_size &#x3D; 8M</span><br><span class="line">myisam_sort_buffer_size &#x3D; 64M</span><br><span class="line">thread_cache_size &#x3D; 8</span><br><span class="line"> </span><br><span class="line">#dns query</span><br><span class="line">skip-name-resolve</span><br><span class="line"> </span><br><span class="line">#connection</span><br><span class="line">max_connections &#x3D; 1000</span><br><span class="line">max_connect_errors &#x3D; 1000</span><br><span class="line">wait_timeout&#x3D; 60</span><br><span class="line"> </span><br><span class="line">#slow-queries</span><br><span class="line">#slow_query_log &#x3D; &#x2F;home&#x2F;mysql_data&#x2F;slow-queries.log</span><br><span class="line">#long_query_time &#x3D; 3</span><br><span class="line">#log-slow-queries &#x3D; &#x2F;home&#x2F;mysql_data&#x2F;mysql-slow-queries.log</span><br><span class="line"> </span><br><span class="line">##timestamp</span><br><span class="line">explicit_defaults_for_timestamp</span><br><span class="line">symbolic-links&#x3D;0</span><br><span class="line"></span><br><span class="line">### log</span><br><span class="line">log-error&#x3D;&#x2F;home&#x2F;mysql&#x2F;data&#x2F;mysqld.log</span><br><span class="line">pid-file&#x3D;&#x2F;home&#x2F;mysql&#x2F;mysqld.pid</span><br><span class="line"> </span><br><span class="line">###chracter</span><br><span class="line">character-set-client-handshake&#x3D;FALSE</span><br><span class="line">init_connect &#x3D; SET collation_connection &#x3D; utf8_general_ci</span><br><span class="line">init_connect &#x3D; SET NAMES utf8</span><br><span class="line">character-set-server &#x3D; utf8</span><br><span class="line">collation-server &#x3D; utf8_general_ci</span><br><span class="line">symbolic-links&#x3D;0</span><br><span class="line"></span><br><span class="line">##Password Policy</span><br><span class="line">#validate_password_policy&#x3D;LOW</span><br><span class="line">#validate_password_policy&#x3D;MEDIUM</span><br><span class="line"> </span><br><span class="line">### MyISAM Spectific options</span><br><span class="line">#default-storage-engine &#x3D; myisam</span><br><span class="line">key_buffer_size &#x3D; 32M</span><br><span class="line">bulk_insert_buffer_size &#x3D; 64M</span><br><span class="line">myisam_sort_buffer_size &#x3D; 128M</span><br><span class="line">myisam_max_sort_file_size &#x3D; 10G</span><br><span class="line">myisam_repair_threads &#x3D; 1</span><br><span class="line"> </span><br><span class="line">### INNODB Spectific options</span><br><span class="line">default-storage-engine &#x3D; InnoDB</span><br><span class="line">#skip-innodb</span><br><span class="line">#innodb_additional_mem_pool_size &#x3D; 16M</span><br><span class="line">innodb_buffer_pool_size &#x3D; 1024MB</span><br><span class="line">innodb_data_file_path &#x3D; ibdata1:10M:autoextend</span><br><span class="line">innodb_write_io_threads &#x3D; 8</span><br><span class="line">innodb_read_io_threads &#x3D; 8</span><br><span class="line">innodb_thread_concurrency &#x3D; 16</span><br><span class="line">innodb_flush_log_at_trx_commit &#x3D; 1</span><br><span class="line">innodb_log_buffer_size &#x3D; 8M</span><br><span class="line">innodb_log_file_size &#x3D; 128M</span><br><span class="line">innodb_log_files_in_group &#x3D; 3</span><br><span class="line">innodb_max_dirty_pages_pct &#x3D; 90</span><br><span class="line">innodb_lock_wait_timeout &#x3D; 120</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">[mysqldump]</span><br><span class="line">default-character-set &#x3D; utf8</span><br><span class="line">max_allowed_packet &#x3D; 512M</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">[mysql]</span><br><span class="line">#no-auto-rehash</span><br><span class="line">default-character-set &#x3D; utf8</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">[myisamchk]</span><br><span class="line">key_buffer_size &#x3D; 512M</span><br><span class="line">sort_buffer_size &#x3D; 512M</span><br><span class="line">read_buffer &#x3D; 8M</span><br><span class="line">write_buffer &#x3D; 8M</span><br></pre></td></tr></table></figure><h1 id="Initialize-Database"><a href="#Initialize-Database" class="headerlink" title="Initialize Database"></a>Initialize Database</h1><p>Database를 mysql user로 초기화 한다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">$ /app/mysql/bin/mysqld --initialize-insecure --basedir=/app/mysql --datadir=/home/mysql/data --user=mysql</span><br><span class="line">$ ll /home/mysql/data/</span><br><span class="line">total 448572</span><br><span class="line">-rw-r----- 1 mysql mysql        56 Nov  5 17:19 auto.cnf</span><br><span class="line">-rw------- 1 mysql mysql      1680 Nov  5 17:19 ca-key.pem</span><br><span class="line">-rw-r--r-- 1 mysql mysql      1112 Nov  5 17:19 ca.pem</span><br><span class="line">-rw-r--r-- 1 mysql mysql      1112 Nov  5 17:19 client-cert.pem</span><br><span class="line">-rw------- 1 mysql mysql      1676 Nov  5 17:19 client-key.pem</span><br><span class="line">-rw-r----- 1 mysql mysql      6100 Nov  5 17:19 ib_buffer_pool</span><br><span class="line">-rw-r----- 1 mysql mysql  10485760 Nov  5 17:19 ibdata1</span><br><span class="line">-rw-r----- 1 mysql mysql 134217728 Nov  5 17:19 ib_logfile0</span><br><span class="line">-rw-r----- 1 mysql mysql 134217728 Nov  5 17:19 ib_logfile1</span><br><span class="line">-rw-r----- 1 mysql mysql 134217728 Nov  5 17:19 ib_logfile2</span><br><span class="line">drwxr-x--- 2 mysql mysql         6 Nov  5 17:19 <span class="comment">#innodb_temp</span></span><br><span class="line">drwxr-x--- 2 mysql mysql       143 Nov  5 17:19 mysql</span><br><span class="line">-rw-r----- 1 mysql mysql      1301 Nov  5 17:19 mysqld.log</span><br><span class="line">-rw-r----- 1 mysql mysql  25165824 Nov  5 17:19 mysql.ibd</span><br><span class="line">drwxr-x--- 2 mysql mysql      8192 Nov  5 17:19 performance_schema</span><br><span class="line">-rw------- 1 mysql mysql      1680 Nov  5 17:19 private_key.pem</span><br><span class="line">-rw-r--r-- 1 mysql mysql       452 Nov  5 17:19 public_key.pem</span><br><span class="line">-rw-r--r-- 1 mysql mysql      1112 Nov  5 17:19 server-cert.pem</span><br><span class="line">-rw------- 1 mysql mysql      1676 Nov  5 17:19 server-key.pem</span><br><span class="line">drwxr-x--- 2 mysql mysql        28 Nov  5 17:19 sys</span><br><span class="line">-rw-r----- 1 mysql mysql  10485760 Nov  5 17:19 undo_001</span><br><span class="line">-rw-r----- 1 mysql mysql  10485760 Nov  5 17:19 undo_002</span><br></pre></td></tr></table></figure><h1 id="Restart-Service-amp-Checking"><a href="#Restart-Service-amp-Checking" class="headerlink" title="Restart Service &amp; Checking"></a>Restart Service &amp; Checking</h1><p>Service 재기동 후 <strong>MySQL</strong>이 제대로 설치되었는지 확인한다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">$ systemctl stop mysql</span><br><span class="line">$ systemctl start mysql</span><br><span class="line">$ /app/mysql/bin/mysql -u root -p</span><br><span class="line">Enter password: </span><br><span class="line">Welcome to the MySQL monitor.  Commands end with ; or \g.</span><br><span class="line">Your MySQL connection id is 8</span><br><span class="line">Server version: 8.0.18 Source distribution</span><br><span class="line"></span><br><span class="line">Copyright (c) 2000, 2019, Oracle and/or its affiliates. All rights reserved.</span><br><span class="line"></span><br><span class="line">Oracle is a registered trademark of Oracle Corporation and/or its</span><br><span class="line">affiliates. Other names may be trademarks of their respective</span><br><span class="line">owners.</span><br><span class="line"></span><br><span class="line">Type <span class="string">&#x27;help;&#x27;</span> or <span class="string">&#x27;\h&#x27;</span> <span class="keyword">for</span> <span class="built_in">help</span>. Type <span class="string">&#x27;\c&#x27;</span> to clear the current input statement.</span><br><span class="line"></span><br><span class="line">mysql&gt; </span><br><span class="line">mysql&gt; </span><br><span class="line">mysql&gt; </span><br><span class="line">mysql&gt; \s</span><br><span class="line">--------------</span><br><span class="line">/app/mysql/bin/mysql  Ver 8.0.18 <span class="keyword">for</span> Linux on x86_64 (Source distribution)</span><br><span class="line"></span><br><span class="line">Connection id:8</span><br><span class="line">Current database:</span><br><span class="line">Current user:root@localhost</span><br><span class="line">SSL:Not <span class="keyword">in</span> use</span><br><span class="line">Current pager:stdout</span><br><span class="line">Using outfile:<span class="string">&#x27;&#x27;</span></span><br><span class="line">Using delimiter:;</span><br><span class="line">Server version:8.0.18 Source distribution</span><br><span class="line">Protocol version:10</span><br><span class="line">Connection:Localhost via UNIX socket</span><br><span class="line">Server characterset:utf8</span><br><span class="line">Db     characterset:utf8</span><br><span class="line">Client characterset:utf8</span><br><span class="line">Conn.  characterset:utf8</span><br><span class="line">UNIX socket:/tmp/mysql.sock</span><br><span class="line">Uptime:17 min 25 sec</span><br><span class="line"></span><br><span class="line">Threads: 2  Questions: 6  Slow queries: 0  Opens: 115  Flush tables: 3  Open tables: 35  Queries per second avg: 0.005</span><br><span class="line">--------------</span><br></pre></td></tr></table></figure><blockquote><p><em>MySQL 시작 시 /tmp/mysql.sock 이 없다고 fail이 날 수 있다. 이때는 tmp 폴더안에 mysql.sock이 있는 path로 링크를 생성하면된다. (초기 설정부터 /tmp에 안들어가게끔 어떻게 설정하지..? 이건 내일하자!)</em></p></blockquote><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="https://xinet.kr/?p=2149">MYSQL 8.0 INSTALL ( mysql 8.0.17 ) / Centos 7</a></li><li><a href="https://idchowto.com/?p=43760">CentOS7에서 Mysql 8.0 소스 설치</a></li><li><a href="https://dev.mysql.com/doc/refman/8.0/en/installing.html">MySQL Documents</a></li></ol><hr><p>2019.11.06 made by <em>jaejun.lee</em></p>]]></content:encoded>
      
      <comments>https://jx2lee.github.io/database-install_mysql/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[Hadoop] Install Hive</title>
      <link>https://jx2lee.github.io/hadoop-install_hive/</link>
      <guid>https://jx2lee.github.io/hadoop-install_hive/</guid>
      <pubDate>Tue, 05 Nov 2019 15:00:00 GMT</pubDate>
      <description>
      
        &lt;p&gt;Hive를 설치하는 과정을 살펴본다&lt;/p&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p>Hive를 설치하는 과정을 살펴본다</p><a id="more"></a><h1 id="Preliminaries"><a href="#Preliminaries" class="headerlink" title="Preliminaries"></a>Preliminaries</h1><h2 id="Add-user-amp-group"><a href="#Add-user-amp-group" class="headerlink" title="Add user &amp; group"></a>Add user &amp; group</h2><p><code>Hive</code> user를 생성한다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ groupadd hdclient</span><br><span class="line">$ usermod -g hdclient sqoop </span><br><span class="line">$ adduser hive --gid 8630 <span class="comment"># hdclient</span></span><br></pre></td></tr></table></figure><h2 id="bash-profile"><a href="#bash-profile" class="headerlink" title=".bash_profile"></a>.bash_profile</h2><p><code>.bash_profile</code>에 <code>Hadoop</code> 및 <code>Hive</code> ENV를 추가한다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Hadoop</span></span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/app/jdk</span><br><span class="line"><span class="built_in">export</span> HADOOP_HOME=/app/hadoop</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$JAVA_HOME</span>/bin:\</span><br><span class="line"><span class="variable">$HADOOP_HOME</span>/bin:\</span><br><span class="line"><span class="variable">$HADOOP_HOME</span>/sbin</span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> HADOOP_PREFIX=/app/hadoop</span><br><span class="line"><span class="built_in">export</span> HADOOP_COMMON_HOME=<span class="variable">$HADOOP_PREFIX</span></span><br><span class="line"><span class="built_in">export</span> HADOOP_CONF_DIR=<span class="variable">$HADOOP_PREFIX</span>/etc/hadoop</span><br><span class="line"><span class="built_in">export</span> HADOOP_HDFS_HOME=<span class="variable">$HADOOP_PREFIX</span></span><br><span class="line"><span class="built_in">export</span> HADOOP_MAPRED_HOME=<span class="variable">$HADOOP_PREFIX</span></span><br><span class="line"><span class="built_in">export</span> HADOOP_YARN_HOME=<span class="variable">$HADOOP_PREFIX</span></span><br><span class="line"><span class="built_in">export</span> YARN_CONF_DIR=<span class="variable">$HADOOP_PREFIX</span>/etc/hadoop</span><br><span class="line"></span><br><span class="line"><span class="comment"># Hive</span></span><br><span class="line"><span class="built_in">export</span> HIVE_HOME=/app/hive</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$JAVA_HOME</span>/bin:<span class="variable">$HIVE_HOME</span>/bin</span><br></pre></td></tr></table></figure><h2 id="Setting-MySQL-for-Hive-MetaStore"><a href="#Setting-MySQL-for-Hive-MetaStore" class="headerlink" title="Setting MySQL for Hive MetaStore"></a>Setting MySQL for Hive MetaStore</h2><p><code>Hive</code> MetaStore를 MySQL로 사용하기위해 새로운 database와 <code>hive</code> user를 생성한다</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="keyword">create</span> <span class="keyword">database</span> hive;</span><br><span class="line">$ <span class="keyword">create</span> <span class="keyword">user</span> hive@<span class="string">&#x27;%&#x27;</span> <span class="keyword">identified</span> <span class="keyword">by</span> <span class="string">&#x27;hive&#x27;</span>;</span><br><span class="line">$ <span class="keyword">grant</span> <span class="keyword">all</span> <span class="keyword">privileges</span> <span class="keyword">on</span> hive.* <span class="keyword">to</span> hive@<span class="string">&#x27;%&#x27;</span>;</span><br><span class="line">$ <span class="keyword">use</span> mysql;</span><br><span class="line">$ <span class="keyword">select</span> host, <span class="keyword">user</span> <span class="keyword">from</span> <span class="keyword">user</span>;</span><br><span class="line"></span><br><span class="line">+<span class="comment">-----------+------------------+</span></span><br><span class="line">| host      | user             |</span><br><span class="line">+<span class="comment">-----------+------------------+</span></span><br><span class="line">| %         | hive             |</span><br><span class="line">| localhost | mysql.infoschema |</span><br><span class="line">| localhost | mysql.session    |</span><br><span class="line">| localhost | mysql.sys        |</span><br><span class="line">| localhost | root             |</span><br><span class="line">+<span class="comment">-----------+------------------+</span></span><br><span class="line">5 rows in <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure><blockquote><p><em>user를 생성할 때 골뱅이 뒤 %는 모든 외부 ip의 접근을 허용한다는 뜻이다.</em></p></blockquote><h2 id="jdbc-to-HIVE-HOME-lib"><a href="#jdbc-to-HIVE-HOME-lib" class="headerlink" title="jdbc to $HIVE_HOME/lib/"></a>jdbc to $HIVE_HOME/lib/</h2><p>MySQL Driver를 해당 PATH로 복사한다.</p><p><code>$ cp tibero6-jdbc.jar /app/hive/lib/</code></p><blockquote><p><em>Driver URL : <a href="https://dev.mysql.com/downloads/connector/j/8.0.html">https://dev.mysql.com/downloads/connector/j/8.0.html</a></em></p></blockquote><h1 id="Hive-Configurations"><a href="#Hive-Configurations" class="headerlink" title="Hive Configurations"></a>Hive Configurations</h1><h2 id="hive-env-sh"><a href="#hive-env-sh" class="headerlink" title="hive-env.sh"></a>hive-env.sh</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ cp <span class="variable">$HIVE_HOME</span>/conf/hive-env.sh.template <span class="variable">$HIVE_HOME</span>/conf/hive-env.sh`</span><br><span class="line">$ vi <span class="variable">$HIVE_HOME</span>/conf/hive-env.sh</span><br><span class="line">HADOOP_HOME=/app/hadoop</span><br></pre></td></tr></table></figure><h2 id="hive-site-xml"><a href="#hive-site-xml" class="headerlink" title="hive-site.xml"></a>hive-site.xml</h2><p><code>hive-site.xml</code>을 아래와 같이 작성한다. <code>Hive</code>의 MetaStore를 외부 서버의 MySQL를 이용할 예정이다. <em>(Hive 설치된 서버와 별개의 서버이다. 만약 같은 서버라면, javax.jdo.option.ConnectionURL의 ip:port값은 localhost:port로 작성한다.)</em></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.local<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:mysql://ip:port/hive?serverTimezone=UTC<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>hive<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>hive<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.urls<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>thrift://node5.dat:10000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><blockquote><p><em>javax.jdo.option.ConnectionURL의 port 뒤 hive는 MetaStore로 사용하기 위한 MySQL DB name이다. 뒤에 serverTimezone arg를 추가한 이유는 추가하지 않으면 SchemaTool 초기화 시 에러 메세지가 뜬다. 결국엔 시간 형식이 맞지 않아 생기는 문제이므로 arg를 추가한다</em></p></blockquote><h2 id="Create-MetaStore-Schema"><a href="#Create-MetaStore-Schema" class="headerlink" title="Create MetaStore Schema"></a>Create MetaStore Schema</h2><p><code>schematool -initSchema -dbType mysql --verbose</code> 을 통해 <code>Hive</code>의 MetaStore를 초기화 한다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">$ schematool -initSchema -dbType mysql --verbose</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line">beeline&gt; Initialization script completed</span><br><span class="line">schemaTool completed</span><br><span class="line">$ mysql -u hive -p</span><br><span class="line">mysql&gt; use hive;</span><br><span class="line">Reading table information <span class="keyword">for</span> completion of table and column names</span><br><span class="line">You can turn off this feature to get a quicker startup with -A</span><br><span class="line">Database changed</span><br><span class="line">mysql&gt; show tables;</span><br><span class="line">+---------------------------+</span><br><span class="line">| Tables_in_hive            |</span><br><span class="line">+---------------------------+</span><br><span class="line">| AUX_TABLE                 |</span><br><span class="line">| BUCKETING_COLS            |</span><br><span class="line">| CDS                       |</span><br><span class="line">| COLUMNS_V2                |</span><br><span class="line">| COMPACTION_QUEUE          |</span><br><span class="line">| COMPLETED_COMPACTIONS     |</span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line">| SERDE_PARAMS              |</span><br><span class="line">| TYPES                     |</span><br><span class="line">| TYPE_FIELDS               |</span><br><span class="line">| VERSION                   |</span><br><span class="line">| WRITE_SET                 |</span><br><span class="line">+---------------------------+</span><br><span class="line">57 rows <span class="keyword">in</span> <span class="built_in">set</span> (0.00 sec)</span><br></pre></td></tr></table></figure><h1 id="Run-HiveServer-amp-MetaStore"><a href="#Run-HiveServer-amp-MetaStore" class="headerlink" title="Run HiveServer &amp; MetaStore"></a>Run HiveServer &amp; MetaStore</h1><p><code>hiveserver2</code>와 <code>metastore</code>를 기동해주면 완료.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">$ hive --service hiveserver2 &amp;</span><br><span class="line">SLF4J: Class path contains multiple SLF4J bindings.</span><br><span class="line">SLF4J: Found binding <span class="keyword">in</span> [jar:file:/app/hive/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: Found binding <span class="keyword">in</span> [jar:file:/app/hadoop/2.9.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: See http://www.slf4j.org/codes.html<span class="comment">#multiple_bindings for an explanation.</span></span><br><span class="line">SLF4J: Actual binding is of <span class="built_in">type</span> [org.apache.logging.slf4j.Log4jLoggerFactory]</span><br><span class="line">Loading class `com.mysql.jdbc.Driver<span class="string">&#x27;. This is deprecated. The new driver class is `com.mysql.cj.jdbc.Driver&#x27;</span>. The driver is automatically registered via the SPI and manual loading of the driver class is generally unnecessary.</span><br><span class="line">$ hive --service metastore &amp;</span><br><span class="line">SLF4J: Class path contains multiple SLF4J bindings.</span><br><span class="line">SLF4J: Found binding <span class="keyword">in</span> [jar:file:/app/hive/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: Found binding <span class="keyword">in</span> [jar:file:/app/hadoop/2.9.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: See http://www.slf4j.org/codes.html<span class="comment">#multiple_bindings for an explanation.</span></span><br><span class="line">SLF4J: Actual binding is of <span class="built_in">type</span> [org.apache.logging.slf4j.Log4jLoggerFactory]</span><br><span class="line">Loading class `com.mysql.jdbc.Driver<span class="string">&#x27;. This is deprecated. The new driver class is `com.mysql.cj.jdbc.Driver&#x27;</span>. The driver is automatically registered via the SPI and manual loading of the driver class is generally unnecessary.</span><br></pre></td></tr></table></figure><hr><p>made by <em>jaejun.lee</em></p>]]></content:encoded>
      
      <comments>https://jx2lee.github.io/hadoop-install_hive/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[Python] Mappers and Reducers</title>
      <link>https://jx2lee.github.io/python-map_reduce/</link>
      <guid>https://jx2lee.github.io/python-map_reduce/</guid>
      <pubDate>Wed, 30 Oct 2019 15:00:00 GMT</pubDate>
      <description>
      
        &lt;p&gt;hackerrank에서 제공하는 Database 카테고리의 MapReduce 문제를 풀어본다&lt;/p&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p>hackerrank에서 제공하는 Database 카테고리의 MapReduce 문제를 풀어본다</p><a id="more"></a><h1 id="문제"><a href="#문제" class="headerlink" title="문제"></a>문제</h1><p><strong>Mappers and Reducers</strong></p><p><a href="http://www.slideshare.net/rantav/introduction-to-map-reduce">Here’s</a> a quick but comprehensive introduction to the idea of splitting tasks into a MapReduce model. The four important functions involved are:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Map (the mapper function)  </span><br><span class="line">EmitIntermediate(the intermediate key,value pairs emitted by the mapper functions)  </span><br><span class="line">Reduce (the reducer function)  </span><br><span class="line">Emit (the final output, after summarization from the Reduce functions)</span><br></pre></td></tr></table></figure><p>We provide you with a single system, single thread version of a basic MapReduce implementation.</p><p><strong>Task</strong></p><p>Joins are</p><p>The input is a number of lines with pairs of name of friends, in the form:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[Friend1] [Friend2]</span><br></pre></td></tr></table></figure><p>The required output is to print the number of friends of each person, in the format shown. The code for the MapReduce class, parts related to IO etc. has already been provided. However, the mapper and reducer functions are incomplete. Your task is to fill up the mapper and reducer functions appropriately, such that the program works, and outputs the list of number of friends of each person , in lexicographical order.</p><p>Also, this program outputs certain information to the error stream. This information has been logged to help beginners gain a better understanding of the the intermediate steps in a map-reduce process.</p><p><strong>Languages Supported</strong></p><p>Currently, we provide the base code in Python.</p><p><strong>Input Format</strong></p><p>A list of single space separated pairs of friend names. We have already written the input handling code to read in this data.</p><p><strong>Output Format</strong></p><p>Again, the output handling part has already been provided in the template code. The Key contains [Person name] and the value contains the number of friends, sorted in lexicographical order. The entities in this list, will naturally be confined to only those people provided in the input data.</p><p><strong>Sample Input</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Joe Sue</span><br><span class="line">Sue Phi</span><br><span class="line">Phi Joe</span><br><span class="line">Phi Alice</span><br></pre></td></tr></table></figure><p><strong>Sample Output</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#123;&quot;key&quot;:&quot;Alice&quot;,&quot;value&quot;:&quot;1&quot;&#125;</span><br><span class="line">&#123;&quot;key&quot;:&quot;Joe&quot;,&quot;value&quot;:&quot;2&quot;&#125;</span><br><span class="line">&#123;&quot;key&quot;:&quot;Phi&quot;,&quot;value&quot;:&quot;3&quot;&#125;</span><br><span class="line">&#123;&quot;key&quot;:&quot;Sue&quot;,&quot;value&quot;:&quot;2&quot;&#125;</span><br></pre></td></tr></table></figure><p><strong>Explanation</strong></p><p>We have computed the number of friends for each person via the Mapper and Reducer functions.</p><h1 id="해결"><a href="#해결" class="headerlink" title="해결"></a>해결</h1><p>full code 는 아래와 같다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> OrderedDict</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MapReduce</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.intermediate = OrderedDict()</span><br><span class="line">        self.result = []</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">emitIntermediate</span>(<span class="params">self, key, value</span>):</span></span><br><span class="line">        self.intermediate.setdefault(key, [])</span><br><span class="line">        self.intermediate[key].append(value)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">emit</span>(<span class="params">self, value</span>):</span></span><br><span class="line">        self.result.append(value)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">execute</span>(<span class="params">self, data, mapper, reducer</span>):</span></span><br><span class="line">        <span class="keyword">for</span> record <span class="keyword">in</span> data:</span><br><span class="line">            mapper(record)</span><br><span class="line">        <span class="keyword">for</span> key <span class="keyword">in</span> self.intermediate:</span><br><span class="line">            reducer(key, self.intermediate[key])</span><br><span class="line"></span><br><span class="line">        self.result.sort()</span><br><span class="line">        print(self.result)</span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> self.result:</span><br><span class="line">            print(<span class="string">&quot;&#123;\&quot;key\&quot;:\&quot;&quot;</span> + item[<span class="number">0</span>] + <span class="string">&quot;\&quot;,\&quot;value\&quot;:\&quot;&quot;</span> + <span class="built_in">str</span>(item[<span class="number">1</span>]) + <span class="string">&quot;\&quot;&#125;&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">mapReducer = MapReduce()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mapper</span>(<span class="params">record</span>):</span></span><br><span class="line">    <span class="comment"># Start writing the Map code here</span></span><br><span class="line">    words = record.split()</span><br><span class="line">    mapReducer.emitIntermediate(words[<span class="number">0</span>], words[<span class="number">1</span>])</span><br><span class="line">    mapReducer.emitIntermediate(words[<span class="number">1</span>], words[<span class="number">0</span>])</span><br><span class="line">    print(mapReducer.intermediate)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reducer</span>(<span class="params">key, list_of_values</span>):</span></span><br><span class="line">    <span class="comment"># Start writing the Reduce code here</span></span><br><span class="line">    mapReducer.emit((key, <span class="built_in">len</span>(list_of_values)))</span><br><span class="line">    print(mapReducer.result)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># inputData = [&#x27;Joe Sue&#x27;, &#x27;Sue Phi&#x27;, &#x27;Phi Joe&#x27;, &#x27;Phi Alice&#x27;]</span></span><br><span class="line">    inputData = []</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> sys.stdin:</span><br><span class="line">        inputData.append(line)</span><br><span class="line">    mapReducer.execute(inputData, mapper, reducer)</span><br></pre></td></tr></table></figure><p>우선, <code>mapReduce</code> class를 살펴본다.</p><h2 id="Class-mapReduce"><a href="#Class-mapReduce" class="headerlink" title="Class : mapReduce"></a>Class : mapReduce</h2><p>clss <code>mapReduce</code>는 <code>init</code> 함수를 포함 총 세 개의 함수를 갖는다.</p><h3 id="Func-init"><a href="#Func-init" class="headerlink" title="Func : init"></a>Func : init</h3><p><strong>init</strong>`함수로 인해 <strong>intermediate, result</strong> 변수를 갖는다. 이는 각각 <u>key-value로 이루어진 dictionary <em>(문제에서 원하는 단어 : 단어 출현 횟수를 의미)</em></u>와 <u>문제 정답에 맞는 형식의 Return</u> 값이다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">    self.intermediate = OrderedDict()</span><br><span class="line">    self.result = []</span><br></pre></td></tr></table></figure><h3 id="Func-emitIntermediate"><a href="#Func-emitIntermediate" class="headerlink" title="Func : emitIntermediate"></a>Func : emitIntermediate</h3><p>key-value 를 입력받아 dictionary에 추가하는 함수이다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">emitIntermediate</span>(<span class="params">self, key, value</span>):</span></span><br><span class="line">    self.intermediate.setdefault(key, [])</span><br><span class="line">    self.intermediate[key].append(value)</span><br></pre></td></tr></table></figure><h3 id="Func-emit"><a href="#Func-emit" class="headerlink" title="Func : emit"></a>Func : emit</h3><p>각 단어의 출현 횟수를 집계한 후 결과값에 담는 함수이다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">emit</span>(<span class="params">self, value</span>):</span></span><br><span class="line">    self.result.append(value)</span><br></pre></td></tr></table></figure><h3 id="Func-execute"><a href="#Func-execute" class="headerlink" title="Func : execute"></a>Func : execute</h3><p>입력받은 데이터를 읽어들여 나중에 우리가 작성해야할 <code>mapper / reducer</code>함수를 이용해 최종 결과값을 알맞는 형태로 출력하는 함수이다. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">execute</span>(<span class="params">self, data, mapper, reducer</span>):</span></span><br><span class="line">    <span class="keyword">for</span> record <span class="keyword">in</span> data:</span><br><span class="line">        mapper(record)</span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> self.intermediate:</span><br><span class="line">        reducer(key, self.intermediate[key])</span><br><span class="line"></span><br><span class="line">    self.result.sort()</span><br><span class="line">    print(self.result)</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> self.result:</span><br><span class="line">        print(<span class="string">&quot;&#123;\&quot;key\&quot;:\&quot;&quot;</span> + item[<span class="number">0</span>] + <span class="string">&quot;\&quot;,\&quot;value\&quot;:\&quot;&quot;</span> + <span class="built_in">str</span>(item[<span class="number">1</span>]) + <span class="string">&quot;\&quot;&#125;&quot;</span>)</span><br></pre></td></tr></table></figure><blockquote><p><em>실행함수(excutable fucntion)라 생각하자</em></p></blockquote><h2 id="Func-mapper"><a href="#Func-mapper" class="headerlink" title="Func : mapper"></a>Func : mapper</h2><p>이제 <code>mapper</code> 를 살펴본다. 입력받은 한 문장은 <code>split</code> 함수를 통해 두 단어로 나누어주고, 첫 번째 단어만 key로 인식하면 안되기 때문에 <code>mapReducer</code> 클래스에서 만든 emitIntermediate 함수를 <strong>두 번</strong> 수행한다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mapper</span>(<span class="params">record</span>):</span></span><br><span class="line">    <span class="comment"># Start writing the Map code here</span></span><br><span class="line">    words = record.split()</span><br><span class="line">    mapReducer.emitIntermediate(words[<span class="number">0</span>], words[<span class="number">1</span>])</span><br><span class="line">    mapReducer.emitIntermediate(words[<span class="number">1</span>], words[<span class="number">0</span>])</span><br><span class="line">    print(mapReducer.intermediate)</span><br></pre></td></tr></table></figure><p>과연 이 mapper 함수가 어떻게 작동되는지 문제에서 제공한 test case를 바탕으로 print 해보면 다음과 같이 출력된다. 즉, 같은 key값을 가지면 value로 append 해나간다. <em>(value값으로 계속해서 단어를 추가하는데 이는 나중에 <code>reducer</code> 함수에서 집계를 할 때 사용한다)</em></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">OrderedDict([(&#39;Joe&#39;, [&#39;Sue&#39;]), (&#39;Sue&#39;, [&#39;Joe&#39;])])</span><br><span class="line">OrderedDict([(&#39;Joe&#39;, [&#39;Sue&#39;]), (&#39;Sue&#39;, [&#39;Joe&#39;, &#39;Phi&#39;]), (&#39;Phi&#39;, [&#39;Sue&#39;])])</span><br><span class="line">OrderedDict([(&#39;Joe&#39;, [&#39;Sue&#39;, &#39;Phi&#39;]), (&#39;Sue&#39;, [&#39;Joe&#39;, &#39;Phi&#39;]), (&#39;Phi&#39;, [&#39;Sue&#39;, &#39;Joe&#39;])])</span><br><span class="line">OrderedDict([(&#39;Joe&#39;, [&#39;Sue&#39;, &#39;Phi&#39;]), (&#39;Sue&#39;, [&#39;Joe&#39;, &#39;Phi&#39;]), (&#39;Phi&#39;, [&#39;Sue&#39;, &#39;Joe&#39;, &#39;Alice&#39;]), (&#39;Alice&#39;, [&#39;Phi&#39;])])</span><br></pre></td></tr></table></figure><h2 id="Func-reducer"><a href="#Func-reducer" class="headerlink" title="Func : reducer"></a>Func : reducer</h2><p>key-value로 이루어진 dictionary를 집계해주는 reducer 함수이다. <code>mapReducer</code> 클래스의 <code>emit</code>함수를 통해 <em>result</em> 변수에 결과값을 저장한다. <strong>이때, 위 <code>mapper</code>함수를 통해 각 key에 대한 value들</strong>의 길이를 <u>key와 함께 append 한다.</u></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reducer</span>(<span class="params">key, list_of_values</span>):</span></span><br><span class="line">    <span class="comment"># Start writing the Reduce code here</span></span><br><span class="line">    mapReducer.emit((key, <span class="built_in">len</span>(list_of_values)))</span><br><span class="line">    print(mapReducer.result)</span><br></pre></td></tr></table></figure><h2 id="Main"><a href="#Main" class="headerlink" title="Main"></a>Main</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># inputData = [&#x27;Joe Sue&#x27;, &#x27;Sue Phi&#x27;, &#x27;Phi Joe&#x27;, &#x27;Phi Alice&#x27;]</span></span><br><span class="line">    inputData = []</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> sys.stdin:</span><br><span class="line">        inputData.append(line)</span><br><span class="line">    mapReducer.execute(inputData, mapper, reducer)</span><br></pre></td></tr></table></figure><blockquote><p><em>이렇게 class 및 function을 직접 짜보면서 설계하는 단계의 중요성을 깨달았다. Python을 이런 방식으로 코딩을 해보는 연습을 해야겠다</em></p></blockquote><h1 id="참고"><a href="#참고" class="headerlink" title="참고"></a>참고</h1><ol><li><a href="https://github.com/cielavenir/procon/blob/master/hackerrank/map-reduce-advanced-count-number-of-friends.py"> https://github.com/cielavenir/procon/blob/master/hackerrank/map-reduce-advanced-count-number-of-friends.py </a></li><li><a href="https://jayzzz.tistory.com/44">하둡 맵리듀스(MapReduce) 알아보자,  https://jayzzz.tistory.com/44</a></li></ol><hr><p>2019.10.31 made by <em>jaejun.lee</em></p>]]></content:encoded>
      
      <comments>https://jx2lee.github.io/python-map_reduce/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[SQL] 15 Days of Learning</title>
      <link>https://jx2lee.github.io/hackerrank-15_days_of_learning/</link>
      <guid>https://jx2lee.github.io/hackerrank-15_days_of_learning/</guid>
      <pubDate>Tue, 29 Oct 2019 15:00:00 GMT</pubDate>
      <description>
      
        &lt;p&gt;&lt;code&gt;hackerrank&lt;/code&gt;에서 제공하는 &lt;code&gt;15 Days of Learning&lt;/code&gt; 를 SELECT sub query을 활용해 해결하였다.&lt;/p&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p><code>hackerrank</code>에서 제공하는 <code>15 Days of Learning</code> 를 SELECT sub query을 활용해 해결하였다.</p><a id="more"></a><h1 id="문제"><a href="#문제" class="headerlink" title="문제"></a>문제</h1><p>Julia conducted a days of learning SQL contest. The start date of the contest was <em>March 01, 2016</em> and the end date was <em>March 15, 2016</em>.</p><p>Write a query to print total number of unique hackers who made at least submission each day (starting on the first day of the contest), and find the <em>hacker_id</em> and <em>name</em> of the hacker who made maximum number of submissions each day. If more than one such hacker has a maximum number of submissions, print the lowest <em>hacker_id</em>. The query should print this information for each day of the contest, sorted by the date.</p><hr><p><strong>Input Format</strong></p><p>The following tables hold contest data:</p><ul><li><em>Hackers:</em> The <em>hacker_id</em> is the id of the hacker, and <em>name</em> is the name of the hacker.<img src="https://s3.amazonaws.com/hr-challenge-images/19597/1458511164-12adec3b8b-ScreenShot2016-03-21at3.26.47AM.png" alt="img"></li><li><em>Submissions:</em> The <em>submission_date</em> is the date of the submission, <em>submission_id</em> is the id of the submission, <em>hacker_id</em> is the id of the hacker who made the submission, and <em>score</em> is the score of the submission. <img src="https://s3.amazonaws.com/hr-challenge-images/19597/1458511251-0b534030b9-ScreenShot2016-03-21at3.26.56AM.png" alt="img"></li></ul><p><strong>Sample Input</strong></p><p>For the following sample input, assume that the end date of the contest was <em>March 06, 2016</em>.</p><p><em>Hackers</em> Table: <img src="https://s3.amazonaws.com/hr-challenge-images/19597/1458511957-814a2c7bf2-ScreenShot2016-03-21at3.27.06AM.png" alt="img"> <em>Submissions</em> Table: <img src="https://s3.amazonaws.com/hr-challenge-images/19597/1458512015-ff6a708164-ScreenShot2016-03-21at3.27.21AM.png" alt="img"></p><p><strong>Sample Output</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">2016-03-01 4 20703 Angela</span><br><span class="line">2016-03-02 2 79722 Michael</span><br><span class="line">2016-03-03 2 20703 Angela</span><br><span class="line">2016-03-04 2 20703 Angela</span><br><span class="line">2016-03-05 1 36396 Frank</span><br><span class="line">2016-03-06 1 20703 Angela</span><br></pre></td></tr></table></figure><p><strong>Explanation</strong></p><p>On <em>March 01, 2016</em> hackers , , , and made submissions. There are unique hackers who made at least one submission each day. As each hacker made one submission, is considered to be the hacker who made maximum number of submissions on this day. The name of the hacker is <em>Angela</em>.</p><p>On <em>March 02, 2016</em> hackers , , and made submissions. Now and were the only ones to submit every day, so there are unique hackers who made at least one submission each day. made submissions, and name of the hacker is <em>Michael</em>.</p><p>On <em>March 03, 2016</em> hackers , , and made submissions. Now and were the only ones, so there are unique hackers who made at least one submission each day. As each hacker made one submission so is considered to be the hacker who made maximum number of submissions on this day. The name of the hacker is <em>Angela</em>.</p><p>On <em>March 04, 2016</em> hackers , , , and made submissions. Now and only submitted each day, so there are unique hackers who made at least one submission each day. As each hacker made one submission so is considered to be the hacker who made maximum number of submissions on this day. The name of the hacker is <em>Angela</em>.</p><p>On <em>March 05, 2016</em> hackers , , and made submissions. Now only submitted each day, so there is only unique hacker who made at least one submission each day. made submissions and name of the hacker is <em>Frank</em>.</p><p>On <em>March 06, 2016</em> only made submission, so there is only unique hacker who made at least one submission each day. made submission and name of the hacker is <em>Angela</em>.</p><h1 id="접근"><a href="#접근" class="headerlink" title="접근"></a>접근</h1><p><code>Join</code>을 이용해 문제를 풀려다 실패하였다. 이에 제공되는 Table이 2개인 점을 활용하여 SELECT 절 내 Sub query를 작성하는 것으로 접근하였다.</p><p>우선, 유일한 제출 날짜 Table로 부터 <em>제출 날짜가 같고 제출 마감일까지 날짜 차이가 같은 유일한 hacker_id</em> 를 조회하는 SELECT 문을 작성한다. 이때, 최종 결과물은 날짜를 기준으로 group화 시킨다.</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">    submission_date,</span><br><span class="line">    (<span class="keyword">select</span> <span class="keyword">count</span>(<span class="keyword">distinct</span> hacker_id)</span><br><span class="line">     <span class="keyword">from</span> submissions <span class="keyword">as</span> s2</span><br><span class="line">     <span class="keyword">where</span> s2.submission_date = s1.submission_date <span class="keyword">and</span></span><br><span class="line">     (<span class="keyword">select</span> <span class="keyword">count</span>(<span class="keyword">distinct</span> s3.submission_date)</span><br><span class="line">      <span class="keyword">from</span> submissions <span class="keyword">as</span> s3</span><br><span class="line">      <span class="keyword">where</span> s3.hacker_id = s2.hacker_id <span class="keyword">and</span> s3.submission_date &lt; s1.submission_date) = <span class="keyword">datediff</span>(s1.submission_date, <span class="string">&#x27;2016-03-01&#x27;</span>)),</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">    (<span class="keyword">select</span> <span class="keyword">distinct</span> submission_date <span class="keyword">from</span> submissions) <span class="keyword">as</span> s1</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> submission_date;</span><br></pre></td></tr></table></figure><p>이후, <em>제출을 가장 많이 한 hacker의 id를 조회한 id table</em> 과 <em>id table을 이용해 hacker 이름을 조회</em> 하는 SELECT Sub query를 추가한다. 이때, id table의 경우 가장 많이 제출한 hacker만 뽑아야 하기 때문에 중복을 피하고자 <code>LIMIT 1</code>을 추가한다.</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">    submission_date,</span><br><span class="line">    (<span class="keyword">select</span> <span class="keyword">count</span>(<span class="keyword">distinct</span> hacker_id)</span><br><span class="line">     <span class="keyword">from</span> submissions <span class="keyword">as</span> s2</span><br><span class="line">     <span class="keyword">where</span> s2.submission_date = s1.submission_date <span class="keyword">and</span></span><br><span class="line">     (<span class="keyword">select</span> <span class="keyword">count</span>(<span class="keyword">distinct</span> s3.submission_date)</span><br><span class="line">      <span class="keyword">from</span> submissions <span class="keyword">as</span> s3</span><br><span class="line">      <span class="keyword">where</span> s3.hacker_id = s2.hacker_id <span class="keyword">and</span> s3.submission_date &lt; s1.submission_date) = <span class="keyword">datediff</span>(s1.submission_date, <span class="string">&#x27;2016-03-01&#x27;</span>)),</span><br><span class="line">    <span class="comment">--append</span></span><br><span class="line">    (<span class="keyword">select</span> hacker_id</span><br><span class="line">     <span class="keyword">from</span> submissions <span class="keyword">as</span> s2</span><br><span class="line">     <span class="keyword">where</span> s2.submission_date = s1.submission_date</span><br><span class="line">     <span class="keyword">group</span> <span class="keyword">by</span> hacker_id</span><br><span class="line">     <span class="keyword">order</span> <span class="keyword">by</span> <span class="keyword">count</span>(submission_id) <span class="keyword">desc</span>, hacker_id <span class="keyword">limit</span> <span class="number">1</span>) <span class="keyword">as</span> <span class="keyword">id</span>,</span><br><span class="line">    (<span class="keyword">select</span> <span class="keyword">name</span> <span class="keyword">from</span> hackers <span class="keyword">where</span> hacker_id = <span class="keyword">id</span>)</span><br><span class="line">    <span class="comment">--/append</span></span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">    (<span class="keyword">select</span> <span class="keyword">distinct</span> submission_date <span class="keyword">from</span> submissions) <span class="keyword">as</span> s1</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> submission_date;</span><br></pre></td></tr></table></figure><blockquote><p><em>문제 접근할 때 Join만 바라보지 않고 SELECT / FROM / WHERE 절에서 Sub query를 작성하는 안목을 키워보자. (물론 hackerrank의 문제는 끝났다..)</em></p></blockquote><h1 id="해결"><a href="#해결" class="headerlink" title="해결"></a>해결</h1><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">    submission_date,</span><br><span class="line">    (<span class="keyword">select</span> <span class="keyword">count</span>(<span class="keyword">distinct</span> hacker_id)</span><br><span class="line">     <span class="keyword">from</span> submissions <span class="keyword">as</span> s2</span><br><span class="line">     <span class="keyword">where</span> s2.submission_date = s1.submission_date <span class="keyword">and</span></span><br><span class="line">     (<span class="keyword">select</span> <span class="keyword">count</span>(<span class="keyword">distinct</span> s3.submission_date)</span><br><span class="line">      <span class="keyword">from</span> submissions <span class="keyword">as</span> s3</span><br><span class="line">      <span class="keyword">where</span> s3.hacker_id = s2.hacker_id <span class="keyword">and</span> s3.submission_date &lt; s1.submission_date) = <span class="keyword">datediff</span>(s1.submission_date, <span class="string">&#x27;2016-03-01&#x27;</span>)),</span><br><span class="line">    (<span class="keyword">select</span> hacker_id</span><br><span class="line">     <span class="keyword">from</span> submissions <span class="keyword">as</span> s2</span><br><span class="line">     <span class="keyword">where</span> s2.submission_date = s1.submission_date</span><br><span class="line">     <span class="keyword">group</span> <span class="keyword">by</span> hacker_id</span><br><span class="line">     <span class="keyword">order</span> <span class="keyword">by</span> <span class="keyword">count</span>(submission_id) <span class="keyword">desc</span>, hacker_id <span class="keyword">limit</span> <span class="number">1</span>) <span class="keyword">as</span> <span class="keyword">id</span>,</span><br><span class="line">    (<span class="keyword">select</span> <span class="keyword">name</span> <span class="keyword">from</span> hackers <span class="keyword">where</span> hacker_id = <span class="keyword">id</span>)</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">    (<span class="keyword">select</span> <span class="keyword">distinct</span> submission_date <span class="keyword">from</span> submissions) <span class="keyword">as</span> s1</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> submission_date;</span><br></pre></td></tr></table></figure><hr><p>2019.10.30 made by <em>jaejun.lee</em></p>]]></content:encoded>
      
      <comments>https://jx2lee.github.io/hackerrank-15_days_of_learning/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[Hadoop] Sqoop import 시 JDBC-90401:Connection refused by the server Error 발생</title>
      <link>https://jx2lee.github.io/hadoop-sqoop_jdbc_error/</link>
      <guid>https://jx2lee.github.io/hadoop-sqoop_jdbc_error/</guid>
      <pubDate>Tue, 29 Oct 2019 15:00:00 GMT</pubDate>
      <description>
      
        &lt;h1 id=&quot;상황&quot;&gt;&lt;a href=&quot;#상황&quot; class=&quot;headerlink&quot; title=&quot;상황&quot;&gt;&lt;/a&gt;상황&lt;/h1&gt;&lt;p&gt;Sqoop을 이용해 Tibero Table을 hdfs 형태로 변환하는 import 과정에서 ERROR가 발생하였다. 상황을 간단히 설명하면, 정보시스템 개발기 DB&lt;em&gt;(Tibero)&lt;/em&gt; Tabel을 팀 서버에 구축한 Hadoop 에&lt;em&gt;(51, 52 : DataNode, 53 : NameNode로 이하 숫자로 표현)&lt;/em&gt; 저장하고자 했다.
      
      </description>
      
      
      <content:encoded><![CDATA[<h1 id="상황"><a href="#상황" class="headerlink" title="상황"></a>상황</h1><p>Sqoop을 이용해 Tibero Table을 hdfs 형태로 변환하는 import 과정에서 ERROR가 발생하였다. 상황을 간단히 설명하면, 정보시스템 개발기 DB<em>(Tibero)</em> Tabel을 팀 서버에 구축한 Hadoop 에<em>(51, 52 : DataNode, 53 : NameNode로 이하 숫자로 표현)</em> 저장하고자 했다.<a id="more"></a>정보시스템에서 허용한 IP는 총 4개였고 그 중 하나인 69<em>(편하게 숫자로.. 대체하겠다)</em>에 <code>Sqoop</code>을 설치하여 Hadoop에 저장하려는 계획이었다. 간략히 각 서버와 현황을 나타내면 아래와 같다.</p><ul><li>Hadoop<ul><li>51, 52 : DataNode</li><li>53 : NameNode</li><li><strong>정보시스템 개발기 DB에 접근이 허용되지 않음</strong></li></ul></li><li>Sqoop<ul><li>69</li><li><strong>정보시스템 개발기 DB에 접근이 허용되지 않음</strong></li></ul></li></ul><p><code>Sqoop import</code> 명령어 <em>(sqoop import –connect jdbc:tibero:thin:@192.168.xx.xx:8629:tibero –driver com.tmax.tibero.jdbc.TbDriver –username xxxx –password xxxx –table PROJECT_INFO –delete-target-dir -m 1)</em> 를 날리면 아래와 같은 에러가 발생하였다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">sqoop@bips:/app/sqoop/sqoop$ sqoop import --connect jdbc:tibero:thin:@192.168.10.84:8629:tibero --driver com.tmax.tibero.jdbc.TbDriver --username tody -P --table PROJECT_INFO --delete-target-dir -m 1</span><br><span class="line">Warning: /app/sqoop/sqoop/../hbase does not exist! HBase imports will fail.</span><br><span class="line">Please <span class="built_in">set</span> <span class="variable">$HBASE_HOME</span> to the root of your HBase installation.</span><br><span class="line">Warning: /app/sqoop/sqoop/../hcatalog does not exist! HCatalog <span class="built_in">jobs</span> will fail.</span><br><span class="line">Please <span class="built_in">set</span> <span class="variable">$HCAT_HOME</span> to the root of your HCatalog installation.</span><br><span class="line">Warning: /app/sqoop/sqoop/../accumulo does not exist! Accumulo imports will fail.</span><br><span class="line">Please <span class="built_in">set</span> <span class="variable">$ACCUMULO_HOME</span> to the root of your Accumulo installation.</span><br><span class="line">Warning: /app/sqoop/sqoop/../zookeeper does not exist! Accumulo imports will fail.</span><br><span class="line">Please <span class="built_in">set</span> <span class="variable">$ZOOKEEPER_HOME</span> to the root of your Zookeeper installation.</span><br><span class="line">19/10/30 09:32:49 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7</span><br><span class="line">Enter password: </span><br><span class="line">19/10/30 09:33:01 WARN sqoop.ConnFactory: Parameter --driver is <span class="built_in">set</span> to an explicit driver however appropriate connection manager is not being <span class="built_in">set</span> (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly <span class="built_in">which</span> connection manager should be used next time.</span><br><span class="line">19/10/30 09:33:01 INFO manager.SqlManager: Using default fetchSize of 1000</span><br><span class="line">19/10/30 09:33:01 INFO tool.CodeGenTool: Beginning code generation</span><br><span class="line">19/10/30 09:33:01 ERROR manager.SqlManager: Error executing statement: java.sql.SQLException: JDBC-90401:Connection refused by the server. - Connection refused (Connection refused)</span><br><span class="line">java.sql.SQLException: JDBC-90401:Connection refused by the server. - Connection refused (Connection refused)</span><br><span class="line">at com.tmax.tibero.jdbc.err.TbError.makeSQLException(Unknown Source)</span><br><span class="line">at com.tmax.tibero.jdbc.err.TbError.newSQLException(Unknown Source)</span><br><span class="line">at com.tmax.tibero.jdbc.comm.TbStream.&lt;init&gt;(Unknown Source)</span><br><span class="line">at com.tmax.tibero.jdbc.comm.TbCommType4.createStream(Unknown Source)</span><br><span class="line">at com.tmax.tibero.jdbc.driver.TbConnection.openConnection(Unknown Source)</span><br><span class="line">at com.tmax.tibero.jdbc.TbDriver.connectInternal(Unknown Source)</span><br><span class="line">at com.tmax.tibero.jdbc.TbDriver.connect(Unknown Source)</span><br><span class="line">at java.sql.DriverManager.getConnection(DriverManager.java:664)</span><br><span class="line">at java.sql.DriverManager.getConnection(DriverManager.java:247)</span><br><span class="line">at org.apache.sqoop.manager.SqlManager.makeConnection(SqlManager.java:904)</span><br><span class="line">at org.apache.sqoop.manager.GenericJdbcManager.getConnection(GenericJdbcManager.java:59)</span><br><span class="line">at org.apache.sqoop.manager.SqlManager.execute(SqlManager.java:763)</span><br><span class="line">at org.apache.sqoop.manager.SqlManager.execute(SqlManager.java:786)</span><br><span class="line">at org.apache.sqoop.manager.SqlManager.getColumnInfoForRawQuery(SqlManager.java:289)</span><br><span class="line">at org.apache.sqoop.manager.SqlManager.getColumnTypesForRawQuery(SqlManager.java:260)</span><br><span class="line">at org.apache.sqoop.manager.SqlManager.getColumnTypes(SqlManager.java:246)</span><br><span class="line">at org.apache.sqoop.manager.ConnManager.getColumnTypes(ConnManager.java:327)</span><br><span class="line">at org.apache.sqoop.orm.ClassWriter.getColumnTypes(ClassWriter.java:1872)</span><br><span class="line">at org.apache.sqoop.orm.ClassWriter.generate(ClassWriter.java:1671)</span><br><span class="line">at org.apache.sqoop.tool.CodeGenTool.generateORM(CodeGenTool.java:106)</span><br><span class="line">at org.apache.sqoop.tool.ImportTool.importTable(ImportTool.java:501)</span><br><span class="line">at org.apache.sqoop.tool.ImportTool.run(ImportTool.java:628)</span><br><span class="line">at org.apache.sqoop.Sqoop.run(Sqoop.java:147)</span><br><span class="line">at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)</span><br><span class="line">at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:183)</span><br><span class="line">at org.apache.sqoop.Sqoop.runTool(Sqoop.java:234)</span><br><span class="line">at org.apache.sqoop.Sqoop.runTool(Sqoop.java:243)</span><br><span class="line">at org.apache.sqoop.Sqoop.main(Sqoop.java:252)</span><br><span class="line">19/10/30 09:33:01 ERROR tool.ImportTool: Import failed: java.io.IOException: No columns to generate <span class="keyword">for</span> ClassWriter</span><br><span class="line">at org.apache.sqoop.orm.ClassWriter.generate(ClassWriter.java:1677)</span><br><span class="line">at org.apache.sqoop.tool.CodeGenTool.generateORM(CodeGenTool.java:106)</span><br><span class="line">at org.apache.sqoop.tool.ImportTool.importTable(ImportTool.java:501)</span><br><span class="line">at org.apache.sqoop.tool.ImportTool.run(ImportTool.java:628)</span><br><span class="line">at org.apache.sqoop.Sqoop.run(Sqoop.java:147)</span><br><span class="line">at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)</span><br><span class="line">at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:183)</span><br><span class="line">at org.apache.sqoop.Sqoop.runTool(Sqoop.java:234)</span><br><span class="line">at org.apache.sqoop.Sqoop.runTool(Sqoop.java:243)</span><br><span class="line">at org.apache.sqoop.Sqoop.main(Sqoop.java:252)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="해결"><a href="#해결" class="headerlink" title="해결"></a>해결</h1><p><strong>결론부터 말하면 해결하지 못하는 문제</strong>이다. Hadoop이 JDBC를 통해 정보시스템 개발기 Tibero에 접근해야 하는데 <code>Sqoop</code>이 아래와 같은 구조를 갖는다.</p><p><img src="https://t1.daumcdn.net/cfile/tistory/255AAE415527751E16" alt=""></p><p><a href="https://t1.daumcdn.net/cfile/tistory/255AAE415527751E16">참고 : https://t1.daumcdn.net/cfile/tistory/255AAE415527751E16</a></p><p>사진은 <code>Sqoop 1</code>의 Architecture이지만, 결국에 Sqoop이 Hadoop의 Map Task에게 태스크를 넘기면 Hadoop이 Database에 접근하는 형태이다. 즉, <strong>정보시스템 DB 접근이 허용되지 않는 Hadoop 환경</strong>에서는 Connection을 허용하지 않는다. 때문에 우리팀이 원래 시도하려던 했던 HDFS 형태로 파일을 떨구고 이를 불러오는 과정을 <em>아예 다른 식으로 접근하거나</em>, <em>우회 방안</em> 을 생각해야 한다. </p><h2 id="결론"><a href="#결론" class="headerlink" title="결론"></a>결론</h2><p>Connection ERROR 해결 방안은 <strong>Hadoop이 Database에 접근 가능</strong>하게 환경을 구성해주면 된다. 하지만, 우리팀의 프로젝트는 이렇게 환경을 재구성하는 것이 쉽지 않다. 이에대해 다음 두 가지 방안이 있는데 구체적이지 않다. <del>(어떠한 방법이 더 효율적인지 더 고민해봐야겠다)</del></p><ul><li>If using Hadoop,<ul><li>Hadoop을 정보시스템 DB에 접근할 수 있는 환경에 설치</li><li>정보시스템 쪽에 접근 가능한 IP 추가 요청</li></ul></li><li>else,<ul><li>자사 제품 사용..</li></ul></li></ul><blockquote><p> <em>역량 강화를 위해서는 Hadoop system을 이용하는 것이 좋다는 개인적인 바람이 있다.</em></p></blockquote><hr><p>2019.10.30 made by <em>jaejun.lee</em></p>]]></content:encoded>
      
      <comments>https://jx2lee.github.io/hadoop-sqoop_jdbc_error/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[Hadoop] Sqoop import/export with Tibero</title>
      <link>https://jx2lee.github.io/hadoop-sqoop_with_tibero/</link>
      <guid>https://jx2lee.github.io/hadoop-sqoop_with_tibero/</guid>
      <pubDate>Tue, 29 Oct 2019 15:00:00 GMT</pubDate>
      <description>
      
        &lt;p&gt;&lt;code&gt;Sqoop&lt;/code&gt;을 이용해 Tibero table을 HDFS로 저장하고 이를 다시 table로 변환하는 테스트를 진행한다.&lt;/p&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p><code>Sqoop</code>을 이용해 Tibero table을 HDFS로 저장하고 이를 다시 table로 변환하는 테스트를 진행한다.</p><a id="more"></a><h1 id="Create-Test-table"><a href="#Create-Test-table" class="headerlink" title="Create Test table"></a>Create Test table</h1><p>우선, <code>Import</code>하려는 table을 생성한다. 중요한 것은 <strong>export하기 위한 table도 생성</strong>해야 한다는 점이다.</p><ul><li>Import table : RECIPES</li><li>Export table : RECIPES_EXP</li></ul><h2 id="Import-table"><a href="#Import-table" class="headerlink" title="Import table"></a>Import table</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="keyword">USERS</span>(</span><br><span class="line">USERNO <span class="built_in">NUMBER</span>,</span><br><span class="line">EMAIL <span class="built_in">VARCHAR2</span>(<span class="number">255</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">PWD <span class="built_in">VARCHAR2</span>(<span class="number">100</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="keyword">NAME</span> <span class="built_in">VARCHAR2</span>(<span class="number">100</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">PNO <span class="built_in">VARCHAR2</span>(<span class="number">100</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">ADDRESS <span class="built_in">VARCHAR2</span>(<span class="number">255</span>)</span><br><span class="line">);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> recipes (recipe_id, recipe_name) <span class="keyword">VALUES</span> (<span class="number">1</span>,<span class="string">&#x27;Tacos&#x27;</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> recipes (recipe_id, recipe_name) <span class="keyword">VALUES</span> (<span class="number">2</span>,<span class="string">&#x27;Tomato Soup&#x27;</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> recipes (recipe_id, recipe_name) <span class="keyword">VALUES</span> (<span class="number">3</span>,<span class="string">&#x27;Grilled Cheese&#x27;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- checking table</span></span><br><span class="line">SQL&gt; SELECT * FROM RECIPES;</span><br><span class="line"> RECIPE_ID RECIPE_NAME                   </span><br><span class="line"><span class="comment">---------- ------------------------------</span></span><br><span class="line">         3 Grilled Cheese</span><br><span class="line">         1 Tacos</span><br><span class="line">         2 Tomato Soup</span><br><span class="line">3 rows selected.</span><br></pre></td></tr></table></figure><h2 id="Export-table"><a href="#Export-table" class="headerlink" title="Export table"></a>Export table</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> recipes_exp (</span><br><span class="line">  recipe_id <span class="built_in">INT</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">  recipe_name <span class="built_in">VARCHAR</span>(<span class="number">30</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">  PRIMARY <span class="keyword">KEY</span> (recipe_id),</span><br><span class="line">  <span class="keyword">UNIQUE</span> (recipe_name)</span><br><span class="line">);</span><br></pre></td></tr></table></figure><h1 id="Sqoop-Import"><a href="#Sqoop-Import" class="headerlink" title="Sqoop Import"></a>Sqoop Import</h1><p>table이 준비되었다면 <code>Sqoop</code>을 이용해 HDFS 형태로 Hadoop에 저장해본다. 명령어와 수행결과는 아래와 같다.</p><h2 id="CMD"><a href="#CMD" class="headerlink" title="CMD"></a>CMD</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sqoop import --connect jdbc:tibero:thin:@192.168.xxx.xx:xxxx:tibero \</span><br><span class="line">--driver com.tmax.tibero.jdbc.TbDriver \</span><br><span class="line">--username tibero --password [password] \</span><br><span class="line">--table RECIPES \</span><br><span class="line">--target-dir /t1/input</span><br></pre></td></tr></table></figure><blockquote><p><em>Tibero 접속을 위한 string은 위와 같이 작성하고, MySQL/PostgreSQL의 경우 driver를 지정하지 않아도 되지만 Oracle/Tibero는 driver를 설정해야 한다. (MySQL/PostgreSQL : direct connect 지원이라고 document에 명시)</em></p></blockquote><h2 id="Result"><a href="#Result" class="headerlink" title="Result"></a>Result</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line">sqoop@bips:~$ sqoop import --connect jdbc:tibero:thin:@192.168.xxx.xx:xxxx:tibero \</span><br><span class="line">&gt; --driver com.tmax.tibero.jdbc.TbDriver \</span><br><span class="line">&gt; --username tibero --password tmax \</span><br><span class="line">&gt; --table RECIPES \</span><br><span class="line">&gt; --target-dir /t1/input</span><br><span class="line">Warning: /app/sqoop/sqoop/../hbase does not exist! HBase imports will fail.</span><br><span class="line">Please <span class="built_in">set</span> <span class="variable">$HBASE_HOME</span> to the root of your HBase installation.</span><br><span class="line">Warning: /app/sqoop/sqoop/../hcatalog does not exist! HCatalog <span class="built_in">jobs</span> will fail.</span><br><span class="line">Please <span class="built_in">set</span> <span class="variable">$HCAT_HOME</span> to the root of your HCatalog installation.</span><br><span class="line">Warning: /app/sqoop/sqoop/../accumulo does not exist! Accumulo imports will fail.</span><br><span class="line">Please <span class="built_in">set</span> <span class="variable">$ACCUMULO_HOME</span> to the root of your Accumulo installation.</span><br><span class="line">Warning: /app/sqoop/sqoop/../zookeeper does not exist! Accumulo imports will fail.</span><br><span class="line">Please <span class="built_in">set</span> <span class="variable">$ZOOKEEPER_HOME</span> to the root of your Zookeeper installation.</span><br><span class="line">19/10/30 10:09:24 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7</span><br><span class="line">19/10/30 10:09:24 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.</span><br><span class="line">19/10/30 10:09:24 WARN sqoop.ConnFactory: Parameter --driver is <span class="built_in">set</span> to an explicit driver however appropriate connection manager is not being <span class="built_in">set</span> (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly <span class="built_in">which</span> connection manager should be used next time.</span><br><span class="line">19/10/30 10:09:24 INFO manager.SqlManager: Using default fetchSize of 1000</span><br><span class="line">19/10/30 10:09:24 INFO tool.CodeGenTool: Beginning code generation</span><br><span class="line">19/10/30 10:09:25 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM RECIPES AS t WHERE 1=0</span><br><span class="line">19/10/30 10:09:25 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM RECIPES AS t WHERE 1=0</span><br><span class="line">19/10/30 10:09:25 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /app/hadoop</span><br><span class="line">Note: /tmp/sqoop-sqoop/compile/905c294a54718643bcf983498f8878ba/RECIPES.java uses or overrides a deprecated API.</span><br><span class="line">Note: Recompile with -Xlint:deprecation <span class="keyword">for</span> details.</span><br><span class="line">19/10/30 10:09:26 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-sqoop/compile/905c294a54718643bcf983498f8878ba/RECIPES.jar</span><br><span class="line">19/10/30 10:09:26 INFO mapreduce.ImportJobBase: Beginning import of RECIPES</span><br><span class="line">19/10/30 10:09:26 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar</span><br><span class="line">19/10/30 10:09:26 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM RECIPES AS t WHERE 1=0</span><br><span class="line">19/10/30 10:09:27 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps</span><br><span class="line">19/10/30 10:09:27 INFO client.RMProxy: Connecting to ResourceManager at node5.dat/192.168.158.53:8050</span><br><span class="line">19/10/30 10:09:31 INFO db.DBInputFormat: Using <span class="built_in">read</span> commited transaction isolation</span><br><span class="line">19/10/30 10:09:31 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(RECIPE_ID), MAX(RECIPE_ID) FROM RECIPES</span><br><span class="line">19/10/30 10:09:31 INFO mapreduce.JobSubmitter: number of splits:4</span><br><span class="line">19/10/30 10:09:31 INFO Configuration.deprecation: yarn.resourcemanager.system-metrics-publisher.enabled is deprecated. Instead, use yarn.system-metrics-publisher.enabled</span><br><span class="line">19/10/30 10:09:31 INFO mapreduce.JobSubmitter: Submitting tokens <span class="keyword">for</span> job: job_1567153359966_0069</span><br><span class="line">19/10/30 10:09:31 INFO impl.YarnClientImpl: Submitted application application_1567153359966_0069</span><br><span class="line">19/10/30 10:09:31 INFO mapreduce.Job: The url to track the job: http://node5.dat:8088/proxy/application_1567153359966_0069/</span><br><span class="line">19/10/30 10:09:31 INFO mapreduce.Job: Running job: job_1567153359966_0069</span><br><span class="line">19/10/30 10:09:35 INFO mapreduce.Job: Job job_1567153359966_0069 running <span class="keyword">in</span> uber mode : <span class="literal">false</span></span><br><span class="line">19/10/30 10:09:35 INFO mapreduce.Job:  map 0% reduce 0%</span><br><span class="line">19/10/30 10:09:39 INFO mapreduce.Job:  map 50% reduce 0%</span><br><span class="line">19/10/30 10:09:40 INFO mapreduce.Job:  map 100% reduce 0%</span><br><span class="line">19/10/30 10:09:41 INFO mapreduce.Job: Job job_1567153359966_0069 completed successfully</span><br><span class="line">19/10/30 10:09:41 INFO mapreduce.Job: Counters: 30</span><br><span class="line">File System Counters</span><br><span class="line">FILE: Number of bytes <span class="built_in">read</span>=0</span><br><span class="line">FILE: Number of bytes written=830592</span><br><span class="line">FILE: Number of <span class="built_in">read</span> operations=0</span><br><span class="line">FILE: Number of large <span class="built_in">read</span> operations=0</span><br><span class="line">FILE: Number of write operations=0</span><br><span class="line">HDFS: Number of bytes <span class="built_in">read</span>=447</span><br><span class="line">HDFS: Number of bytes written=39</span><br><span class="line">HDFS: Number of <span class="built_in">read</span> operations=16</span><br><span class="line">HDFS: Number of large <span class="built_in">read</span> operations=0</span><br><span class="line">HDFS: Number of write operations=8</span><br><span class="line">Job Counters </span><br><span class="line">Launched map tasks=4</span><br><span class="line">Other <span class="built_in">local</span> map tasks=4</span><br><span class="line">Total time spent by all maps <span class="keyword">in</span> occupied slots (ms)=8334</span><br><span class="line">Total time spent by all reduces <span class="keyword">in</span> occupied slots (ms)=0</span><br><span class="line">Total time spent by all map tasks (ms)=8334</span><br><span class="line">Total vcore-milliseconds taken by all map tasks=8334</span><br><span class="line">Total megabyte-milliseconds taken by all map tasks=8534016</span><br><span class="line">Map-Reduce Framework</span><br><span class="line">Map input records=3</span><br><span class="line">Map output records=3</span><br><span class="line">Input split bytes=447</span><br><span class="line">Spilled Records=0</span><br><span class="line">Failed Shuffles=0</span><br><span class="line">Merged Map outputs=0</span><br><span class="line">GC time elapsed (ms)=162</span><br><span class="line">CPU time spent (ms)=3530</span><br><span class="line">Physical memory (bytes) snapshot=827170816</span><br><span class="line">Virtual memory (bytes) snapshot=8600104960</span><br><span class="line">Total committed heap usage (bytes)=585629696</span><br><span class="line">File Input Format Counters </span><br><span class="line">Bytes Read=0</span><br><span class="line">File Output Format Counters </span><br><span class="line">Bytes Written=39</span><br><span class="line">19/10/30 10:09:41 INFO mapreduce.ImportJobBase: Transferred 39 bytes <span class="keyword">in</span> 14.9856 seconds (2.6025 bytes/sec)</span><br><span class="line">19/10/30 10:09:41 INFO mapreduce.ImportJobBase: Retrieved 3 records.</span><br></pre></td></tr></table></figure><h2 id="Checking"><a href="#Checking" class="headerlink" title="Checking"></a>Checking</h2><p>제대로 hadoop에 저장되어있는지 확인해본다. <code>hdfs</code>명령어를 이용해 파일이 정상적으로 저장되었는지 확인한다.</p><p><code>hdfs dfs -cat /t1/input/*</code> or <code>hdfs dfs -ls /t1/input</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">sqoop@bips:~$ hdfs dfs -cat /t1/input/*</span><br><span class="line">1,Tacos</span><br><span class="line">2,Tomato Soup</span><br><span class="line">3,Grilled Cheese</span><br><span class="line">sqoop@bips:~$ hdfs dfs -ls /t1/input</span><br><span class="line">Found 5 items</span><br><span class="line">-rw-r--r--   2 sqoop supergroup          0 2019-10-30 10:09 /t1/input/_SUCCESS</span><br><span class="line">-rw-r--r--   2 sqoop supergroup          8 2019-10-30 10:09 /t1/input/part-m-00000</span><br><span class="line">-rw-r--r--   2 sqoop supergroup          0 2019-10-30 10:09 /t1/input/part-m-00001</span><br><span class="line">-rw-r--r--   2 sqoop supergroup         14 2019-10-30 10:09 /t1/input/part-m-00002</span><br><span class="line">-rw-r--r--   2 sqoop supergroup         17 2019-10-30 10:09 /t1/input/part-m-00003</span><br></pre></td></tr></table></figure><h1 id="Sqoop-Export"><a href="#Sqoop-Export" class="headerlink" title="Sqoop Export"></a>Sqoop Export</h1><p>이번엔 HDFS를 Tibero table로 다시 변환하는 작업을 진행한다. 명령어와 수행결과는 다음과 같다.</p><h2 id="CMD-1"><a href="#CMD-1" class="headerlink" title="CMD"></a>CMD</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sqoop <span class="built_in">export</span> --connect jdbc:tibero:thin:@192.168.xxx.xx:xxx:tibero \</span><br><span class="line">--driver com.tmax.tibero.jdbc.TbDriver \</span><br><span class="line">--username tibero --password ?? \</span><br><span class="line">--table RECIPES_EXP \</span><br><span class="line">--export-dir /t1/input</span><br></pre></td></tr></table></figure><blockquote><p><em>위에서도 언급했듯이 export table이 Tibero에 이미 존재해야 한다.</em></p></blockquote><h2 id="Result-1"><a href="#Result-1" class="headerlink" title="Result"></a>Result</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line">sqoop@bips:~$ sqoop <span class="built_in">export</span> --connect jdbc:tibero:thin:@192.168.158.53:8729:tibero \</span><br><span class="line">&gt; --driver com.tmax.tibero.jdbc.TbDriver \</span><br><span class="line">&gt; --username tibero --password tmax \</span><br><span class="line">&gt; --table RECIPES_EXP \</span><br><span class="line">&gt; --export-dir /t1/input</span><br><span class="line">Warning: /app/sqoop/sqoop/../hbase does not exist! HBase imports will fail.</span><br><span class="line">Please <span class="built_in">set</span> <span class="variable">$HBASE_HOME</span> to the root of your HBase installation.</span><br><span class="line">Warning: /app/sqoop/sqoop/../hcatalog does not exist! HCatalog <span class="built_in">jobs</span> will fail.</span><br><span class="line">Please <span class="built_in">set</span> <span class="variable">$HCAT_HOME</span> to the root of your HCatalog installation.</span><br><span class="line">Warning: /app/sqoop/sqoop/../accumulo does not exist! Accumulo imports will fail.</span><br><span class="line">Please <span class="built_in">set</span> <span class="variable">$ACCUMULO_HOME</span> to the root of your Accumulo installation.</span><br><span class="line">Warning: /app/sqoop/sqoop/../zookeeper does not exist! Accumulo imports will fail.</span><br><span class="line">Please <span class="built_in">set</span> <span class="variable">$ZOOKEEPER_HOME</span> to the root of your Zookeeper installation.</span><br><span class="line">19/10/30 10:40:23 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7</span><br><span class="line">19/10/30 10:40:23 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.</span><br><span class="line">19/10/30 10:40:23 WARN sqoop.ConnFactory: Parameter --driver is <span class="built_in">set</span> to an explicit driver however appropriate connection manager is not being <span class="built_in">set</span> (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly <span class="built_in">which</span> connection manager should be used next time.</span><br><span class="line">19/10/30 10:40:23 INFO manager.SqlManager: Using default fetchSize of 1000</span><br><span class="line">19/10/30 10:40:23 INFO tool.CodeGenTool: Beginning code generation</span><br><span class="line">19/10/30 10:40:24 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM RECIPES_EXP AS t WHERE 1=0</span><br><span class="line">19/10/30 10:40:24 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM RECIPES_EXP AS t WHERE 1=0</span><br><span class="line">19/10/30 10:40:24 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /app/hadoop</span><br><span class="line">Note: /tmp/sqoop-sqoop/compile/bc292d345cc3a16972516454f904b6df/RECIPES_EXP.java uses or overrides a deprecated API.</span><br><span class="line">Note: Recompile with -Xlint:deprecation <span class="keyword">for</span> details.</span><br><span class="line">19/10/30 10:40:25 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-sqoop/compile/bc292d345cc3a16972516454f904b6df/RECIPES_EXP.jar</span><br><span class="line">19/10/30 10:40:25 INFO mapreduce.ExportJobBase: Beginning <span class="built_in">export</span> of RECIPES_EXP</span><br><span class="line">19/10/30 10:40:25 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar</span><br><span class="line">19/10/30 10:40:25 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM RECIPES_EXP AS t WHERE 1=0</span><br><span class="line">19/10/30 10:40:25 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative</span><br><span class="line">19/10/30 10:40:25 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative</span><br><span class="line">19/10/30 10:40:25 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps</span><br><span class="line">19/10/30 10:40:25 INFO client.RMProxy: Connecting to ResourceManager at node5.dat/192.168.158.53:8050</span><br><span class="line">19/10/30 10:40:29 INFO input.FileInputFormat: Total input files to process : 4</span><br><span class="line">19/10/30 10:40:29 INFO input.FileInputFormat: Total input files to process : 4</span><br><span class="line">19/10/30 10:40:29 INFO mapreduce.JobSubmitter: number of splits:4</span><br><span class="line">19/10/30 10:40:29 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative</span><br><span class="line">19/10/30 10:40:29 INFO Configuration.deprecation: yarn.resourcemanager.system-metrics-publisher.enabled is deprecated. Instead, use yarn.system-metrics-publisher.enabled</span><br><span class="line">19/10/30 10:40:29 INFO mapreduce.JobSubmitter: Submitting tokens <span class="keyword">for</span> job: job_1567153359966_0071</span><br><span class="line">19/10/30 10:40:30 INFO impl.YarnClientImpl: Submitted application application_1567153359966_0071</span><br><span class="line">19/10/30 10:40:30 INFO mapreduce.Job: The url to track the job: http://node5.dat:8088/proxy/application_1567153359966_0071/</span><br><span class="line">19/10/30 10:40:30 INFO mapreduce.Job: Running job: job_1567153359966_0071</span><br><span class="line">19/10/30 10:40:35 INFO mapreduce.Job: Job job_1567153359966_0071 running <span class="keyword">in</span> uber mode : <span class="literal">false</span></span><br><span class="line">19/10/30 10:40:35 INFO mapreduce.Job:  map 0% reduce 0%</span><br><span class="line">19/10/30 10:40:40 INFO mapreduce.Job:  map 75% reduce 0%</span><br><span class="line">19/10/30 10:40:41 INFO mapreduce.Job:  map 100% reduce 0%</span><br><span class="line">19/10/30 10:40:41 INFO mapreduce.Job: Job job_1567153359966_0071 completed successfully</span><br><span class="line">19/10/30 10:40:41 INFO mapreduce.Job: Counters: 31</span><br><span class="line">File System Counters</span><br><span class="line">FILE: Number of bytes <span class="built_in">read</span>=0</span><br><span class="line">FILE: Number of bytes written=829388</span><br><span class="line">FILE: Number of <span class="built_in">read</span> operations=0</span><br><span class="line">FILE: Number of large <span class="built_in">read</span> operations=0</span><br><span class="line">FILE: Number of write operations=0</span><br><span class="line">HDFS: Number of bytes <span class="built_in">read</span>=656</span><br><span class="line">HDFS: Number of bytes written=0</span><br><span class="line">HDFS: Number of <span class="built_in">read</span> operations=22</span><br><span class="line">HDFS: Number of large <span class="built_in">read</span> operations=0</span><br><span class="line">HDFS: Number of write operations=0</span><br><span class="line">Job Counters </span><br><span class="line">Launched map tasks=4</span><br><span class="line">Other <span class="built_in">local</span> map tasks=1</span><br><span class="line">Data-local map tasks=3</span><br><span class="line">Total time spent by all maps <span class="keyword">in</span> occupied slots (ms)=14227</span><br><span class="line">Total time spent by all reduces <span class="keyword">in</span> occupied slots (ms)=0</span><br><span class="line">Total time spent by all map tasks (ms)=14227</span><br><span class="line">Total vcore-milliseconds taken by all map tasks=14227</span><br><span class="line">Total megabyte-milliseconds taken by all map tasks=14568448</span><br><span class="line">Map-Reduce Framework</span><br><span class="line">Map input records=3</span><br><span class="line">Map output records=3</span><br><span class="line">Input split bytes=586</span><br><span class="line">Spilled Records=0</span><br><span class="line">Failed Shuffles=0</span><br><span class="line">Merged Map outputs=0</span><br><span class="line">GC time elapsed (ms)=425</span><br><span class="line">CPU time spent (ms)=4140</span><br><span class="line">Physical memory (bytes) snapshot=811102208</span><br><span class="line">Virtual memory (bytes) snapshot=8589045760</span><br><span class="line">Total committed heap usage (bytes)=606601216</span><br><span class="line">File Input Format Counters </span><br><span class="line">Bytes Read=0</span><br><span class="line">File Output Format Counters </span><br><span class="line">Bytes Written=0</span><br><span class="line">19/10/30 10:40:41 INFO mapreduce.ExportJobBase: Transferred 656 bytes <span class="keyword">in</span> 15.7366 seconds (41.6862 bytes/sec)</span><br><span class="line">19/10/30 10:40:41 INFO mapreduce.ExportJobBase: Exported 3 records.</span><br><span class="line">​```</span><br></pre></td></tr></table></figure><h2 id="Checking-1"><a href="#Checking-1" class="headerlink" title="Checking"></a>Checking</h2><p>제대로 테이블로 데이터가 들어갔는지 확인해본다. <code>Sqoop</code>에서 <code>--query</code> argument를 주어 <code>RECIPES_EXP</code>를 조회해보자</p><p><code>sqoop eval --connect jdbc:tibero:thin:@192.168.xxx.xx:xxxx:tibero --driver com.tmax.tibero.jdbc.TbDriver --username tibero --password ?? --query &#39;SELECT * FROM RECIPES_EXP&#39;</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">sqoop@bips:~$ sqoop <span class="built_in">eval</span> --connect jdbc:tibero:thin:@192.168.158.53:8729:tibero \</span><br><span class="line">&gt; --driver com.tmax.tibero.jdbc.TbDriver \</span><br><span class="line">&gt; --username tibero --password tmax --query <span class="string">&#x27;SELECT * FROM RECIPES_EXP&#x27;</span></span><br><span class="line">Warning: /app/sqoop/sqoop/../hbase does not exist! HBase imports will fail.</span><br><span class="line">Please <span class="built_in">set</span> <span class="variable">$HBASE_HOME</span> to the root of your HBase installation.</span><br><span class="line">Warning: /app/sqoop/sqoop/../hcatalog does not exist! HCatalog <span class="built_in">jobs</span> will fail.</span><br><span class="line">Please <span class="built_in">set</span> <span class="variable">$HCAT_HOME</span> to the root of your HCatalog installation.</span><br><span class="line">Warning: /app/sqoop/sqoop/../accumulo does not exist! Accumulo imports will fail.</span><br><span class="line">Please <span class="built_in">set</span> <span class="variable">$ACCUMULO_HOME</span> to the root of your Accumulo installation.</span><br><span class="line">Warning: /app/sqoop/sqoop/../zookeeper does not exist! Accumulo imports will fail.</span><br><span class="line">Please <span class="built_in">set</span> <span class="variable">$ZOOKEEPER_HOME</span> to the root of your Zookeeper installation.</span><br><span class="line">19/10/30 10:41:54 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7</span><br><span class="line">19/10/30 10:41:54 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.</span><br><span class="line">19/10/30 10:41:54 WARN sqoop.ConnFactory: Parameter --driver is <span class="built_in">set</span> to an explicit driver however appropriate connection manager is not being <span class="built_in">set</span> (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly <span class="built_in">which</span> connection manager should be used next time.</span><br><span class="line">19/10/30 10:41:54 INFO manager.SqlManager: Using default fetchSize of 1000</span><br><span class="line">-----------------------------------------------</span><br><span class="line">| RECIPE_ID            | RECIPE_NAME          | </span><br><span class="line">-----------------------------------------------</span><br><span class="line">| 3                    | Grilled Cheese       | </span><br><span class="line">| 1                    | Tacos                | </span><br><span class="line">| 2                    | Tomato Soup          | </span><br><span class="line">-----------------------------------------------</span><br></pre></td></tr></table></figure><p><strong>export 가 정상 작동됐음을 확인할 수 있다</strong></p><h1 id="참고"><a href="#참고" class="headerlink" title="참고"></a>참고</h1><ol><li><a href="https://dlwjdcks5343.tistory.com/116">플밍장군님 블로그 :  https://dlwjdcks5343.tistory.com/116 </a></li><li><a href="https://blog.voidmainvoid.net/175">AndersonChoi 님 블로그 :  https://blog.voidmainvoid.net/175 </a></li><li><a href="https://sqoop.apache.org/docs/1.4.2/SqoopUserGuide.html#_free_form_query_imports">Sqoop Documents :  https://sqoop.apache.org/docs/1.4.2/SqoopUserGuide.html#_free_form_query_imports</a></li></ol><hr><p>2019.10.30 made by <em>jaejun.lee</em></p>]]></content:encoded>
      
      <comments>https://jx2lee.github.io/hadoop-sqoop_with_tibero/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[SQL] Interviews</title>
      <link>https://jx2lee.github.io/hackerrank-interviews/</link>
      <guid>https://jx2lee.github.io/hackerrank-interviews/</guid>
      <pubDate>Sun, 27 Oct 2019 15:00:00 GMT</pubDate>
      <description>
      
        &lt;p&gt;&lt;code&gt;hackerrank&lt;/code&gt;에서 제공하는 &lt;code&gt;Interviews&lt;/code&gt; 문제를 multiple join 및 group by를 활용해 해결하였다.&lt;/p&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p><code>hackerrank</code>에서 제공하는 <code>Interviews</code> 문제를 multiple join 및 group by를 활용해 해결하였다.</p><a id="more"></a><h1 id="문제"><a href="#문제" class="headerlink" title="문제"></a>문제</h1><p>Samantha interviews many candidates from different colleges using coding challenges and contests. Write a query to print the <em>contest_id</em>, <em>hacker_id</em>, <em>name</em>, and the sums of <em>total_submissions</em>, <em>total_accepted_submissions</em>, <em>total_views</em>, and <em>total_unique_views</em> for each contest sorted by <em>contest_id</em>. Exclude the contest from the result if all four sums are .</p><p><strong>Note:</strong> A specific contest can be used to screen candidates at more than one college, but each college only holds  screening contest.</p><hr><p><strong>Input Format</strong></p><p>The following tables hold interview data:</p><ul><li><em>Contests:</em> The <em>contest_id</em> is the id of the contest, <em>hacker_id</em> is the id of the hacker who created the contest, and <em>name</em> is the name of the hacker. <img src="https://s3.amazonaws.com/hr-challenge-images/19596/1458517426-e017c3460e-ScreenShot2016-03-21at4.57.47AM.png" alt="img"></li><li><em>Colleges:</em> The <em>college_id</em> is the id of the college, and <em>contest_id</em> is the id of the contest that Samantha used to screen the candidates. <img src="https://s3.amazonaws.com/hr-challenge-images/19596/1458517503-fd4aa63111-ScreenShot2016-03-21at4.57.56AM.png" alt="img"></li><li><em>Challenges:</em> The <em>challenge_id</em> is the id of the challenge that belongs to one of the contests whose contest_id Samantha forgot, and <em>college_id</em> is the id of the college where the challenge was given to candidates. <img src="https://s3.amazonaws.com/hr-challenge-images/19596/1458517661-a642f750ce-ScreenShot2016-03-21at4.58.04AM.png" alt="img"></li><li><em>View_Stats:</em> The <em>challenge_id</em> is the id of the challenge, <em>total_views</em> is the number of times the challenge was viewed by candidates, and <em>total_unique_views</em> is the number of times the challenge was viewed by unique candidates. <img src="https://s3.amazonaws.com/hr-challenge-images/19596/1458517983-b4302286a8-ScreenShot2016-03-21at4.58.15AM.png" alt="img"></li><li><em>Submission_Stats:</em> The <em>challenge_id</em> is the id of the challenge, <em>total_submissions</em> is the number of submissions for the challenge, and <em>total_accepted_submission</em> is the number of submissions that achieved full scores. <img src="https://s3.amazonaws.com/hr-challenge-images/19596/1458518090-80983c916a-ScreenShot2016-03-21at4.58.27AM.png" alt="img"></li></ul><hr><p><strong>Sample Input</strong></p><p><em>Contests</em> Table: <img src="https://s3.amazonaws.com/hr-challenge-images/19596/1458519044-d788f8a6ee-ScreenShot2016-03-21at4.58.39AM.png" alt="img"> <em>Colleges</em> Table: <img src="https://s3.amazonaws.com/hr-challenge-images/19596/1458519098-912836d6ac-ScreenShot2016-03-21at4.59.22AM.png" alt="img"> <em>Challenges*Table: <img src="https://s3.amazonaws.com/hr-challenge-images/19596/1458519120-c531743caf-ScreenShot2016-03-21at4.59.32AM.png" alt="img"> *View_Stats</em> Table: <img src="https://s3.amazonaws.com/hr-challenge-images/19596/1458519152-107a67866b-ScreenShot2016-03-21at4.59.43AM.png" alt="img"><em>Submission_Stats</em> Table: <img src="https://s3.amazonaws.com/hr-challenge-images/19596/1458519173-091aba871a-ScreenShot2016-03-21at4.59.55AM.png" alt="img"></p><p><strong>Sample Output</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">66406 17973 Rose 111 39 156 56</span><br><span class="line">66556 79153 Angela 0 0 11 10</span><br><span class="line">94828 80275 Frank 150 38 41 15</span><br></pre></td></tr></table></figure><p><strong>Explanation</strong></p><p>The contest  is used in the college . In this college , challenges  and  are asked, so from the <em>view</em> and <em>submission</em> stats:</p><ul><li>Sum of total submissions </li><li>Sum of total accepted submissions </li><li>Sum of total views </li><li>Sum of total unique views </li></ul><p>Similarly, we can find the sums for contests  and .</p><h1 id="접근"><a href="#접근" class="headerlink" title="접근"></a>접근</h1><p>Join 유형 중 <code>Left Join</code>을 활용하여 해결하였다. 다수의 table을 특정 키를 기준으로 <code>Join</code>하는 것이 다소 헷갈릴 수 있지만 차근차근 <code>Join</code>하면 문제를 쉽게 해결할 수 있다.</p><p>우선, <code>contests</code> table을 기준으로 <code>colleges</code>, <code>challenges</code> table과 <code>Left Join</code>을 수행한다. 각 key는 <em>contest_id 와 college_id</em> 이다.</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> a.contest_id, a.hacker_id, a.name,</span><br><span class="line"><span class="keyword">from</span> contests <span class="keyword">as</span> a</span><br><span class="line"><span class="keyword">left</span> <span class="keyword">join</span> colleges <span class="keyword">as</span> b <span class="keyword">on</span> a.contest_id = b.contest_id</span><br><span class="line"><span class="keyword">left</span> <span class="keyword">join</span> challenges <span class="keyword">as</span> c <span class="keyword">on</span> b.college_id = c.college_id;</span><br></pre></td></tr></table></figure><p>다음 <em>total_views<em>와 *total_unique_views</em>를 구하기 위해 <code>view_stats</code> table을 *challenge_id</em> 기준으로 group by 한다. 이후 결과 테이블과 <code>Left Join</code>을 수행한다. 단, key는 <em>challenge_id</em>이다.</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> a.contest_id, a.hacker_id, a.name,</span><br><span class="line">    <span class="keyword">sum</span>(total_views) <span class="keyword">as</span> total_views,</span><br><span class="line">    <span class="keyword">sum</span>(total_unique_views) <span class="keyword">as</span> total_unique_views</span><br><span class="line"><span class="keyword">from</span> contests <span class="keyword">as</span> a</span><br><span class="line"><span class="keyword">left</span> <span class="keyword">join</span> colleges <span class="keyword">as</span> b <span class="keyword">on</span> a.contest_id = b.contest_id</span><br><span class="line"><span class="keyword">left</span> <span class="keyword">join</span> challenges <span class="keyword">as</span> c <span class="keyword">on</span> b.college_id = c.college_id</span><br><span class="line"><span class="keyword">left</span> <span class="keyword">join</span> ( <span class="keyword">select</span> challenge_id, <span class="keyword">sum</span>(total_views) <span class="keyword">as</span> total_views, <span class="keyword">sum</span>(total_unique_views) <span class="keyword">as</span> total_unique_views</span><br><span class="line">            <span class="keyword">from</span> view_stats</span><br><span class="line">            <span class="keyword">group</span> <span class="keyword">by</span> challenge_id ) <span class="keyword">as</span> d <span class="keyword">on</span> c.challenge_id = d.challenge_id;</span><br></pre></td></tr></table></figure><p><code>view_stats</code> table Join과 같은 방법으로 <code>submission_stats</code> table을 정제한 후 <code>Left Join</code>을 수행한다.</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> a.contest_id, a.hacker_id, a.name,</span><br><span class="line">    <span class="keyword">sum</span>(total_submissions) <span class="keyword">as</span> total_submissions,</span><br><span class="line">    <span class="keyword">sum</span>(total_accepted_submissions) <span class="keyword">as</span> total_accepted_submissions,</span><br><span class="line">    <span class="keyword">sum</span>(total_views) <span class="keyword">as</span> total_views,</span><br><span class="line">    <span class="keyword">sum</span>(total_unique_views) <span class="keyword">as</span> total_unique_views</span><br><span class="line"><span class="keyword">from</span> contests <span class="keyword">as</span> a</span><br><span class="line"><span class="keyword">left</span> <span class="keyword">join</span> colleges <span class="keyword">as</span> b <span class="keyword">on</span> a.contest_id = b.contest_id</span><br><span class="line"><span class="keyword">left</span> <span class="keyword">join</span> challenges <span class="keyword">as</span> c <span class="keyword">on</span> b.college_id = c.college_id</span><br><span class="line"><span class="keyword">left</span> <span class="keyword">join</span> ( <span class="keyword">select</span> challenge_id, <span class="keyword">sum</span>(total_views) <span class="keyword">as</span> total_views, <span class="keyword">sum</span>(total_unique_views) <span class="keyword">as</span> total_unique_views</span><br><span class="line">            <span class="keyword">from</span> view_stats</span><br><span class="line">            <span class="keyword">group</span> <span class="keyword">by</span> challenge_id ) <span class="keyword">as</span> d <span class="keyword">on</span> c.challenge_id = d.challenge_id</span><br><span class="line"><span class="keyword">left</span> <span class="keyword">join</span> ( <span class="keyword">select</span> challenge_id, <span class="keyword">sum</span>(total_submissions) <span class="keyword">as</span> total_submissions, <span class="keyword">sum</span>(total_accepted_submissions) <span class="keyword">as</span> total_accepted_submissions</span><br><span class="line">            <span class="keyword">from</span> submission_stats</span><br><span class="line">            <span class="keyword">group</span> <span class="keyword">by</span> challenge_id ) <span class="keyword">as</span> e <span class="keyword">on</span> c.challenge_id = e.challenge_id;</span><br></pre></td></tr></table></figure><p>마지막으로 <em>contest_id, hacker_id, name</em>을 기준으로 group by를 수행하고, 문제의 조건인 네 가지 summation이 0보다 큰 경우만 조회하는  <code>having</code>을 추가하여 완성한다.</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> a.contest_id, a.hacker_id, a.name,</span><br><span class="line">    <span class="keyword">sum</span>(total_submissions) <span class="keyword">as</span> total_submissions,</span><br><span class="line">    <span class="keyword">sum</span>(total_accepted_submissions) <span class="keyword">as</span> total_accepted_submissions,</span><br><span class="line">    <span class="keyword">sum</span>(total_views) <span class="keyword">as</span> total_views,</span><br><span class="line">    <span class="keyword">sum</span>(total_unique_views) <span class="keyword">as</span> total_unique_views</span><br><span class="line"><span class="keyword">from</span> contests <span class="keyword">as</span> a</span><br><span class="line"><span class="keyword">left</span> <span class="keyword">join</span> colleges <span class="keyword">as</span> b <span class="keyword">on</span> a.contest_id = b.contest_id</span><br><span class="line"><span class="keyword">left</span> <span class="keyword">join</span> challenges <span class="keyword">as</span> c <span class="keyword">on</span> b.college_id = c.college_id</span><br><span class="line"><span class="keyword">left</span> <span class="keyword">join</span> ( <span class="keyword">select</span> challenge_id, <span class="keyword">sum</span>(total_views) <span class="keyword">as</span> total_views, <span class="keyword">sum</span>(total_unique_views) <span class="keyword">as</span> total_unique_views</span><br><span class="line">            <span class="keyword">from</span> view_stats</span><br><span class="line">            <span class="keyword">group</span> <span class="keyword">by</span> challenge_id ) <span class="keyword">as</span> d <span class="keyword">on</span> c.challenge_id = d.challenge_id</span><br><span class="line"><span class="keyword">left</span> <span class="keyword">join</span> ( <span class="keyword">select</span> challenge_id, <span class="keyword">sum</span>(total_submissions) <span class="keyword">as</span> total_submissions, <span class="keyword">sum</span>(total_accepted_submissions) <span class="keyword">as</span> total_accepted_submissions</span><br><span class="line">            <span class="keyword">from</span> submission_stats</span><br><span class="line">            <span class="keyword">group</span> <span class="keyword">by</span> challenge_id ) <span class="keyword">as</span> e <span class="keyword">on</span> c.challenge_id = e.challenge_id</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> a.contest_id, a.hacker_id, a.name</span><br><span class="line"><span class="keyword">having</span> (total_submissions + total_accepted_submissions + total_views + total_unique_views) &gt; <span class="number">0</span>;</span><br></pre></td></tr></table></figure><h1 id="해결"><a href="#해결" class="headerlink" title="해결"></a>해결</h1><p><a href="https://github.com/BlakeBrown/HackerRank-Solutions/blob/master/SQL/5_Advanced%20Join/4_Interviews/Interviews.mysql">참고</a></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> a.contest_id, a.hacker_id, a.name,</span><br><span class="line">    <span class="keyword">sum</span>(total_submissions) <span class="keyword">as</span> total_submissions,</span><br><span class="line">    <span class="keyword">sum</span>(total_accepted_submissions) <span class="keyword">as</span> total_accepted_submissions,</span><br><span class="line">    <span class="keyword">sum</span>(total_views) <span class="keyword">as</span> total_views,</span><br><span class="line">    <span class="keyword">sum</span>(total_unique_views) <span class="keyword">as</span> total_unique_views</span><br><span class="line"><span class="keyword">from</span> contests <span class="keyword">as</span> a</span><br><span class="line"><span class="keyword">left</span> <span class="keyword">join</span> colleges <span class="keyword">as</span> b <span class="keyword">on</span> a.contest_id = b.contest_id</span><br><span class="line"><span class="keyword">left</span> <span class="keyword">join</span> challenges <span class="keyword">as</span> c <span class="keyword">on</span> b.college_id = c.college_id</span><br><span class="line"><span class="keyword">left</span> <span class="keyword">join</span> ( <span class="keyword">select</span> challenge_id, <span class="keyword">sum</span>(total_views) <span class="keyword">as</span> total_views, <span class="keyword">sum</span>(total_unique_views) <span class="keyword">as</span> total_unique_views</span><br><span class="line">            <span class="keyword">from</span> view_stats</span><br><span class="line">            <span class="keyword">group</span> <span class="keyword">by</span> challenge_id ) <span class="keyword">as</span> d <span class="keyword">on</span> c.challenge_id = d.challenge_id</span><br><span class="line"><span class="keyword">left</span> <span class="keyword">join</span> ( <span class="keyword">select</span> challenge_id, <span class="keyword">sum</span>(total_submissions) <span class="keyword">as</span> total_submissions, <span class="keyword">sum</span>(total_accepted_submissions) <span class="keyword">as</span> total_accepted_submissions</span><br><span class="line">            <span class="keyword">from</span> submission_stats</span><br><span class="line">            <span class="keyword">group</span> <span class="keyword">by</span> challenge_id ) <span class="keyword">as</span> e <span class="keyword">on</span> c.challenge_id = e.challenge_id</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> a.contest_id, a.hacker_id, a.name</span><br><span class="line"><span class="keyword">having</span> (total_submissions + total_accepted_submissions + total_views + total_unique_views) &gt; <span class="number">0</span>;</span><br></pre></td></tr></table></figure><hr><p>2019.10.28 made by <em>jaejun.lee</em></p>]]></content:encoded>
      
      <comments>https://jx2lee.github.io/hackerrank-interviews/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[Hadoop] Install Hadoop using Docker</title>
      <link>https://jx2lee.github.io/hadoop-install_hadoop_using_docker/</link>
      <guid>https://jx2lee.github.io/hadoop-install_hadoop_using_docker/</guid>
      <pubDate>Thu, 24 Oct 2019 15:00:00 GMT</pubDate>
      <description>
      
        &lt;p&gt;&lt;strong&gt;Hadoop&lt;/strong&gt; 환경을 구성해보자!&lt;/p&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p><strong>Hadoop</strong> 환경을 구성해보자!</p><a id="more"></a><p>드디어 엔지니어로써의 능력을 배양할 수 있는 <code>환경 구성</code>이다. 우선, 회사에서 지급받은 서버로 이미 하둡 환경이 구축되어 있지만, 개인 할당받은 서버에서 <code>Docker</code>를 이용해 실습 환경을 구축하고자 한다.</p><blockquote><p><em>Docker Hub 에서 스타가 가장 많은 이미지를 이용할 것이다. Docker를 이용하는 것은 <code>혹여나 나중에도 써먹을 경우</code>를 대비한 것이다.</em></p></blockquote><h1 id="PLAN"><a href="#PLAN" class="headerlink" title="PLAN"></a>PLAN</h1><p>3개의 DataNode와 1개의 NameNode로 구성된 하둡 환경을 구축한다. 여러 개 서버를 연결하는 구조 대신, 쉽게 환경을 바꾸고 입맛대로 수정이 가능한 <strong>docker</strong>를 이용해 구성한다. docker를 공부해보자는 의미도 있고 새롭게 환경을 재구성할 때 유용할 것 같다.</p><h1 id="ENVIRONMENT"><a href="#ENVIRONMENT" class="headerlink" title="ENVIRONMENT"></a>ENVIRONMENT</h1><p>hadoop 폴더를 생성하여 아래와 같은 구조를 갖는다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[kuber@node2 hadoop]$ tree .</span><br><span class="line">.</span><br><span class="line">├── base</span><br><span class="line">│   ├── core-site.xml</span><br><span class="line">│   └── Dockerfile</span><br><span class="line">├── data-node</span><br><span class="line">│   ├── Dockerfile</span><br><span class="line">│   ├── hdfs-site.xml</span><br><span class="line">│   └── install.sh</span><br><span class="line">├── docker-compose.yml</span><br><span class="line">└── name-node</span><br><span class="line">    ├── Dockerfile</span><br><span class="line">    ├── hdfs-site.xml</span><br><span class="line">    └── install.sh</span><br><span class="line"></span><br><span class="line">3 directories, 9 files</span><br></pre></td></tr></table></figure><p>hadoop의 기본 환경을 구성하는 <code>base</code>와 이를 활용해 <code>name / data -node</code> 폴더를 구성하였고, Dockerfile을 작성하여 직접 image를 build하고 <code>docker-compose</code>를 활용해 배포한다. </p><h2 id="BASE"><a href="#BASE" class="headerlink" title="BASE"></a>BASE</h2><p>각 component의 밑바당이 되는 이미지를 구성하는 단계이다. 이는 <code>base</code> 폴더에서 수행하며, 고려사항은 다음과 같다.</p><ul><li>Hadoop 설치를 위한 binary</li><li>Java</li></ul><p>이미지 빌드를 위해 <code>Dockerfile</code>, <code>core-site.xml</code>을 작성해보도록 한다.</p><h3 id="Dockerfile"><a href="#Dockerfile" class="headerlink" title="Dockerfile"></a>Dockerfile</h3><p><code>Dockerfile</code>은 아래와 같은 순서로 작성하였다.</p><blockquote><p><em>Dockerfile 작성을 많이 해버릇 해야겠다. 참고한 블로그에서 사용한 내용을 복사 붙여넣지 않고 직접 작성하니 어느정도 흐름은 파악하였다</em></p></blockquote><ul><li><p>환경변수 설정</p><ul><li><code>HADOOP_VERSION</code> : hadoop version을 의미</li><li><code>HADOOP_URL</code> : hadoop 설치 binary 다운을 위한 url</li></ul></li><li><p>환경변수를 이용해 download 및 압축해제</p></li><li><p>링크파일 생성</p></li><li><p>호스트(in base directory) 파일을 container에 추가</p></li><li><p>hadoop 실행을 위한 환경변수 설정</p><ul><li><code>HADOOP_PREFIX</code> : hadoop root directory</li><li><code>HADOOP_CONF_DIR</code> : hadoop config directory</li><li><code>JAVA_HOME</code> : Java directory</li></ul></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ENV for installation</span></span><br><span class="line">ENV HADOOP_VERSION=2.9.2</span><br><span class="line">ENV HADOOP_URL=http://mirror.apache-kr.org/hadoop/common/hadoop-<span class="variable">$HADOOP_VERSION</span>/hadoop-<span class="variable">$HADOOP_VERSION</span>.tar.gz</span><br><span class="line"></span><br><span class="line"><span class="comment">## Download Hadoop on /app/hadoop</span></span><br><span class="line">RUN curl -fSL <span class="string">&quot;<span class="variable">$HADOOP_URL</span>&quot;</span> -o /tmp/hadoop.tar.gz \</span><br><span class="line">&amp;&amp; tar -xvf /tmp/hadoop.tar.gz -C /opt/ \</span><br><span class="line">&amp;&amp; rm /tmp/hadoop.tar.gz</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># make directory &amp; symbolic link</span></span><br><span class="line">RUN ln -s /opt/hadoop-<span class="variable">$HADOOP_VERSION</span> /opt/hadoop \</span><br><span class="line">&amp;&amp; mkdir /opt/hadoop/dfs \</span><br><span class="line">&amp;&amp; ln -s /opt/hadoop-<span class="variable">$HADOOP_VERSION</span>/etc/hadoop /etc/hadoop \</span><br><span class="line">&amp;&amp; rm -rf /opt/hadoop/share/doc</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># copy local-site.xml file to container</span></span><br><span class="line">ADD core-site.xml /etc/hadoop/</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># ENV for run</span></span><br><span class="line">ENV HADOOP_PREFIX /opt/hadoop</span><br><span class="line">ENV HADOOP_CONF_DIR /etc/hadoop</span><br><span class="line">ENV PATH <span class="variable">$HADOOP_PREFIX</span>/bin/:<span class="variable">$PATH</span></span><br><span class="line">ENV JAVA_HOME /usr/lib/jvm/zulu-8-amd64</span><br></pre></td></tr></table></figure><h3 id="core-site-xml"><a href="#core-site-xml" class="headerlink" title="core-site.xml"></a>core-site.xml</h3><p><code>core-site.xml</code>은 아래와 같이 작성한다.</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">connfiguration</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://namenode:9000/<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>NameNode URI</span><br><span class="line">    <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span>  </span><br><span class="line"><span class="tag">&lt;/<span class="name">connfiguration</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li><p><code>fs.defaultFS</code> : NameNode의 위치를 찾는 설정으로 읽기/쓰기 요청을 할 때 사용되는 항목</p></li><li><p>URI hostname 은 namenode라 설정하였는데, NameNode container의 host name을 지정한 것</p></li></ul><blockquote><p><em><code>connfiguration</code> tag 명을 제대로 확인하도록 하자. (내 경우 connfiguration -&gt; configuration으로 이미지를 빌드 후 실행하였더니 container가 정상적으로 작동하지 않았다)</em></p></blockquote><h3 id="Build-hadoop-base-2-9-2"><a href="#Build-hadoop-base-2-9-2" class="headerlink" title="Build hadoop-base:2.9.2"></a>Build hadoop-base:2.9.2</h3><p>이제 Docker image로 빌드할 차례이다. <code>base</code> 폴더로 접근 후 아래와 같은 명령어를 통해 build를 수행한다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[kuber@node2 hadoop]$ <span class="built_in">cd</span> base/</span><br><span class="line">[kuber@node2 base]$ docker build -t hadoop-base:2.9.2 .</span><br></pre></td></tr></table></figure><h2 id="NAMENODE"><a href="#NAMENODE" class="headerlink" title="NAMENODE"></a>NAMENODE</h2><p>base image를 생성하였다면, 이를 이용해 NameNode Container를 빌드하기 위한 환경을 구성한다. 고려 사항은 다음과 같다.</p><ul><li>NameNode 용 <code>hdfs-site.xml</code></li><li>FsImage, EditLog 저장을 위한 로컬 파일 시스템 경로</li><li>NameNode의 첫 구동 확인 <em>(첫 구동이 아니라면 포맷 후 구동 필요)</em></li></ul><p>NameNode 이미지 빌드를 위해 <code>Dockerfile</code>, <code>hdfs-site.xml</code>, <code>install.sh</code>를 작성해보도록 한다.</p><h3 id="Dockerfile-1"><a href="#Dockerfile-1" class="headerlink" title="Dockerfile"></a>Dockerfile</h3><p><code>Dockerfile</code>은 아래와 같은 순서로 작성하였다.</p><ul><li>이전에 만든 hadoop-base:2.9.2 image를 불러온다</li><li>Web UI 응답 여부 확인을 위한 HEALTHCHECK</li><li>호스트(in name-node directory) 파일을 container에 추가</li><li>FSIMage, EditLog 파일 경로 연결</li><li>포트 노출</li><li>명령어 등록</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">FROM hadoop-base:2.9.2</span><br><span class="line"></span><br><span class="line"><span class="comment"># CON NameNode Web UI</span></span><br><span class="line">HEALTHCHECK --interval=30s --timeout=30s --retries=3 CMD curl -f http://localhost:50070/ || <span class="built_in">exit</span> 1</span><br><span class="line"></span><br><span class="line"><span class="comment"># COPY hdfs-site.xml</span></span><br><span class="line">ADD hdfs-site.xml /etc/hadoop/</span><br><span class="line"></span><br><span class="line"><span class="comment"># FSImage/EditLog path -&gt; volume</span></span><br><span class="line">RUN mkdir /opt/hadoop/dfs/name</span><br><span class="line">VOLUME /opt/hadoop/dfs/name</span><br><span class="line"></span><br><span class="line"><span class="comment"># COPY shell scrip</span></span><br><span class="line">ADD install.sh /install.sh</span><br><span class="line">RUN chmod a+x /install.sh</span><br><span class="line"></span><br><span class="line"><span class="comment"># EXPOSE Port</span></span><br><span class="line">EXPOSE 50070 9000</span><br><span class="line"></span><br><span class="line"><span class="comment"># ADD command line for run</span></span><br><span class="line">CMD [<span class="string">&quot;/install.sh&quot;</span>, <span class="string">&quot;opt/hadoop/dfs/name&quot;</span>]</span><br></pre></td></tr></table></figure><h3 id="hdfs-site-xml"><a href="#hdfs-site-xml" class="headerlink" title="hdfs-site.xml"></a>hdfs-site.xml</h3><p><code>hdfs-site.xml</code>은 아래와 같이 작성한다.</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:///opt/hadoop/dfs/name<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.blocksize<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>10485760<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.client.use.datanode.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.rpc-bind-host<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>0.0.0.0<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.servicerpc-bind-host<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>0.0.0.0<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-bind-host<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>0.0.0.0<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.https-bind-host<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>0.0.0.0<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li><p><code>dfs.namenode.name.dir</code> : FSImage / EditLog 파일을 저장하는 경로</p></li><li><p><code>dfs.blocksize</code> : HDFS 파일 블록의 크기로 본 환경에서는 10MB로 설정하였다<em>(default : 128MB)</em></p><p>(기타 항목들은 ( <a href="https://blog.geunho.dev/posts/hadoop-docker-test-env-hdfs/">https://blog.geunho.dev/posts/hadoop-docker-test-env-hdfs/</a> ) 확인)</p></li></ul><h3 id="install-sh"><a href="#install-sh" class="headerlink" title="install.sh"></a>install.sh</h3><p> <code>install.sh</code>은 NameNode의 네임스페이스의 포맷 여부를 확인하는 쉘 스크립트이다. 만약 네임스페이스가 포맷되어 있다면 NameNode를 구동하고, 포맷되어있지 않다면 포맷을 진행한 후 구동한다.</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">! /bin/bash</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> SET namespace directory</span></span><br><span class="line">NAME_DIR=$1</span><br><span class="line">echo $NAME_DIR</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> CHECK <span class="keyword">if</span> dir is empty</span></span><br><span class="line">if [ &quot;$(ls -A $NAME_DIR)&quot; ]; then</span><br><span class="line">echo &quot;NameNode is already formatted !!&quot;</span><br><span class="line">else</span><br><span class="line">echo &quot;Format NameNode..&quot;</span><br><span class="line"><span class="meta">$</span><span class="bash">HADOOP_PREFIX/bin/hdfs --config <span class="variable">$HADOOP_CONF_DIR</span> namenode -format</span></span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> RUN</span></span><br><span class="line"><span class="meta">$</span><span class="bash">HADOOP_PREFIX/bin/hdfs --config <span class="variable">$HADOOP_CONF_DIR</span> namenode</span></span><br></pre></td></tr></table></figure><h3 id="Build-hadoop-namenode-2-9-2"><a href="#Build-hadoop-namenode-2-9-2" class="headerlink" title="Build hadoop-namenode:2.9.2"></a>Build hadoop-namenode:2.9.2</h3><p>이제 Docker image로 빌드할 차례이다. <code>name-node</code> 폴더로 접근 후 아래와 같은 명령어를 통해 build를 수행한다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[kuber@node2 hadoop]$ <span class="built_in">cd</span> name-node/</span><br><span class="line">[kuber@node2 name-node]$ docker build -t hadoop-namenode:2.9.2 .</span><br></pre></td></tr></table></figure><h2 id="DATANODE"><a href="#DATANODE" class="headerlink" title="DATANODE"></a>DATANODE</h2><p>NameNode 이미지 생성과 마찬가지로, base image를 이용해 DataNode image를 생성해본다. 고려사항은 아래와 같다.</p><ul><li>DataNode 용 <code>hdfs-site.xml</code></li><li>파일 블록 저장을 위한 경로</li></ul><p>DataNode 이미지 빌드를 위해 <code>Dockerfile</code>, <code>hdfs-site.xml</code>, <code>install.sh</code>를 작성해보도록 한다.</p><h3 id="Dockerfile-2"><a href="#Dockerfile-2" class="headerlink" title="Dockerfile"></a>Dockerfile</h3><p><code>Dockerfile</code>은 아래와 같은 순서로 작성하였다.</p><ul><li>이전에 만든 hadoop-base:2.9.2 image를 불러온다</li><li>Web UI 응답 여부 확인을 위한 HEALTHCHECK</li><li>host(in name-node directory) 파일을 container에 추가</li><li>FSIMage, EditLog 파일 경로 연결</li><li>port 노출</li><li>cmd 등록</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">FROM hadoop-base:2.9.2</span><br><span class="line"></span><br><span class="line"><span class="comment"># CONN NameNode Web UI</span></span><br><span class="line">HEALTHCHECK --interval=30s --timeout=30s --retries=3 CMD curl -f http://localhost:50075/ || <span class="built_in">exit</span> 1</span><br><span class="line"></span><br><span class="line"><span class="comment"># </span></span><br><span class="line">RUN mkdir /opt/hadoop/dfs/data</span><br><span class="line">VOLUME /opt/hadoop/dfs/data</span><br><span class="line"></span><br><span class="line"><span class="comment"># COPY shell scrip</span></span><br><span class="line">ADD install.sh /install.sh</span><br><span class="line">RUN chmod a+x /install.sh</span><br><span class="line"></span><br><span class="line"><span class="comment"># EXPOSE Port</span></span><br><span class="line">EXPOSE 50075 50010</span><br><span class="line"></span><br><span class="line"><span class="comment"># ADD command line for run</span></span><br><span class="line">CMD [<span class="string">&quot;/install.sh&quot;</span>]</span><br></pre></td></tr></table></figure><h3 id="hdfs-site-xml-1"><a href="#hdfs-site-xml-1" class="headerlink" title="hdfs-site.xml"></a>hdfs-site.xml</h3><p><code>hdfs-site.xml</code> 아래와 같이 작성한다. container 에 datanode의 dir path와 blocksize를 지정한다.</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:///opt/hadoop/dfs/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.blocksize<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>10485760<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span>  </span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.use.datanode.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="install-sh-1"><a href="#install-sh-1" class="headerlink" title="install.sh"></a>install.sh</h3><p>DataNode의 <code>install.sh</code> 은 별거 없다. DataNode를 구동하는 명령어를 추가하여 작성한다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#! /bin/sh</span></span><br><span class="line"></span><br><span class="line"><span class="variable">$HADOOP_PREFIX</span>/bin/hdfs --config <span class="variable">$HADOOP_CONF_DIR</span> datanode</span><br></pre></td></tr></table></figure><h3 id="Build-hadoop-datanode-2-9-2"><a href="#Build-hadoop-datanode-2-9-2" class="headerlink" title="Build hadoop-datanode:2.9.2"></a>Build hadoop-datanode:2.9.2</h3><p>이제 Docker image로 빌드할 차례이다. <code>data-node</code> 폴더로 접근 후 아래와 같은 명령어를 통해 build를 수행한다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[kuber@node2 hadoop]$ <span class="built_in">cd</span> data-node/</span><br><span class="line">[kuber@node2 data-node]$ docker build -t hadoop-datanode:2.9.2 .</span><br><span class="line">Sending build context to Docker daemon 4.096 kB</span><br><span class="line">Step 1/8 : FROM hadoop-base:2.9.2</span><br><span class="line"> ---&gt; 765c9acb59fd</span><br><span class="line">Step 2/8 : HEALTHCHECK --interval=30s --timeout=30s --retries=3 CMD curl -f http://localhost:50075/ || <span class="built_in">exit</span> 1</span><br><span class="line"> ---&gt; Running <span class="keyword">in</span> e2b20d7d5fd1</span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line">Successfully built 3f1372bf4fdb</span><br></pre></td></tr></table></figure><p><strong>container 구동을 위한 준비가 거의 끝나간다</strong></p><h1 id="RUN"><a href="#RUN" class="headerlink" title="RUN"></a>RUN</h1><p>빌드된 이미지를 하나하나 실행<em>(ex. docker run ~)</em>해도 되지만, <code>docker compose</code>라는 툴을 이용해 한꺼번에 배포해보도록 한다. yml 형식의 스크립트를 작성하여 NadeNode와 DataNode를 한 번에 배포할 것이다.</p><h2 id="Install-docker-compose"><a href="#Install-docker-compose" class="headerlink" title="Install docker-compose"></a>Install <code>docker-compose</code></h2><p>docker 설치 시 자동으로 설치된 줄 알았는데, 설치가 안되있었다. 아래 명령어를 통해 <code>docker-compose</code>를 설치한다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[kuber@node2 hadoop]$ sudo curl -L <span class="string">&quot;https://github.com/docker/compose/releases/download/1.24.1/docker-compose-<span class="subst">$(uname -s)</span>-<span class="subst">$(uname -m)</span>&quot;</span> -o /usr/<span class="built_in">local</span>/bin/docker-compose</span><br><span class="line">  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current</span><br><span class="line">                                 Dload  Upload   Total   Spent    Left  Speed</span><br><span class="line">100   617    0   617    0     0   1382      0 --:--:-- --:--:-- --:--:--  1380</span><br><span class="line">100 15.4M  100 15.4M    0     0  2327k      0  0:00:06  0:00:06 --:--:-- 3535k</span><br><span class="line">[kuber@node2 hadoop]$ sudo chmod +x /usr/<span class="built_in">local</span>/bin/docker-compose</span><br><span class="line">[kuber@node2 hadoop]$ sudo ln -s /usr/<span class="built_in">local</span>/bin/docker-compose /usr/bin/docker-compose</span><br></pre></td></tr></table></figure><h2 id="docker-compose-yml"><a href="#docker-compose-yml" class="headerlink" title="docker-compose.yml"></a>docker-compose.yml</h2><p><code>docker-compose.yml</code>은 docker 실행 옵션들을 미리 적어둔 파일이다. <em>services`</em> 부분은 우리가 구동할 NameNode 및 DataNode에 관련된 옵션들을 작성하고, 계획에서 언급한 것처럼 DataNode 3개 구동을 위해 01/02/03으로 구분하여 작성한다.</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">version: &quot;3.4&quot;</span><br><span class="line"></span><br><span class="line">x-datanode_base: &amp;datanode_base</span><br><span class="line">  image: hadoop-datanode:2.9.2</span><br><span class="line">  networks:</span><br><span class="line">    - bridge</span><br><span class="line"></span><br><span class="line">services:</span><br><span class="line">  namenode:</span><br><span class="line">    image: hadoop-namenode:2.9.2</span><br><span class="line">    container_name: namenode</span><br><span class="line">    hostname: namenode</span><br><span class="line">    ports:</span><br><span class="line">      - &quot;50070:50070&quot;</span><br><span class="line">      - &quot;9000:9000&quot;</span><br><span class="line">    volumes:</span><br><span class="line">      - namenode:/opt/hadoop/dfs/name</span><br><span class="line">      - /tmp:/tmp</span><br><span class="line">    networks:</span><br><span class="line">      - bridge</span><br><span class="line"></span><br><span class="line">  datanode01:</span><br><span class="line">    &lt;&lt;: *datanode_base</span><br><span class="line">    container_name: datanode01</span><br><span class="line">    hostname: datanode01</span><br><span class="line">    volumes:</span><br><span class="line">      - datanode01:/opt/hadoop/dfs/data</span><br><span class="line">  datanode02:</span><br><span class="line">    &lt;&lt;: *datanode_base</span><br><span class="line">    container_name: datanode02</span><br><span class="line">    hostname: datanode02</span><br><span class="line">    volumes:</span><br><span class="line">      - datanode02:/opt/hadoop/dfs/data</span><br><span class="line">  datanode03:</span><br><span class="line">    &lt;&lt;: *datanode_base</span><br><span class="line">    container_name: datanode03</span><br><span class="line">    hostname: datanode03</span><br><span class="line">    volumes:</span><br><span class="line">      - datanode03:/opt/hadoop/dfs/data</span><br><span class="line"></span><br><span class="line">volumes:</span><br><span class="line">  namenode:</span><br><span class="line">  datanode01:</span><br><span class="line">  datanode02:</span><br><span class="line">  datanode03:</span><br><span class="line"></span><br><span class="line">networks:</span><br><span class="line">  bridge:</span><br></pre></td></tr></table></figure><blockquote><p><em>version의 경우, 자신의 서버에 설치된 docker engine release에 따라 format이 정해져있으므로 이 <a href="https://docs.docker.com/compose/compose-file/compose-versioning/">문서</a>를 참고</em></p></blockquote><h2 id="Run-Container-using-docker-compose"><a href="#Run-Container-using-docker-compose" class="headerlink" title="Run Container using docker-compose"></a>Run Container using docker-compose</h2><p>docker-compose를 이용해 배포해 보도록 하자. 볼륨을 생성하고 NameNode / DataNode가 구동되었다는 메세지가 보일 것이다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[kuber@node2 hadoop]$ docker-compose up -d</span><br><span class="line">Creating volume <span class="string">&quot;hadoop_datanode01&quot;</span> with default driver</span><br><span class="line">Creating volume <span class="string">&quot;hadoop_datanode02&quot;</span> with default driver</span><br><span class="line">Creating volume <span class="string">&quot;hadoop_datanode03&quot;</span> with default driver</span><br><span class="line">namenode is up-to-date</span><br><span class="line">Creating datanode01 ... <span class="keyword">done</span></span><br><span class="line">Creating datanode02 ... <span class="keyword">done</span></span><br><span class="line">Creating datanode03 ... <span class="keyword">done</span></span><br></pre></td></tr></table></figure><h1 id="Installation-Check"><a href="#Installation-Check" class="headerlink" title="Installation Check"></a>Installation Check</h1><p>노드들이 정상 작동하는지 확인해보는 단계이다. 구동을 했으면 제대로 되는지 확인하는게 중요하겠죠? 아래 순서와 같이 설치 확인을 진행한다.</p><h2 id="NameNode-컨테이너의-hadoop-client-실행-확인"><a href="#NameNode-컨테이너의-hadoop-client-실행-확인" class="headerlink" title="NameNode 컨테이너의 hadoop client 실행 확인"></a>NameNode 컨테이너의 hadoop client 실행 확인</h2><p>NameNode의 컨테이너에 접속해 커맨드라인을 확인하는 명령어<em>(docker exec)</em>를 통해 확인해본다. 그러면 아래와 같이 Usage가 출력되는 것을 확인할 수 있다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">[kuber@node2 hadoop]$ docker <span class="built_in">exec</span> namenode /opt/hadoop/bin/hadoop</span><br><span class="line">Usage: hadoop [--config confdir] [COMMAND | CLASSNAME]</span><br><span class="line">  CLASSNAME            run the class named CLASSNAME</span><br><span class="line"> or</span><br><span class="line">  <span class="built_in">where</span> COMMAND is one of:</span><br><span class="line">  fs                   run a generic filesystem user client</span><br><span class="line">  version              <span class="built_in">print</span> the version</span><br><span class="line">  jar &lt;jar&gt;            run a jar file</span><br><span class="line">                       note: please use <span class="string">&quot;yarn jar&quot;</span> to launch</span><br><span class="line">                             YARN applications, not this <span class="built_in">command</span>.</span><br><span class="line">  checknative [-a|-h]  check native hadoop and compression libraries availability</span><br><span class="line">  distcp &lt;srcurl&gt; &lt;desturl&gt; copy file or directories recursively</span><br><span class="line">  archive -archiveName NAME -p &lt;parent path&gt; &lt;src&gt;* &lt;dest&gt; create a hadoop archive</span><br><span class="line">  classpath            prints the class path needed to get the</span><br><span class="line">                       Hadoop jar and the required libraries</span><br><span class="line">  credential           interact with credential providers</span><br><span class="line">  daemonlog            get/<span class="built_in">set</span> the <span class="built_in">log</span> level <span class="keyword">for</span> each daemon</span><br><span class="line">  trace                view and modify Hadoop tracing settings</span><br><span class="line"></span><br><span class="line">Most commands <span class="built_in">print</span> <span class="built_in">help</span> when invoked w/o parameters.</span><br></pre></td></tr></table></figure><p>이처럼 매 번 docker exec 명령어를 작성하는 건 정말 귀찮을 것이다. alias를 등록해 간편하게 명령어를 날려보자.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">[kuber@node2 hadoop]$ vi ~/.bash_profile </span><br><span class="line"><span class="comment"># bash_profile</span></span><br><span class="line"><span class="built_in">alias</span> hadoop=<span class="string">&quot;docker exec namenode /opt/hadoop/bin/hadoop&quot;</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line">[kuber@node2 hadoop]$ <span class="built_in">source</span> ~/.bash_profile </span><br><span class="line">[kuber@node2 hadoop]$ hadoop</span><br><span class="line">Usage: hadoop [--config confdir] [COMMAND | CLASSNAME]</span><br><span class="line">  CLASSNAME            run the class named CLASSNAME</span><br><span class="line"> or</span><br><span class="line">  <span class="built_in">where</span> COMMAND is one of:</span><br><span class="line">  fs                   run a generic filesystem user client</span><br><span class="line">  version              <span class="built_in">print</span> the version</span><br><span class="line">  jar &lt;jar&gt;            run a jar file</span><br><span class="line">                       note: please use <span class="string">&quot;yarn jar&quot;</span> to launch</span><br><span class="line">                             YARN applications, not this <span class="built_in">command</span>.</span><br><span class="line">  checknative [-a|-h]  check native hadoop and compression libraries availability</span><br><span class="line">  distcp &lt;srcurl&gt; &lt;desturl&gt; copy file or directories recursively</span><br><span class="line">  archive -archiveName NAME -p &lt;parent path&gt; &lt;src&gt;* &lt;dest&gt; create a hadoop archive</span><br><span class="line">  classpath            prints the class path needed to get the</span><br><span class="line">                       Hadoop jar and the required libraries</span><br><span class="line">  credential           interact with credential providers</span><br><span class="line">  daemonlog            get/<span class="built_in">set</span> the <span class="built_in">log</span> level <span class="keyword">for</span> each daemon</span><br><span class="line">  trace                view and modify Hadoop tracing settings</span><br><span class="line"></span><br><span class="line">Most commands <span class="built_in">print</span> <span class="built_in">help</span> when invoked w/o parameters.</span><br></pre></td></tr></table></figure><h2 id="폴더-생성-조회-삭제-확인"><a href="#폴더-생성-조회-삭제-확인" class="headerlink" title="폴더 생성/조회/삭제 확인"></a>폴더 생성/조회/삭제 확인</h2><p>hadoop 명령어를 통해 폴더를 생성하고 조회하며 마지막으로 삭제하는 작업을 해본다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[kuber@node2 hadoop]$ hadoop fs -mkdir -p /tmp/<span class="built_in">test</span>/app</span><br><span class="line">[kuber@node2 hadoop]$ hadoop fs -ls -R /tmp</span><br><span class="line">drwxr-xr-x   - root supergroup          0 2019-10-25 05:15 /tmp/<span class="built_in">test</span></span><br><span class="line">drwxr-xr-x   - root supergroup          0 2019-10-25 05:15 /tmp/<span class="built_in">test</span>/app</span><br><span class="line">[kuber@node2 hadoop]$ hadoop fs -rm -r /tmp/<span class="built_in">test</span>/app</span><br><span class="line">Deleted /tmp/<span class="built_in">test</span>/app</span><br><span class="line">[kuber@node2 hadoop]$ hadoop fs -ls -R /tmp</span><br><span class="line">drwxr-xr-x   - root supergroup          0 2019-10-25 05:16 /tmp/<span class="built_in">test</span></span><br></pre></td></tr></table></figure><h2 id="Web-UI"><a href="#Web-UI" class="headerlink" title="Web UI"></a>Web UI</h2><p>NameNode와 DataNode 상태를 Web에서 확인할 수 있다. container 실행을 위해 작성한 <code>docker-compose.yml</code> 안에 NameNode의 port<em>(50070)</em>를 이용해 접속을 하면 아래와 같은 화면이 보일 것이다.</p><ul><li><p><code>NameNode</code> overview</p><p><img src="/image/namenode-web.png" alt=""></p></li><li><p><code>DataNode</code> overview</p><p><img src="/image/datanode-web.png" alt=""></p></li></ul><h1 id="Ref"><a href="#Ref" class="headerlink" title="Ref."></a>Ref.</h1><ol><li><a href="https://blog.geunho.dev/posts/hadoop-docker-test-env-hdfs/">김근호님 블로그 : Docker로 Hadoop 테스트 환경 구축하기 - HDFS</a></li><li><a href="https://docs.docker.com/compose/install/">Docker documenst</a></li></ol><hr><p>made by <em>jaejun.lee</em></p>]]></content:encoded>
      
      <comments>https://jx2lee.github.io/hadoop-install_hadoop_using_docker/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[Database] OLAP</title>
      <link>https://jx2lee.github.io/hackerrank-olap/</link>
      <guid>https://jx2lee.github.io/hackerrank-olap/</guid>
      <pubDate>Tue, 22 Oct 2019 15:00:00 GMT</pubDate>
      <description>
      
        &lt;p&gt;hackerrank에서 제공하는 Database 카테고리의 OLAP 문제를 풀어보고 개념을 정리한다.&lt;/p&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p>hackerrank에서 제공하는 Database 카테고리의 OLAP 문제를 풀어보고 개념을 정리한다.</p><a id="more"></a><h1 id="OLAP-The-Total-view"><a href="#OLAP-The-Total-view" class="headerlink" title="OLAP - The Total view"></a>OLAP - The Total view</h1><h2 id="문제"><a href="#문제" class="headerlink" title="문제"></a>문제</h2><p>Which of these provides a total view of the organization?</p><p>1) OLAP<br>2) OLTP<br><strong>3) Data Warehousing</strong><br>4) Database</p><h2 id="풀이"><a href="#풀이" class="headerlink" title="풀이"></a>풀이</h2><p>왜 틀렸는지를 모르겠는 문제이다. <code>1번, 2번, 4번</code>이 결국 <code>3번 : Data Warehousing</code>에 포함된 내용이다. 각각의 개념을 간단히 정리해보면 아래와 같다. <em>(4번의 경우 <code>PASS</code>)</em></p><blockquote><p><strong>OLAP</strong><br>데이터 집계를 효율화하는 접근 방법 중 하나로, <code>다차원 모델</code>구조를 <code>MDX (Multidimensional expressions)</code> 등의 쿼리 언어로 집계한다. 다차원 모델 구조를 <em>OLAP 큐브</em>라 하며 이러한 큐브를 이용해 크로스 집계하는 구조가 <strong>OLAP</strong>이다.</p><p><strong>OLTP (Online Transaction Processing)</strong><br>정의는 <code>실시간으로 서버(DB)가 자료를 처리하는 과정</code> 인데, 사실 OLAP vs OLTP를 비교하는 것을 주로 보았는데 정확한 의미를 모르겠다. OLAP은 하나의 기술로 보는것인지, OLTP는 <code>기술이 아닌 실시간성 처리 과정</code>으로 봐야하는지는 좀 더 살펴본 이후에 자세히 정리해야겠다.</p><p> <strong>Data Warehousing</strong><br> Data Warehouse를 설계하고 사용하는 과정을 뜻하는 단어이다. <code>Data Warehouse</code> 특징을 살펴보면 다음과 같다.</p><ul><li>Web Server 또는 RDB와 달리 <code>대량 데이터 장기 보존</code> 최적화</li><li>정리된 데이터 전송 기능은 뛰어나지만, 소량 데이터의 경우 적합하지 않음</li><li>업무 처리에 있어 함부로 사용해 시스템 과부하 초래는 위험함, 이러한 문제로 필요한 데이터만을 추출하여 <code>데이터 마트 (Data Mart)</code>를 구성함</li></ul></blockquote><p>[참고] : 빅데이터를 지탱하는 기술, Wikipedia</p><h1 id="OLAP-OLAP-Operation-Types"><a href="#OLAP-OLAP-Operation-Types" class="headerlink" title="OLAP - OLAP Operation Types"></a>OLAP - OLAP Operation Types</h1><h2 id="문제-1"><a href="#문제-1" class="headerlink" title="문제"></a>문제</h2><p>Consider a fact table DataPoints(D1,D2,D3,x), and the following three queries:</p><p>Q1: Select D1,D2,D3,Sum(x) From DataPoints Group By D1,D2,D3<br>Q2: Select D1,D2,D3,Sum(x) From DataPoints Group By D1,D2,D3 WITH CUBE<br>Q3: Select D1,D2,D3,Sum(x) From DataPoints Group By D1,D2,D3 WITH ROLLUP</p><p>Suppose attributes D1, D2, and D3 have n1, n2, and n3 different values respectively, and assume that each possible combination of values appears at least once in the table DataPoints. The number of tuples in the result of each of the three queries above can be specified as an arithmetic formula involving n1, n2, and n3. Pick the one tuple (a,b,c,d,e,f) in the list below such that when n1=a, n2=b, and n3=c, then the result sizes of queries Q1, Q2, and Q3 are d, e, and f respectively.</p><p>1) (2, 2, 2, 6, 18, 8)<br>2) (2, 2, 2, 8, 64, 15)<br>3) (5, 10, 10, 500, 1000, 550)<br><strong>4) (4, 7, 3, 84, 160, 117)</strong></p><h2 id="풀이-1"><a href="#풀이-1" class="headerlink" title="풀이"></a>풀이</h2><p>문제를 잘못 이해해 푸는데 오래 걸렸다. 주어진 보기 4개 tuple의 앞에 3개는 각각 D1, D2, D3칼럼의 value 들이었다. 따라서, 각 지문의 3개 숫자 <em>(ex. 1번 보기는 2,2,2 -&gt; D1, D2, D3)</em> 로 operation <code>CUBE</code> 및 <code>ROLL UP</code> 을 수행한 후 조회되는 행의 갯수를 맞추는 문제이다. <strong>Q1</strong>은 쉽게 구할 수 있었고 <code>CUBE</code> 및 <code>ROLLUP</code> 연산을 구글을 통해 살펴보았다.</p><p>우선 <code>CUBE</code> operation은  모든 차원에서 모든 속성 조합을 사용한다. 이는 곧, <code>NULL</code> 구문을 사용하기 때문에 group by로 조회수와는 다르게 아래와 같이 계산된다.</p><p><code>(n1 + 1) * (n2 + 1) * (n3 + 1)</code></p><p>마지막으로 <code>ROLLUP</code> operation은 <code>NULL</code>이 있는 속성을 포함하여 속성 tuple을 생성한다. 이에 기존 <code>CUBE</code> 연산을 통해 나온 수와 <code>ROLLUP</code>연산을 통해 계산되는 tuple 수를 더해준다. 식은 아래와 같다.</p><p><code>[기존 CUBE로 계산된 수] + n1 * (n2 + 1) + 1</code></p><blockquote><p>ROLLUP 이라 함은 Drill Down (operation 중 하나)과 달리 작은 범위에서 큰 범위의 단계적 접근 분석 방법을 말한다 <em>(ex. 번지 -&gt; 동 -&gt; 구 -&gt; 시도 -&gt; 광역)</em>. 위에 기존 CUBE 연산을 통한 값과 그 뒤에 새로운 추가된 수를 더하는 내용이 확실히 이해가 가지 않아 나중에 정리해야할 것 같다.</p></blockquote><hr><p>2019.10.23 made by <em>jaejun.lee</em></p>]]></content:encoded>
      
      <comments>https://jx2lee.github.io/hackerrank-olap/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[Hadoop] Hadoop Overview</title>
      <link>https://jx2lee.github.io/hadoop-introduction_to_hadoop/</link>
      <guid>https://jx2lee.github.io/hadoop-introduction_to_hadoop/</guid>
      <pubDate>Mon, 21 Oct 2019 15:00:00 GMT</pubDate>
      <description>
      
        &lt;p&gt;&lt;strong&gt;Hadoop&lt;/strong&gt; 관련 책을 읽으며 개념을 간단히 정리한다.&lt;/p&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p><strong>Hadoop</strong> 관련 책을 읽으며 개념을 간단히 정리한다.</p><a id="more"></a><p>맨날 하둡 공부해야지.. 공부해야지 하다가 <a href="http://www.yes24.com/Product/goods/66277191"><em>빅데이터를 지탱하는 기술</em></a> 책을 읽으며 정리하고자 한다. 무작정 실습부터 하기 보다는, 기본적인 개념을 익히고 <strong>Hadoop</strong> 환경을 구성할 것이다.</p><blockquote><p><em>우선 내용 전체적으로 도서를 참고해 작성하고 추가적인 부분은 구글 서치를 통해 채워넣을 예정이다.  최대한 책을 기반으로 개념을 정리하는 것을 목표로 한다.</em></p></blockquote><p><strong>Contents</strong>:  </p><ul><li><a href="#introductions">Hadoop 이란</a></li><li><a href="#components">Hadoop 기본 구성 요소</a><ul><li><a href="#hdfsyarn">분산 파일 시스템 (HDFS) 과 리소스 관리자 (YARN)</a></li><li><a href="#mrhive">분산 데이터 처리 (MapReduce) 및 쿼리 엔진 (Hive)</a></li><li><a href="#hive">Hive on Tez</a></li><li><a href="#queryengine">대화형 쿼리 엔진 (Impala &amp; Presto)</a></li></ul></li></ul><h1 id="Hadoop-이란"><a href="#Hadoop-이란" class="headerlink" title=" Hadoop 이란"></a><a name="introductions"></a> Hadoop 이란</h1><p>개념을 배우는데 있어서 역사는 크게 중요하지 않다고 생각한다. 책에서는 역사가 기술되어있지만 나는 정의<u><em>(수학과 아니랄까봐)</em></u> 부터 짚어보고자 한다. <code>Hadoop</code>은 단일 소프트웨어가 아닌, <strong>분산 시스템</strong> 을 구성하는 다수의 소프트웨어로 이루어진 <strong>집합체</strong> 이다. <code>Wiki</code>백과에 의하면, <code>대량의 자료를 처리할 수 있는 큰 컴퓨터 클러스터에서 동작하는 분산 응용 프로그램을 지원하는 프레임워크</code>라고 소개한다. 일맥 상통한다.</p><blockquote><p><em>좀 더 자세히 어떠한 언어로 작성되었는지를 표현하였지, 의미는 같다</em></p></blockquote><p>2013년 <code>Hadoop2</code>부터  <code>YARN</code>이라는 <code>리소트관리자</code> 상에서 분산 애플리케이션이 동작하는 구성으로 설계되어, <strong>대규모 분산시스템을 구축하기 위한 플랫폼</strong> 역할을 맡고 있다.</p><p><img src="/image/hadoop-ecosystem.png" alt="그림 - 빅데이터 관련 Apache 프로젝트"></p><p>[그림] - 빅데이터 관련 Apache 프로젝트 (참고 : 빅데이터를 지탱하는 기술)</p><h1 id="Hadoop-기본-구성-요소"><a href="#Hadoop-기본-구성-요소" class="headerlink" title=" Hadoop 기본 구성 요소"></a><a name="components"></a> Hadoop 기본 구성 요소</h1><p>기본 구성 요소로는 <strong>분산 파일 시스템</strong> <em>(distributed file system)<em>인 <code>HDFS(Hadoop Distributed File System)</code>, *</em>리소스 관리자** *(resource manager)</em> 인 <code>YARN(Yet Another Resource Negotiator)</code>, <strong>분산 데이터 처리</strong> <em>(distributed data processing)</em> 기반 <code>MapReduce</code> 3가지다. 이외 구성요소<em>(프로젝트라고 표현하기도 함)</em>는 Hadoop과 독립적으로 개발되어 분산 애플리케이션으로 동작한다.</p><p>즉, 위에서 소개한 프로젝트에서 분산 파일 시스템으로는 <code>HDFS</code>를 사용하고 resource manager로는 <code>Mesos</code>를, 분산 데이터 처리에는 <code>Spark</code>를 사용할 수 있다. 자신에게 맞고 상황에 맞는 프로젝트를 구성하는 것이 Hadoop을 중심으로 하는 데이터 처리의 특징이다.</p><h2 id="분산-파일-시스템-HDFS-과-리소스-관리자-YARN"><a href="#분산-파일-시스템-HDFS-과-리소스-관리자-YARN" class="headerlink" title=" 분산 파일 시스템 (HDFS) 과 리소스 관리자 (YARN)"></a><a name="hdfsyarn"></a> 분산 파일 시스템 (HDFS) 과 리소스 관리자 (YARN)</h2><p>Hadoop에서 처리되는 데이터는 대부분 <code>HDFS</code>에 저장된다. 보통 파일 서버와 비슷한 개념이지만, <code>다수 컴퓨터에 파일을 복사하여 중복성을 높인다</code>는 특징이 있다.</p><blockquote><p>HDFS는 <strong>블록 구조의 file system</strong>이다. 파일을 특정 크기 블록으로 나누어 분산된 서버에 저장한다. 크기는 64MB 에서 Hadoop2 부터는 128M로 증가하였다(<a href="https://yookeun.github.io/java/2015/05/24/hadoop-hdfs/">참고</a>)</p></blockquote><p>한편, CPU나 메모리 등의 계산 리소스는 resource manager인 <code>YARN</code>에 의해 관리된다. <code>YARN</code>은 CPU 코어와 메모리를 <strong>컨테이너 (Container)</strong> 단위로 관리한다 <em>(여기서 Container는 Docker Container와는 다르다. 어떤 호스트에서 어떤 프로세스를 실행시킬 것인지 결정하는 앱 수준의 기술)</em>. Hadoop에서  분산 앱을 실행하면 <code>YARN</code>이 클러스터 전체의 부하를 보고 비어 있는 호스트부터 컨테이너를 할당한다.</p><blockquote><p>즉, 리소스 관리자인 <code>YARN</code>은 어느 애플리케이션에 얼마만큼의 리소스를 할당할 지 관리함으로써 모든 애플리케이션이 <strong>차질없이 실행되도록 제어</strong> 한다</p></blockquote><h2 id="분산-데이터-처리-MapReduce-및-쿼리-엔진-Hive"><a href="#분산-데이터-처리-MapReduce-및-쿼리-엔진-Hive" class="headerlink" title=" 분산 데이터 처리 (MapReduce) 및 쿼리 엔진 (Hive)"></a><a name="mrhive"></a> 분산 데이터 처리 (MapReduce) 및 쿼리 엔진 (Hive)</h2><ul><li><code>MapReduce</code><br>YARN 상에서 동작하는 분산 애플리케이션 중 하나로 데이터 처리를 실행하는 데 사용한다. 임의의 java 프로그램을 실행할 수 있기 때문에 <em>비구조화 데이터 (Unstructured Data)</em> 가공에 적합하다. 초기 목적은 대량의 데이터를 <em>Batch*처리하기 위함이었다. 한 번 실행하면 대량의 데이터를 읽을 수 있지만, 작은 프로그램을 *(작은 데이터가 존재하는)</em> 을 실행하면 과한 오버헤드로 몇 초 안에 끝나버리는 쿼리에는 어울리지 않다.</li></ul><p><img src="https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2017/04/apache-hadoop-outputformat-introduction-1024x536-1-1.jpg" alt="MapReduce Process"><br>[그림] - MapReduce Process</p><ul><li><code>쿼리 엔진 (Hive)</code><br>Hive는 SQL 등 쿼리 언어에 의한 데이터 집계가 목적으로 설계된 쿼리 엔진 중 하나이다. 이는 SQL 쿼리를 자동으로 MapReduce 프로그램으로 변환시킨다. 실행 특성 상 MapReduce에 의존하고 있다. </li></ul><blockquote><p>쿼리 엔진 Hive도 결국 MapReduce에 의존하고 있기 때문에, 시간이 오래 걸리는 대량의 데이터를 처리하는 <strong>배치 처리에는 적합</strong>하나, <strong>애드 훅 분석을 위한 쿼리<em>(간단하고 바로바로 볼 수 있는)</em>를 여러 번 수행하는 데 적절하지 않다</strong></p></blockquote><center><img src="https://miro.medium.com/max/560/1*ncMKJ3Mdg-QKJQ21zavJeQ.png"></center><p>[그림] - Hive Architecture (<a href="https://medium.com/@yigiterbas/apache-hive-and-applications-1-31735b8823c7">https://medium.com/@yigiterbas/apache-hive-and-applications-1-31735b8823c7</a>)</p><h2 id="Hive-on-Tez"><a href="#Hive-on-Tez" class="headerlink" title=" Hive on Tez"></a><a name="hive"></a> Hive on Tez</h2><p>Hive 가속화를 위해 개발된 것으로 MapReduce에서 보인 몇 가지 단점을 해결하고 고속화를 실현하고 있다.</p><blockquote><p>예를 들어, MapReduce의 경우 하나의 stage가 끝날 때 까지 다음의 처리를 진행할 수 없었다. 이에 <code>Tez</code>는 stage 종료를 기다리지 않고 처리가 끝난 데이터를 차례대로 후속 처리로 넘겨 전체 쿼리 시간의 단축을 실현했다.</p></blockquote><p>현재의 Hive는 MapReduce 뿐 아니라 Tez를 사용해도 동작하므로 Hive를 <code>Hive on Tez</code> 와 <code>Hive on MR</code>로 구분한다. <em>(MR은 MapReduce 줄임말)</em></p><p><img src="https://t1.daumcdn.net/cfile/tistory/2616EF345874E63A0C" alt="Hive on MR &amp; Hive on Tez Process"><br>[그림] - Hive on MR &amp; Hive on Tez Process</p><h2 id="대화형-쿼리-엔진-Impala-amp-Presto"><a href="#대화형-쿼리-엔진-Impala-amp-Presto" class="headerlink" title=" 대화형 쿼리 엔진 (Impala &amp; Presto)"></a><a name="queryengine"></a> 대화형 쿼리 엔진 (Impala &amp; Presto)</h2><p>Hive 고속화가 아닌 대화형 쿼리 실행만을 위한 엔진도 있다. 그 중 <code>Impala</code>와 <code>Presto</code>가 대표적이다. <code>Imapala</code>와 <code>Presto</code>를 간단히 살펴보면 다음과 같다.</p><ul><li><p><strong>Impala</strong></p><p>Impala는 크게 <code>impalad</code>와 <code>impala state store</code> 프로세스로 구성한다.</p><ul><li><code>impalad</code>는 분산 질의 엔진 역할을 담당하는 프로세스로, Hadoop 클러스터 내 데이터노드 위에서 질의에 대한 plan 설계와 질의 처리 작업을 수행한다.</li><li><code>impala state store</code> 는 각 데이터 노드에서 수행되는 <code>impalad</code>에 대한 메타데이터를 유지하는 역할을 담당한다. <code>impalad</code> 프로세스가 클러스터 내에 추가 또는 제거될 때 <code>impala state store</code> 프로세스를 통해 메타데이터가 업데이트된다.</li></ul><blockquote><p><code>impalad</code> : 분산 질의 엔진, <code>impala state store</code> : Impalad의 메타데이터 관리</p></blockquote></li></ul><p><img src="https://d2.naver.com/content/images/2015/06/helloworld-246342-1.png" alt=""></p><p>[그림] - impala high-level architecture (<a href="http://blog.cloudera.com/blog/2012/10/cloudera-impala-real-time-queries-in-apache-hadoop-for-real/">원본출처</a>)</p><ul><li><p><strong>Presto</strong></p><p>Presto는 크게 <code>Coordinator</code>와 <code>Worker</code>로 구성된다.</p><ul><li><code>Coordinator</code>는 SQL query 분석, query 계획과 Presto Worker 노드 (worker)를 관리한다. REST API를 사용하여 Worker 및 Client와 통신한다.</li><li><code>Worker</code>는 작업을 실행하고 데이터를 처리한다. Worker가 수행한 결과를 Coordinator를 거쳐 Client에게 전달하며 Coordinator와 마찬가지로 REST API를 사용해 통신한다.</li></ul></li></ul><p><img src="http://labs.gree.jp/blog/wp-content/uploads/2014/12/presto_arch-600x372.png" alt=""></p><p>[그림] - Presto architecture (<a href="http://labs.gree.jp/blog/2014/12/12838/">출처</a>)</p><p>이러한 대화형 쿼리 엔진은 Hive 와는 달리 순간 최대 속도를 높이기 위해 모든 오버헤드를 제거하여, 리소스를 최대한 활용하여 쿼리를 실행한다 <em>(이는 Hive의 단점으로 언급한 부분을 해결한다)</em> . 그 결과, 대화형 쿼리 엔진은 MPP DB와 비교해도 손색없는 응답 시간을 얻을 수 있다. </p><blockquote><p>Hadoop에서는 쿼리 엔진을 목적에 따라 구분한다. 대량의 비구조화 데이터를 가공하는 무거운 배치 처리에는 높은 처리량으로 리소스를 활용할 수 있는 <code>Hive</code>를, 구조화 및 완성된 데이터를 대화식으로 집계를 원할 땐 지연이 적은 <code>Impala</code>와 <code>Presto</code>가 적합하다.</p></blockquote><hr><p>2019.10.22 made by <em>jaejun.lee</em></p>]]></content:encoded>
      
      <comments>https://jx2lee.github.io/hadoop-introduction_to_hadoop/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[Python] 등굣길</title>
      <link>https://jx2lee.github.io/programmers-tortoise/</link>
      <guid>https://jx2lee.github.io/programmers-tortoise/</guid>
      <pubDate>Thu, 17 Oct 2019 15:00:00 GMT</pubDate>
      <description>
      
        &lt;p&gt;등교하는데 가능한 루트의 최솟값을 구하는 문제를 풀어본다.&lt;/p&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p>등교하는데 가능한 루트의 최솟값을 구하는 문제를 풀어본다.</p><a id="more"></a><h1 id="문제-설명"><a href="#문제-설명" class="headerlink" title="문제 설명"></a>문제 설명</h1><h2 id="문제-설명-1"><a href="#문제-설명-1" class="headerlink" title="문제 설명"></a>문제 설명</h2><p>계속되는 폭우로 일부 지역이 물에 잠겼습니다. 물에 잠기지 않은 지역을 통해 학교를 가려고 합니다. 집에서 학교까지 가는 길은 m x n 크기의 격자모양으로 나타낼 수 있습니다.</p><p>아래 그림은 m = 4, n = 3 인 경우입니다.</p><p><img src="https://grepp-programmers.s3.amazonaws.com/files/ybm/056f54e618/f167a3bc-e140-4fa8-a8f8-326a99e0f567.png" alt="image0.png"></p><p>가장 왼쪽 위, 즉 집이 있는 곳의 좌표는 (1, 1)로 나타내고 가장 오른쪽 아래, 즉 학교가 있는 곳의 좌표는 (m, n)으로 나타냅니다.</p><p>격자의 크기 m, n과 물이 잠긴 지역의 좌표를 담은 2차원 배열 puddles이 매개변수로 주어집니다. 집에서 학교까지 갈 수 있는 최단경로의 개수를 1,000,000,007로 나눈 나머지를 return 하도록 solution 함수를 작성해주세요.</p><h2 id="제한사항"><a href="#제한사항" class="headerlink" title="제한사항"></a>제한사항</h2><ul><li>격자의 크기 m, n은 1 이상 100 이하인 자연수입니다.<ul><li>m과 n이 모두 1인 경우는 입력으로 주어지지 않습니다.</li></ul></li><li>물에 잠긴 지역은 0개 이상 10개 이하입니다.</li><li>집과 학교가 물에 잠긴 경우는 입력으로 주어지지 않습니다.</li></ul><h2 id="입출력-예"><a href="#입출력-예" class="headerlink" title="입출력 예"></a>입출력 예</h2><table><thead><tr><th>m</th><th>n</th><th>puddles</th><th>return</th></tr></thead><tbody><tr><td>4</td><td>3</td><td>[[2, 2]]</td><td>4</td></tr></tbody></table><h2 id="입출력-예-설명"><a href="#입출력-예-설명" class="headerlink" title="입출력 예 설명"></a>입출력 예 설명</h2><p><img src="https://grepp-programmers.s3.amazonaws.com/files/ybm/32c67958d5/729216f3-f305-4ad1-b3b0-04c2ba0b379a.png" alt="image1.png"></p><h1 id="문제-접근"><a href="#문제-접근" class="headerlink" title="문제 접근"></a>문제 접근</h1><p>Dynamic Programming 문제. 오랜만에 파이썬 알고리즘 문제를 풀었다. 쉬운 레벨이라 생각해 도전하였지만 역시나 구글검색행.. 우선 코드에 사용한 변수들을 살펴보자</p><blockquote><p> <code>grid</code> : 격자 <em>(index error를 방지하기 위해 +1 만큼 더 생성)</em></p></blockquote><p>여기서 조심해야 할 것은, 문제에서 제공하는 m,n이 행과 열이라고 생각할 수 있는데 그 반대이다. 이 점을 명심하고 문제를 풀어야 index error를 방지하고 문제를 해결할 수 있다.</p><p>물이 고여있는 좌표에는 <code>-1</code>로 대체한다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> puddles:</span><br><span class="line"><span class="keyword">for</span> x, y <span class="keyword">in</span> puddles:</span><br><span class="line">grid[y][x] = -<span class="number">1</span></span><br></pre></td></tr></table></figure><p>이제 (a,b) = (a-1, b) + (a, b-1) 식을 구현하는데 위에 언급한 것 처럼 행과 열을 조심해서 <code>for</code>문을 수행해야 한다. 행을 기준으로 열을 채워나가는 구조로 (1,1) 인 부분은 <code>continue</code>로 수정하지 않게 설정한다. 또한, 물이 있는 경우는 0으로 바꿔 횟수가 커지기 않게 방지하고 마지막으로 위 식을 작성하면 grid 변수는 원하는 대로 채워질 것이다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n+<span class="number">1</span>):</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, m+<span class="number">1</span>):</span><br><span class="line"><span class="keyword">if</span> i == j == <span class="number">1</span>:</span><br><span class="line"><span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">if</span> grid[j][i] == -<span class="number">1</span>:</span><br><span class="line">            grid[j][i] = <span class="number">0</span></span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">grid[j][i] = (grid[j][i-<span class="number">1</span>] + grid[j-<span class="number">1</span>][i])%<span class="number">1000000007</span></span><br></pre></td></tr></table></figure><p>행과 열 순서가 바꿔 있기 때문에 마지막 return 값도 [m][n]이 아닌 [n][m]으로 <code>return</code> 해야한다.</p><p><code>return grid[n][m]</code></p><p>full code는 하기와 같다</p><h1 id="문제-해결"><a href="#문제-해결" class="headerlink" title="문제 해결"></a>문제 해결</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">solution</span>(<span class="params">m, n, puddles</span>):</span></span><br><span class="line">    grid = [[<span class="number">0</span>] * (m+<span class="number">1</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n+<span class="number">1</span>)]</span><br><span class="line">    grid[<span class="number">1</span>][<span class="number">1</span>] = <span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> puddles:</span><br><span class="line">        <span class="keyword">for</span> x, y <span class="keyword">in</span> puddles:</span><br><span class="line">            grid[y][x] = -<span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n+<span class="number">1</span>):</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, m+<span class="number">1</span>):</span><br><span class="line">            <span class="keyword">if</span> i == j == <span class="number">1</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="keyword">if</span> grid[j][i] == -<span class="number">1</span>:</span><br><span class="line">                grid[j][i] = <span class="number">0</span></span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            grid[j][i] = (grid[j][i-<span class="number">1</span>] + grid[j-<span class="number">1</span>][i])%<span class="number">1000000007</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> grid[n][m]</span><br></pre></td></tr></table></figure><hr><p>2019.10.18 made by <em>jaejun.lee</em></p>]]></content:encoded>
      
      <comments>https://jx2lee.github.io/programmers-tortoise/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[SQL] Print Prime Number</title>
      <link>https://jx2lee.github.io/hackerrank-print_prime_number/</link>
      <guid>https://jx2lee.github.io/hackerrank-print_prime_number/</guid>
      <pubDate>Mon, 14 Oct 2019 15:00:00 GMT</pubDate>
      <description>
      
        &lt;p&gt;&lt;code&gt;hackerrank&lt;/code&gt;에서 제공하는 &lt;code&gt;Print Prime Number&lt;/code&gt; 문제를 사용자 변수를 활용해 해결하였다.&lt;/p&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p><code>hackerrank</code>에서 제공하는 <code>Print Prime Number</code> 문제를 사용자 변수를 활용해 해결하였다.</p><a id="more"></a><h1 id="문제"><a href="#문제" class="headerlink" title="문제"></a>문제</h1><p>Write a query to print all  <em>prime numbers</em>  less than or equal to  1000. Print your result on a single line, and use the ampersand () character as your separator (instead of a space).</p><p>For example, the output for all prime numbers  &lt;= 10 would be:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2&amp;3&amp;5&amp;7</span><br></pre></td></tr></table></figure><h1 id="접근"><a href="#접근" class="headerlink" title="접근"></a>접근</h1><p>1000 이하의 소수를 구하는 문제로, sql query로는 처음 풀어본다. 아래와 같은 순서로 풀어볼 수 있다. (참고한 자료는 url을 잃어버렸다. 죄송합니다)</p><p><strong>첫 번째</strong>, <code>prime number</code>를 구하기 위한 <code>num</code> 변수를 2 이상 1000이하 까지 조회하는 부분이다. <code>information_schema</code>의 테이블을 이용해 <code>num := num + 1</code> 을 조회하면 아래와 같다.</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">    @num1 :=@num1 + <span class="number">1</span> <span class="keyword">as</span> num1</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">    information_schema.tables t1,</span><br><span class="line">    information_schema.tables t2,</span><br><span class="line">    (<span class="keyword">select</span> @num1 := <span class="number">1</span>) tmp</span><br><span class="line">;</span><br></pre></td></tr></table></figure><p><strong>두 번째</strong>, 소수가 아닌 수를 걸러내기 위해 <code>exists</code> 문을 작성한다. <code>div</code> 변수를 <code>num</code> 변수와 같이 조회하는 문을 이용해 소수<em>(약수는 나와 그 수 밖에 없는 특징 : floor(num / div) != num / div )</em>를 구한다. 이때, <code>where</code>절에 속해야 한다.</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">where</span><br><span class="line">    num1 &lt;= 1000</span><br><span class="line">and not exists (</span><br><span class="line">    <span class="keyword">select</span></span><br><span class="line">        *</span><br><span class="line">    <span class="keyword">from</span></span><br><span class="line">        (</span><br><span class="line">            <span class="keyword">select</span></span><br><span class="line">                @num2 :=@num2 + <span class="number">1</span> <span class="keyword">as</span> num2</span><br><span class="line">            <span class="keyword">from</span></span><br><span class="line">                information_schema.tables <span class="keyword">as</span> t1,</span><br><span class="line">                information_schema.tables <span class="keyword">as</span> t2,</span><br><span class="line">                (<span class="keyword">select</span> @num2 := <span class="number">1</span>) tmp2</span><br><span class="line">            <span class="keyword">limit</span> <span class="number">1000</span></span><br><span class="line">        ) t2</span><br><span class="line">    <span class="keyword">where</span></span><br><span class="line">        <span class="keyword">floor</span>(num1 / num2) = (num1 / num2)</span><br><span class="line">    <span class="keyword">and</span> num2 * num2 &lt;= num1</span><br><span class="line">    <span class="keyword">and</span> num2 &gt; <span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>이제 적절히 두 sql 문을 합쳐주면 된다. 테이블명이 중복되지 않게, 이미 사용한 사용자 변수 또한 중복되지 않게 두 query를 섞어주면 문제를 해결할 수 있다.</p><blockquote><p>특히, 사용자 변수 num1, num2 를 같은 것으로 실행하니 오류가 발생하였다. 앞으로 주의할 것! 사용자 변수는 <strong>모두 다르게</strong></p></blockquote><h1 id="해결"><a href="#해결" class="headerlink" title="해결"></a>해결</h1><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">    <span class="keyword">group_concat</span>(num1 separator <span class="string">&#x27;&amp;&#x27;</span>)</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">    (</span><br><span class="line">        <span class="keyword">select</span></span><br><span class="line">            @num1 :=@num1 + <span class="number">1</span> <span class="keyword">as</span> num1</span><br><span class="line">        <span class="keyword">from</span></span><br><span class="line">            information_schema.tables <span class="keyword">as</span> t1,</span><br><span class="line">            information_schema.tables <span class="keyword">as</span> t2,</span><br><span class="line">            (<span class="keyword">select</span> @num1 := <span class="number">1</span>) tmp1</span><br><span class="line">    ) t1</span><br><span class="line"><span class="keyword">where</span></span><br><span class="line">    num1 &lt;= <span class="number">1000</span></span><br><span class="line"><span class="keyword">and</span> <span class="keyword">not</span> <span class="keyword">exists</span> (</span><br><span class="line">    <span class="keyword">select</span></span><br><span class="line">        *</span><br><span class="line">    <span class="keyword">from</span></span><br><span class="line">        (</span><br><span class="line">            <span class="keyword">select</span></span><br><span class="line">                @num2 :=@num2 + <span class="number">1</span> <span class="keyword">as</span> num2</span><br><span class="line">            <span class="keyword">from</span></span><br><span class="line">                information_schema.tables <span class="keyword">as</span> t1,</span><br><span class="line">                information_schema.tables <span class="keyword">as</span> t2,</span><br><span class="line">                (<span class="keyword">select</span> @num2 := <span class="number">1</span>) tmp2</span><br><span class="line">            <span class="keyword">limit</span> <span class="number">1000</span></span><br><span class="line">        ) t2</span><br><span class="line">    <span class="keyword">where</span></span><br><span class="line">        <span class="keyword">floor</span>(num1 / num2) = (num1 / num2)</span><br><span class="line">    <span class="keyword">and</span> num2 * num2 &lt;= num1</span><br><span class="line">    <span class="keyword">and</span> num2 &gt; <span class="number">1</span>);</span><br></pre></td></tr></table></figure><hr><p>2019.10.15 made by <em>jaejun.lee</em></p><!--stackedit_data:eyJoaXN0b3J5IjpbLTEyOTA5NzU5MTgsMTI0NjM5MjY2MCwtNDA2MDA4Nzc5LDQxOTEwMjYxLDEwNTE0NzkxNzYsLTE1NDYzMzA1NDcsLTM5NjczNjYzMSwtOTg3ODQ3ODA4LC0xMjE0MzMzNTUyLC0xNDI4Nzk5NDAsMTYyNTQ1NTc4LDM2ODE5MDkwOCwtODg3NjM2NTgwLDIwODcxODE4NDEsNzQ1MjIyMDhdfQ==-->]]></content:encoded>
      
      <comments>https://jx2lee.github.io/hackerrank-print_prime_number/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[SQL] Symmetric Pairs &amp; Draw The Triangle 1</title>
      <link>https://jx2lee.github.io/hackerrank-symmetric_pairs_draw_triangle_1/</link>
      <guid>https://jx2lee.github.io/hackerrank-symmetric_pairs_draw_triangle_1/</guid>
      <pubDate>Sun, 13 Oct 2019 15:00:00 GMT</pubDate>
      <description>
      
        &lt;p&gt;&lt;code&gt;hackerrank&lt;/code&gt;에서 제공하는 &lt;code&gt;Symmetric Pairs &amp;amp; Draw The Triangle 1&lt;/code&gt; 문제를 정리한다.&lt;/p&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p><code>hackerrank</code>에서 제공하는 <code>Symmetric Pairs &amp; Draw The Triangle 1</code> 문제를 정리한다.</p><a id="more"></a><h1 id="Symmetric-Pairs"><a href="#Symmetric-Pairs" class="headerlink" title="Symmetric Pairs"></a>Symmetric Pairs</h1><h2 id="문제"><a href="#문제" class="headerlink" title="문제"></a>문제</h2><p>You are given a table,  <em>Functions</em>, containing two columns:  <em>X</em> and  <em>Y</em>.</p><p><img src="https://s3.amazonaws.com/hr-challenge-images/12892/1443818798-51909e977d-1.png" alt=""></p><p>Two pairs  <em>(X1, Y1)</em>  and  <em>(X2, Y2)</em>  are said to be  <em>symmetric</em>  <em>pairs</em>  if <em>X1  = Y2</em>  and  <em>X2  = Y1</em>.</p><p>Write a query to output all such  <em>symmetric</em>  <em>pairs</em>  in ascending order by the value of  <em>X</em>.</p><p><strong>Sample Input</strong></p><p><img src="https://s3.amazonaws.com/hr-challenge-images/12892/1443818693-b384c24e35-2.png" alt=""></p><p><strong>Sample Output</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">20 20</span><br><span class="line">20 21</span><br><span class="line">22 23</span><br></pre></td></tr></table></figure><h2 id="접근"><a href="#접근" class="headerlink" title="접근"></a>접근</h2><p>문제는 <code>1) x = y</code>인 짝들과 <code>2) x != y</code> 인 짝들의 <em>union</em>으로 접근하였다.<br>우선, <code>1) x = y</code>인 경우는 아래 쿼리로 표현할 수 있다.</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> x, y</span><br><span class="line"><span class="keyword">from</span> functions <span class="keyword">as</span> f1</span><br><span class="line"><span class="keyword">where</span> x = y <span class="keyword">and</span></span><br><span class="line">    (<span class="keyword">select</span> <span class="keyword">count</span>(*) <span class="keyword">from</span> functions <span class="keyword">where</span> x = f1.x <span class="keyword">and</span> y = f1.x) &gt; <span class="number">1</span></span><br><span class="line"><span class="string">``</span><span class="string">` </span></span><br><span class="line"><span class="string">*count(*)  &gt; 1*인 이유는 나온 갯수가 2개 이상인 짝들만 뽑아줘야 하므로 `</span><span class="keyword">where</span><span class="string">`절에 조건을 추가한 것이다. 이후 `</span><span class="number">2</span>) x != y<span class="string">` 는 아래와 같다.</span></span><br><span class="line"><span class="string">`</span><span class="string">``</span><span class="keyword">sql</span></span><br><span class="line"><span class="keyword">select</span> f1.x, f1.y</span><br><span class="line"><span class="keyword">from</span> functions <span class="keyword">as</span> f1, functions <span class="keyword">as</span> f2</span><br><span class="line"><span class="keyword">where</span> f1.x = f2.y <span class="keyword">and</span> f1.y = f2.x <span class="keyword">and</span> f1.x &lt; f1.y</span><br></pre></td></tr></table></figure><p>주목해야하는 부분은 <code>f1.x &lt; f1.y</code>인 부분으로, 뽑아내는 짝들의 <code>x</code>값이 <code>y</code>보다 작은 짝들만 찾아주게 되면 <code>1) x = y</code>인 부분은 제외할 수 있다. 이와 같은 두 쿼리를 <code>union</code>으로 묶어주고 마지막 <code>order by</code>를 통해 정렬만 하면 문제가 해결된다.</p><h2 id="해결"><a href="#해결" class="headerlink" title="해결"></a>해결</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> x, y</span><br><span class="line"><span class="keyword">from</span> functions <span class="keyword">as</span> f1</span><br><span class="line"><span class="keyword">where</span> x = y <span class="keyword">and</span></span><br><span class="line">    (<span class="keyword">select</span> <span class="keyword">count</span>(*) <span class="keyword">from</span> functions <span class="keyword">where</span> x = f1.x <span class="keyword">and</span> y = f1.x) &gt; <span class="number">1</span></span><br><span class="line"><span class="keyword">union</span></span><br><span class="line"><span class="keyword">select</span> f1.x, f1.y</span><br><span class="line"><span class="keyword">from</span> functions <span class="keyword">as</span> f1, functions <span class="keyword">as</span> f2</span><br><span class="line"><span class="keyword">where</span> f1.x = f2.y <span class="keyword">and</span> f1.y = f2.x <span class="keyword">and</span> f1.x &lt; f1.y</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> x;</span><br></pre></td></tr></table></figure><h1 id="Draw-The-Triangle-1"><a href="#Draw-The-Triangle-1" class="headerlink" title="Draw The Triangle 1"></a>Draw The Triangle 1</h1><h2 id="문제-1"><a href="#문제-1" class="headerlink" title="문제"></a>문제</h2><p><em>P(R)</em>  represents a pattern drawn by Julia in  <em>R</em>  rows. The following pattern represents  <em>P(5)</em>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">* * * * * </span><br><span class="line">* * * * </span><br><span class="line">* * * </span><br><span class="line">* * </span><br><span class="line">*</span><br></pre></td></tr></table></figure><p>Write a query to print the pattern  <em>P(20)</em>.</p><h2 id="접근-1"><a href="#접근-1" class="headerlink" title="접근"></a>접근</h2><p><em>사용자 정의 변수</em>를 이용해 접근하였다.<br><code>i</code>라는 변수를 21로 선언하고, <code>repeat</code>함수를 이용해 <code>i &gt; 0</code>일 때까지 반복하는 쿼리를 작성하였다. 4문장으로 쉽게 풀리는 문제인데, <em>사용자 정의 변수</em>에 대해 다시 한 번 생각해보자는 의미로 정리하였고 다음에는 꼭 틀리지 말자.</p><h2 id="해결-1"><a href="#해결-1" class="headerlink" title="해결"></a>해결</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> @i = <span class="number">21</span>;</span><br><span class="line"><span class="keyword">select</span> <span class="keyword">repeat</span>(<span class="string">&#x27;* &#x27;</span>, @i := @i - <span class="number">1</span>)</span><br><span class="line"><span class="keyword">from</span> information_schema.tables</span><br><span class="line"><span class="keyword">where</span> @i &gt; <span class="number">0</span>;</span><br></pre></td></tr></table></figure><hr><p>2019.10.14 made by <em>jaejun.lee</em></p>]]></content:encoded>
      
      <comments>https://jx2lee.github.io/hackerrank-symmetric_pairs_draw_triangle_1/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[SQL] Contest Leaderboard</title>
      <link>https://jx2lee.github.io/hackerrank-contest_leaderboard/</link>
      <guid>https://jx2lee.github.io/hackerrank-contest_leaderboard/</guid>
      <pubDate>Wed, 09 Oct 2019 15:00:00 GMT</pubDate>
      <description>
      
        &lt;p&gt;&lt;code&gt;hackerrank&lt;/code&gt;에서 제공하는 &lt;code&gt;Contest Leaderboard&lt;/code&gt; 문제를 &lt;code&gt;Group by&lt;/code&gt;를 활용해 해결하였다.&lt;/p&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p><code>hackerrank</code>에서 제공하는 <code>Contest Leaderboard</code> 문제를 <code>Group by</code>를 활용해 해결하였다.</p><a id="more"></a><h1 id="문제"><a href="#문제" class="headerlink" title="문제"></a>문제</h1><p>You did such a great job helping Julia with her last coding contest challenge that she wants you to work on this one, too!</p><p>The total score of a hacker is the sum of their maximum scores for all of the challenges. Write a query to print the  <em>hacker_id</em>,  <em>name</em>, and total score of the hackers ordered by the descending score. If more than one hacker achieved the same total score, then sort the result by ascending  <em>hacker_id</em>. Exclude all hackers with a total score of  from your result.</p><p><strong>Input Format</strong></p><p>The following tables contain contest data:</p><ul><li><p><em>Hackers:</em>  The  <em>hacker_id</em>  is the id of the hacker, and  <em>name</em>  is the name of the hacker.  <img src="https://s3.amazonaws.com/hr-challenge-images/19503/1458522826-a9ddd28469-ScreenShot2016-03-21at6.40.27AM.png" alt=""></p></li><li><p><em>Submissions:</em>  The  <em>submission_id</em>  is the id of the submission,  <em>hacker_id</em>  is the id of the hacker who made the submission,  <em>challenge_id</em>  is the id of the challenge for which the submission belongs to, and  <em>score</em>  is the score of the submission.  <img src="https://s3.amazonaws.com/hr-challenge-images/19503/1458523022-771511df90-ScreenShot2016-03-21at6.40.37AM.png" alt=""></p></li></ul><p><strong>Sample Input</strong></p><p><em>Hackers</em>  Table:  <img src="https://s3.amazonaws.com/hr-challenge-images/19503/1458523374-7ecc39010f-ScreenShot2016-03-21at6.51.56AM.png" alt=""></p><p><em>Submissions</em>  Table:  <img src="https://s3.amazonaws.com/hr-challenge-images/19503/1458523388-0896218137-ScreenShot2016-03-21at6.51.45AM.png" alt=""></p><p><strong>Sample Output</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">4071 Rose 191</span><br><span class="line">74842 Lisa 174</span><br><span class="line">84072 Bonnie 100</span><br><span class="line">4806 Angela 89</span><br><span class="line">26071 Frank 85</span><br><span class="line">80305 Kimberly 67</span><br><span class="line">49438 Patrick 43</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><strong>Explanation</strong></p><p>Hacker  <em>4071</em>  submitted solutions for challenges  <em>19797</em>  and  <em>49593</em>, so the total score  .</p><p>Hacker  <em>74842</em>  submitted solutions for challenges  <em>19797</em>  and  <em>63132</em>, so the total score</p><p>Hacker  <em>84072</em>  submitted solutions for challenges  <em>49593</em>  and  <em>63132</em>, so the total score  .</p><p>The total scores for hackers  <em>4806</em>,  <em>26071</em>,  <em>80305</em>, and  <em>49438</em>  can be similarly calculated.</p><h1 id="접근"><a href="#접근" class="headerlink" title="접근"></a>접근</h1><p>우선은, <code>challenge_id / hacker_id 별 score의 최댓값</code>을 구하고 이를 <code>hackers</code> 테이블과 조인한다.</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> h.hacker_id, h.name, <span class="keyword">sum</span>(m.score) <span class="keyword">as</span> total_score</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">    (<span class="keyword">select</span> hacker_id, challenge_id, <span class="keyword">max</span>(score) <span class="keyword">as</span> score</span><br><span class="line">    <span class="keyword">from</span> submissions</span><br><span class="line">    <span class="keyword">group</span> <span class="keyword">by</span> challenge_id, hacker_id) <span class="keyword">as</span> m</span><br><span class="line"><span class="keyword">join</span> hackers <span class="keyword">as</span> h <span class="keyword">on</span> h.hacker_id = m.hacker_id</span><br></pre></td></tr></table></figure><p>이후 <code>max score</code>들의 <code>total_score</code>를 구하기 위해 <code>hacker_id / name 을 key로 하여 group by</code> 한다.</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> h.hacker_id, h.name, <span class="keyword">sum</span>(m.score) <span class="keyword">as</span> total_score</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">    (<span class="keyword">select</span> hacker_id, challenge_id, <span class="keyword">max</span>(score) <span class="keyword">as</span> score</span><br><span class="line">    <span class="keyword">from</span> submissions</span><br><span class="line">    <span class="keyword">group</span> <span class="keyword">by</span> challenge_id, hacker_id) <span class="keyword">as</span> m</span><br><span class="line"><span class="keyword">join</span> hackers <span class="keyword">as</span> h <span class="keyword">on</span> h.hacker_id = m.hacker_id</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> h.hacker_id, h.name</span><br></pre></td></tr></table></figure><p>마지막으로 문제에 따라 정렬만 하면 된다.</p><blockquote><p><em>(total_score &gt; 0인 조건도 추가)</em></p></blockquote><h1 id="해결"><a href="#해결" class="headerlink" title="해결"></a>해결</h1><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> h.hacker_id, h.name, <span class="keyword">sum</span>(m.score) <span class="keyword">as</span> total_score</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">    (<span class="keyword">select</span> hacker_id, challenge_id, <span class="keyword">max</span>(score) <span class="keyword">as</span> score</span><br><span class="line">    <span class="keyword">from</span> submissions</span><br><span class="line">    <span class="keyword">group</span> <span class="keyword">by</span> challenge_id, hacker_id) <span class="keyword">as</span> m</span><br><span class="line"><span class="keyword">join</span> hackers <span class="keyword">as</span> h <span class="keyword">on</span> h.hacker_id = m.hacker_id</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> h.hacker_id, h.name</span><br><span class="line"><span class="keyword">having</span> total_score &gt; <span class="number">0</span></span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> total_score <span class="keyword">desc</span>, h.hacker_id;</span><br></pre></td></tr></table></figure><hr><p>2019.10.10 made by <em>jaejun.lee</em></p>]]></content:encoded>
      
      <comments>https://jx2lee.github.io/hackerrank-contest_leaderboard/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[SQL] SQL Project Planning</title>
      <link>https://jx2lee.github.io/hackerrank-project_planning/</link>
      <guid>https://jx2lee.github.io/hackerrank-project_planning/</guid>
      <pubDate>Wed, 09 Oct 2019 15:00:00 GMT</pubDate>
      <description>
      
        &lt;p&gt;&lt;code&gt;hackerrank&lt;/code&gt;에서 제공하는 &lt;code&gt;SQL Project Planning&lt;/code&gt; 문제를 정리한다.&lt;/p&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p><code>hackerrank</code>에서 제공하는 <code>SQL Project Planning</code> 문제를 정리한다.</p><a id="more"></a><h1 id="문제"><a href="#문제" class="headerlink" title="문제"></a>문제</h1><p>You are given a table,  <em>Projects</em>, containing three columns:  <em>Task_ID</em>,  <em>Start_Date</em>  and  <em>End_Date</em>. It is guaranteed that the difference between the  <em>End_Date</em>  and the  <em>Start_Date</em>  is equal to  <em>1</em>  day for each row in the table.</p><p><img src="https://s3.amazonaws.com/hr-challenge-images/12894/1443819551-639948acc0-1.png" alt=""></p><p>If the  <em>End_Date</em>  of the tasks are consecutive, then they are part of the same project. Samantha is interested in finding the total number of different projects completed.</p><p>Write a query to output the start and end dates of projects listed by the number of days it took to complete the project in ascending order. If there is more than one project that have the same number of completion days, then order by the start date of the project.</p><p><strong>Sample Input</strong></p><p><img src="https://s3.amazonaws.com/hr-challenge-images/12894/1443819440-1c40e943a1-2.png" alt=""></p><p><strong>Sample Output</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">2015-10-28 2015-10-29</span><br><span class="line">2015-10-30 2015-10-31</span><br><span class="line">2015-10-13 2015-10-15</span><br><span class="line">2015-10-01 2015-10-04</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><strong>Explanation</strong></p><p>The example describes following  <em>four</em>  projects:</p><ul><li><em>Project 1</em>: Tasks  <em>1</em>,  <em>2</em>  and  <em>3</em>  are completed on consecutive days, so these are part of the project. Thus start date of project is  <em>2015-10-01</em>  and end date is  <em>2015-10-04</em>, so it took  <em>3 days</em>  to complete the project.</li><li><em>Project 2</em>: Tasks  <em>4</em> and <em>5</em> are completed on consecutive days, so these are part of the project. Thus, the start date of project is <em>2015-10-13</em> and end date is <em>2015-10-15</em>, so it took <em>2 days</em> to complete the project.</li><li><em>Project 3</em>: Only task  <em>6</em> is part of the project. Thus, the start date of project is <em>2015-10-28</em> and end date is <em>2015-10-29</em>, so it took <em>1 day</em> to complete the project.</li><li><em>Project 4</em>: Only task <em>7</em> is part of the project. Thus, the start date of project is <em>2015-10-30</em> and end date is <em>2015-10-31</em>, so it took <em>1 day</em> to complete the project.</li></ul><h1 id="접근"><a href="#접근" class="headerlink" title="접근"></a>접근</h1><p>프로젝트의 시작과 끝 날짜를 출력하는 문제. 테이블에는 각각의 <code>task</code>들이 <code>start_date, end_date</code>로 구성되었고 같은 프로젝트는 각 <code>task</code>가 이어질 수 있다. 처음 접근했을 때 <code>join</code>을 이용해 <code>p1, p2</code> 테이블로부터 <code>p1.end_date=p2.start_date</code>를 이용하였지만 실패하여 <a href="https://nifannn.github.io/2017/10/24/SQL-Notes-Hackerrank-Projects/">블로그</a>를 참고하였다.</p><p>우선 <code>start_date</code>가 <code>end_date</code>에 포함되지 않는 날짜를 확인한다. 이는 프로젝트의 시작일 것이다</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> start_date</span><br><span class="line"><span class="keyword">from</span> projects</span><br><span class="line"><span class="keyword">where</span> start_date <span class="keyword">not</span> <span class="keyword">in</span></span><br><span class="line">(<span class="keyword">select</span> end_date <span class="keyword">from</span> projects);</span><br></pre></td></tr></table></figure><p>마찬가지로 <code>end_date</code>가 <code>start_date</code>에 포함되지 않는 날짜를 확인한다. 이는 프로젝트이 끝일 것이다.</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> end_date</span><br><span class="line"><span class="keyword">from</span> projects</span><br><span class="line"><span class="keyword">where</span> end_date <span class="keyword">not</span> <span class="keyword">in</span></span><br><span class="line">(<span class="keyword">select</span> start_date <span class="keyword">from</span> projects);</span><br></pre></td></tr></table></figure><p>그런 다음 각 프로젝트에 대해 (시작 날짜, 종료 날짜) 쌍을 찾아야한다. 그 전에 프로젝트의 시작 날짜와 종료 날짜를 <code>교차</code>시켜 <code>모든 잠재적 쌍</code>을 생성한다. 또한, 동일한 프로젝트의 경우 종료 날짜는 프로젝트 시작 날짜보다 큰 프로젝트의 모든 종료 날짜 중 가장 작아야한다.</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> start_date, <span class="keyword">min</span>(end_date)</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">    (<span class="keyword">select</span> start_date <span class="keyword">from</span> projects <span class="keyword">where</span> start_date <span class="keyword">not</span> <span class="keyword">in</span> (<span class="keyword">select</span> end_date <span class="keyword">from</span> projects)) <span class="keyword">as</span> t1,</span><br><span class="line">    (<span class="keyword">select</span> end_date <span class="keyword">from</span> projects <span class="keyword">where</span> end_date <span class="keyword">not</span> <span class="keyword">in</span> (<span class="keyword">select</span> start_date <span class="keyword">from</span> projects)) <span class="keyword">as</span> t2</span><br><span class="line"><span class="keyword">where</span> start_date &lt; end_date</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> start_date</span><br></pre></td></tr></table></figure><p>마지막 문제 조건 중 프로젝트 수행 기간이 짧은 순서로, 수행 기간이 같다면 시작 날짜를 기준으로 정렬하면 해결할 수 있다. <em>(datediff 함수 이용 - 두 날짜 데이터 차이값 생성)</em></p><ul><li>ADD <code>order by datediff(min(end_date), start_date), start_date</code></li></ul><h1 id="해결"><a href="#해결" class="headerlink" title="해결"></a>해결</h1><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> start_date, <span class="keyword">min</span>(end_date)</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">    (<span class="keyword">select</span> start_date <span class="keyword">from</span> projects <span class="keyword">where</span> start_date <span class="keyword">not</span> <span class="keyword">in</span> (<span class="keyword">select</span> end_date <span class="keyword">from</span> projects)) <span class="keyword">as</span> t1,</span><br><span class="line">    (<span class="keyword">select</span> end_date <span class="keyword">from</span> projects <span class="keyword">where</span> end_date <span class="keyword">not</span> <span class="keyword">in</span> (<span class="keyword">select</span> start_date <span class="keyword">from</span> projects)) <span class="keyword">as</span> t2</span><br><span class="line"><span class="keyword">where</span> start_date &lt; end_date</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> start_date</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> <span class="keyword">datediff</span>(<span class="keyword">min</span>(end_date), start_date), start_date;</span><br></pre></td></tr></table></figure><hr><p>2019.10.10 made by <em>jaejun.lee</em></p>]]></content:encoded>
      
      <comments>https://jx2lee.github.io/hackerrank-project_planning/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[SQL] Challenges</title>
      <link>https://jx2lee.github.io/hackerrank-challenges/</link>
      <guid>https://jx2lee.github.io/hackerrank-challenges/</guid>
      <pubDate>Sun, 06 Oct 2019 15:00:00 GMT</pubDate>
      <description>
      
        &lt;p&gt;&lt;code&gt;hackerrank&lt;/code&gt;에서 제공하는 &lt;code&gt;Challenges&lt;/code&gt; 문제를 &lt;code&gt;Join&lt;/code&gt;을 활용해 해결하였다.&lt;/p&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p><code>hackerrank</code>에서 제공하는 <code>Challenges</code> 문제를 <code>Join</code>을 활용해 해결하였다.</p><a id="more"></a><h1 id="문제"><a href="#문제" class="headerlink" title="문제"></a>문제</h1><p>Julia asked her students to create some coding challenges. Write a query to print the  <em>hacker_id</em>,  <em>name</em>, and the total number of challenges created by each student. Sort your results by the total number of challenges in descending order. If more than one student created the same number of challenges, then sort the result by  <em>hacker_id</em>. If more than one student created the same number of challenges and the count is less than the maximum number of challenges created, then exclude those students from the result.</p><p><strong>Input Format</strong></p><p>The following tables contain challenge data:</p><ul><li><p><em>Hackers:</em>  The  <em>hacker_id</em>  is the id of the hacker, and  <em>name</em>  is the name of the hacker.  <img src="https://s3.amazonaws.com/hr-challenge-images/19506/1458521004-cb4c077dd3-ScreenShot2016-03-21at6.06.54AM.png" alt=""></p></li><li><p><em>Challenges:</em>  The  <em>challenge_id</em>  is the id of the challenge, and  <em>hacker_id</em>  is the id of the student who created the challenge.  <img src="https://s3.amazonaws.com/hr-challenge-images/19506/1458521079-549341d9ec-ScreenShot2016-03-21at6.07.03AM.png" alt=""></p></li></ul><hr><p><strong>Sample Input 0</strong></p><p><em>Hackers</em>  Table:  <img src="https://s3.amazonaws.com/hr-challenge-images/19506/1458521384-34c6866dae-ScreenShot2016-03-21at6.07.15AM.png" alt="">  <em>Challenges</em>  Table:  <img src="https://s3.amazonaws.com/hr-challenge-images/19506/1458521410-befa8e1cd9-ScreenShot2016-03-21at6.07.25AM.png" alt=""></p><p><strong>Sample Output 0</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">21283 Angela 6</span><br><span class="line">88255 Patrick 5</span><br><span class="line">96196 Lisa 1</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><strong>Sample Input 1</strong></p><p><em>Hackers</em>  Table:  <img src="https://s3.amazonaws.com/hr-challenge-images/19506/1458521469-87036deea3-ScreenShot2016-03-21at6.07.48AM.png" alt="">  <em>Challenges</em>  Table:  <img src="https://s3.amazonaws.com/hr-challenge-images/19506/1458521490-358215cf0b-ScreenShot2016-03-21at6.07.58AM.png" alt=""></p><p><strong>Sample Output 1</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">12299 Rose 6</span><br><span class="line">34856 Angela 6</span><br><span class="line">79345 Frank 4</span><br><span class="line">80491 Patrick 3</span><br><span class="line">81041 Lisa 1</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><strong>Explanation</strong></p><p>For  <em>Sample Case 0</em>, we can get the following details:<br><img src="https://s3.amazonaws.com/hr-challenge-images/19506/1458521677-fd04c384c0-ScreenShot2016-03-21at6.07.38AM.png" alt=""><br>Students  and  both created  challenges, but the maximum number of challenges created is  so these students are excluded from the result.</p><p>For  <em>Sample Case 1</em>, we can get the following details:<br><img src="https://s3.amazonaws.com/hr-challenge-images/19506/1458521836-24039e7523-ScreenShot2016-03-21at6.08.08AM.png" alt=""><br>Students  and  both created  challenges. Because  is the maximum number of challenges created, these students are included in the result.</p><h1 id="접근"><a href="#접근" class="headerlink" title="접근"></a>접근</h1><p><code>group by</code> 조건 중 <code>1) 최댓값이 하나여야하고</code>, <code>2) 최댓값 아래로 중복되는 횟수를 가지면 안된다</code> 를 만족하는 것이 어려웠다. <code>count</code> 값을 <code>OR</code>을 이용해 만족하는 범위로 <code>having</code>절을 작성하여 해결할 수 있었다. 순서는 아래와 같이 풀었다.</p><ul><li><code>hackers</code> 테이블과 <code>challenges</code> 테이블을 <code>join</code> 및 <code>group by</code> <em>(group by key는 id / name)</em></li><li><code>having</code> 절 <code>cnt</code> 조건을 <code>OR</code>로 작성 <em>(1. max value, 2. not duplicated)</em> </li><li>문제 조건에 맞는 <code>order by</code> 추가</li></ul><h1 id="해결"><a href="#해결" class="headerlink" title="해결"></a>해결</h1><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">    c.hacker_id, h.name, <span class="keyword">count</span>(c.challenge_id) <span class="keyword">as</span> cnt</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">    challenges <span class="keyword">as</span> c</span><br><span class="line"><span class="keyword">join</span> hackers <span class="keyword">as</span> h <span class="keyword">on</span> c.hacker_id = h.hacker_id</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">    c.hacker_id, h.name</span><br><span class="line"><span class="keyword">having</span></span><br><span class="line">    cnt =  (<span class="keyword">select</span></span><br><span class="line">                <span class="keyword">count</span>(c1.challenge_id)</span><br><span class="line">            <span class="keyword">from</span></span><br><span class="line">                challenges <span class="keyword">as</span> c1</span><br><span class="line">            <span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">                c1.hacker_id</span><br><span class="line">            <span class="keyword">order</span> <span class="keyword">by</span></span><br><span class="line">                <span class="keyword">count</span>(c1.challenge_id) <span class="keyword">desc</span> <span class="keyword">limit</span> <span class="number">1</span>) <span class="keyword">or</span></span><br><span class="line">    cnt <span class="keyword">not</span> <span class="keyword">in</span> (<span class="keyword">select</span></span><br><span class="line">                    <span class="keyword">count</span>(c2.challenge_id)</span><br><span class="line">                <span class="keyword">from</span></span><br><span class="line">                    challenges <span class="keyword">as</span> c2</span><br><span class="line">                <span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">                    c2.hacker_id</span><br><span class="line">                <span class="keyword">having</span></span><br><span class="line">                    c2.hacker_id &lt;&gt; c.hacker_id)</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span></span><br><span class="line">    cnt <span class="keyword">desc</span>, c.hacker_id;</span><br></pre></td></tr></table></figure><hr><p>2019.10.07 made by <em>jaejun.lee</em></p>]]></content:encoded>
      
      <comments>https://jx2lee.github.io/hackerrank-challenges/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[SQL] Ollivander&#39;s Inventory</title>
      <link>https://jx2lee.github.io/hackerrank-ollivanders_inventory/</link>
      <guid>https://jx2lee.github.io/hackerrank-ollivanders_inventory/</guid>
      <pubDate>Sun, 06 Oct 2019 15:00:00 GMT</pubDate>
      <description>
      
        &lt;p&gt;&lt;code&gt;hackerrank&lt;/code&gt;에서 제공하는 &lt;code&gt;Ollivander&amp;#39;s Inventory&lt;/code&gt; 문제를 &lt;code&gt;Group by&lt;/code&gt;를 활용해 해결하였다.&lt;/p&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p><code>hackerrank</code>에서 제공하는 <code>Ollivander&#39;s Inventory</code> 문제를 <code>Group by</code>를 활용해 해결하였다.</p><a id="more"></a><h1 id="문제"><a href="#문제" class="headerlink" title="문제"></a>문제</h1><p>Harry Potter and his friends are at Ollivander’s with Ron, finally replacing Charlie’s old broken wand.</p><p>Hermione decides the best way to choose is by determining the minimum number of gold galleons needed to buy each  <em>non-evil</em>  wand of high power and age. Write a query to print the  <em>id</em>,  <em>age</em>,  <em>coins_needed</em>, and  <em>power</em>  of the wands that Ron’s interested in, sorted in order of descending  <em>power</em>. If more than one wand has same power, sort the result in order of descending  <em>age</em>.</p><p><strong>Input Format</strong></p><p>The following tables contain data on the wands in Ollivander’s inventory:</p><ul><li><p><em>Wands:</em>  The  <em>id</em>  is the id of the wand,  <em>code</em>  is the code of the wand,  <em>coins_needed</em>  is the total number of gold galleons needed to buy the wand, and  <em>power</em>  denotes the quality of the wand (the higher the power, the better the wand is).  <img src="https://s3.amazonaws.com/hr-challenge-images/19502/1458538092-b2a8163a74-ScreenShot2016-03-08at12.13.39AM.png" alt=""></p></li><li><p><em>Wands_Property:</em>  The  <em>code</em>  is the code of the wand,  <em>age</em>  is the age of the wand, and  <em>is_evil</em>  denotes whether the wand is good for the dark arts. If the value of  <em>is_evil</em>  is  <em>0</em>, it means that the wand is not evil. The mapping between  <em>code</em>  and  <em>age</em>  is one-one, meaning that if there are two pairs,  and  , then  and  .<img src="https://s3.amazonaws.com/hr-challenge-images/19502/1458538221-18c4092b7d-ScreenShot2016-03-08at12.13.53AM.png" alt=""></p></li></ul><hr><p><strong>Sample Input</strong></p><p><em>Wands</em>  Table:  <img src="https://s3.amazonaws.com/hr-challenge-images/19502/1458538559-51bf29644e-ScreenShot2016-03-21at10.34.41AM.png" alt="">  <em>Wands_Property</em>  Table:  <img src="https://s3.amazonaws.com/hr-challenge-images/19502/1458538583-fd514566f9-ScreenShot2016-03-21at10.34.28AM.png" alt=""></p><p><strong>Sample Output</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">9 45 1647 10</span><br><span class="line">12 17 9897 10</span><br><span class="line">1 20 3688 8</span><br><span class="line">15 40 6018 7</span><br><span class="line">19 20 7651 6</span><br><span class="line">11 40 7587 5</span><br><span class="line">10 20 504 5</span><br><span class="line">18 40 3312 3</span><br><span class="line">20 17 5689 3</span><br><span class="line">5 45 6020 2</span><br><span class="line">14 40 5408 1</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><strong>Explanation</strong></p><p>The data for wands of  <em>age 45</em>  (code 1):  <img src="https://s3.amazonaws.com/hr-challenge-images/19502/1458539700-2f319702ab-ScreenShot2016-03-21at11.23.06AM.png" alt=""></p><ul><li>The minimum number of galleons needed for</li><li>The minimum number of galleons needed for</li></ul><p>The data for wands of  <em>age 40</em>  (code 2):  <img src="https://s3.amazonaws.com/hr-challenge-images/19502/1458539909-ab79f7ff95-ScreenShot2016-03-21at11.23.14AM.png" alt=""></p><ul><li>The minimum number of galleons needed for</li><li>The minimum number of galleons needed for</li><li>The minimum number of galleons needed for</li><li>The minimum number of galleons needed for</li></ul><p>The data for wands of  <em>age 20</em>  (code 4):  <img src="https://s3.amazonaws.com/hr-challenge-images/19502/1458540035-d950b9c900-ScreenShot2016-03-21at11.23.25AM.png" alt=""></p><ul><li>The minimum number of galleons needed for</li><li>The minimum number of galleons needed for</li><li>The minimum number of galleons needed for</li></ul><p>The data for wands of  <em>age 17</em>  (code 5):  <img src="https://s3.amazonaws.com/hr-challenge-images/19502/1458540132-79fd7b916b-ScreenShot2016-03-21at11.23.34AM.png" alt=""></p><ul><li>The minimum number of galleons needed for</li><li>The minimum number of galleons needed for</li></ul><h1 id="접근"><a href="#접근" class="headerlink" title="접근"></a>접근</h1><p><code>determining the minimum number of gold galleons needed</code> 부분을 놓쳤다. <code>Wands</code> 테이블에서 <code>code/power</code> 별 coins_needed의 <code>최솟값</code>을 찾은 테이블과 <code>Wands / Wands_property</code> 테이블을 조인하여 <code>order by</code>만 추가하여 쿼리를 완성하면 된다.</p><ul><li>code, power, min(coins_needed) 를 code/power 별 <code>group by</code></li><li>wands/wands_property 테이블과 join <em>(wands 테이블 조인 시 code/coins_needed 일치)</em></li><li><code>order by</code> 로 power/age descending</li></ul><h1 id="해결"><a href="#해결" class="headerlink" title="해결"></a>해결</h1><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> w.id, p.age, w.coins_needed, w.power</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">    (<span class="keyword">select</span> code, <span class="keyword">power</span>, <span class="keyword">min</span>(coins_needed) <span class="keyword">as</span> coins_needed</span><br><span class="line">    <span class="keyword">from</span> wands</span><br><span class="line">    <span class="keyword">group</span> <span class="keyword">by</span> code, <span class="keyword">power</span>) <span class="keyword">as</span> m</span><br><span class="line"><span class="keyword">join</span> wands <span class="keyword">as</span> w <span class="keyword">on</span> w.code = m.code <span class="keyword">and</span> w.coins_needed = m.coins_needed</span><br><span class="line"><span class="keyword">join</span> wands_property <span class="keyword">as</span> p <span class="keyword">on</span> p.code = m.code</span><br><span class="line"><span class="keyword">where</span> p.is_evil = <span class="number">0</span></span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> m.power <span class="keyword">desc</span>, p.age <span class="keyword">desc</span>;</span><br></pre></td></tr></table></figure><hr><p>2019.10.07 made by <em>jaejun.lee</em></p>]]></content:encoded>
      
      <comments>https://jx2lee.github.io/hackerrank-ollivanders_inventory/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[Python] 단어 변환</title>
      <link>https://jx2lee.github.io/programmers-disk_controller/</link>
      <guid>https://jx2lee.github.io/programmers-disk_controller/</guid>
      <pubDate>Sun, 06 Oct 2019 15:00:00 GMT</pubDate>
      <description>
      
        &lt;p&gt;특정 기준을 가지고 단어를 변환할 때 최소 횟수를 구하는 문제를 풀어본다.&lt;/p&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p>특정 기준을 가지고 단어를 변환할 때 최소 횟수를 구하는 문제를 풀어본다.</p><a id="more"></a><h1 id="문제-설명"><a href="#문제-설명" class="headerlink" title="문제 설명"></a>문제 설명</h1><p>하드디스크는 한 번에 하나의 작업만 수행할 수 있습니다. 디스크 컨트롤러를 구현하는 방법은 여러 가지가 있습니다. 가장 일반적인 방법은 요청이 들어온 순서대로 처리하는 것입니다.<br>예를들어</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">- 0ms 시점에 3ms가 소요되는 A작업 요청</span><br><span class="line">- 1ms 시점에 9ms가 소요되는 B작업 요청</span><br><span class="line">- 2ms 시점에 6ms가 소요되는 C작업 요청</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>와 같은 요청이 들어왔습니다. 이를 그림으로 표현하면 아래와 같습니다.<br><img src="https://grepp-programmers.s3.amazonaws.com/files/production/b68eb5cec6/38dc6a53-2d21-4c72-90ac-f059729c51d5.png" alt="Screen Shot 2018-09-13 at 6.34.58 PM.png"></p><p>한 번에 하나의 요청만을 수행할 수 있기 때문에 각각의 작업을 요청받은 순서대로 처리하면 다음과 같이 처리 됩니다.<br><img src="https://grepp-programmers.s3.amazonaws.com/files/production/5e677b4646/90b91fde-cac4-42c1-98b8-8f8431c52dcf.png" alt="Screen Shot 2018-09-13 at 6.38.52 PM.png"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">- A: 3ms 시점에 작업 완료 (요청에서 종료까지 : 3ms)</span><br><span class="line">- B: 1ms부터 대기하다가, 3ms 시점에 작업을 시작해서 12ms 시점에 작업 완료(요청에서 종료까지 : 11ms)</span><br><span class="line">- C: 2ms부터 대기하다가, 12ms 시점에 작업을 시작해서 18ms 시점에 작업 완료(요청에서 종료까지 : 16ms)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>이 때 각 작업의 요청부터 종료까지 걸린 시간의 평균은 10ms(= (3 + 11 + 16) / 3)가 됩니다.</p><p>하지만 A → C → B 순서대로 처리하면<br><img src="https://grepp-programmers.s3.amazonaws.com/files/production/9eb7c5a6f1/a6cff04d-86bb-4b5b-98bf-6359158940ac.png" alt="Screen Shot 2018-09-13 at 6.41.42 PM.png"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">- A: 3ms 시점에 작업 완료(요청에서 종료까지 : 3ms)</span><br><span class="line">- C: 2ms부터 대기하다가, 3ms 시점에 작업을 시작해서 9ms 시점에 작업 완료(요청에서 종료까지 : 7ms)</span><br><span class="line">- B: 1ms부터 대기하다가, 9ms 시점에 작업을 시작해서 18ms 시점에 작업 완료(요청에서 종료까지 : 17ms)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>이렇게 A → C → B의 순서로 처리하면 각 작업의 요청부터 종료까지 걸린 시간의 평균은 9ms(= (3 + 7 + 17) / 3)가 됩니다.</p><p>각 작업에 대해 [작업이 요청되는 시점, 작업의 소요시간]을 담은 2차원 배열 jobs가 매개변수로 주어질 때, 작업의 요청부터 종료까지 걸린 시간의 평균을 가장 줄이는 방법으로 처리하면 평균이 얼마가 되는지 return 하도록 solution 함수를 작성해주세요. (단, 소수점 이하의 수는 버립니다)</p><h2 id="제한-사항"><a href="#제한-사항" class="headerlink" title="제한 사항"></a>제한 사항</h2><ul><li>jobs의 길이는 1 이상 500 이하입니다.</li><li>jobs의 각 행은 하나의 작업에 대한 [작업이 요청되는 시점, 작업의 소요시간] 입니다.</li><li>각 작업에 대해 작업이 요청되는 시간은 0 이상 1,000 이하입니다.</li><li>각 작업에 대해 작업의 소요시간은 1 이상 1,000 이하입니다.</li><li>하드디스크가 작업을 수행하고 있지 않을 때에는 먼저 요청이 들어온 작업부터 처리합니다.</li></ul><h2 id="입출력-예"><a href="#입출력-예" class="headerlink" title="입출력 예"></a>입출력 예</h2><p>jobs</p><p>return</p><p>[[0, 3], [1, 9], [2, 6]]</p><p>9</p><h2 id="입출력-예-설명"><a href="#입출력-예-설명" class="headerlink" title="입출력 예 설명"></a>입출력 예 설명</h2><p>문제에 주어진 예와 같습니다.</p><ul><li>0ms 시점에 3ms 걸리는 작업 요청이 들어옵니다.</li><li>1ms 시점에 9ms 걸리는 작업 요청이 들어옵니다.</li><li>2ms 시점에 6ms 걸리는 작업 요청이 들어옵니다.</li></ul><h1 id="문제-접근"><a href="#문제-접근" class="headerlink" title="문제 접근"></a>문제 접근</h1><p><code>heap</code> 을 이용해야 하는 문제. 사실 이에 대한 개념이 부족하여 공부하려 했지만 방대하길래 우선 패쓰하고 블로그를 참고하였다. 시간이 된다면 <code>heap</code> 에 대해 글을 정리하고 우선 아래와 같은 변수를 통해 해결하였다.</p><ul><li>변수<ul><li><code>in_</code>, <code>out_</code> : 작업을 시작/종료한 시간</li><li><code>ans&#39;</code> : 총 작업 시간</li><li><code>n</code> : 총 작업의 갯수</li><li><code>cnt</code> : <code>heap</code> 구조에서 자료가 빠져나간 횟수로 while문에 사용</li><li><code>wt</code> : <code>heap</code>구조로 작업의 종료 시간을 담는다.</li></ul></li></ul><h1 id="문제-해결"><a href="#문제-해결" class="headerlink" title="문제 해결"></a>문제 해결</h1><p><a href="https://codedrive.tistory.com/129">참고 블로그</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> heapq</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">solution</span>(<span class="params">jobs</span>):</span></span><br><span class="line">    in_, out_, ans, cnt = -<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">    wt = []</span><br><span class="line">    n = <span class="built_in">len</span>(jobs)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span> cnt &lt; n:</span><br><span class="line">        <span class="keyword">for</span> job <span class="keyword">in</span> jobs:</span><br><span class="line">            <span class="keyword">if</span> in_ &lt; job[<span class="number">0</span>] &lt;= out_ :</span><br><span class="line">                ans += (out_ - job[<span class="number">0</span>])</span><br><span class="line">                heapq.heappush(wt, job[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(wt) &gt; <span class="number">0</span>:</span><br><span class="line">            ans += <span class="built_in">len</span>(wt) * wt[<span class="number">0</span>] <span class="comment">#len(wt)를 곱하는 이유는 대기열에 들어간 작업도 작업 중인 시간을 더해야하므로 wt 길이를 곱해준다.</span></span><br><span class="line">            in_ = out_</span><br><span class="line">            out_ += heapq.heappop(wt)</span><br><span class="line">            cnt += <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            out_ += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> ans // n</span><br></pre></td></tr></table></figure><hr><p>2019.10.07 made by <em>jaejun.lee</em></p>]]></content:encoded>
      
      <comments>https://jx2lee.github.io/programmers-disk_controller/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[SQL] The Report</title>
      <link>https://jx2lee.github.io/hackerrank-the_report/</link>
      <guid>https://jx2lee.github.io/hackerrank-the_report/</guid>
      <pubDate>Thu, 03 Oct 2019 15:00:00 GMT</pubDate>
      <description>
      
        &lt;p&gt;&lt;code&gt;hackerrank&lt;/code&gt;에서 제공하는 &lt;code&gt;The Report&lt;/code&gt; 문제를 &lt;code&gt;Join&lt;/code&gt;을 활용해 해결하였다.&lt;/p&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p><code>hackerrank</code>에서 제공하는 <code>The Report</code> 문제를 <code>Join</code>을 활용해 해결하였다.</p><a id="more"></a><h1 id="문제"><a href="#문제" class="headerlink" title="문제"></a>문제</h1><p>You are given two tables: <em>Students</em> and <em>Grades</em>. <em>Students</em> contains three columns <em>ID</em>, <em>Name</em> and <em>Marks</em>.</p><p><img src="https://s3.amazonaws.com/hr-challenge-images/12891/1443818166-a5c852caa0-1.png" alt="img"></p><p><em>Grades</em> contains the following data:</p><p><img src="https://s3.amazonaws.com/hr-challenge-images/12891/1443818137-69b76d805c-2.png" alt="img"></p><p><em>Ketty</em> gives <em>Eve</em> a task to generate a report containing three columns: <em>Name</em>, <em>Grade</em> and <em>Mark</em>. <em>Ketty</em> doesn’t want the NAMES of those students who received a grade lower than <em>8</em>. The report must be in descending order by grade – i.e. higher grades are entered first. If there is more than one student with the same grade (8-10) assigned to them, order those particular students by their name alphabetically. Finally, if the grade is lower than 8, use “NULL” as their name and list them by their grades in descending order. If there is more than one student with the same grade (1-7) assigned to them, order those particular students by their marks in ascending order.</p><p>Write a query to help Eve.</p><p><strong>Sample Input</strong></p><p><img src="https://s3.amazonaws.com/hr-challenge-images/12891/1443818093-b79f376ec1-3.png" alt="img"></p><p><strong>Sample Output</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Maria 10 99</span><br><span class="line">Jane 9 81</span><br><span class="line">Julia 9 88 </span><br><span class="line">Scarlet 8 78</span><br><span class="line">NULL 7 63</span><br><span class="line">NULL 7 68</span><br></pre></td></tr></table></figure><p><strong>Note</strong></p><p>Print “NULL”  as the name if the grade is less than 8.</p><p><strong>Explanation</strong></p><p>Consider the following table with the grades assigned to the students:</p><p><img src="https://s3.amazonaws.com/hr-challenge-images/12891/1443818026-0b3af8db30-4.png" alt="img"></p><p>So, the following students got <em>8</em>, <em>9</em> or <em>10</em> grades:</p><ul><li><em>Maria (grade 10)</em></li><li><em>Jane (grade 9)</em></li><li><em>Julia (grade 9)</em></li><li><em>Scarlet (grade 8)</em></li></ul><h1 id="접근"><a href="#접근" class="headerlink" title="접근"></a>접근</h1><p><code>order by</code> 에 name이 <code>NULL</code>인 학생들 중 같은 Grade이면 점수를 <code>오름차순</code> 정렬할 때 헷갈렸다. <code>name</code> 정렬 후 <code>marks</code>로 정렬하면 끝나는 문제. 그리고 처음엔 <code>Union</code>으로 문제를 접근했는데 <code>case</code>문으로 쉽게 풀 수 있었다.</p><h1 id="해결"><a href="#해결" class="headerlink" title="해결"></a>해결</h1><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">    <span class="keyword">case</span></span><br><span class="line">        <span class="keyword">when</span> g.grade &lt; <span class="number">8</span> <span class="keyword">then</span> <span class="literal">null</span></span><br><span class="line">        <span class="keyword">else</span> s.name</span><br><span class="line">    <span class="keyword">end</span>, g.grade, s.marks</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">    students <span class="keyword">as</span> s</span><br><span class="line"><span class="keyword">join</span> grades <span class="keyword">as</span> g <span class="keyword">on</span> s.marks <span class="keyword">between</span> g.min_mark <span class="keyword">and</span> g.max_mark</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span></span><br><span class="line">    g.grade <span class="keyword">desc</span>, s.name, s.marks;</span><br></pre></td></tr></table></figure><hr><p>2019.10.04 made by <em>jaejun.lee</em></p>]]></content:encoded>
      
      <comments>https://jx2lee.github.io/hackerrank-the_report/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[SQL] Weather Observation Station 20</title>
      <link>https://jx2lee.github.io/hackerrank-weather_observation_station_20/</link>
      <guid>https://jx2lee.github.io/hackerrank-weather_observation_station_20/</guid>
      <pubDate>Tue, 01 Oct 2019 15:00:00 GMT</pubDate>
      <description>
      
        &lt;p&gt;&lt;code&gt;hackerrank&lt;/code&gt;에서 제공하는 &lt;code&gt;Weather Observation Station 20&lt;/code&gt; 문제를 &lt;code&gt;사용자 정의 변수&lt;/code&gt;를 활용해 해결하였다.&lt;/p&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p><code>hackerrank</code>에서 제공하는 <code>Weather Observation Station 20</code> 문제를 <code>사용자 정의 변수</code>를 활용해 해결하였다.</p><a id="more"></a><h1 id="문제"><a href="#문제" class="headerlink" title="문제"></a>문제</h1><p>A <em>median</em> is defined as a number separating the higher half of a data set from the lower half. Query the <em>median</em> of the <em>Northern Latitudes</em> (<em>LAT_N</em>) from <strong>STATION</strong> and round your answer to  decimal places.</p><p><strong>Input Format</strong></p><p>The <strong>STATION</strong> table is described as follows:</p><p><img src="https://s3.amazonaws.com/hr-challenge-images/9336/1449345840-5f0a551030-Station.jpg" alt="Station.jpg"></p><p>where <em>LAT_N</em> is the northern latitude and <em>LONG_W</em> is the western longitude.</p><h1 id="접근"><a href="#접근" class="headerlink" title="접근"></a>접근</h1><p><code>사용자 정의 변수</code>를 이용해 <code>median</code>을 구하는 문제이다.</p><p><code>row index</code>가 1부터 시작하며 <code>LAT_N</code>을 기준으로 <code>sorting</code>된 테이블에서, <code>index</code>가 <code>@ct/2.0, @ct/2.0+1</code> 범위일 경우 조회하는 query를 작성하였다. 여기선 <code>@ct</code>는 median 계산을 위해 테이블 전체 행을 뜻한다.</p><h1 id="해결"><a href="#해결" class="headerlink" title="해결"></a>해결</h1><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> @row_id = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">set</span> @ct = (<span class="keyword">select</span> <span class="keyword">count</span>(*) <span class="keyword">from</span> station);</span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line"><span class="keyword">round</span>(<span class="keyword">avg</span>(LAT_N), <span class="number">4</span>)</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">(<span class="keyword">select</span> * <span class="keyword">from</span> station <span class="keyword">order</span> <span class="keyword">by</span> LAT_N) <span class="keyword">as</span> <span class="keyword">sample</span></span><br><span class="line"><span class="keyword">where</span></span><br><span class="line">(<span class="keyword">select</span> @row_id := @row_id + <span class="number">1</span>) <span class="keyword">between</span> @ct/<span class="number">2.0</span> <span class="keyword">and</span> @ct/<span class="number">2.0</span> + <span class="number">1</span>;</span><br></pre></td></tr></table></figure><p>2019.10.02 made by <em>jaejun.lee</em></p>]]></content:encoded>
      
      <comments>https://jx2lee.github.io/hackerrank-weather_observation_station_20/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[Python] 예산</title>
      <link>https://jx2lee.github.io/programmers-budgets/</link>
      <guid>https://jx2lee.github.io/programmers-budgets/</guid>
      <pubDate>Mon, 30 Sep 2019 15:00:00 GMT</pubDate>
      <description>
      
        &lt;p&gt;정해진 총액 이하에서 가능한 한 최대 예산을 배정하는 문제를 풀어본다.&lt;/p&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p>정해진 총액 이하에서 가능한 한 최대 예산을 배정하는 문제를 풀어본다.</p><a id="more"></a><h2 id="문제-설명"><a href="#문제-설명" class="headerlink" title="문제 설명"></a>문제 설명</h2><h3 id="문제-설명-1"><a href="#문제-설명-1" class="headerlink" title="문제 설명"></a>문제 설명</h3><p>국가의 역할 중 하나는 여러 지방의 예산요청을 심사하여 국가의 예산을 분배하는 것입니다. 국가예산의 총액은 미리 정해져 있어서 모든 예산요청을 배정해 주기는 어려울 수도 있습니다. 그래서 정해진 총액 이하에서 <strong>가능한 한 최대의</strong> 총 예산을 다음과 같은 방법으로 배정합니다.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1. 모든 요청이 배정될 수 있는 경우에는 요청한 금액을 그대로 배정합니다.</span><br><span class="line">2. 모든 요청이 배정될 수 없는 경우에는 특정한 정수 상한액을 계산하여 그 이상인 예산요청에는 모두 상한액을 배정합니다. </span><br><span class="line">   상한액 이하의 예산요청에 대해서는 요청한 금액을 그대로 배정합니다. </span><br></pre></td></tr></table></figure><p>예를 들어, 전체 국가예산이 485이고 4개 지방의 예산요청이 각각 120, 110, 140, 150일 때, 상한액을 127로 잡으면 위의 요청들에 대해서 각각 120, 110, 127, 127을 배정하고 그 합이 484로 가능한 최대가 됩니다.<br>각 지방에서 요청하는 예산이 담긴 배열 budgets과 총 예산 M이 매개변수로 주어질 때, 위의 조건을 모두 만족하는 상한액을 return 하도록 solution 함수를 작성해주세요.</p><h3 id="제한-사항"><a href="#제한-사항" class="headerlink" title="제한 사항"></a>제한 사항</h3><ul><li>지방의 수는 3 이상 100,000 이하인 자연수입니다.</li><li>각 지방에서 요청하는 예산은 1 이상 100,000 이하인 자연수입니다.</li><li>총 예산은 <code>지방의 수</code> 이상 1,000,000,000 이하인 자연수입니다.</li></ul><h3 id="입출력-예"><a href="#입출력-예" class="headerlink" title="입출력 예"></a>입출력 예</h3><table><thead><tr><th>budgets</th><th>M</th><th>return</th></tr></thead><tbody><tr><td>[120, 110, 140, 150]</td><td>485</td><td>127</td></tr></tbody></table><p><a href="https://www.digitalculture.or.kr/koi/selectOlymPiadDissentList.do">출처</a></p><hr><p>※ 공지 - 2019년 3월 15일, 테스트케이스가 강화되었습니다.</p><p>이번 업데이트로 인해 지방의 수가 최대 10,000개에서 100,000개로 늘어났으며, 이에 따라 테스트케이스가 수정되었습니다.</p><p>이로 인해 이전에 통과하던 코드가 더 이상 통과하지 않을 수 있습니다.</p><h2 id="문제-접근"><a href="#문제-접근" class="headerlink" title="문제 접근"></a>문제 접근</h2><p><code>이분 탐색</code>으로 문제를 해결할 수 있다.. <code>left</code>와 <code>rigth</code>의 중간값 <code>mid</code>를 구해 매 번 총액 <code>M</code>을 최대한 맞춘다. 만약 <code>M</code>보다 작으면 <code>left +1</code>, <code>M</code>보다 크면 <code>right -1</code>로 이분 탐색한다.</p><ul><li>변수 설명<ul><li><code>res</code> : <code>left mid right</code>에 따른 총 예산액</li><li><code>left, mid, right</code> : 최소, 최대에 따른 중간값<em>(이분 탐색을 위해)</em></li></ul></li></ul><h2 id="문제-해결"><a href="#문제-해결" class="headerlink" title="문제 해결"></a>문제 해결</h2><h3 id="틀린-코드"><a href="#틀린-코드" class="headerlink" title="틀린 코드"></a>틀린 코드</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">solution</span>(<span class="params">budgets, M</span>):</span></span><br><span class="line">    sorted_budgets = <span class="built_in">sorted</span>(budgets)</span><br><span class="line">    res = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> M &gt; <span class="number">0</span>:</span><br><span class="line">        tmp = sorted_budgets[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">if</span> tmp &lt; M // <span class="built_in">len</span>(sorted_budgets):</span><br><span class="line">            res = tmp</span><br><span class="line">            M -= tmp</span><br><span class="line">            sorted_budgets.pop(<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            res = M // <span class="built_in">len</span>(sorted_budgets)</span><br><span class="line">            M -= res</span><br><span class="line">            sorted_budgets.pop(<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(sorted_budgets) == <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> res</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span></span><br></pre></td></tr></table></figure><h3 id="맞은-코드"><a href="#맞은-코드" class="headerlink" title="맞은 코드"></a>맞은 코드</h3><p><a href="https://codedrive.tistory.com/47">참고 blog</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">solution</span>(<span class="params">budgets, M</span>):</span></span><br><span class="line">    left, right, tmp = <span class="number">0</span>, <span class="built_in">max</span>(budgets), <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> right &gt;= left:</span><br><span class="line">        mid = (left + right) // <span class="number">2</span></span><br><span class="line">        res = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> budget <span class="keyword">in</span> budgets:</span><br><span class="line">            <span class="keyword">if</span> mid &gt; budget:</span><br><span class="line">                res += budget</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                res += mid</span><br><span class="line">        <span class="keyword">if</span> res &gt; M:</span><br><span class="line">            right = mid - <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            answer = mid</span><br><span class="line">            left = mid + <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> answer</span><br></pre></td></tr></table></figure><hr><p>2019.10.01 made by <em>jaejun.lee</em></p>]]></content:encoded>
      
      <comments>https://jx2lee.github.io/programmers-budgets/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[Python] 단어 변환</title>
      <link>https://jx2lee.github.io/programmers-convert_word/</link>
      <guid>https://jx2lee.github.io/programmers-convert_word/</guid>
      <pubDate>Mon, 30 Sep 2019 15:00:00 GMT</pubDate>
      <description>
      
        &lt;p&gt;특정 기준을 가지고 단어를 변환할 때 최소 횟수를 구하는 문제를 풀어본다.&lt;/p&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p>특정 기준을 가지고 단어를 변환할 때 최소 횟수를 구하는 문제를 풀어본다.</p><a id="more"></a><h2 id="문제-설명"><a href="#문제-설명" class="headerlink" title="문제 설명"></a>문제 설명</h2><p>두 개의 단어 begin, target과 단어의 집합 words가 있습니다. 아래와 같은 규칙을 이용하여 begin에서 target으로 변환하는 가장 짧은 변환 과정을 찾으려고 합니다.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1. 한 번에 한 개의 알파벳만 바꿀 수 있습니다.</span><br><span class="line">2. words에 있는 단어로만 변환할 수 있습니다.</span><br></pre></td></tr></table></figure><p>예를 들어 begin이 hit, target가 cog, words가 [hot,dot,dog,lot,log,cog]라면 hit -&gt; hot -&gt; dot -&gt; dog -&gt; cog와 같이 4단계를 거쳐 변환할 수 있습니다.</p><p>두 개의 단어 begin, target과 단어의 집합 words가 매개변수로 주어질 때, 최소 몇 단계의 과정을 거쳐 begin을 target으로 변환할 수 있는지 return 하도록 solution 함수를 작성해주세요.</p><h3 id="제한사항"><a href="#제한사항" class="headerlink" title="제한사항"></a>제한사항</h3><ul><li>각 단어는 알파벳 소문자로만 이루어져 있습니다.</li><li>각 단어의 길이는 3 이상 10 이하이며 모든 단어의 길이는 같습니다.</li><li>words에는 3개 이상 50개 이하의 단어가 있으며 중복되는 단어는 없습니다.</li><li>begin과 target은 같지 않습니다.</li><li>변환할 수 없는 경우에는 0를 return 합니다.</li></ul><h3 id="입출력-예"><a href="#입출력-예" class="headerlink" title="입출력 예"></a>입출력 예</h3><table><thead><tr><th>begin</th><th>target</th><th>words</th><th>return</th></tr></thead><tbody><tr><td>hit</td><td>cog</td><td>[hot, dot, dog, lot, log, cog]</td><td>4</td></tr><tr><td>hit</td><td>cog</td><td>[hot, dot, dog, lot, log]</td><td>0</td></tr></tbody></table><h3 id="입출력-예-설명"><a href="#입출력-예-설명" class="headerlink" title="입출력 예 설명"></a>입출력 예 설명</h3><p>예제 #1<br>문제에 나온 예와 같습니다.</p><p>예제 #2<br>target인 cog는 words 안에 없기 때문에 변환할 수 없습니다.</p><h2 id="문제-접근"><a href="#문제-접근" class="headerlink" title="문제 접근"></a>문제 접근</h2><p><code>DFS/BFS</code> 문제로 실패하여 블로그를 참고하였다. <code>words</code>가 빈 리스타가 될 때까지 탐색을 통해 <code>문자 한 개만 변형된 단어</code>를 <code>cnt</code>변수를 이용해 찾고, <code>tmp</code>를 계속해서 업데이트 해나간다. 이후 <code>tmp</code>안에 <code>target word</code>가 포함되면 <code>while 횟수</code>를 <code>return</code>하고, 아니면 <code>answer = tmp</code>로 계속해서 찾아나가는 방법으로 풀 수 있다.</p><ul><li>변수<ul><li><code>answer</code> : 탐색을 시작하는 단어를 저장하는 변수</li><li><code>res</code> : 문제의 결괏값</li><li><code>tmp</code> : 변환될 단어들의 후보</li><li><code>cnt</code> : 한 문자만 다른 단어를 뽑기위한 변수 <em>(cnt == 1)</em></li></ul></li></ul><h2 id="문제-해결"><a href="#문제-해결" class="headerlink" title="문제 해결"></a>문제 해결</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">solution</span>(<span class="params">begin, target, words</span>):</span></span><br><span class="line">    answer = [begin]</span><br><span class="line">    res = <span class="number">0</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> target <span class="keyword">in</span> words:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span> words:</span><br><span class="line">        <span class="keyword">for</span> ans <span class="keyword">in</span> answer:</span><br><span class="line">            tmp = []</span><br><span class="line">            <span class="keyword">for</span> word <span class="keyword">in</span> words:</span><br><span class="line">                cnt = <span class="number">0</span></span><br><span class="line">                <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(ans)):</span><br><span class="line">                    <span class="keyword">if</span> ans[i] != word[i]:</span><br><span class="line">                        cnt += <span class="number">1</span></span><br><span class="line">                    <span class="keyword">if</span> cnt == <span class="number">2</span>:</span><br><span class="line">                        <span class="keyword">break</span></span><br><span class="line">                <span class="keyword">if</span> cnt == <span class="number">1</span>:</span><br><span class="line">                    tmp.append(word)</span><br><span class="line">                    words.remove(word)</span><br><span class="line">        res += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> target <span class="keyword">in</span> tmp:</span><br><span class="line">            <span class="keyword">return</span> res</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            answer = tmp</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span></span><br></pre></td></tr></table></figure><hr><p>2019.10.01 made by <em>jaejun.lee</em></p>]]></content:encoded>
      
      <comments>https://jx2lee.github.io/programmers-convert_word/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[SQL] Occupations</title>
      <link>https://jx2lee.github.io/hackerrank-occupations/</link>
      <guid>https://jx2lee.github.io/hackerrank-occupations/</guid>
      <pubDate>Thu, 26 Sep 2019 15:00:00 GMT</pubDate>
      <description>
      
        &lt;p&gt;&lt;code&gt;hackerrank&lt;/code&gt;에서 제공하는 &lt;code&gt;Occupations&lt;/code&gt; 문제를 &lt;code&gt;pivot&lt;/code&gt;을 활용해 해결하였다.&lt;/p&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p><code>hackerrank</code>에서 제공하는 <code>Occupations</code> 문제를 <code>pivot</code>을 활용해 해결하였다.</p><a id="more"></a><h1 id="문제"><a href="#문제" class="headerlink" title="문제"></a>문제</h1><p><a href="https://en.wikipedia.org/wiki/Pivot_table">Pivot</a> the <em>Occupation</em> column in <strong>OCCUPATIONS</strong> so that each <em>Name</em> is sorted alphabetically and displayed underneath its corresponding <em>Occupation</em>. The output column headers should be <em>Doctor</em>, <em>Professor</em>, <em>Singer</em>, and <em>Actor</em>, respectively.</p><p><strong>Note:</strong> Print <strong>NULL</strong> when there are no more names corresponding to an occupation.</p><p><strong>Input Format</strong></p><p>The <strong>OCCUPATIONS</strong> table is described as follows:</p><p><img src="https://s3.amazonaws.com/hr-challenge-images/12889/1443816414-2a465532e7-1.png" alt="img"></p><p><em>Occupation</em> will only contain one of the following values: <strong>Doctor</strong>, <strong>Professor</strong>, <strong>Singer</strong> or <strong>Actor</strong>.</p><p><strong>Sample Input</strong></p><p><img src="https://s3.amazonaws.com/hr-challenge-images/12890/1443817648-1b2b8add45-2.png" alt="img"></p><p><strong>Sample Output</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Jenny    Ashley     Meera  Jane</span><br><span class="line">Samantha Christeen  Priya  Julia</span><br><span class="line">NULL     Ketty      NULL   Maria</span><br></pre></td></tr></table></figure><p><strong>Explanation</strong></p><p>The first column is an alphabetically ordered list of Doctor names.<br>The second column is an alphabetically ordered list of Professor names.<br>The third column is an alphabetically ordered list of Singer names.<br>The fourth column is an alphabetically ordered list of Actor names.<br>The empty cell data for columns with less than the maximum number of names per occupation (in this case, the Professor and Actor columns) are filled with <strong>NULL</strong> values.</p><h3 id="접근"><a href="#접근" class="headerlink" title="접근"></a>접근</h3><ul><li>변수 설정과<em>(set @~)</em> case문을 이용<ul><li>set @[변수명] = [값]</li><li>case 절은 위 문제 참고</li></ul></li><li>from 절에 <code>min</code>대신 <code>max</code>를 해도 결과는 동일</li></ul><h3 id="해결"><a href="#해결" class="headerlink" title="해결"></a>해결</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> @drow=<span class="number">0</span>, @prow=<span class="number">0</span>, @srow=<span class="number">0</span>, @arow=<span class="number">0</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">min</span>(doctor), <span class="keyword">min</span>(professor), <span class="keyword">min</span>(singer), <span class="keyword">min</span>(actor)</span><br><span class="line"><span class="keyword">from</span>(</span><br><span class="line">    <span class="keyword">select</span> <span class="keyword">case</span> occupation</span><br><span class="line">        <span class="keyword">when</span> <span class="string">&#x27;Doctor&#x27;</span>       <span class="keyword">then</span> @drow := @drow + <span class="number">1</span></span><br><span class="line">        <span class="keyword">when</span> <span class="string">&#x27;Professor&#x27;</span>    <span class="keyword">then</span> @prow := @prow + <span class="number">1</span></span><br><span class="line">        <span class="keyword">when</span> <span class="string">&#x27;Singer&#x27;</span>       <span class="keyword">then</span> @srow := @srow + <span class="number">1</span></span><br><span class="line">        <span class="keyword">when</span> <span class="string">&#x27;Actor&#x27;</span>        <span class="keyword">then</span> @arow := @arow + <span class="number">1</span></span><br><span class="line">        <span class="keyword">end</span> <span class="keyword">as</span> <span class="keyword">row</span>,</span><br><span class="line">        <span class="keyword">if</span>(occupation=<span class="string">&#x27;Doctor&#x27;</span>, <span class="keyword">name</span>, <span class="literal">null</span>) <span class="keyword">as</span> doctor,</span><br><span class="line">        <span class="keyword">if</span>(occupation=<span class="string">&#x27;Professor&#x27;</span>, <span class="keyword">name</span>, <span class="literal">null</span>) <span class="keyword">as</span> professor,</span><br><span class="line">        <span class="keyword">if</span>(occupation=<span class="string">&#x27;Singer&#x27;</span>, <span class="keyword">name</span>, <span class="literal">null</span>) <span class="keyword">as</span> singer,</span><br><span class="line">        <span class="keyword">if</span>(occupation=<span class="string">&#x27;Actor&#x27;</span>, <span class="keyword">name</span>, <span class="literal">null</span>) <span class="keyword">as</span> actor</span><br><span class="line">    <span class="keyword">from</span> occupations</span><br><span class="line">    <span class="keyword">order</span> <span class="keyword">by</span> <span class="keyword">name</span></span><br><span class="line">) <span class="keyword">as</span> a</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> <span class="keyword">row</span>;</span><br></pre></td></tr></table></figure><hr><p>2019.09.27 made by <em>jaejun.lee</em></p>]]></content:encoded>
      
      <comments>https://jx2lee.github.io/hackerrank-occupations/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[SQL] Type of Triangle</title>
      <link>https://jx2lee.github.io/hackerrank-type_of_triangle/</link>
      <guid>https://jx2lee.github.io/hackerrank-type_of_triangle/</guid>
      <pubDate>Thu, 26 Sep 2019 15:00:00 GMT</pubDate>
      <description>
      
        &lt;p&gt;&lt;code&gt;hackerrank&lt;/code&gt;에서 제공하는 &lt;code&gt;Type of Triangle&lt;/code&gt; 문제를 &lt;code&gt;case&lt;/code&gt;를 활용해 해결하였다.&lt;/p&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p><code>hackerrank</code>에서 제공하는 <code>Type of Triangle</code> 문제를 <code>case</code>를 활용해 해결하였다.</p><a id="more"></a><h1 id="문제"><a href="#문제" class="headerlink" title="문제"></a>문제</h1><p>Write a query identifying the <em>type</em> of each record in the <strong>TRIANGLES</strong> table using its three side lengths. Output one of the following statements for each record in the table:</p><ul><li><strong>Equilateral</strong>: It’s a triangle with  sides of equal length.</li><li><strong>Isosceles</strong>: It’s a triangle with  sides of equal length.</li><li><strong>Scalene</strong>: It’s a triangle with  sides of differing lengths.</li><li><strong>Not A Triangle</strong>: The given values of <em>A</em>, <em>B</em>, and <em>C</em> don’t form a triangle.</li></ul><p><strong>Input Format</strong></p><p>The <strong>TRIANGLES</strong> table is described as follows:</p><p><img src="https://s3.amazonaws.com/hr-challenge-images/12887/1443815629-ac2a843fb7-1.png" alt="img"></p><p>Each row in the table denotes the lengths of each of a triangle’s three sides.</p><p><strong>Sample Input</strong></p><p><img src="https://s3.amazonaws.com/hr-challenge-images/12887/1443815827-cbfc1ca12b-2.png" alt="img"></p><p><strong>Sample Output</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Isosceles</span><br><span class="line">Equilateral</span><br><span class="line">Scalene</span><br><span class="line">Not A Triangle</span><br></pre></td></tr></table></figure><p><strong>Explanation</strong></p><p>Values in the tuple  form an Isosceles triangle, because .<br>Values in the tuple  form an Equilateral triangle, because . Values in the tuple  form a Scalene triangle, because .<br>Values in the tuple  cannot form a triangle because the combined value of sides  and  is not larger than that of side </p><h1 id="접근"><a href="#접근" class="headerlink" title="접근"></a>접근</h1><p><code>IF</code>문을 사용하려다 <code>select</code>에 <code>case::when-then</code>을 이용하였다. 그리고 <code>Not a triangle</code> 조건을 먼저 주지않고 나중에 준다면<em>(Isosceles 이후에 조건을 삽입)</em> 결과값이 달라지는 오류가 발생한다. <code>case</code>문법은 아래와 같다.</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">case [column(선택)]</span><br><span class="line">when ~ then ~</span><br><span class="line">when ~ then ~</span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><h1 id="해결"><a href="#해결" class="headerlink" title="해결"></a>해결</h1><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line"><span class="keyword">case</span></span><br><span class="line">    <span class="keyword">when</span> a=b <span class="keyword">and</span> b=c <span class="keyword">then</span> <span class="string">&quot;Equilateral&quot;</span></span><br><span class="line">    <span class="keyword">when</span> a+b&lt;=c <span class="keyword">then</span> <span class="string">&quot;Not A Triangle&quot;</span></span><br><span class="line">    <span class="keyword">when</span> a+c&lt;=b <span class="keyword">then</span> <span class="string">&quot;Not A Triangle&quot;</span></span><br><span class="line">    <span class="keyword">when</span> b+c&lt;=a <span class="keyword">then</span> <span class="string">&quot;Not A Triangle&quot;</span></span><br><span class="line">    <span class="keyword">when</span> a=b <span class="keyword">and</span> a&lt;&gt;c <span class="keyword">then</span> <span class="string">&quot;Isosceles&quot;</span></span><br><span class="line">    <span class="keyword">when</span> a=c <span class="keyword">and</span> c&lt;&gt;b <span class="keyword">then</span> <span class="string">&quot;Isosceles&quot;</span></span><br><span class="line">    <span class="keyword">when</span> b=c <span class="keyword">and</span> c&lt;&gt;a <span class="keyword">then</span> <span class="string">&quot;Isosceles&quot;</span></span><br><span class="line">    <span class="keyword">when</span> a&lt;&gt;b <span class="keyword">and</span> b&lt;&gt;c <span class="keyword">then</span> <span class="string">&quot;Scalene&quot;</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"><span class="keyword">from</span> triangles;</span><br></pre></td></tr></table></figure><hr><p>2019.09.27 made by <em>jaejun.lee</em></p>]]></content:encoded>
      
      <comments>https://jx2lee.github.io/hackerrank-type_of_triangle/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[Python] 단속카메라</title>
      <link>https://jx2lee.github.io/programmers-camera/</link>
      <guid>https://jx2lee.github.io/programmers-camera/</guid>
      <pubDate>Thu, 26 Sep 2019 15:00:00 GMT</pubDate>
      <description>
      
        &lt;p&gt;단속카메라를 일정 조건에 맞게 최소로 설치하는 문제를 풀어본다&lt;/p&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p>단속카메라를 일정 조건에 맞게 최소로 설치하는 문제를 풀어본다</p><a id="more"></a><h1 id="문제"><a href="#문제" class="headerlink" title="문제"></a>문제</h1><h2 id="문제-설명"><a href="#문제-설명" class="headerlink" title="문제 설명"></a>문제 설명</h2><p>고속도로를 이동하는 모든 차량이 고속도로를 이용하면서 단속용 카메라를 한 번은 만나도록 카메라를 설치하려고 합니다.</p><p>고속도로를 이동하는 차량의 경로 routes가 매개변수로 주어질 때, 모든 차량이 한 번은 단속용 카메라를 만나도록 하려면 최소 몇 대의 카메라를 설치해야 하는지를 return 하도록 solution 함수를 완성하세요.</p><p><strong>제한사항</strong></p><ul><li>차량의 대수는 1대 이상 10,000대 이하입니다.</li><li>routes에는 차량의 이동 경로가 포함되어 있으며 routes[i][0]에는 i번째 차량이 고속도로에 진입한 지점, routes[i][1]에는 i번째 차량이 고속도로에서 나간 지점이 적혀 있습니다.</li><li>차량의 진입/진출 지점에 카메라가 설치되어 있어도 카메라를 만난것으로 간주합니다.</li><li>차량의 진입 지점, 진출 지점은 -30,000 이상 30,000 이하입니다.</li></ul><p><strong>입출력 예</strong></p><table><thead><tr><th>routes</th><th>return</th></tr></thead><tbody><tr><td>[[-20,15], [-14,-5], [-18,-13], [-5,-3]]</td><td>2</td></tr></tbody></table><p><strong>입출력 예 설명</strong></p><p>-5 지점에 카메라를 설치하면 두 번째, 네 번째 차량이 카메라를 만납니다.</p><p>-15 지점에 카메라를 설치하면 첫 번째, 세 번째 차량이 카메라를 만납니다.</p><h1 id="문제-접근"><a href="#문제-접근" class="headerlink" title="문제 접근"></a>문제 접근</h1><p> Greedy 알고리즘으로 쉽게 해결할 수 있는 문제. 입력받은 <code>list</code>를 <code>sorting (도착 지점을 기준으로)</code>하고 <code>tmp</code>변수와 출발지점을 비교해 작다면 해당 범위에 포함되지 않으므로 <code>answer</code>을 추가하며 <code>tmp</code>를 갱신해주면 된다.</p><h1 id="문제-해결"><a href="#문제-해결" class="headerlink" title="문제 해결"></a>문제 해결</h1><p>아래 코드로 해결하였다. <em>(<a href="[https://ga0n.tistory.com/entry/%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%A8%B8%EC%8A%A4-%EB%8B%A8%EC%86%8D%EC%B9%B4%EB%A9%94%EB%9D%BC](https://ga0n.tistory.com/entry/프로그래머스-단속카메라)">참고</a>)</em></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">solution</span>(<span class="params">routes</span>):</span></span><br><span class="line">    routes = <span class="built_in">sorted</span>(routes, key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>])</span><br><span class="line">    answer = <span class="number">0</span></span><br><span class="line">    tmp = -<span class="number">100000000000</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> route <span class="keyword">in</span> routes:</span><br><span class="line">        <span class="keyword">if</span> tmp &lt; route[<span class="number">0</span>]:</span><br><span class="line">            answer += <span class="number">1</span></span><br><span class="line">            tmp = route[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> answer</span><br></pre></td></tr></table></figure><hr><p>2019.09.27 made by <em>jaejun.lee</em></p>]]></content:encoded>
      
      <comments>https://jx2lee.github.io/programmers-camera/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[Python] 가장 먼 노드</title>
      <link>https://jx2lee.github.io/programmers-node/</link>
      <guid>https://jx2lee.github.io/programmers-node/</guid>
      <pubDate>Thu, 26 Sep 2019 15:00:00 GMT</pubDate>
      <description>
      
        &lt;p&gt;노드를 연결하는 그래프를 작성하고 노드 1에서 가장 멀리 떨어진 노드 갯수를 구하는 문제를 풀어본다&lt;/p&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p>노드를 연결하는 그래프를 작성하고 노드 1에서 가장 멀리 떨어진 노드 갯수를 구하는 문제를 풀어본다</p><a id="more"></a><h1 id="문제"><a href="#문제" class="headerlink" title="문제"></a>문제</h1><h2 id="문제-설명"><a href="#문제-설명" class="headerlink" title="문제 설명"></a>문제 설명</h2><p>n개의 노드가 있는 그래프가 있습니다. 각 노드는 1부터 n까지 번호가 적혀있습니다. 1번 노드에서 가장 멀리 떨어진 노드의 갯수를 구하려고 합니다. 가장 멀리 떨어진 노드란 최단경로로 이동했을 때 간선의 개수가 가장 많은 노드들을 의미합니다.</p><p>노드의 개수 n, 간선에 대한 정보가 담긴 2차원 배열 vertex가 매개변수로 주어질 때, 1번 노드로부터 가장 멀리 떨어진 노드가 몇 개인지를 return 하도록 solution 함수를 작성해주세요.</p><h2 id="제한사항"><a href="#제한사항" class="headerlink" title="제한사항"></a>제한사항</h2><ul><li>노드의 개수 n은 2 이상 20,000 이하입니다.</li><li>간선은 양방향이며 총 1개 이상 50,000개 이하의 간선이 있습니다.</li><li>vertex 배열 각 행 [a, b]는 a번 노드와 b번 노드 사이에 간선이 있다는 의미입니다.</li></ul><h2 id="입출력-예"><a href="#입출력-예" class="headerlink" title="입출력 예"></a>입출력 예</h2><table><thead><tr><th>n</th><th>vertex</th><th>return</th></tr></thead><tbody><tr><td>6</td><td>[[3, 6], [4, 3], [3, 2], [1, 3], [1, 2], [2, 4], [5, 2]]</td><td>3</td></tr></tbody></table><h2 id="입출력-예-설명"><a href="#입출력-예-설명" class="headerlink" title="입출력 예 설명"></a>입출력 예 설명</h2><p>예제의 그래프를 표현하면 아래 그림과 같고, 1번 노드에서 가장 멀리 떨어진 노드는 4,5,6번 노드입니다.</p><p><img src="https://grepp-programmers.s3.amazonaws.com/files/ybm/fadbae38bb/dec85ab5-0273-47b3-ba73-fc0b5f6be28a.png" alt="image.png"></p><h1 id="문제-접근"><a href="#문제-접근" class="headerlink" title="문제 접근"></a>문제 접근</h1><p>그래프 관련 문제를 처음 풀어보았다. 어떻게 접근해야 될지를 몰라 <a href="https://codedrive.tistory.com/189">구글에서 찾은 이 블로그</a>를 우선 참고했다. 해결 방법의 간단한 스케치는 <code>1) 각 노드별 인접한 노드 index 구하기</code>, <code>2) queue를 이용해 방문여부(is_visit)이 False인 경우 True로 바꿔주며 distance, queue를 업데이트</code>, <code>3) distance 변수를 sorting하고 max값을 count하여 return</code> 이다. 좀 더 자세히 살펴보면 아래와 같다.</p><ul><li>변수 설정<ul><li><code>graph</code> : 인접한 노드를 나타내는 변수</li><li><code>distance</code> : 노드 1에서 각 노드<code>index</code> 까지의 거리</li><li><code>is_visit</code> : 방문 여부 <em>(모두 False로 초기화, 노드 1은 True로 바꾸고 시작)</em></li><li><code>queue</code> : 큐 변수</li></ul></li><li>graph 변수 채우기 <em>(연결된 노드 append)</em></li><li>queue가 빈 리스트가 될 때까지<ul><li><code>i</code> : queue의 맨 첫 번째 <code>index</code> 추출</li><li><code>j</code>가 <code>graph[i]</code>안 원소일 때 <em>(for)</em><ul><li>if <code>is_visit[j]</code> : False,<ul><li><code>is_visit[j]</code> = False</li><li><code>queue</code>에 <code>j (노드 index)</code> append</li><li><code>distance</code> update (+1)</li></ul></li></ul></li></ul></li><li><code>distance</code> 정렬 후 첫 번째 값<em>(max)</em> 카운트 값 return</li></ul><h1 id="문제-해결"><a href="#문제-해결" class="headerlink" title="문제 해결"></a>문제 해결</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">solution</span>(<span class="params">n, edge</span>):</span></span><br><span class="line">    graph =[  [] <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n + <span class="number">1</span>) ]</span><br><span class="line">    distances = [ <span class="number">0</span> <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n) ]</span><br><span class="line">    is_visit = [<span class="literal">False</span> <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n)]</span><br><span class="line">    queue = [<span class="number">0</span>]</span><br><span class="line">    is_visit[<span class="number">0</span>] = <span class="literal">True</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 연결된 node append</span></span><br><span class="line">    <span class="keyword">for</span> (a, b) <span class="keyword">in</span> edge:</span><br><span class="line">        graph[a-<span class="number">1</span>].append(b-<span class="number">1</span>)</span><br><span class="line">        graph[b-<span class="number">1</span>].append(a-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># queue를 이용한 distance 계산</span></span><br><span class="line">    <span class="keyword">while</span> queue:</span><br><span class="line">        i = queue.pop(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> graph[i]:</span><br><span class="line">            <span class="keyword">if</span> is_visit[j] == <span class="literal">False</span>:</span><br><span class="line">                is_visit[j] = <span class="literal">True</span></span><br><span class="line">                queue.append(j)</span><br><span class="line">                distances[j] = distances[i] + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># max distance를 계산한 후 count 결과 return</span></span><br><span class="line">    distances.sort(reverse=<span class="literal">True</span>)</span><br><span class="line">    answer = distances.count(distances[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> answer</span><br></pre></td></tr></table></figure><hr><p>2019.09.27 made by <em>jaejun.lee</em></p>]]></content:encoded>
      
      <comments>https://jx2lee.github.io/programmers-node/#disqus_thread</comments>
    </item>
    
  </channel>
</rss>
